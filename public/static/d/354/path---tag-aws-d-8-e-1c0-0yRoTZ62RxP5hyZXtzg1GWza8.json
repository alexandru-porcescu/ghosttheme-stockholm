{"data":{"ghostTag":{"slug":"aws","name":"AWS","visibility":"public","feature_image":null,"description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673735","title":"Reselling AWS Load Balancing","slug":"reselling-aws-load-balancer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","custom_excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","created_at_pretty":"12 November, 2018","published_at_pretty":"12 November, 2018","updated_at_pretty":"13 November, 2018","created_at":"2018-11-11T19:49:57.000-05:00","published_at":"2018-11-11T20:16:49.000-05:00","updated_at":"2018-11-12T23:05:57.000-05:00","meta_title":"Reselling AWS Load Balancing | Hackers and Slackers","meta_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","og_title":"Reselling AWS Load Balancing | Hackers and Slackers","twitter_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","twitter_title":"Reselling AWS Load Balancing | Hackers and Slackers","authors":[{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"}],"primary_author":{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Let's say we have a hosting service for users who bring their own domain name.\nIn this scenario we'd like to be able to service customers no matter who manages\ntheir DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore\nplace, etc.\n\nAt the same time, we'd also like to provide Load balancing so no one-user can\noverload any of our systems. This means, instead of having a customer's domain\nname point directly to the system where their webserver or app resides, it will\npoint the HTTP connection to a Load Balancer which is prepared to handle serious\nconnection load before divvying it out to whichever cluster of systems is ready\nto deliver the user's content. \n\nIn an ideal world, we would have the user point their domain name to the Load\nBalancer's IP address. Very simple DNS A-Record adjustment. \n\nIn the real world, these type of cloud load balancers run over several ip\naddresses that rotate over time. So, if we were to place one of these IP\naddresses in a domain name's A-Record, it would soon be useless as it rotates\nout. Instead, the cloud load balancer offers us an end point (also an A-Record)\nsuch as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static\nwhile they dynamically rotate the IP addresses the entrypoint leads to. \n\nThe catch? You can't place an A-Record in another DNS A-Record, you can only\nplace an IP address in an A-Record. the DNS A-Record is simply a key-value pair\nwhere the key  is the Domain name (yoursiteEndpoint.com) and the value  is an IP\naddress (and nothing else). \n\nThen how do we leverage a cloud load balancer for our customers?\n\n{workaround}  Each customer with their own domain name must make the following\nchanges in their DNS provider records. \n\n * Make a CNAME Record called \"www\" which leads to the AWS Load Balancer\n   A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)\n * Setup DNS forwarding so customersite.com forwards to www.customersite.com\n\nThe Problem:  The customer will literally be entering evident AWS data into\ntheir config, and it's much more information to update than just an IP address\nin an A-Record. \n\nMore Options: \n\n{Route 53 Nameservers}  You have to automate Route 53, adding a new Hosted Zone\nbased on the customer's domain name, retrieve and deliver the Route 53 Hosted\nZone nameservers to the customer so the customer can update their DNS records at\ntheir service of choice.\n\nThe Problem:  Lots more automation and costs, AWS 500 Hosted Zone limit, more\ncustomer sync interaction\n\nMy Ultimatum:\nMake my own Load Balancer out of a network-enhanced AWS EC2 instance. I will\ngive two options for the customers - the simple A-record update to EC2-instance\nstatic IP. If they want DDoS protection and load balancing, they can do the \n{workaround}  step above additionally. If they decide not to do {workaround} \nstep above, the customer understands that we are leaving leaving the uptime\ncompletely up to the EC2 instance IP address.  Also, forget that Route 53\nnameservers update BS, as that is way too much additional business logic\nautomation and costs for reselling standpoint.","html":"<p>Let's say we have a hosting service for users who bring their own domain name. In this scenario we'd like to be able to service customers no matter who manages their DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore place, etc.</p><p>At the same time, we'd also like to provide Load balancing so no one-user can overload any of our systems. This means, instead of having a customer's domain name point directly to the system where their webserver or app resides, it will point the HTTP connection to a Load Balancer which is prepared to handle serious connection load before divvying it out to whichever cluster of systems is ready to deliver the user's content. </p><p>In an ideal world, we would have the user point their domain name to the Load Balancer's IP address. Very simple DNS A-Record adjustment. </p><p>In the real world, these type of cloud load balancers run over several ip addresses that rotate over time. So, if we were to place one of these IP addresses in a domain name's A-Record, it would soon be useless as it rotates out. Instead, the cloud load balancer offers us an end point (also an A-Record) such as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static while they dynamically rotate the IP addresses the entrypoint leads to. </p><p>The catch? You can't place an A-Record in another DNS A-Record, you can only place an IP address in an A-Record. the DNS A-Record is simply a key-value pair where the <em>key</em> is the Domain name (yoursiteEndpoint.com) and the <em>value</em> is an IP address (and nothing else). </p><p>Then how do we leverage a cloud load balancer for our customers?</p><p><em>{workaround}</em> Each customer with their own domain name must make the following changes in their DNS provider records. </p><ul><li>Make a CNAME Record called \"www\" which leads to the AWS Load Balancer A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)</li><li>Setup DNS forwarding so customersite.com forwards to www.customersite.com</li></ul><p><em>The Problem:</em><strong> </strong>The customer will literally be entering evident AWS data into their config, and it's much more information to update than just an IP address in an A-Record. </p><p>More Options: </p><p><em>{Route 53 Nameservers}</em> You have to automate Route 53, adding a new Hosted Zone based on the customer's domain name, retrieve and deliver the Route 53 Hosted Zone nameservers to the customer so the customer can update their DNS records at their service of choice.</p><p><em>The Problem:</em> Lots more automation and costs, AWS 500 Hosted Zone limit, more customer sync interaction</p><h2 id=\"my-ultimatum-\">My Ultimatum:</h2><p>Make my own Load Balancer out of a network-enhanced AWS EC2 instance. I will give two options for the customers - the simple A-record update to EC2-instance static IP. If they want DDoS protection and load balancing, they can do the <em>{workaround}</em> step above additionally. If they decide not to do <em>{workaround}</em> step above, the customer understands that we are leaving leaving the uptime completely up to the EC2 instance IP address.  Also, forget that Route 53 nameservers update BS, as that is way too much additional business logic automation and costs for reselling standpoint. </p>","url":"https://hackersandslackers.com/reselling-aws-load-balancer/","uuid":"5dd15e2a-c368-4717-900c-e85095331c4b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be8ce3574f90031d0a61650"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673733","title":"Python-Lambda: The Essential Library for AWS Cloud Functions","slug":"improve-your-aws-lambda-workflow-with-python-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","custom_excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","created_at_pretty":"07 November, 2018","published_at_pretty":"08 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-07T16:01:48.000-05:00","published_at":"2018-11-07T19:13:20.000-05:00","updated_at":"2019-01-05T13:22:03.000-05:00","meta_title":"Simplify Lambda Deployment with python-lambda | Hackers and Slackers","meta_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","og_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","twitter_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","twitter_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our series about building AWS APIs\n[https://hackersandslackers.com/tag/aws-api/], we've covered a lot of ground\naround learning the AWS ecosystem. Now that we're all feeling a bit more\ncomfortable, it may be time to let everybody in on the world's worst-kept\nsecret: Almost nobody builds architecture by interacting with the AWS UI\ndirectly. There are plenty examples of how this is done, with the main example\nbeing HashiCorp:  an entire business model based around the premise that AWS has\na shitty UI, to the point where it's easier to write code to make things which\nwill host your code. What a world.\n\nIn the case of creating Python Lambda functions, the \"official\" (aka: manual)\nworkflow of deploying your code to AWS is something horrible like this:\n\n * You start a project locally and begin development.\n * You opt to use virtualenv, because you're well aware that you're going to\n   need the source for any packages you use available.\n * When you're ready to 'deploy' to AWS, you copy all your dependencies from \n   /site-packages  and move them into your root directory, temporarily creating\n   an abomination of a project structure.\n * With your project fully bloated and confused, you cherry pick the files\n   needed to zip into an archive.\n * Finally, you upload your code via zip either to Lambda directory or to S3,\n   only to run your code, realize its broken, and need to start all over.\n\nThere Must be a Better Way\nIndeed there is, and surprisingly enough the solution is 100% Python (sorry\nHashiCorp, we'll talk another time). This \"better way\" is my personal method of\nleveraging the following:\n\n * The official AWS CLI\n   [https://docs.aws.amazon.com/cli/latest/userguide/installing.html].\n * Pipenv [https://pipenv.readthedocs.io/en/latest/]  as an environment manager.\n * Python's python-lambda [https://github.com/nficano/python-lambda]  package:\n   the magic behind it all.\n\nObligatory \"Installing the CLI\" Recap\nFirst off, make sure you're using a compatible version of Python on your system,\nas AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by\nthe way, Python 2.7 doesn't count as compatible - let it die before your career\ndoes).\n\n$ pip3 install awscli --upgrade --user\n\n\nIf you're working off an EC2 instance, it has come to my attention pip3 does not\ncome preinstalled. Remember to run: * $ apt update\n * $ apt upgrade\n * $ apt install python3-pip\n\nYou may be prompted to run apt install awscli  as well.Awesome, now that we have\nthe CLI installed on the real  version of Python, we need to store your\ncredentials. Your Access Key ID and Secret Access Key can be found in your IAM\npolicy manager.\n\n$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:\n\nOn both Linux and OSX, this should generate files found under cd ~/.aws  which\nwill be referenced by default whenever you use an AWS service moving forward.\n\nSet Up Your Environment\nAs mentioned, we'll use pipenv  for easy environment management. We'll create an\nenvironment using Lambda's preferred Python version:\n\n$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n\n\nSomething you should be aware of at the time of writing: Pip's latest version,\n18.1, is actually a breaking change  for Pipenv. Thus, the first thing we should\ndo is force usage of pip 18.0 (is there even a fix for this yet?). This is\nsolved by typing pip3 install pip==18.0  with the Pipenv shell activated. Now\nlet's get to the easy part.\n\npython-lambda: The Savior of AWS\nSo far we've made our lives easier in two ways: we're keeping our AWS\ncredentials safe and far away from ourselves, and we have what is by far the\nsuperior Python package management solution. But this is all foreplay leading up\nto python-lambda:\n\n$ pip3 install python-lambda\n\n\nThis library alone is about to do you the following favors:\n\n * Initiate your Lambda project structure for you.\n * Isolate Lambda configuration to a  config.yaml  file, covering everything\n   from the name of your entry point, handler function, and even\n   program-specific variables.\n * Allow you to run tests locally, where a test.json  file simulates a request\n   being made to your function locally.\n * Build a production-ready zip file with all dependencies completely separated \n   from your beautiful file structure.\n * The ability to deploy directly  to S3 or Lambda with said zip file from\n   command-line.\n\nCheck out the commands for yourself:\n\nCommands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n\n\nInitiate your project\nRunning lambda init  will generate the following file structure:\n\n.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n\n\nChecking out the entry point: service.py\npython-lambda starts you off with a basic handler as an example of a working\nproject. Feel free to rename service.py  and its handler function to whatever\nyou please, as we can configure that in a bit.\n\n# -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n\n\nEasy configuration via configure.yaml\nThe base config generated by lambda init  looks like this:\n\nregion: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n\n\nLook familiar? These are all the properties you would normally have to set up\nvia the UI. As an added bonus, you can store values (such as S3 bucket names for\nboto3) in this file as well. That's dope.\n\nSetting up event.json\nThe default event.json  is about as simplistic as you can get, and naturally not\nvery helpful at first (it isn't meant to be). These are the contents:\n\n{\n  \"pi\": 3.14,\n  \"e\": 2.718\n}\n\n\nWe can replace this a real test JSON which we can grab from Lambda itself.\nHere's an example of a Cloudwatch event we can use instead:\n\n{\n  \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n  \"detail-type\": \"Scheduled Event\",\n  \"source\": \"aws.events\",\n  \"account\": \"{{account-id}}\",\n  \"time\": \"1970-01-01T00:00:00Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n  ],\n  \"pi\": 3.14,\n  \"e\": 2.718\n  \"detail\": {}\n}\n\n\nRemember that event.json  is what is being passed to our handler as the event \nparameter. Thus, now we can run our Lambda function locally  to see if it works:\n\n$ lambda invoke\n5.8580000000000005\n\n\nPretty cool if you ask me.\n\nDeploy it, Ship it, Roll Credits\nAfter you express your coding genius, remember to output pip freeze >\nrequirements.txt. python-lambda  will use this as a reference for which packages\nneed to be included. This is neat because we can use Pipenv and the benefits of\nthe workflow it provides while still easily outputting what we need to deploy. \n\nBecause we already specified which Lambda we're going to deploy to in \nconfig.yaml, we can deploy to that Lambda immediately. lambda deploy  will use\nthe zip upload method, whereas lambda deploy-s3  will store your source on S3.\n\nIf you'd like to deploy the function yourself, run with lambda build  which will\nzip your source code plus dependencies  neatly into a /dist  directory. Suddenly\nwe never have to compromise our project structure, and now we can easily source\ncontrol our Lambdas by .gitignoring our build folders while hanging on to our\nPipfiles.\n\nHere's to hoping you never need to deploy Lambdas using any other method ever\nagain. Cheers.","html":"<p>In our series about building <a href=\"https://hackersandslackers.com/tag/aws-api/\">AWS APIs</a>, we've covered a lot of ground around learning the AWS ecosystem. Now that we're all feeling a bit more comfortable, it may be time to let everybody in on the world's worst-kept secret: Almost nobody builds architecture by interacting with the AWS UI directly. There are plenty examples of how this is done, with the main example being <strong>HashiCorp:</strong> an entire business model based around the premise that AWS has a shitty UI, to the point where it's easier to write code to make things which will host your code. What a world.</p><p>In the case of creating Python Lambda functions, the \"official\" (aka: manual) workflow of deploying your code to AWS is something horrible like this:</p><ul><li>You start a project locally and begin development.</li><li>You opt to use <strong>virtualenv, </strong>because you're well aware that you're going to need the source for any packages you use available.</li><li>When you're ready to 'deploy' to AWS, you <em>copy all your dependencies from </em><code>/site-packages</code> <em>and move them into your root directory</em>, temporarily creating an abomination of a project structure.</li><li>With your project fully bloated and confused, you cherry pick the files needed to zip into an archive.</li><li>Finally, you upload your code via zip either to Lambda directory or to S3, only to run your code, realize its broken, and need to start all over.</li></ul><h2 id=\"there-must-be-a-better-way\">There Must be a Better Way</h2><p>Indeed there is, and surprisingly enough the solution is 100% Python (sorry HashiCorp, we'll talk another time). This \"better way\" is my personal method of leveraging the following:</p><ul><li>The official <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\">AWS CLI</a>.</li><li><a href=\"https://pipenv.readthedocs.io/en/latest/\">Pipenv</a> as an environment manager.</li><li>Python's <strong><a href=\"https://github.com/nficano/python-lambda\">python-lambda</a></strong> package: the magic behind it all.</li></ul><h3 id=\"obligatory-installing-the-cli-recap\">Obligatory \"Installing the CLI\" Recap</h3><p>First off, make sure you're using a compatible version of Python on your system, as AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by the way, <em>Python 2.7 </em>doesn't count as compatible - let it die before your career does).</p><pre><code class=\"language-python\">$ pip3 install awscli --upgrade --user\n</code></pre>\n<div class=\"protip\">\n    If you're working off an EC2 instance, it has come to my attention pip3 does not come preinstalled. Remember to run:\n<ul>\n    <li><code>$ apt update</code></li>\n    <li><code>$ apt upgrade</code></li>\n    <li><code>$ apt install python3-pip</code></li>\n</ul>\n    \n    You may be prompted to run <code>apt install awscli</code> as well.\n</div><p>Awesome, now that we have the CLI installed on the <em>real</em> version of Python, we need to store your credentials. Your Access Key ID and Secret Access Key can be found in your IAM policy manager.</p><pre><code>$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:</code></pre><p>On both Linux and OSX, this should generate files found under <code>cd ~/.aws</code> which will be referenced by default whenever you use an AWS service moving forward.</p><h2 id=\"set-up-your-environment\">Set Up Your Environment</h2><p>As mentioned, we'll use <code>pipenv</code> for easy environment management. We'll create an environment using Lambda's preferred Python version:</p><pre><code class=\"language-python\">$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n</code></pre>\n<p>Something you should be aware of at the time of writing: Pip's latest version, 18.1, is actually a <em>breaking change</em> for Pipenv. Thus, the first thing we should do is force usage of pip 18.0 (is there even a fix for this yet?). This is solved by typing <code>pip3 install pip==18.0</code> with the Pipenv shell activated. Now let's get to the easy part.</p><h2 id=\"python-lambda-the-savior-of-aws\">python-lambda: The Savior of AWS</h2><p>So far we've made our lives easier in two ways: we're keeping our AWS credentials safe and far away from ourselves, and we have what is by far the superior Python package management solution. But this is all foreplay leading up to <code>python-lambda</code>:</p><pre><code class=\"language-bash\">$ pip3 install python-lambda\n</code></pre>\n<p>This library alone is about to do you the following favors:</p><ul><li>Initiate your Lambda project structure for you.</li><li>Isolate Lambda configuration to a<em> config.yaml</em> file, covering everything from the name of your entry point, handler function, and even program-specific variables.</li><li>Allow you to run tests locally, where a <em>test.json</em> file simulates a request being made to your function locally.</li><li>Build a production-ready zip file with all dependencies <em>completely separated </em>from your beautiful file structure.</li><li>The ability to deploy <em>directly</em> to S3 or Lambda with said zip file from command-line.</li></ul><p>Check out the commands for yourself:</p><pre><code class=\"language-bash\">Commands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n</code></pre>\n<h3 id=\"initiate-your-project\">Initiate your project</h3><p>Running <code>lambda init</code> will generate the following file structure:</p><pre><code class=\"language-bash\">.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n</code></pre>\n<h3 id=\"checking-out-the-entry-point-service-py\">Checking out the entry point: service.py</h3><p>python-lambda starts you off with a basic handler as an example of a working project. Feel free to rename <code>service.py</code> and its handler function to whatever you please, as we can configure that in a bit.</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n</code></pre>\n<h3 id=\"easy-configuration-via-configure-yaml\">Easy configuration via configure.yaml</h3><p>The base config generated by <code>lambda init</code> looks like this:</p><pre><code class=\"language-yaml\">region: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n</code></pre>\n<p>Look familiar? These are all the properties you would normally have to set up via the UI. As an added bonus, you can store values (such as S3 bucket names for boto3) in this file as well. That's dope.</p><h3 id=\"setting-up-event-json\">Setting up event.json</h3><p>The default <code>event.json</code> is about as simplistic as you can get, and naturally not very helpful at first (it isn't meant to be). These are the contents:</p><pre><code class=\"language-json\">{\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n}\n</code></pre>\n<p>We can replace this a real test JSON which we can grab from Lambda itself. Here's an example of a Cloudwatch event we can use instead:</p><pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;cdc73f9d-aea9-11e3-9d5a-835b769c0d9c&quot;,\n  &quot;detail-type&quot;: &quot;Scheduled Event&quot;,\n  &quot;source&quot;: &quot;aws.events&quot;,\n  &quot;account&quot;: &quot;{{account-id}}&quot;,\n  &quot;time&quot;: &quot;1970-01-01T00:00:00Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [\n    &quot;arn:aws:events:us-east-1:123456789012:rule/ExampleRule&quot;\n  ],\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n  &quot;detail&quot;: {}\n}\n</code></pre>\n<p>Remember that <code>event.json</code> is what is being passed to our handler as the <code>event</code> parameter. Thus, now we can run our Lambda function <em>locally</em> to see if it works:</p><pre><code class=\"language-bash\">$ lambda invoke\n5.8580000000000005\n</code></pre>\n<p>Pretty cool if you ask me.</p><h2 id=\"deploy-it-ship-it-roll-credits\">Deploy it, Ship it, Roll Credits</h2><p>After you express your coding genius, remember to output <code>pip freeze &gt; requirements.txt</code>. <strong>python-lambda</strong> will use this as a reference for which packages need to be included. This is neat because we can use Pipenv and the benefits of the workflow it provides while still easily outputting what we need to deploy. </p><p>Because we already specified which Lambda we're going to deploy to in <code>config.yaml</code>, we can deploy to that Lambda immediately. <code>lambda deploy</code> will use the zip upload method, whereas <code>lambda deploy-s3</code> will store your source on S3.</p><p>If you'd like to deploy the function yourself, run with <code>lambda build</code> which will zip your source code <em>plus dependencies</em> neatly into a /<em>dist</em> directory. Suddenly we never have to compromise our project structure, and now we can easily source control our Lambdas by <em>.gitignoring </em>our build folders while hanging on to our Pipfiles.</p><p>Here's to hoping you never need to deploy Lambdas using any other method ever again. Cheers.</p>","url":"https://hackersandslackers.com/improve-your-aws-lambda-workflow-with-python-lambda/","uuid":"08ad7706-8dd7-4475-875e-880c017de8d5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be352bc2aa81b1606ab77a7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673651","title":"Building an API with Amazon's API Gateway","slug":"creating-apis-with-api-gateway","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","custom_excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","created_at_pretty":"13 May, 2018","published_at_pretty":"29 October, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-05-13T17:29:07.000-04:00","published_at":"2018-10-29T19:41:00.000-04:00","updated_at":"2019-01-05T13:28:10.000-05:00","meta_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","meta_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","og_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","twitter_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","twitter_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our last adventure, we ventured off to create our very own cloud database\n[https://hackersandslackers.com/setting-up-mysql-on-aws/]  by using Amazon's RDS \n service. We've also briefly covered\n[https://hackersandslackers.com/building-an-api-using-aws/]  the general concept\nbehind what Lambda functions. In case you've already forgotten, Lambdas are\nbasically just chunks of code in the cloud; think of them as tiny virtual\nservers, which have already been configured (and locked down) to serve one\nspecific purpose. Because that's literally what it is.\n\nThe data being stored in RDS is ultimately what we're targeting, and Lambdas \nserve as the in-between logic to serve up, modify, or add to the proper data.\nThe only piece missing from the picture is API Gateway. \n\nAs the name suggests, API Gateway  is the, uh, gateway  that users or systems\ninteract with to obtain what they're seeking. It is (hopefully) the only part of\nthis VPC structure an external user can interact with:\n\nSimple API to interact with RDS.Serving as a \"gateway\" is obviously what all\nAPIs so, but the term is also true in the sense that API Gateway  is completely\nconfigured via UI, thus engineers of any programming background can safely\nmodify endpoints, methods,  CORs  configurations, or any of the high-level API\nstructure without being locked into a programming language. API Gateway  is\ntherefore very much an enterprise-geared product: it lends itself to large teams\nand scaling. That said, if it were to be compared to building an API via a\nframework designed to do such things (such as Express or Flask) the experience\nis undoubtedly more clunky. The trade-off being made for speed is immediate\nvisibility, assurance, and a higher chance for collaboration.\n\nThe Challenge of Building a \"Well-Designed\" API\nGood APIs are premeditated. A complex API might accept multiple methods per\nendpoint, allow advanced filtering of results, or handle advanced\nAuthentication. Neither of us have the time to attempt covering all of those\nthings in detail, but I will  leave you with the knowledge that all these\nfeatures are very much possible.\n\nThe API Gateway  interface is where you'd get started. Let's blow through the\nworld's most inappropriately fast explanation of building APIs ever ,and check\nout the UI:\n\nIt ain't pretty, but it works. * Your APIs are listed on the left. You can create more than one, if you're\n   some sort of sadist.\n * The Resources pane is the full structure of your API. At the highest level,\n   'resources' refers to Endpoints,  which are the URLs your API will ultimately\n   expose.\n * Every Endpoint  can contain whichever Methods  you choose to associate with\n   them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST\n   method could contain entirely unrelated logic from a PUT method: its your\n   responsibility to make sure your API design makes sense.\n * Finally, each Method has their expected Request and Response  structures\n   defined individually, which what the horribly designed box diagram is\n   attempting to explain on the right. The box on the left labeled CLIENT refers\n   to the requester, where the box on the right represents the triggered action.\n\nThis UI is your bread and butter. I hope you're strapped in, because walking\nthrough this interface is going to be hella boring for all of us.\n\nCreating a Method Request\nThe first step to creating an endpoint (let's say a GET endpoint) is to set the\nexpectation for what the user will send to us:\n\nAwe yea, authorization. 1. Authorization  allows you to restrict users from using your API unless they\n    follow your IAM policy.\n 2. Request Validator  lets you chose if you'd like this validation to happen\n    via the body, query string parameters, headers, or all of the above.\n 3. API Keys  are useful if you're creating an API to sell commercially or\n    enforce limited access. If your business model revolves around selling an\n    API, you can realistically do this.\n 4. Query String Parameters  are... actually forget it, you know this by now.\n 5. See above.\n 6. If preferred, the Request Body can be assigned a model,  which is\n    essentially a JSON schema. If a request is made to your endpoint which does\n    not match the request body model, it is a malformed request. We'll cover \n    models  in the advanced course, once somebody actually starts paying me to\n    write this stuff.\n\nMethod Execution: AKA \"What do we do with this?\"\nSet the game plan. 1. Integration Type  specifies which AWS service will be accepting or affected\n    by this request. The vast majority of the time, this will be Lambda. If\n    you're wondering why other AWS Services aren't present, this has been made\n    intentional over time as just about any AWS service you can interact with\n    will still need logic to do anything useful: you can't just shove a JSON\n    object in a database's face and expect to get results. Unless you're using\n    MongoDB or something.\n 2. Lambda Proxies  are generally a bad idea. They auto-format your Lambda's \n    request  and response  body to follow a very  specific structure, which is\n    presumably intended to help speed up or standardize development. The\n    downside is these structures are bloated and most likely contain useless\n    information. To get an idea of what these structures look like, check them\n    out here.\n 3. The Region  your Lambda hosted lives in.\n 4. Name of the Lamba Function  your request will be directed to.\n 5. Execution role  refers to the IAM role your Lambda policy will be a part of.\n    This is kind of an obnoxious concept, but your function has permissions as\n    though it were a user. This is presumably Amazon's way of thinking ahead to\n    extending human rights to robots.\n 6. Caller Credentials  refers to API keys, assuming you chose to use them. If\n    this is checked, the API will not be usable without an API key, thus making\n    it difficult to test\n 7. Credentials Cache  probably refers to expiring credentials or something, I'm\n    sure you'll figure it out.\n 8. Timeout  can be increased if you're dealing with an API call that takes a\n    lot of time to respond, such as occasions with heavy data sets.\n 9. URL Paths probably do something, I don't know. Who really cares?\n\nINTERMISSION: The Part Where Things Happen\nThe next step in the flow would be where the AWS service we selected to handle\nthe request would do its thing. We'll get into that next time.\n\nResponse Codes and Headers\nKeep it 200 baby. 1. While AWS provides users with standard error codes  and generic errors, you\n    can add your own specific error/success messages. Props to whoever puts in\n    the effort.\n 2. Header Mappings  are the headings returned with the response. For example,\n    this is where you might solve cross-domain issues via the \n    Access-Control-Allow-Origin  header.\n\n3. Mapping Templates  are the Content-Type  of the response returned, most\ncommonly application/json.\n\nMethod Response\nI almost never spend time hereThis step is a continuation of the previous step.\nI'm not entirely sure what the point in splitting this into two screens is, but\nI'm guessing its not important.\n\nReap Your Rewards\nAt long last, this brings us to the end of our journey. This is presumably where\n you've executed a successful AWS test or something. However, there's a final\nstep before you go live; deploying your API:\n\nDeploy your API to a live \"stage\" and retire.Next time we'll cover the logical,\nless boring part of writing actual code behind these endpoints.","html":"<p>In our last adventure, we ventured off to create our very own cloud <a href=\"https://hackersandslackers.com/setting-up-mysql-on-aws/\">database</a> by using Amazon's <strong>RDS</strong> service. We've also <a href=\"https://hackersandslackers.com/building-an-api-using-aws/\">briefly covered</a> the general concept behind what <strong>Lambda functions</strong>. In case you've already forgotten, Lambdas are basically just chunks of code in the cloud; think of them as tiny virtual servers, which have already been configured (and locked down) to serve one specific purpose. Because that's literally what it is.</p><p>The data being stored in <strong>RDS </strong>is ultimately what we're targeting, and <strong>Lambdas</strong> serve as the in-between logic to serve up, modify, or add to the proper data. The only piece missing from the picture is <strong>API Gateway</strong>. </p><p>As the name suggests, <strong>API Gateway</strong> is the, uh, <em>gateway</em> that users or systems interact with to obtain what they're seeking. It is (hopefully) the only part of this VPC structure an external user can interact with:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_o-1.jpg\" class=\"kg-image\"><figcaption>Simple API to interact with RDS.</figcaption></figure><p>Serving as a \"gateway\" is obviously what all APIs so, but the term is also true in the sense that <strong>API Gateway</strong> is completely configured via UI, thus engineers of any programming background can safely modify <em>endpoints</em>, <em>methods,</em> <em>CORs</em> configurations, or any of the high-level API structure without being locked into a programming language. <strong>API Gateway</strong> is therefore very much an enterprise-geared product: it lends itself to large teams and scaling. That said, if it were to be compared to building an API via a framework designed to do such things (such as Express or Flask) the experience is undoubtedly more clunky. The trade-off being made for speed is immediate visibility, assurance, and a higher chance for collaboration.</p><h2 id=\"the-challenge-of-building-a-well-designed-api\">The Challenge of Building a \"Well-Designed\" API</h2><p>Good APIs are premeditated. A complex API might accept multiple methods per endpoint, allow advanced filtering of results, or handle advanced Authentication. Neither of us have the time to attempt covering all of those things in detail, but I <em>will</em> leave you with the knowledge that all these features are very much possible.  </p><p>The <strong>API Gateway</strong> interface is where you'd get started. Let's blow through the world's most inappropriately fast explanation of building APIs ever ,and check out the UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_overview3.png\" class=\"kg-image\"><figcaption>It ain't pretty, but it works.</figcaption></figure><ul><li>Your <strong>APIs </strong>are listed on the left. You can create more than one, if you're some sort of sadist.</li><li>The <strong>Resources </strong>pane is the full structure of your API. At the highest level, 'resources' refers to <strong>Endpoints,</strong> which are the URLs your API will ultimately expose.</li><li>Every <strong>Endpoint</strong> can contain whichever <strong>Methods</strong> you choose to associate with them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST method could contain entirely unrelated logic from a PUT method: its your responsibility to make sure your API design makes sense.</li><li>Finally, each <strong>Method </strong>has their expected <strong>Request </strong>and <strong>Response</strong> structures defined individually, which what the horribly designed box diagram is attempting to explain on the right. The box on the left labeled CLIENT refers to the requester, where the box on the right represents the triggered action.</li></ul><p>This UI is your bread and butter. I hope you're strapped in, because walking through this interface is going to be hella boring for all of us.</p><h3 id=\"creating-a-method-request\">Creating a Method Request</h3><p>The first step to creating an endpoint (let's say a GET endpoint) is to set the expectation for what the user will send to us:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodrequest_o.jpg\" class=\"kg-image\"><figcaption>Awe yea, authorization.</figcaption></figure><ol><li><strong>Authorization</strong> allows you to restrict users from using your API unless they follow your IAM policy.</li><li><strong>Request Validator</strong> lets you chose if you'd like this validation to happen via the body, query string parameters, headers, or all of the above.</li><li><strong>API Keys</strong> are useful if you're creating an API to sell commercially or enforce limited access. If your business model revolves around selling an API, you can realistically do this.</li><li><strong>Query String Parameters</strong> are... actually forget it, you know this by now.</li><li>See above.</li><li>If preferred, the <strong>Request Body </strong>can be assigned a <strong>model,</strong> which is essentially a JSON schema. If a request is made to your endpoint which does not match the request body model, it is a malformed request. We'll cover <strong>models</strong> in the advanced course, once somebody actually starts paying me to write this stuff.</li></ol><h3 id=\"method-execution-aka-what-do-we-do-with-this\">Method Execution: AKA \"What do we do with this?\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodexecution_o.jpg\" class=\"kg-image\"><figcaption>Set the game plan.</figcaption></figure><ol><li><strong>Integration Type</strong> specifies which AWS service will be accepting or affected by this request. The vast majority of the time, this will be Lambda. If you're wondering why other AWS Services aren't present, this has been made intentional over time as just about any AWS service you can interact with will still need logic to do anything useful: you can't just shove a JSON object in a database's face and expect to get results. Unless you're using MongoDB or something.</li><li><strong>Lambda Proxies</strong> are generally a bad idea. They auto-format your Lambda's <em>request</em> and <em>response</em> body to follow a very  specific structure, which is presumably intended to help speed up or standardize development. The downside is these structures are bloated and most likely contain useless information. To get an idea of what these structures look like, check them out <a href=\"https://github.com/bbilger/jrestless/tree/master/aws/gateway/jrestless-aws-gateway-handler#response-schema\">here</a>.</li><li>The <strong>Region</strong> your Lambda hosted lives in.</li><li>Name of the <strong>Lamba Function</strong> your request will be directed to.</li><li><strong>Execution role</strong> refers to the IAM role your Lambda policy will be a part of. This is kind of an obnoxious concept, but your function has permissions as though it were a user. This is presumably Amazon's way of thinking ahead to extending human rights to robots.</li><li><strong>Caller Credentials</strong> refers to API keys, assuming you chose to use them. If this is checked, the API will not be usable without an API key, thus making it difficult to test</li><li><strong>Credentials Cache</strong> probably refers to expiring credentials or something, I'm sure you'll figure it out.</li><li><strong>Timeout</strong> can be increased if you're dealing with an API call that takes a lot of time to respond, such as occasions with heavy data sets.</li><li><strong>URL Paths </strong>probably do something, I don't know. Who really cares?</li></ol><h3 id=\"intermission-the-part-where-things-happen\">INTERMISSION: The Part Where Things Happen</h3><p>The next step in the flow would be where the AWS service we selected to handle the request would do its thing. We'll get into that next time.</p><h3 id=\"response-codes-and-headers\">Response Codes and Headers</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/response_o.jpg\" class=\"kg-image\"><figcaption>Keep it 200 baby.</figcaption></figure><ol><li>While AWS provides users with standard <strong>error codes</strong> and generic errors, you can add your own specific error/success messages. Props to whoever puts in the effort.</li><li><strong>Header Mappings</strong> are the headings returned with the response. For example, this is where you might solve cross-domain issues via the <em>Access-Control-Allow-Origin</em> header.</li></ol><p>3. <strong>Mapping Templates</strong> are the <em>Content-Type</em> of the response returned, most commonly <em>application/json.</em></p><h3 id=\"method-response\">Method Response</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-8.54.51-PM_o.png\" class=\"kg-image\"><figcaption>I almost never spend time here</figcaption></figure><p>This step is a continuation of the previous step. I'm not entirely sure what the point in splitting this into two screens is, but I'm guessing its not important.</p><h2 id=\"reap-your-rewards\">Reap Your Rewards</h2><p>At long last, this brings us to the end of our journey. This is presumably where  you've executed a successful AWS test or something. However, there's a final step before you go live; deploying your API:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-9.02.52-PM_o.png\" class=\"kg-image\"><figcaption>Deploy your API to a live \"stage\" and retire.</figcaption></figure><p>Next time we'll cover the logical, less boring part of writing actual code behind these endpoints.</p>","url":"https://hackersandslackers.com/creating-apis-with-api-gateway/","uuid":"ce9c1023-431b-4580-b3ca-1a3e2074f9c5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5af8ae23092feb404eb9981e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673719","title":"Creating an AMI with HashiCorp Packer","slug":"hashicorp-packer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","excerpt":"HashiCorp's version control for infrastructure .","custom_excerpt":"HashiCorp's version control for infrastructure .","created_at_pretty":"02 October, 2018","published_at_pretty":"03 October, 2018","updated_at_pretty":"30 December, 2018","created_at":"2018-10-02T16:05:02.000-04:00","published_at":"2018-10-03T07:00:00.000-04:00","updated_at":"2018-12-30T07:00:11.000-05:00","meta_title":"Creating an AMI with HashiCorp Packer | Hackers and Slackers","meta_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","og_title":"Creating an AMI with HashiCorp Packer","twitter_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","twitter_title":"Creating an AMI with HashiCorp Packer","authors":[{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null}],"primary_author":{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Hashicorp","slug":"hashicorp","description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","feature_image":null,"meta_description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","meta_title":"Hashicorp Suite | Hackers and Slackers","visibility":"public"}],"plaintext":"Why use Packer [https://www.packer.io/]? Infrastructure as code has become part\nof the buzzword bingo surrounding operational teams and their desired optimal\nworkflows.\n\nOne could theoretically just start with a base AMI and manually update it and\nthen re-save it as a new AMI, but this process is not repeatable.  We can check\nin our desired infrastructure states as code to version control.  This is good\npractice for change control management.  We can readily see what worked before\nand what was changed in the latest update.  If something catastrophic happens or\nwe encounter unforeseen issues, we can always rollback to a previous state.\n\nI'm the first new guy on our ops team in a few years.  We work with a base image\nto create our EC2 instances and that image does not have my ssh keys.  In our\ncurrent workflow, when I spin up a new instance using our latest base AMI, I\ncan't ssh to the box because my key isn't on there.  Amazon has also released\nAmazon Linux 2, so we had a card to update the base AMI in the backlog.  I\npicked up this task and found the HashiCorp tool to be very powerful and useful.\n\n{\n  \"description\": \"Builds a Base Image for EC2 AWS provisioner\",\n  \"variables\":{\n    \"hostname\": \"cne-aws-trusty64\",\n    \"config_dir\": \".\"\n  },\n\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-04681a1dbd79675a5\",\n      \"instance_type\": \"m5.xlarge\",\n      \"ssh_username\": \"ec2-user\",\n      \"ami_name\": \"snapdragon-v3.6.9\",\n      \"subnet_id\": \"subnet-0000000000\",\n      \"tags\": {\n        \"OS_Version\": \"Amazon Linux 2\",\n        \"Release\": \"2017-12\",\n        \"Builder\": \"packer\"\n      },\n      \"ssh_timeout\": \"60m\"\n    }\n  ],\n\n  \"provisioners\": [\n    {\n      \"type\": \"shell\",\n        \"scripts\": [\n          \"scripts/setup-example.sh\"\n        ]\n    } \n  ]\n}\n\n\nIn our setup script, we install dependencies and a configuration management tool\nadds users, and updates permissions as needed for all of our applications.  It's\nbasically the equivalent of whatever you would do manually to achieve a desired\nstate.","html":"<p><strong>Why use <a href=\"https://www.packer.io/\">Packer</a>? </strong>Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.</p><p>One could theoretically just start with a base AMI and manually update it and then re-save it as a new AMI, but this process is not repeatable.  We can check in our desired infrastructure states as code to version control.  This is good practice for change control management.  We can readily see what worked before and what was changed in the latest update.  If something catastrophic happens or we encounter unforeseen issues, we can always rollback to a previous state.</p><p>I'm the first new guy on our ops team in a few years.  We work with a base image to create our EC2 instances and that image does not have my ssh keys.  In our current workflow, when I spin up a new instance using our latest base AMI, I can't ssh to the box because my key isn't on there.  Amazon has also released Amazon Linux 2, so we had a card to update the base AMI in the backlog.  I picked up this task and found the HashiCorp tool to be very powerful and useful.</p><pre><code>{\n  &quot;description&quot;: &quot;Builds a Base Image for EC2 AWS provisioner&quot;,\n  &quot;variables&quot;:{\n    &quot;hostname&quot;: &quot;cne-aws-trusty64&quot;,\n    &quot;config_dir&quot;: &quot;.&quot;\n  },\n\n  &quot;builders&quot;: [\n    {\n      &quot;type&quot;: &quot;amazon-ebs&quot;,\n      &quot;region&quot;: &quot;us-east-1&quot;,\n      &quot;source_ami&quot;: &quot;ami-04681a1dbd79675a5&quot;,\n      &quot;instance_type&quot;: &quot;m5.xlarge&quot;,\n      &quot;ssh_username&quot;: &quot;ec2-user&quot;,\n      &quot;ami_name&quot;: &quot;snapdragon-v3.6.9&quot;,\n      &quot;subnet_id&quot;: &quot;subnet-0000000000&quot;,\n      &quot;tags&quot;: {\n        &quot;OS_Version&quot;: &quot;Amazon Linux 2&quot;,\n        &quot;Release&quot;: &quot;2017-12&quot;,\n        &quot;Builder&quot;: &quot;packer&quot;\n      },\n      &quot;ssh_timeout&quot;: &quot;60m&quot;\n    }\n  ],\n\n  &quot;provisioners&quot;: [\n    {\n      &quot;type&quot;: &quot;shell&quot;,\n        &quot;scripts&quot;: [\n          &quot;scripts/setup-example.sh&quot;\n        ]\n    } \n  ]\n}\n</code></pre>\n<p>In our setup script, we install dependencies and a configuration management tool adds users, and updates permissions as needed for all of our applications.  It's basically the equivalent of whatever you would do manually to achieve a desired state.</p>","url":"https://hackersandslackers.com/hashicorp-packer/","uuid":"0c50713b-1c8c-4071-b65d-e57fd87f3536","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bb3cf6e7ae39d0d60547523"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673683","title":"Read and Write to S3 Buckets via NodeJS","slug":"accessing-private-s3-objects-with-node","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","custom_excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","created_at_pretty":"21 June, 2018","published_at_pretty":"22 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-21T19:26:44.000-04:00","published_at":"2018-06-22T07:30:00.000-04:00","updated_at":"2019-03-28T05:25:16.000-04:00","meta_title":"Accessing Private S3 Objects with Node | Hackers and Slackers","meta_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","og_title":"Accessing Private S3 Objects with Node","twitter_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","twitter_title":"Accessing Private S3 Objects with Node","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"We here at H+S are dedicated to one simple cause: creating posts about oddly\nspecific programming scenarios. Somewhere in the world as sad soul is looking to\nprogrammatically access files from an S3 server while keeping their bucket\nprivate. To that person: we heard you.\n\nThere are plenty of reasons you'd want to access files in S3. For example, let's\nsay you read that post\n[https://hackersandslackers.com/using-pandas-with-aws-lambda/]  about using\nPandas in a Lambda function. Since you're already familiar with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], you may hypothetically be in a\nposition to export data from a DB query to a CSV saved in S3. I bet you can\nguess what I've been doing lately.\n\nConfigure the AWS CLI on your VPS\nThe easiest and safest way to interact with other AWS services on your EC2\ninstance (or VPS of choice) is via the AWS CLI. This is easily installed as a\nglobal Python3 library:\n\n$ pip3 install awscli\n\n\nWith the CLI installed we'll be able to do something truly magical: set our AWS\nconfiguration globally. This means that any time we use interact with a\nmicroservice  (such as S3), the boto3  library will always look to the files\nstored in ~/.aws/  for our keys and secrets, without us specifying.  This\ncritical from a security perspective as it removes all  mentions of credentials\nfrom our codebase: including the location of said secrets.\n\nUse $ aws configure  to kickstart the process:\n\n$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n\n\nThis creates a couple config files for us. If we never need to modify these\nfiles, they can be found here:\n\n$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n\n\nNode Time\nWe'll assume you have an app set up with some basic routing, such as the\nbarebones ExpressJS set up.\n\nIn your app we'll need to add 2 dependencies:\n\n$ npm install --save aws-sdk\n$ npm install --save aws-config\n\n\nNow we'll create a route.\n\nvar awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n\n\nNotice the empty curly brackets in new AWS.S3({}). If we had decided to\nbarbarically hardcode our credentials into our source code, normally those\nvalues would live between those brackets as an object. When the brackets are\nempty, the AWS library automagically knows to look to our AWS credentials file\nfor our access and secret keys. \n\nThis is how you'd do things the wrong way, just in case you wanted to be\nentertained:\n\nvar s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n\n\nYeah, that totally won't get committed somewhere by accident. Shake-my-head fam.\n\nThat's pretty much it: this route will prompt a download of the target file upon\nhitting the route. As much as I'm sure we'd all love to sit here and go through\nmore complicated use cases, let's just avoid Callback Hell altogether and enjoy\nthe rest of our day.\n\nHell will have to wait until next time.","html":"<p>We here at H+S are dedicated to one simple cause: creating posts about oddly specific programming scenarios. Somewhere in the world as sad soul is looking to programmatically access files from an S3 server while keeping their bucket private. To that person: we heard you.</p><p>There are plenty of reasons you'd want to access files in S3. For example, let's say you read <a href=\"https://hackersandslackers.com/using-pandas-with-aws-lambda/\">that post</a> about using Pandas in a Lambda function. Since you're already familiar with <a href=\"https://hackersandslackers.com/using-pymysql/\">PyMySQL</a>, you may hypothetically be in a position to export data from a DB query to a CSV saved in S3. I bet you can guess what I've been doing lately.</p><h2 id=\"configure-the-aws-cli-on-your-vps\">Configure the AWS CLI on your VPS</h2><p>The easiest and safest way to interact with other AWS services on your EC2 instance (or VPS of choice) is via the AWS CLI. This is easily installed as a global Python3 library:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ pip3 install awscli\n</code></pre>\n<!--kg-card-end: markdown--><p>With the CLI installed we'll be able to do something truly magical: set our AWS configuration globally. This means that any time we use interact with a microservice  (such as S3), the <strong>boto3</strong> library will always look to the files stored in <code>~/.aws/</code> for our keys and secrets, without us specifying.  This critical from a security perspective as it removes <em>all</em> mentions of credentials from our codebase: including the location of said secrets.</p><p>Use <code>$ aws configure</code> to kickstart the process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n</code></pre>\n<!--kg-card-end: markdown--><p>This creates a couple config files for us. If we never need to modify these files, they can be found here:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"node-time\">Node Time</h2><p>We'll assume you have an app set up with some basic routing, such as the barebones ExpressJS set up.</p><p>In your app we'll need to add 2 dependencies:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install --save aws-sdk\n$ npm install --save aws-config\n</code></pre>\n<!--kg-card-end: markdown--><p>Now we'll create a route.</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Notice the empty curly brackets in <code>new AWS.S3({})</code>. If we had decided to barbarically hardcode our credentials into our source code, normally those values would live between those brackets as an object. When the brackets are empty, the AWS library automagically knows to look to our AWS credentials file for our access and secret keys. </p><p>This is how you'd do things the <strong><em>wrong </em></strong>way, just in case you wanted to be entertained:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Yeah, that totally won't get committed somewhere by accident. Shake-my-head fam.</p><p>That's pretty much it: this route will prompt a download of the target file upon hitting the route. As much as I'm sure we'd all love to sit here and go through more complicated use cases, let's just avoid Callback Hell altogether and enjoy the rest of our day.</p><p>Hell will have to wait until next time.</p>","url":"https://hackersandslackers.com/accessing-private-s3-objects-with-node/","uuid":"210f5e64-7599-43d0-a148-d68373a9d3c4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2c34345f0bc81011d7cfc6"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673682","title":"Using Pandas with AWS Lambda Functions","slug":"using-pandas-with-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","custom_excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","created_at_pretty":"20 June, 2018","published_at_pretty":"21 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-20T18:21:31.000-04:00","published_at":"2018-06-21T07:30:00.000-04:00","updated_at":"2019-03-28T08:49:25.000-04:00","meta_title":"Using Pandas with AWS Lambda Functions | Hackers and Slackers","meta_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","og_title":"Using Pandas with AWS Lambda","twitter_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-2.jpg","twitter_title":"Using Pandas with AWS Lambda","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In one corner we have Pandas: Python's beloved data analysis library. In the\nother, AWS: the unstoppable cloud provider we're obligated to use for all\neternity. We should have known this day would come.\n\nWhile not the prettiest workflow, uploaded Python package dependencies for usage\nin AWS Lambda is typically straightforward. We install the packages locally to a\nvirtual env, package them with our app logic, and upload a neat CSV to Lambda.\nIn some cases this doesn't always work: some packages result in a cryptic error\nmessage with absolutely no helpful instruction. Pandas is one of those packages.\n\nWhy is this? I can't exactly speak to that, but I can speak to how to fix it.\n\nSpin up an EC2 Instance\nCertain Python packages need to be installed and compiled on an EC2 instance in\norder to work properly with AWS microservices. I wish I could say that this fun\nlittle fact is well-documented somewhere in AWS with a perfectly good\nexplanation. It's not, and it doesn't.  It's probably best not to ask questions.\n\nSpin up a free tier EC2 instance, update your system packages, and make sure\nPython3 is installed. Some people theorize that the Python dependency package\nerrors happen when said dependencies are installed via versions of Python which\ndiffer from the version AWS is running. Those people are wrong.  I've already\nwasted the time to debunk this. They are liars.\n\nWith Python installed,  create a virtual environment inside any empty directory:\n\n$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n\n\nWith the environment active, install pandas via pip3 install pandas. This will\nsave pandas and all its dependencies to the site-packages  folder our\nenvironment is running from, resulting in a URL such as this: \npandasenv/lib/python3.6/site-packages.\n\nPandas is actually 5 packages total. We're going to add each of these libraries\nto a zip file by installing zip, and adding each folder to the zip file\none-by-one. Finally, we'll apply some liberal permissions to the zip file we\njust created so we can grab it via FTP.\n\n$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n\n\nThis should be ready for you to FTP in your instance and grab as a zip file now\n(assuming you want to work locally). Alternatively, we could always copy those\npackages into the directory we'd like to work out of and zip everything once\nwe're done.\n\nUpload Source Code to S3\nAt this point, you should have been able to grab the AWS friendly version of\nPandas which is ready to be included in the final source code which will become\nyour Lambda Function.  You might notice that pandas alone nearly 30Mb: which is\nroughly the file size of countless intelligent people creating their life's\nwork. When Lambda Functions go above this file size, it's best to upload our\nfinal package (with source and dependencies) as a zip file to S3, and link it to\nLambda that way. This is considerably faster than the alternative of uploading\nthe zip to Lambda directly.\n\nBonus Round: Saving Exports\nWhat? You want to save a CSV result of all the cool stuff you're doing in\nPandas? You really are needy.\n\nBecause AWS is invoking the function, any attempt to read_csv()  will be\nworthless to us. To get around this, we can use boto3  to write files to an S3\nbucket instead:\n\nimport pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n\n\nWord of Advice\nThis isn't the prettiest process in the world, but we're somewhat at fault here.\nLambda functions are intended to be small tidbits of logic aimed to serve a\nsingle simple purpose. We just jammed 30Mbs of Python libraries into that simple\npurpose.\n\nThere are alternatives to Pandas that are better suited for usage in Lambda,\nsuch as Toolz  (thanks to Snkia for the heads up). Enjoy your full Pandas\nlibrary for now, but remember to feel bad about what you’ve done for next time.","html":"<p>In one corner we have Pandas: Python's beloved data analysis library. In the other, AWS: the unstoppable cloud provider we're obligated to use for all eternity. We should have known this day would come.</p><p>While not the prettiest workflow, uploaded Python package dependencies for usage in AWS Lambda is typically straightforward. We install the packages locally to a virtual env, package them with our app logic, and upload a neat CSV to Lambda. In some cases this doesn't always work: some packages result in a cryptic error message with absolutely no helpful instruction. Pandas is one of those packages.</p><p>Why is this? I can't exactly speak to that, but I can speak to how to fix it.</p><h2 id=\"spin-up-an-ec2-instance\">Spin up an EC2 Instance</h2><p>Certain Python packages need to be installed and compiled on an EC2 instance in order to work properly with AWS microservices. I wish I could say that this fun little fact is well-documented somewhere in AWS with a perfectly good explanation. It's not, and it doesn't.  It's probably best not to ask questions.</p><p>Spin up a free tier EC2 instance, update your system packages, and make sure Python3 is installed. Some people theorize that the Python dependency package errors happen when said dependencies are installed via versions of Python which differ from the version AWS is running. <em>Those people are wrong.</em> I've already wasted the time to debunk this. They are liars.</p><p>With Python installed,  create a virtual environment inside any empty directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n</code></pre>\n<!--kg-card-end: markdown--><p>With the environment active, install pandas via <code>pip3 install pandas</code>. This will save pandas and all its dependencies to the <em>site-packages</em> folder our environment is running from, resulting in a URL such as this: <code>pandasenv/lib/python3.6/site-packages</code>.</p><p>Pandas is actually 5 packages total. We're going to add each of these libraries to a zip file by installing <code>zip</code>, and adding each folder to the zip file one-by-one. Finally, we'll apply some liberal permissions to the zip file we just created so we can grab it via FTP.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n</code></pre>\n<!--kg-card-end: markdown--><p>This should be ready for you to FTP in your instance and grab as a zip file now (assuming you want to work locally). Alternatively, we could always copy those packages into the directory we'd like to work out of and zip everything once we're done.</p><h3 id=\"upload-source-code-to-s3\">Upload Source Code to S3</h3><p>At this point, you should have been able to grab the <em>AWS friendly </em>version of Pandas which is ready to be included in the final source code which will become your Lambda Function.  You might notice that pandas alone nearly <em>30Mb</em>: which is roughly the file size of countless intelligent people creating their life's work. When Lambda Functions go above this file size, it's best to upload our final package (with source and dependencies) as a zip file to S3, and link it to Lambda that way. This is considerably faster than the alternative of uploading the zip to Lambda directly.</p><h2 id=\"bonus-round-saving-exports\">Bonus Round: Saving Exports</h2><p>What? You want to save a CSV result of all the cool stuff you're doing in Pandas? You really are needy.</p><p>Because AWS is invoking the function, any attempt to <code>read_csv()</code> will be worthless to us. To get around this, we can use <strong>boto3</strong> to write files to an S3 bucket instead:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"word-of-advice\">Word of Advice</h3><p>This isn't the prettiest process in the world, but we're somewhat at fault here. Lambda functions are intended to be small tidbits of logic aimed to serve a single simple purpose. We just jammed 30Mbs of Python libraries into that simple purpose.</p><p>There are alternatives to Pandas that are better suited for usage in Lambda, such as <em>Toolz</em> (thanks to Snkia for the heads up). Enjoy your full Pandas library for now, but remember to feel bad about what you’ve done for next time.</p>","url":"https://hackersandslackers.com/using-pandas-with-aws-lambda/","uuid":"3d2d6592-5614-4485-b9cd-15d905a28c46","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2ad36cded32f5af8fd674d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867366f","title":"AWS Adventures: Why Can't I SSH Into My Damn EC2 Instance?","slug":"aws-adventures-why-cant-i-ssh-into-my-damn-ec2-instance","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/ec2@2x.jpg","excerpt":"Watch as I struggle with the cloud!  My pain, your gain!","custom_excerpt":"Watch as I struggle with the cloud!  My pain, your gain!","created_at_pretty":"06 June, 2018","published_at_pretty":"06 June, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-06-06T19:27:37.000-04:00","published_at":"2018-06-06T19:41:47.000-04:00","updated_at":"2018-07-24T22:06:03.000-04:00","meta_title":"AWS Adventures: Why Can't I SSH Into My Damn EC2 Instance? | Hackers and Slackers","meta_description":"Watch as I struggle with the cloud!  My pain, your gain!","og_description":"Watch as I struggle with the cloud!  My pain, your gain!","og_image":"https://hackersandslackers.com/content/images/2018/06/ec2@2x.jpg","og_title":"AWS Adventures: Why Can't I SSH Into My Damn EC2 Instance?","twitter_description":"Watch as I struggle with the cloud!  My pain, your gain!","twitter_image":"https://hackersandslackers.com/content/images/2018/06/ec2@2x.jpg","twitter_title":"AWS Adventures: Why Can't I SSH Into My Damn EC2 Instance?","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Sometimes I find myself responsible for setting up and maintaining my own\ninfrastructure for doing data stuff. In light of this, I've taken it upon myself\nto learn more about AWS in order for this experience to be less terrifying. I've\nbeen following a course on Udemy for the AWS Solutions Architect Certification.\nSo far, so good.\n\nCut to the EC2 section. Has you spin up a little EC2 instance and SSH into it. I\nwould find myself SSHing in, then after a few minutes my connection would be\nreset and then any further attempts would be met with a Resource temporarily\nunavailable.\n\nI googled around. Lots of stuff that didn't work. I logged into a client's\nconsole that I had access to, tried to see if there were any settings that were\ndifferent from mine. What on Earth did I mess with? I first made an AWS account\nin 2012, and go into phases of messing with my personal account and forgetting\nit exists. I must have hit some kind of \"Make it so every server you launch is\nunreachable and useless\" button at some point, then forgotten where this button\nwas.\n\nIt's a tough little jungle. There's numerous different versions of security on\nAWS. Then, finally, I noticed an angel on StackOverflow posted that you should\ncheck to see whether you have a public DNS address. Turns out I didn't! \nhttps://stackoverflow.com/questions/20941704/ec2-instance-has-no-public-dns\n\n 1. Go to console.aws.amazon.com\n 2. Go To Services -> VPC\n 3. Open Your VPCs\n 4. select your VPC connected to your EC2 and\n 5. select Actions => Edit DNS Hostnames ---> Change DNS hostnames: to YES\n\nAaand it worked!\n\nHow did it get like this? I deleted and re-made the VPC numerous times, so it\nwas off by default for some reason. When did I mess with this? Why did I mess\nwith this? Why did it work at all instead of blocking me from the beginning? Who\nknows!\n\nP.S. Digging the \"code journal\" style of tech talk, among other reasons because\nI know I'm not going to accidentally plagiarize some tutorial I've read.\n\nUPDATE:\nJust tried to log back in with my settings from before, and wouldnchaknowit, it\nsuddenly started being wacky again! Exact same problems too - can log in for a\nbit shortly after the instance starts, then connection reset, then temporarily\nunavailable.\n\nI decided to look at the System Logs (accessible from Actions -> Instance\nSettings -> Get System Log). I noticed that it was blank. And when logs showed\nup, coincidentally my connection was reset again! The last line of the system\nlog was ip-172-30-0-53 login:. A clue, mayhaps! Maybe it has something to do\nwith the little intranet it was logged into!\n\nFrantic googling. Eventually, I discover that when I created my account, there\nwas a system called \"EC2 Classic\" in place (it presumably was not called this at\nthe time). They eventually added the VPC feature (short and sweet: VPC stands\nfor either \"Volatile Psionic Cloak\" or \"Virtual Private Cloud\", and is one of\nthe several terms that mean \"a place where your VMs run\" (each one has its own\nsubtle idiosyncracies. I will be bitching about them again, I promise you.) Why\ndoesn't this text editor do paren-highlighting? (print-str \"I love lisp\")  Eh,\nlet's say this is enough.)\n\nSooo, VPCs put a spanner in the works of how EC2 classic worked. If you made\nyour AWS account after VPC was introduced, you had to do stuff with VPC.\nHowever, if your account was old enough that you had access to EC2 Classic, they\nlet you use both systems so as not to nuke old infrastructure you might have\nhad. Note that whether you were \"VPC Only\" or \"VPC And EC2 Classic\" is something\nthat's flagged at the account level.\n\nI think it MIGHT have been okay, but while trying to log in, I deleted the VPC\nthat was present cuz I read that it'd spawn a new one that was set to default\noptions. Now, this did happen, except that the one I deleted was my \"Default\nVPC\". I do not fully understand the signifigance of this, but it seems to be\nwhat was getting in my way. So, simple - just make a new Default VPC! However,\nif you have an \"EC2 And VPC\" account, you cannot make a new Default VPC (for\nreasons that are mysterious to me).\n\nRemember when I said \"VPC and EC2 Classic\" status was at the account level?\nUnfortunately, that means you need to contact AWS support in order to change it.\nIf you've ever used normal Amazon customer support, you're probably accustomed\nto it being pretty good - you just open a chat window, say your stuff didn't\narrive, then they overnight the missing delivery. AWS support (if you're not\npaying extra money) is not like that. I put in the request on Wednesday night,\nand I've had several rounds of confirming \"Yes, I want you to do this\", each\nwith at least a day-long turn-around. It is now Sunday, and I got a message from\nyesterday saying that my account is finally in the process of being moved over.\nHopefully it all goes off without a hitch, but we'll see!","html":"<p>Sometimes I find myself responsible for setting up and maintaining my own infrastructure for doing data stuff.  In light of this, I've taken it upon myself to learn more about AWS in order for this experience to be less terrifying.  I've been following a course on Udemy for the AWS Solutions Architect Certification.  So far, so good.</p>\n<p>Cut to the EC2 section.  Has you spin up a little EC2 instance and SSH into it.  I would find myself SSHing in, then after a few minutes my connection would be reset and then any further attempts would be met with a <code>Resource temporarily unavailable</code>.</p>\n<p>I googled around.  Lots of stuff that didn't work.  I logged into a client's console that I had access to, tried to see if there were any settings that were different from mine.  What on Earth did I mess with?  I first made an AWS account in 2012, and go into phases of messing with my personal account and forgetting it exists.  I must have hit some kind of &quot;Make it so every server you launch is unreachable and useless&quot; button at some point, then forgotten where this button was.</p>\n<p>It's a tough little jungle.  There's numerous different versions of security on AWS.  Then, finally, I noticed an angel on StackOverflow posted that you should check to see whether you have a public DNS address.  Turns out I didn't!  <a href=\"https://stackoverflow.com/questions/20941704/ec2-instance-has-no-public-dns\">https://stackoverflow.com/questions/20941704/ec2-instance-has-no-public-dns</a></p>\n<ol>\n<li>Go to console.aws.amazon.com</li>\n<li>Go To Services -&gt; VPC</li>\n<li>Open Your VPCs</li>\n<li>select your VPC connected to your EC2 and</li>\n<li>select Actions =&gt; Edit DNS Hostnames ---&gt; Change DNS hostnames: to YES</li>\n</ol>\n<p>Aaand it worked!</p>\n<p>How did it get like this?  I deleted and re-made the VPC numerous times, so it was off by default for some reason.  When did I mess with this?  Why did I mess with this?  Why did it work at all instead of blocking me from the beginning?  Who knows!</p>\n<p>P.S. Digging the &quot;code journal&quot; style of tech talk, among other reasons because I know I'm not going to accidentally plagiarize some tutorial I've read.</p>\n<p>UPDATE:<br>\nJust tried to log back in with my settings from before, and wouldnchaknowit, it suddenly started being wacky again!  Exact same problems too - can log in for a bit shortly after the instance starts, then connection reset, then temporarily unavailable.</p>\n<p>I decided to look at the System Logs (accessible from Actions -&gt; Instance Settings -&gt; Get System Log).  I noticed that it was blank.  And when logs showed up, coincidentally my connection was reset again!  The last line of the system log was <code>ip-172-30-0-53 login:</code>.  A clue, mayhaps!  Maybe it has something to do with the little intranet it was logged into!</p>\n<p>Frantic googling.  Eventually, I discover that when I created my account, there was a system called &quot;EC2 Classic&quot; in place (it presumably was not called this at the time).  They eventually added the VPC feature (short and sweet: VPC stands for either &quot;Volatile Psionic Cloak&quot; or &quot;Virtual Private Cloud&quot;, and is one of the several terms that mean &quot;a place where your VMs run&quot; (each one has its own subtle idiosyncracies.  I will be bitching about them again, I promise you.)  Why doesn't this text editor do paren-highlighting?  <code>(print-str &quot;I love lisp&quot;)</code> Eh, let's say this is enough.)</p>\n<p>Sooo, VPCs put a spanner in the works of how EC2 classic worked.  If you made your AWS account after VPC was introduced, you had to do stuff with VPC.  However, if your account was old enough that you had access to EC2 Classic, they let you use both systems so as not to nuke old infrastructure you might have had.  Note that whether you were &quot;VPC Only&quot; or &quot;VPC And EC2 Classic&quot; is something that's flagged at the account level.</p>\n<p>I think it MIGHT have been okay, but while trying to log in, I deleted the VPC that was present cuz I read that it'd spawn a new one that was set to default options.  Now, this did happen, except that the one I deleted was my &quot;Default VPC&quot;.  I do not fully understand the signifigance of this, but it seems to be what was getting in my way.  So, simple - just make a new Default VPC!  However, if you have an &quot;EC2 And VPC&quot; account, you cannot make a new Default VPC (for reasons that are mysterious to me).</p>\n<p>Remember when I said &quot;VPC and EC2 Classic&quot; status was at the account level?  Unfortunately, that means you need to contact AWS support in order to change it.  If you've ever used normal Amazon customer support, you're probably accustomed to it being pretty good - you just open a chat window, say your stuff didn't arrive, then they overnight the missing delivery.  AWS support (if you're not paying extra money) is not like that.  I put in the request on Wednesday night, and I've had several rounds of confirming &quot;Yes, I want you to do this&quot;, each with at least a day-long turn-around.  It is now Sunday, and I got a message from yesterday saying that my account is finally in the process of being moved over.  Hopefully it all goes off without a hitch, but we'll see!</p>\n","url":"https://hackersandslackers.com/aws-adventures-why-cant-i-ssh-into-my-damn-ec2-instance/","uuid":"62262c95-6926-4f0c-96c7-cb4269f40a43","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b186de91e7fe93906b2e8f7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867364d","title":"Preparing your AWS Project to Build an API","slug":"building-an-api-using-aws","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/awsapi-1-3.jpg","excerpt":"Get your AWS instance configured, and become familiar with the services needed to build APIs.","custom_excerpt":"Get your AWS instance configured, and become familiar with the services needed to build APIs.","created_at_pretty":"06 May, 2018","published_at_pretty":"06 May, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-05-06T07:30:47.000-04:00","published_at":"2018-05-06T08:58:41.000-04:00","updated_at":"2019-03-28T08:54:39.000-04:00","meta_title":"Preparing your AWS Project to Build an API | Hackers and Slackers","meta_description":"Get your AWS instance configured, and become familiar with the services needed to build APIs.","og_description":"Get your AWS instance configured, and become familiar with the services needed to build APIs.","og_image":"https://hackersandslackers.com/content/images/2019/03/awsapi-1-3.jpg","og_title":"Preparing your AWS Project to Build an API | Hackers and Slackers","twitter_description":"Get your AWS instance configured, and become familiar with the services needed to build APIs.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/awsapi-1-2.jpg","twitter_title":"Preparing your AWS Project to Build an API | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},"tags":[{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"There comes a surreal moment in nearly every profession in which perspective\nviolently forces itself into our self-awareness. People with cooler jobs\nprobably have that moment when they save their first patient or launch their\nfirst rocket. For me, the idea of building an API was this moment in software\ndevelopment. All those past black boxes which spat out results your life\ndepended on: we can make those now.\n\nFeel free to remain unfazed by this as I'm sure most are... for those of us who\ncan't remember how they became an \"engineer\" in the first place, API design\nfeels like reaching a final frontier. Granted, this may happen at least 20 more\ntimes in your software career, so don’t pause too long now.\n\nIf you managed to catch the tutorial on setting up RDS on AWS\n[https://hackersandslackers.com/setting-up-mysql-on-aws/], you're already a step\nahead. We're going to make our way through this slowly, which is probably a good\nidea even if you've done this before. There are so many pitfalls and nuances in\nAWS's architecture that I'm not even sure I fully understand what's happening,\neven when everything works.\n\nQuick Overview\nHere are the services we'll be using to hook this baby up:\n\n *   RDS:  Amazon's cloud-hosted relational databases. These databases come\n   preconfigured with an endpoint, which can be made accessible to other AWS\n   services or even to external manipulation. \n *   Lambda:  Snippets of code which can be invoked without setting up a web\n   server (hence: \"serverless\"). Lambda functions are intended to serve simple,\n   specific functions (such as serving as the logic for an endpoint, for\n   instance). Lambda functions play well with other AWS services: we'll be using\n   this as the glue between our API and interacting with the Database.\n *   API Gateway:  Amazon's visual editor for creating an API. API Gateway\n   allows developers to architect the structure and logic of APIs without having\n   to worry about setting up routes via code.\n *   IAM:  Amazon's headache of a user & permissions manager. IAM is needed to\n   specify exactly with AWS services have access to other services, which users\n   are permitted to interact with your API, etc. \n\nGameplan\nSo here's the deal. Our RDS  database will be where all the data we provide and\nreceive will live. Lambda  functions will be the snippets of code actually\ninteracting with information from the database; all of our queries will be done\nthrough Lambda. API Gateway  will control the \"design\" of the API, as in the\nstructure of endpoints, their respective methods, and how all these should\ninteract with Lambda.\n\nIt sounds simple enough, but the devil is in the details. And trust me, there\nare a lot of details.\n\nSetting the Correct Role Permissions\nUsually, I'd say we should jump into the fun stuff and deal with the details\nwhen we get to them. I won't let you steer down that road with AWS... let's\navoid smashing our heads on keyboards where possible and kick things off slow.\n\nIf you were to attempt to create a Lambda function off the bat, the first prompt\nto appear would demand a \"role\" to be specified. Roles are one of the types of\npermission packages (?) we mentioned earlier. Roles limit exactly which services\nyour Lambda function can interact with off the bat. Start with the wrong role,\nand you won't be able to do much of anything.\n\nHead over the IAM console\n[https://console.aws.amazon.com/iam/home?region=us-east-1#/home]  to set up an\nappropriate role:\n\nWhat a God-awful way to handle permissions.Let's pause for a moment to take this\nall in. You'll see we have users, groups, roles, policies and a whole bunch of\nother garbage. Policies can be attached to roles. Policies can also be attached\nto users, and also attached to groups. Users can be in groups. Wait, so what if\na user has a bunch of policies, but then joins a group with a bunch of policies?\nWhat even is a \"policy\"? These are the real questions. The short answer is none\nof it makes sense; it's just extra job security for those who make it work.\n\nClick on \"Roles\" in the sidebar. Create a role. Select \"Lambda\" and click next.\n\nThis interface only seems to get worse.Ok cool. The role  we're creating is basically just going to be a collection of\npermissions we can attached directly to the role. Go ahead and attach these:\n\n * AmazonVPCFullAccess\n * AmazonAPIGatewayInvokeFullAccess\n * AmazonRDSFullAccess\n * AWSLambdaFullAccess\n * CloudWatchLogsFullAccess\n\nSave the role, and remember what you name it. You'll need it.\n\nGetting Started with Lambda Functions\nGo back to the Lambda console. It's game time. We're going to create a function\nfrom scratch (sadly, I haven't found any of the blueprints to be very useful\njust yet).\n\nIgnore Amazon's silly Blueprints.Under “runtime”, you’ll need to pick which\nprogramming language we’ll be writing our function in. I’m doing Python 3\nbecause I don’t mess with semicolons, ya dig. Most people seem to stick with\nNode, which makes sense: Node is much faster at runtime, especially when you\nconsider that AWS runs Node natively. The choice is your preference.\n\nAha, see the “Role” dropdown? This is what I warned you about. Select the role\nyou just created earlier from existing roles.\n\nLambda Function Editor\nWelcome to Lambda's web UIBehold, the Lambda visual editor. That tree you're\nseeing is a representation of the integration this function will handle. The\ncurrent function is the box top-middle, the trigger is on the left, and the list\nof potential AWS services we can touch is on the right; these were automatically\npopulated by that role I forced you to create. You're welcome.\n\nNOTE:  The entire interface below this section depends on which service you've\nclicked in the tree. It's not the most intuitive at first. I have my Lambda\nfunction selected, so that's the interface I can interact with below.\n\nInline Code Editor\nWe can create Lambdas directly in the browser, or by uploading souce via zip\nfile.Real quick, we need to go over what each field here does. The dropdown\ncurrently set to \"edit code inline\" can be expanded, which gives you the option\nto upload a zip file of source code. THIS WILL DELETE ALL PREEXISTING WORK. \nThey don't tell you that, hah. Ha hah. I recommend doing everything offline to\nbe uploaded later - this needs to be done with python packages anyway.\n\nHandler  specifies which function should be called upon initialization.\n\"lambda_function\" is referring to the function, so \"handler\" here specifies that\nthe function handler within lambda_function.py  is what will get called upon\nexecution.\n\nOur Lambda’s VPC Settings\nScroll down until you hit this pretty little gem:\n\nAWS is filled with complicated network concepts, and zero attempts to explain\nthem.We need to specify the VPC this function will interact with. If you created\nan RDS already, go ahead select the VPC you created. Add a bunch of subnets\n(whichever ones). Finally, select a security group. Remember that the\npermissions of this group determine whether or not your VPC is allowed to speak\nto this function. If you're struggling with this, check out the AWS MySQL post\nagain. I'm not going to link it twice in one post, sorry. I have self-respect\nyou know.\n\nThat's Enough For Now\nThere's a lot to take in when playing around in AWS. The combination of\ngibberish terminology and horrible documentation doesn't exactly make for solid\nuser experience. If you're new to AWS and any of this seems frustrating, know\nit's supposed to be. Amazon owns you, and they hate you. Kind of like God.\n\nI'd suggest messing around the interface, and maybe even check out API Gateway a\nbit to get a feel for how that stuff looks. They set you up with a cute demo to\nmake you think it's going to be easy, so maybe you'll enjoy that. Next time,\nwe're going to crank out some Lambdas.","html":"<p>There comes a surreal moment in nearly every profession in which perspective violently forces itself into our self-awareness. People with cooler jobs probably have that moment when they save their first patient or launch their first rocket. For me, the idea of building an API was this moment in software development. All those past black boxes which spat out results your life depended on: we can make those now.</p><p>Feel free to remain unfazed by this as I'm sure most are... for those of us who can't remember how they became an \"engineer\" in the first place, API design feels like reaching a final frontier. Granted, this may happen at least 20 more times in your software career, so don’t pause too long now.</p><p>If you managed to catch the tutorial on <a href=\"https://hackersandslackers.com/setting-up-mysql-on-aws/\">setting up RDS on AWS</a>, you're already a step ahead. We're going to make our way through this slowly, which is probably a good idea even if you've done this before. There are so many pitfalls and nuances in AWS's architecture that I'm not even sure I fully understand what's happening, even when everything works.</p><h2 id=\"quick-overview\">Quick Overview</h2><p>Here are the services we'll be using to hook this baby up:</p><ul><li> <strong>RDS:</strong> Amazon's cloud-hosted relational databases. These databases come preconfigured with an endpoint, which can be made accessible to other AWS services or even to external manipulation. </li><li> <strong>Lambda:</strong> Snippets of code which can be invoked without setting up a web server (hence: \"<em>serverless</em>\"). Lambda functions are intended to serve simple, specific functions (such as serving as the logic for an endpoint, for instance). Lambda functions play well with other AWS services: we'll be using this as the glue between our API and interacting with the Database.</li><li> <strong>API Gateway:</strong> Amazon's visual editor for creating an API. API Gateway allows developers to architect the structure and logic of APIs without having to worry about setting up routes via code.</li><li> <strong>IAM:</strong> Amazon's headache of a user &amp; permissions manager. IAM is needed to specify exactly with AWS services have access to other services, which users are permitted to interact with your API, etc. </li></ul><h3 id=\"gameplan\">Gameplan</h3><p>So here's the deal. Our <strong>RDS</strong> database will be where all the data we provide and receive will live. <strong>Lambda</strong> functions will be the snippets of code actually interacting with information from the database; all of our queries will be done through Lambda. <strong>API Gateway</strong> will control the \"design\" of the API, as in the structure of endpoints, their respective methods, and how all these should interact with Lambda.</p><p>It sounds simple enough, but the devil is in the details. And trust me, there are a lot of details.</p><h2 id=\"setting-the-correct-role-permissions\">Setting the Correct Role Permissions</h2><p>Usually, I'd say we should jump into the fun stuff and deal with the details when we get to them. I won't let you steer down that road with AWS... let's avoid smashing our heads on keyboards where possible and kick things off slow.</p><p>If you were to attempt to create a Lambda function off the bat, the first prompt to appear would demand a \"<strong>role</strong>\" to be specified. Roles are one of the types of permission packages (?) we mentioned earlier. Roles limit exactly which services your Lambda function can interact with off the bat. Start with the wrong role, and you won't be able to do much of anything.</p><p>Head over the <a href=\"https://console.aws.amazon.com/iam/home?region=us-east-1#/home\">IAM console</a> to set up an appropriate role:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-08.06.13.png\" class=\"kg-image\" alt=\"Screenshot-2018-05-06-08.06.13\"><figcaption>What a God-awful way to handle permissions.</figcaption></figure><!--kg-card-end: image--><p>Let's pause for a moment to take this all in. You'll see we have users, groups, roles, policies and a whole bunch of other garbage. Policies can be attached to roles. Policies can also be attached to users, and also attached to groups. Users can be in groups. Wait, so what if a user has a bunch of policies, but then joins a group with a bunch of policies? What even is a \"policy\"? These are the real questions. The short answer is none of it makes sense; it's just extra job security for those who make it work.</p><p>Click on \"Roles\" in the sidebar. Create a role. Select \"Lambda\" and click <em>next</em>.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-08.18.21.png\" class=\"kg-image\" alt=\"Screenshot-2018-05-06-08.18.21\"><figcaption>This interface only seems to get worse.</figcaption></figure><!--kg-card-end: image--><p>Ok cool. The <strong>role</strong> we're creating is basically just going to be a collection of permissions we can attached directly to the role. Go ahead and attach these:</p><ul><li>AmazonVPCFullAccess</li><li>AmazonAPIGatewayInvokeFullAccess</li><li>AmazonRDSFullAccess</li><li>AWSLambdaFullAccess</li><li>CloudWatchLogsFullAccess</li></ul><p>Save the role, and remember what you name it. You'll need it.</p><h2 id=\"getting-started-with-lambda-functions\">Getting Started with Lambda Functions</h2><p>Go back to the Lambda console. It's game time. We're going to create a function from scratch (sadly, I haven't found any of the blueprints to be very useful just yet).</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-08.25.16.png\" class=\"kg-image\" alt=\"Lambda\"><figcaption>Ignore Amazon's silly Blueprints.</figcaption></figure><!--kg-card-end: image--><p>Under “runtime”, you’ll need to pick which programming language we’ll be writing our function in. I’m doing Python 3 because I don’t mess with semicolons, ya dig. Most people seem to stick with Node, which makes sense: Node is much faster at runtime, especially when you consider that AWS runs Node natively. The choice is your preference.</p><p>Aha, see the “Role” dropdown? This is what I warned you about. Select the role you just created earlier from existing roles.</p><h3 id=\"lambda-function-editor\">Lambda Function Editor</h3><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screenshot-2018-05-06-08.30.00.png\" class=\"kg-image\"><figcaption>Welcome to Lambda's web UI</figcaption></figure><!--kg-card-end: image--><p>Behold, the Lambda visual editor. That tree you're seeing is a representation of the integration this function will handle. The current function is the box top-middle, the trigger is on the left, and the list of potential AWS services we can touch is on the right; these were automatically populated by that role I forced you to create. You're welcome.</p><p><strong>NOTE:</strong> The entire interface below this section depends on which service you've clicked in the tree. It's not the most intuitive at first. I have my Lambda function selected, so that's the interface I can interact with below.</p><h3 id=\"inline-code-editor\">Inline Code Editor</h3><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-08.35.20.png\" class=\"kg-image\" alt=\"Function code\"><figcaption>We can create Lambdas directly in the browser, or by uploading souce via zip file.</figcaption></figure><!--kg-card-end: image--><p>Real quick, we need to go over what each field here does. The dropdown currently set to \"edit code inline\" can be expanded, which gives you the option to upload a zip file of source code. <em>THIS WILL DELETE ALL PREEXISTING WORK.</em> They don't tell you that, hah. Ha hah. I recommend doing everything offline to be uploaded later - this needs to be done with python packages anyway.</p><p><strong>Handler</strong> specifies which function should be called upon initialization. \"lambda_function\" is referring to the function, so \"handler\" here specifies that the function handler within <code>lambda_function.py</code> is what will get called upon execution.</p><h3 id=\"our-lambda-s-vpc-settings\">Our Lambda’s VPC Settings</h3><p>Scroll down until you hit this pretty little gem:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-08.41.02.png\" class=\"kg-image\" alt=\"VPC\"><figcaption>AWS is filled with complicated network concepts, and zero attempts to explain them.</figcaption></figure><!--kg-card-end: image--><p>We need to specify the VPC this function will interact with. If you created an RDS already, go ahead select the VPC you created. Add a bunch of subnets (whichever ones). Finally, select a <em>security group</em>. Remember that the permissions of this group determine whether or not your VPC is allowed to speak to this function. If you're struggling with this, check out the AWS MySQL post again. I'm not going to link it twice in one post, sorry. I have self-respect you know.</p><h2 id=\"that-s-enough-for-now\">That's Enough For Now</h2><p>There's a lot to take in when playing around in AWS. The combination of gibberish terminology and horrible documentation doesn't exactly make for solid user experience. If you're new to AWS and any of this seems frustrating, know it's supposed to be. Amazon owns you, and they hate you. Kind of like God.</p><p>I'd suggest messing around the interface, and maybe even check out API Gateway a bit to get a feel for how that stuff looks. They set you up with a cute demo to make you think it's going to be easy, so maybe you'll enjoy that. Next time, we're going to crank out some Lambdas.</p>","url":"https://hackersandslackers.com/building-an-api-using-aws/","uuid":"45101fbc-527b-432b-994e-31d855e76aff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5aeee767926f095edfccda8e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673648","title":"MySQL on the Cloud with AWS RDS","slug":"setting-up-mysql-on-aws","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","excerpt":"Spinning up a standalone MySQL Database with Amazon.","custom_excerpt":"Spinning up a standalone MySQL Database with Amazon.","created_at_pretty":"30 April, 2018","published_at_pretty":"01 May, 2018","updated_at_pretty":"27 November, 2018","created_at":"2018-04-29T23:12:26.000-04:00","published_at":"2018-04-30T20:14:57.000-04:00","updated_at":"2018-11-27T03:54:21.000-05:00","meta_title":"Setting up MySQL on AWS | Hackers and Slackers","meta_description":"Spinning up a standalone MySQL Database with Amazon","og_description":"Spinning up a standalone MySQL Database with Amazon","og_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","og_title":"Setting up MySQL on AWS","twitter_description":"Spinning up a standalone MySQL Database with Amazon","twitter_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","twitter_title":"Setting up MySQL on AWS","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"Last time we became familiar with the handiwork of setting up MySQL locally,\nnavigating databases via command line, and exposing your database to external\naccess. While badass, it has come to my attention that most people don't bother\ndoing things this way. Unless you're getting deep into some heavy architecture,\nmost people opt to use cloud services such as AWS to set up databases which are\nintended to be interacted with by multiple services.\n\nA perfect example is one we ran into over the weekend while working on this very\nblog. We're running a Ghost instance, which is respectably complex\nproduction-ready app. For a bunch of guys just looking to make some stupid blog\nwidgets, it became obvious that reverse engineering the undocumented inner\nworkings of an open source node app was a rabbit hole of complexity.\n\nHosting on AWS\nIn our case, AWS is useful for enforcing separation of concerns. Instead of\nbuilding new logic into a live app, we can build that logic elsewhere in a way\nthat's reusable across multiple apps.\n\nThe end goal here is simply to read/write to a database. That said, there's\nstill a fair amount of complexity involved. We'll need to leverage the following\nAWS services:\n\n * RDS (Relational Database Service): A cloud hosted database\n * API Gateway: An interface for building APIs\n * Lambda: The necessary serverless connector between RDS and Gateway\n * IAM: Amazon's god-awful user and policy manager\n\nFor now, all we're going to worry about is RDS.\n\nData is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs\nHead to the AWS console and create a new RDS instance. Once prompted, go with\nMySQL:\n\nAs though some of these are real options. Please.Stick with MySQL Production  on the next screen.\n\nDo everything in production. AlwaysConfiguration Settings\nThis is where we set our configurations. You'll notice immediately how\nconvoluted AWS tends to be with their naming conventions. I personally hate how\nintentionally unintuitive all of AWS tends to be (what the hell is a \ndb.t2.medium)? This sort of absurdity is just something we need to deal with\nforever. Amazon is technically outside the realm of enforceable Monopoly laws,\nand there's no reason to believe their reign of mediocre products and talking\nrobots is ever going to end.\n\n * License: Select general-public-license\n * Version: Choose whichever, just don't do an old one\n * Instance class: Most of these instances are huge and unnecessary. Go with\n   something small: I would also advise looking at the pricing plan.\n * Multi AZ: Create a replica.\n * Storage type: General.\n * Allocated storage: Feel free to allocate more for latency.\n * Publicly Accessible: True.\n\nGod I love configuring esoteric shit.Once configuration is complete, it takes a\ngood amount of time for the database to be created. While we wait, let's move on\nto creating to a user to access this. We can do this with IAM: another AWS\nproduct with an even more terrible interface.\n\nAccess\nFair warning: user roles and permissions are the worst part of AWS. I could\nwrite an entire series on how deep this mess of a scheme goes, but quite\nhonestly I still barely understand what I'm doing most of the time.\n\nCreating a User\nCreate a new user that will access the database. Go to the Users panel  and\ncreate a user:\n\nModifying permission policies\nPermissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS\nhas some default policies that we can leverage for our purposes, so this should\nluckily be somewhat straightforward.\n\nPolicies can also be combined so that users have multiple policies across AWS\nproducts.\n\nNative Client\nOnce your DB pops up in AWS, we're going to need to get you a GUI to modify your\nDB. Don't even try to be a hotshot by setting up all your tables via command\nline. It sucks, it's slower, and nobody is impressed. Don't bother downloading\nthe AWS CLI either. Do not pass GO. Do not collect 500 dollars.\n\nIn case you need to install MySQL locally, an OSX download can be found here.\nCome to think of it, that step was probably unnecessary. I'm not sure why I did\nthat.\n\nI settled on Sequel Pro [https://www.sequelpro.com/]  as a client. It's good\nenough, and their logo looks like pancakes. That's really the only metric I\nneeded tbh.\n\nTo connect to your database, you'll need to retrieve the endpoint and port\nnumber from your RDS console:\n\nConnect to that ish:\n\nHopefully everything went well! If not, I'm sure the problem will be a quick and\neasy fix. Surely it won't involve mindlessly swapping permissions for an entire\nday. You defintely won't somehow end up corrupting your .bash_profile, making\nPython invisible to your OS, and effectively destroying your computer. Only an\nidiot would do something like that. Yesterday evening.\n\nGo ahead and get accustomed to the UI of Sequel Pro - it's pretty\nstraightforward, and ten thousand million times less effort than creating tables\nvia terminal. Create columns under the \"structure\" tab - the terminology should\nimmediately seem familiar if you've been following the series until this point.\n\nProtip: Issues with Security Groups\nIf you're running into an issue connecting to your DB externally, I happened to\nrun in to a nice little issue the other day with security groups. RDS instances\nlimit what kinds of connections they accept via \"security groups.\" This is yet\nanother layer of AWS security hassle where you'll need to specify which hosts\nare permitted to access your DB, by type of connection, port range, etc.\n\nIf you'd like to get this over with as soon as possible, this configuration will\nopen you up to the entire world:\n\nHappy Trails\nNext time we're going to sink deeper into this rabbit hole by exploring the\nwonderful world of serverless functions. Setting up AWS Lambda will allow us to\nconfigure endpoints which will allow us to read and write data to our brand new\ntable in the sky.\n\nWe'll still need to get into API Gateway after that, but let's not think about\nthat just yet. Let's not address the absurd amount of time and effort we're\nabout to spend just to make a god damn widget that shows Github commits.","html":"<p>Last time we became familiar with the handiwork of setting up MySQL locally, navigating databases via command line, and exposing your database to external access. While badass, it has come to my attention that most people don't bother doing things this way. Unless you're getting deep into some heavy architecture, most people opt to use cloud services such as AWS to set up databases which are intended to be interacted with by multiple services.</p><p>A perfect example is one we ran into over the weekend while working on this very blog. We're running a Ghost instance, which is respectably complex production-ready app. For a bunch of guys just looking to make some stupid blog widgets, it became obvious that reverse engineering the undocumented inner workings of an open source node app was a rabbit hole of complexity.</p><h2 id=\"hosting-on-aws\">Hosting on AWS</h2><p>In our case, AWS is useful for enforcing separation of concerns. Instead of building new logic into a live app, we can build that logic elsewhere in a way that's reusable across multiple apps.</p><p>The end goal here is simply to read/write to a database. That said, there's still a fair amount of complexity involved. We'll need to leverage the following AWS services:</p><ul><li>RDS (Relational Database Service): A cloud hosted database</li><li>API Gateway: An interface for building APIs</li><li>Lambda: The necessary serverless connector between RDS and Gateway</li><li>IAM: Amazon's god-awful user and policy manager</li></ul><p>For now, all we're going to worry about is RDS.</p><h2 id=\"data-is-the-new-kale-is-the-new-money-is-the-new-bitcoin-oil-gold-etfs\">Data is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs</h2><p>Head to the AWS console and create a new RDS instance. Once prompted, go with MySQL:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.13.png\" class=\"kg-image\" alt=\"Database Type\"><figcaption>As though some of these are real options. Please.</figcaption></figure><p>Stick with <strong>MySQL Production</strong> on the next screen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.22.png\" class=\"kg-image\" alt=\"Use case\"><figcaption>Do everything in production. Always</figcaption></figure><h3 id=\"configuration-settings\">Configuration Settings</h3><p>This is where we set our configurations. You'll notice immediately how convoluted AWS tends to be with their naming conventions. I personally hate how intentionally unintuitive all of AWS tends to be (what the hell is a <em>db.t2.medium</em>)? This sort of absurdity is just something we need to deal with forever. Amazon is technically outside the realm of enforceable Monopoly laws, and there's no reason to believe their reign of mediocre products and talking robots is ever going to end.</p><ul><li><strong>License</strong>: Select <em>general-public-license</em></li><li><strong>Version</strong>: Choose whichever, just don't do an old one</li><li><strong>Instance class</strong>: Most of these instances are huge and unnecessary. Go with something small: I would also advise looking at the pricing plan.</li><li><strong>Multi AZ</strong>: Create a replica.</li><li><strong>Storage type</strong>: General.</li><li><strong>Allocated storage</strong>: Feel free to allocate more for latency.</li><li><strong>Publicly Accessible</strong>: True.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.46.png\" class=\"kg-image\" alt=\"Configuration\"><figcaption>God I love configuring esoteric shit.</figcaption></figure><p>Once configuration is complete, it takes a good amount of time for the database to be created. While we wait, let's move on to creating to a user to access this. We can do this with IAM: another AWS product with an even more terrible interface.</p><h2 id=\"access\">Access</h2><p>Fair warning: user roles and permissions are the worst part of AWS. I could write an entire series on how deep this mess of a scheme goes, but quite honestly I still barely understand what I'm doing most of the time.</p><h3 id=\"creating-a-user\">Creating a User</h3><p>Create a new user that will access the database. Go to the <a href=\"https://console.aws.amazon.com/iam/home?region=us-east-1#/users\">Users panel</a> and create a user:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.37.20.png\" class=\"kg-image\" alt=\"Users\"></figure><h3 id=\"modifying-permission-policies\">Modifying permission policies</h3><p>Permissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS has some default policies that we can leverage for our purposes, so this should luckily be somewhat straightforward.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.39.15.png\" class=\"kg-image\" alt=\"Permissions\"></figure><p>Policies can also be combined so that users have multiple policies across AWS products.</p><h2 id=\"native-client\">Native Client</h2><p>Once your DB pops up in AWS, we're going to need to get you a GUI to modify your DB. Don't even try to be a hotshot by setting up all your tables via command line. It sucks, it's slower, and nobody is impressed. Don't bother downloading the AWS CLI either. Do not pass GO. Do not collect 500 dollars.</p><p>In case you need to install MySQL locally, an OSX download can be found <a href=\"https://dev.mysql.com/downloads/mysql/5.5.html#macosx-dmg\">here</a>. Come to think of it, that step was probably unnecessary. I'm not sure why I did that.</p><p>I settled on <a href=\"https://www.sequelpro.com/\">Sequel Pro</a> as a client. It's good enough, and their logo looks like pancakes. That's really the only metric I needed tbh.</p><p>To connect to your database, you'll need to retrieve the endpoint and port number from your RDS console:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.44.png\" class=\"kg-image\" alt=\"Endpoint\"></figure><p>Connect to that ish:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.28.png\" class=\"kg-image\" alt=\"Sequel Pro\"></figure><p>Hopefully everything went well! If not, I'm sure the problem will be a quick and easy fix. Surely it won't involve mindlessly swapping permissions for an entire day. You defintely won't somehow end up corrupting your .bash_profile, making Python invisible to your OS, and effectively destroying your computer. Only an idiot would do something like that. Yesterday evening.</p><p>Go ahead and get accustomed to the UI of Sequel Pro - it's pretty straightforward, and ten thousand million times less effort than creating tables via terminal. Create columns under the \"structure\" tab - the terminology should immediately seem familiar if you've been following the series until this point.</p><h2 id=\"protip-issues-with-security-groups\">Protip: Issues with Security Groups</h2><p>If you're running into an issue connecting to your DB externally, I happened to run in to a nice little issue the other day with security groups. RDS instances limit what kinds of connections they accept via \"security groups.\" This is yet another layer of AWS security hassle where you'll need to specify which hosts are permitted to access your DB, by type of connection, port range, etc.</p><p>If you'd like to get this over with as soon as possible, this configuration will open you up to the entire world:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-07.15.26.png\" class=\"kg-image\" alt=\"Security Groups\"></figure><h2 id=\"happy-trails\">Happy Trails</h2><p>Next time we're going to sink deeper into this rabbit hole by exploring the wonderful world of serverless functions. Setting up AWS Lambda will allow us to configure endpoints which will allow us to read and write data to our brand new table in the sky.</p><p>We'll still need to get into API Gateway after that, but let's not think about that just yet. Let's not address the absurd amount of time and effort we're about to spend just to make a god damn widget that shows Github commits.</p>","url":"https://hackersandslackers.com/setting-up-mysql-on-aws/","uuid":"bf9a9804-206a-4556-ade4-b7cbdd896ecc","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5ae6899aed09bd1cb7110e51"}}]}},"pageContext":{"slug":"aws","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}