{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673709","title":"Using Random Forests for Feature Selection with Categorical Features","slug":"random-forests-for-feature-selection","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","custom_excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","created_at_pretty":"24 September, 2018","published_at_pretty":"24 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-23T20:23:32.000-04:00","published_at":"2018-09-24T07:30:00.000-04:00","updated_at":"2019-02-19T03:48:04.000-05:00","meta_title":"Using Random Forests for Feature Selection | Hackers and Slackers","meta_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","og_title":"Using Random Forests for Feature Selection","twitter_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","twitter_title":"Using Random Forests for Feature Selection","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Notebook here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb]\n.  Helper functions here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py]\n.\n\nOne of the best features of Random Forests is that it has built-in Feature\nSelection.  Explicability is one of the things we often lose when we go from\ntraditional statistics to Machine Learning, but Random Forests lets us actually\nget some insight into our dataset instead of just having to treat our model as a\nblack box.\n\nOne problem, though - it doesn't work that well for categorical features.  Since\nyou'll generally have to One-Hot Encode a categorical feature (for instance,\nturn something with 7 categories into 7 variables that are a \"True/False\"),\nyou'll wind up with a bunch of small features.  This gets tough to read,\nespecially if you're dealing with a lot of categories.  It also makes that\nfeature look less important than it is - rather than appearing near the top,\nyou'll maybe have 17 weak-seeming features near the bottom - which gets worse if\nyou're filtering it so that you only see features above a certain threshold.\n\nSoo, here's some helper functions for adding up their importance and displaying\nthem as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my\nmy own One-Hot function, rather than using Scikit's builtin one.\n\nFirst, let's grab a dataset.  I'm using this\n[https://www.kaggle.com/c/avazu-ctr-prediction]  Kaggle dataset because it has a\ngood number of categorical predictors.  I'm also only using the first 500 rows\nbecause the whole dataset is like ~ 1 GB.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)\n\nLet's just use the Categorical variables as our predictors because that's what\nwe're focusing on, but in actual usage you don't have to make them the same.\n\npredVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)\n\nLet's use log_loss  as our metric, because I saw this\n[https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512] \n blog post that used it for this dataset.\n\nfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 \n\nVariable\n Importance\n 0\n device_model\n 0.843122\n 1\n site_category\n 0.083392\n 2\n app_category\n 0.037216\n 3\n device_type\n 0.025057\n 4\n device_conn_type\n 0.011213","html":"<p><em>Notebook <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb\">here</a>.  Helper functions <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py\">here</a>.</em></p><p>One of the best features of Random Forests is that it has built-in Feature Selection.  Explicability is one of the things we often lose when we go from traditional statistics to Machine Learning, but Random Forests lets us actually get some insight into our dataset instead of just having to treat our model as a black box.</p><p>One problem, though - it doesn't work that well for categorical features.  Since you'll generally have to One-Hot Encode a categorical feature (for instance, turn something with 7 categories into 7 variables that are a \"True/False\"), you'll wind up with a bunch of small features.  This gets tough to read, especially if you're dealing with a lot of categories.  It also makes that feature look less important than it is - rather than appearing near the top, you'll maybe have 17 weak-seeming features near the bottom - which gets worse if you're filtering it so that you only see features above a certain threshold.</p><p>Soo, here's some helper functions for adding up their importance and displaying them as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my my own One-Hot function, rather than using Scikit's builtin one.</p><p>First, let's grab a dataset.  I'm using <a href=\"https://www.kaggle.com/c/avazu-ctr-prediction\">this</a> Kaggle dataset because it has a good number of categorical predictors.  I'm also only using the first 500 rows because the whole dataset is like ~ 1 GB.</p><pre><code>import pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)</code></pre><p>Let's just use the Categorical variables as our predictors because that's what we're focusing on, but in actual usage you don't have to make them the same.</p><pre><code>predVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)</code></pre><p>Let's use <code>log_loss</code> as our metric, because I saw <a href=\"https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\">this</a> blog post that used it for this dataset.</p><pre><code>from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 </code></pre><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>device_model</td>\n      <td>0.843122</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>site_category</td>\n      <td>0.083392</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>app_category</td>\n      <td>0.037216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>device_type</td>\n      <td>0.025057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>device_conn_type</td>\n      <td>0.011213</td>\n    </tr>\n  </tbody>\n</table>","url":"https://hackersandslackers.com/random-forests-for-feature-selection/","uuid":"26ebccb3-ab41-44cf-8d57-bf995100b088","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ba82e84a1cf0b13cf2e9886"}},"pageContext":{"slug":"random-forests-for-feature-selection"}}