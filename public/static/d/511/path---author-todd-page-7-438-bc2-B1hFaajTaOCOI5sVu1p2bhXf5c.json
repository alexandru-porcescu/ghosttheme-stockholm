{"data":{"ghostAuthor":{"slug":"todd","name":"Todd Birchard","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","cover_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/fox_o_o.jpg","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","location":"New York City","website":"https://toddbirchard.com","twitter":"@ToddRBirchard","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867364b","title":"Python's Requests Library: Bring Your Scripts to the Web","slug":"pythons-requests-library","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/snek@2x.jpg","excerpt":"Get familiar with what might become your most used Python library.","custom_excerpt":"Get familiar with what might become your most used Python library.","created_at_pretty":"01 May, 2018","published_at_pretty":"02 May, 2018","updated_at_pretty":"25 November, 2018","created_at":"2018-05-01T18:38:59.000-04:00","published_at":"2018-05-01T20:10:32.000-04:00","updated_at":"2018-11-25T09:55:28.000-05:00","meta_title":"Python's Requests Library | Hackers and Slackers","meta_description":"Get familiar with what might become your most used Python library","og_description":"APIs like a snake","og_image":"https://hackersandslackers.com/content/images/2018/05/snek@2x.jpg","og_title":"Python's Requests Library","twitter_description":"APIs like a snake","twitter_image":"https://hackersandslackers.com/content/images/2018/05/snek@2x.jpg","twitter_title":"Python's Requests Library","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"Atlassian","slug":"atlassian","description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","feature_image":null,"meta_description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","meta_title":"Atlassian Development for JIRA and Confluence. | Hackers and Slackers","visibility":"public"}],"plaintext":"Last episode [https://hackersandslackers.com/making-ajax-calls-with-jquery/]  we\ncovered every programming noob's favorite 'A-ha' moment: making GET requests\nusing AJAX. Stepping stones such as these can serve as great turning points in a\ncareer, but they also expose how little we still know. For instance, when we\nintegrated the functional logic of APIs on the client side, we actually broke a\ncardinal rule: storing and passing private keys on the client side like an\nidiot. Does that make everything we learned useless? Not entirely, but kinda\nyeah.\n\nToday we'll do the equivalent in Python by using the requests  library. Requests\nis successor to Urllib, both of which are simple tools to retrieve or modify\ninformation on the web, most commonly in the case of APIs.\n\nWe'll be using JIRA's API as an example of how to format GET and POST requests.\nJIRA's API in particular is an excellent example of a powerful and useful API.\nThere's a ton we can do, thus a perfect demonstration of how much power one\nlibrary can give you.\n\nBatteries Not Included\nEven I sometimes forget that requests is not a built-in Python library. Make\nsure requests  is installed in your environment via pip install requests.\n\nCreate a file in your directory called creds.py  to store your credentials. Make\nsure to add that file to your .gitignore  if you plan on committing anything any\ntime soon.\n\n# creds.py\nusername = fake.user\npassword = securepassword123\n\n\nThe only libraries we need to import are requests  and json. Make sure you\nimport your credentials from the file you created earlier.\n\nimport requests\nimport json\nfrom creds import username, password\n\n\nGET Requests\nAs long as you have a URL, you can make a GET request. The requests library will\nreturn the content of any page it hits; if you make a request to an HTML page,\nyour response will be that page's HTML source.\n\nWhen we know what sort of data we're expecting to receive back, we can specify\nthe expected content type  by passing the headers  argument, and specifying the \nContent-Type. Authentication is handled via passing arguments as well,\nspecifically the auth  argument. Take a look at what you can pass in a GET\nrequest:\n\nCommon GET Arguments\n * url:  The URL we will either retrieve or pass the information along to.\n * parameters  (optional):  Depending on the API, some URLs can accept a\n   dictionary of variables to be passed along with the URL. These are called\n   query strings; you notice these all the time whenever you come across a URL\n   that looks like nonsense... that nonsense is information!\n * headers  (optional): A collection of metadata sent along with the request.\n   Our browsers send HTTP headers every time we visit a site, but the scope of\n   what a header value might cover ranges from tokens to content types.\n * auth  (optional):  Method for logging in if needed. Basic/Digest/Custom HTTP\n   Auth.\n\nLet’s GET Some\nWe're going to make a relatively simple request to pull open tickets from a JIRA\nproject called EXM.\n\nThis request will:\n\n * Accept our destination's base URL\n * Append 'search/' (the endpoint for searching issues)\n * Pass two parameters:  A query to return issues A flag to show the issue\n   history\n * Authenticate with our username/password\n * Print the result\n\nimport requests\nimport json\nfrom creds import username\nfrom creds import password\n\nbase_url = 'https://examplejira.com/rest/api/2/'\nheaders = {'Content-Type': 'application/json'}\nparams = {\n    'jql': 'project = EXM AND resolution is not EMPTY',\n    'expand': 'changelog',\n}\n\nreq = requests.get(base_url + 'search/', headers=headers, params=params, auth=(username, password))\n\nprint(req.content)\n\n\nNotice that setting a variable equal to the request will equal the result of\nthat request. Printing r  alone would return a numerical status code (200, 404,\netc). The response that comes back from request such as r  are actually complex\nobjects — printing r.json()  will display the contents of the response as a JSON\nobject. Alternatively, r.text  returns the raw response as a string.\n\nIf your response comes back with an error, remember that you can always debug\nyour requests via Postman [https://www.getpostman.com/].\n\nIf all went well with our request, r.json()  should return something similar to\nthe following:\n\n{  \n   \"expand\":\"schema,names\",\n   \"startAt\":0,\n   \"maxResults\":50,\n   \"total\":63,\n   \"issues\":[  \n      {  \n         \"expand\":\"operations,versionedRepresentations,editmeta,changelog,renderedFields\",\n         \"id\":\"10558\",\n         \"self\":\"https://hackersandslackers.atlassian.net/rest/api/2/issue/10558\",\n         \"key\":\"HSB-63\",\n         \"fields\":{  \n            \"issuetype\":{  \n               \"self\":\"https://hackersandslackers.atlassian.net/rest/api/2/issuetype/10007\",\n               \"id\":\"10007\",\n               \"description\":\"Non-development related content task\",\n               \"iconUrl\":\"https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&avatarId=10306&avatarType=issuetype\",\n               \"name\":\"Content\",\n               \"subtask\":false,\n               \"avatarId\":10306\n            },\n         }\n    ]\n}\n\n\nThe entirety of the request is probably much longer (depending on how many\nissues you have). Notice how JIRA will only return a maximum of 50 results\nunless otherwise specified (this is one of the parameters they accept). Feel\nfree to check out JIRA's API documentation to see what else you can do, but be\nwarned: their docs kind of suck.\n\nRetrieving information is cool, but modifying it is even better. Here's a use\ncase which might be immediately useful: creating a user.\n\nPOST Requests\nIn addition to the arguments GET requests can receive, POST requests can also\naccept arguments like as data.  This is where we tell the API the specifics of\nwhat we're trying to do.\n\nCommon POST Arguments\n * url: Endpoint URL.\n * params  (optional): Dictionary of variables to be passed as parameters of a\n   query string.\n * body  (optional): A JSON or  ML object sent in the body of the Request.\n * headers  (optional):  Dictionary of HTTP Headers to send with the Request.\n * auth  (optional):  Auth to enable Basic/Digest/Custom HTTP Auth.\n\nLet There be Users\nThe main difference between this request and the last will be what we pass via\nthe data  argument. For example's sake we'll be creating a user named bro  with\nthe appropriate broiest details.\n\nTake special note of json.dumps(userdata). If an endpoint is expecting JSON (it\nprobably is) we need to explicitly convert our dictionary of values to JSON\nbefore making this request.\n\nimport requests\nimport json\nfrom creds import username\nfrom creds import password\n\nbase_url = \"https://examplejira.com/rest/api/2/\"\nheaders = {'Content-Type': 'application/json'}\nuserdata = {\n  'username': 'bro',\n  'name': 'Bro',\n  'password': '32456456',\n  'email': 'bro@broiest.com',\n  \"notification\" : \"true\"\n}\n\nreq = requests.post(base_url + 'user/', data=json.dumps(userdata), headers=headers, auth=(username, password))\n\nprint(req.content)\n\n\n\nYou just created a user. That's basically like giving birth to a child.\nCongratulations.\n\nAdvanced POST Requests\nAs fun as it is to create bro users in JIRA instances, one-off usage of APIs\nlike this isn't really useful. We haven't done anything that we couldn't have\njust done ourselves via the UI.\n\nTo spice things up, here's a very real use case: importing a list of users via a\nCSV. As we speak, people in corporations around the world are manually adding\nthousands of users by hand to internal SaaS products. Don't be that person.\n\nThis request will do the following:\n\n * Use pandas  to open users.csv   (presumably this CSV should have columns for\n   name, email, etc)\n * Generate a random password using secrets\n * Use the CSV to create accounts with each user's information\n * Output the result to users_created.csv\n * \n\n# JIRA User Import\n\nimport pandas as pd\nimport requests\nimport secrets\nimport json\n\n# store credentials\nfrom creds import username\nfrom creds import password\n\n# dataframe from csv\nuser_df = pd.read_csv('users.csv')\n\n# store results of import\nrows_list = []\n\nheaders = {'Content-Type': 'application/json'}\nbase_url = \"https://examplejira.com/rest/api/2/\"\n\n# generate 20-character password\ndef generate_password():\n    alphabet = string.ascii_letters + string.digits\n    password = ''.join(secrets.choice(alphabet) for i in range(20))\n    return password\n\n# iterate and create users\nfor index, row in user_df.iterrows():\n    userdata = {\n        \"name\": row['email'].split('@')[0],\n        \"password\": generate_password(),\n        \"emailAddress\": row['email'],\n        \"displayName\": row['name'],\n        \"notification\" : \"true\"\n    }\n    req = requests.post(base_url + 'user/', data=json.dumps(userdata), headers=headers, auth=(jirauser, password))\n    rows_list.append(userdata) # adds row to array to be tracked\n    # create & export results to a csv\n    users_imported_df = pd.DataFrame(rows_list)\n    users_imported_df.to_csv('users_created.csv')\n\n\n\nIf this worked for you, take a moment to put something in perspective: you just\nautomated somebody's entire 9-5 job in a few minutes.\n\nAlso feel free to reflect on our purpose as a species. If automating this was so\nstraightforward, why do so many of us choose not to automate more tasks? Is our\nentire economy a hoax created to grant the masses an illusion of free will? Are\nwe running around in circles trying to solve problems we create ourselves, to\npay the bills which come with being employed? Finally: if robots are clearly\nthis superior, is there a purpose for the human race at all?\n\nNow you're asking the real questions. Hail Megatron.","html":"<p><a href=\"https://hackersandslackers.com/making-ajax-calls-with-jquery/\">Last episode</a> we covered every programming noob's favorite 'A-ha' moment: making GET requests using AJAX. Stepping stones such as these can serve as great turning points in a career, but they also expose how little we still know. For instance, when we integrated the functional logic of APIs on the client side, we actually broke a cardinal rule: storing and passing private keys on the client side like an idiot. Does that make everything we learned useless? Not entirely, but kinda yeah.</p><p>Today we'll do the equivalent in Python by using the <em>requests</em> library. Requests is successor to Urllib, both of which are simple tools to retrieve or modify information on the web, most commonly in the case of APIs.</p><p>We'll be using JIRA's API as an example of how to format GET and POST requests. JIRA's API in particular is an excellent example of a powerful and useful API. There's a ton we can do, thus a perfect demonstration of how much power one library can give you.</p><h2 id=\"batteries-not-included\">Batteries Not Included</h2><p>Even I sometimes forget that requests is not a built-in Python library. Make sure <strong>requests</strong> is installed in your environment via <code>pip install requests</code>.</p><p>Create a file in your directory called <code>creds.py</code> to store your credentials. Make sure to add that file to your <code>.gitignore</code> if you plan on committing anything any time soon.</p><pre><code class=\"language-python\"># creds.py\nusername = fake.user\npassword = securepassword123\n</code></pre>\n<p>The only libraries we need to import are <code>requests</code> and <code>json</code>. Make sure you import your credentials from the file you created earlier.</p><pre><code class=\"language-python\">import requests\nimport json\nfrom creds import username, password\n</code></pre>\n<h2 id=\"get-requests\">GET Requests</h2><p>As long as you have a URL, you can make a GET request. The requests library will return the content of any page it hits; if you make a request to an HTML page, your response will be that page's HTML source.</p><p>When we know what sort of data we're expecting to receive back, we can specify the expected <em>content type</em> by passing the <em>headers</em> argument, and specifying the <em>Content-Type</em>. Authentication is handled via passing arguments as well, specifically the <em>auth</em> argument. Take a look at what you can pass in a GET request:</p><h3 id=\"common-get-arguments\">Common GET Arguments</h3><ul><li><strong>url:</strong> The URL we will either retrieve or pass the information along to.</li><li><strong>parameters</strong> (optional):  Depending on the API, some URLs can accept a dictionary of variables to be passed along with the URL. These are called query strings; you notice these all the time whenever you come across a URL that looks like nonsense... that nonsense is information!</li><li><strong>headers</strong> (optional): A collection of metadata sent along with the request. Our browsers send HTTP headers every time we visit a site, but the scope of what a header value might cover ranges from tokens to content types.</li><li><strong>auth</strong> (optional):  Method for logging in if needed. Basic/Digest/Custom HTTP Auth.</li></ul><h3 id=\"let-s-get-some\">Let’s GET Some</h3><p>We're going to make a relatively simple request to pull open tickets from a JIRA project called <em>EXM.</em></p><p>This request will:</p><ul><li>Accept our destination's base URL</li><li>Append 'search/' (the endpoint for searching issues)</li><li>Pass two parameters:  A query to return issues A flag to show the issue history</li><li>Authenticate with our username/password</li><li>Print the result</li></ul><pre><code class=\"language-python\">import requests\nimport json\nfrom creds import username\nfrom creds import password\n\nbase_url = 'https://examplejira.com/rest/api/2/'\nheaders = {'Content-Type': 'application/json'}\nparams = {\n    'jql': 'project = EXM AND resolution is not EMPTY',\n    'expand': 'changelog',\n}\n\nreq = requests.get(base_url + 'search/', headers=headers, params=params, auth=(username, password))\n\nprint(req.content)\n</code></pre>\n<p>Notice that setting a variable equal to the request will equal the result of that request. Printing <code>r</code> alone would return a numerical status code (200, 404, etc). The response that comes back from request such as <code>r</code> are actually complex objects — printing <code>r.json()</code> will display the contents of the response as a JSON object. Alternatively, <code>r.text</code> returns the raw response as a string.</p><p>If your response comes back with an error, remember that you can always debug your requests via <a href=\"https://www.getpostman.com/\" rel=\"noopener\">Postman</a>.</p><p>If all went well with our request, <code>r.json()</code> should return something similar to the following:</p><pre><code class=\"language-json\">{  \n   &quot;expand&quot;:&quot;schema,names&quot;,\n   &quot;startAt&quot;:0,\n   &quot;maxResults&quot;:50,\n   &quot;total&quot;:63,\n   &quot;issues&quot;:[  \n      {  \n         &quot;expand&quot;:&quot;operations,versionedRepresentations,editmeta,changelog,renderedFields&quot;,\n         &quot;id&quot;:&quot;10558&quot;,\n         &quot;self&quot;:&quot;https://hackersandslackers.atlassian.net/rest/api/2/issue/10558&quot;,\n         &quot;key&quot;:&quot;HSB-63&quot;,\n         &quot;fields&quot;:{  \n            &quot;issuetype&quot;:{  \n               &quot;self&quot;:&quot;https://hackersandslackers.atlassian.net/rest/api/2/issuetype/10007&quot;,\n               &quot;id&quot;:&quot;10007&quot;,\n               &quot;description&quot;:&quot;Non-development related content task&quot;,\n               &quot;iconUrl&quot;:&quot;https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&amp;avatarId=10306&amp;avatarType=issuetype&quot;,\n               &quot;name&quot;:&quot;Content&quot;,\n               &quot;subtask&quot;:false,\n               &quot;avatarId&quot;:10306\n            },\n         }\n    ]\n}\n</code></pre>\n<p>The entirety of the request is probably much longer (depending on how many issues you have). Notice how JIRA will only return a maximum of 50 results unless otherwise specified (this is one of the parameters they accept). Feel free to check out JIRA's API documentation to see what else you can do, but be warned: their docs kind of suck.</p><p>Retrieving information is cool, but modifying it is even better. Here's a use case which might be immediately useful: creating a user.</p><h2 id=\"post-requests\">POST Requests</h2><p>In addition to the arguments GET requests can receive, POST requests can also accept arguments like as <em>data.</em> This is where we tell the API the specifics of what we're trying to do.</p><h3 id=\"common-post-arguments\">Common POST Arguments</h3><ul><li><strong>url</strong>: Endpoint URL.</li><li><strong>params</strong> (optional): Dictionary of variables to be passed as parameters of a query string.</li><li><strong>body</strong> (optional): A JSON or  ML object sent in the body of the Request.</li><li><strong>headers</strong> (optional):  Dictionary of HTTP Headers to send with the Request.</li><li><strong>auth</strong> (optional):  Auth to enable Basic/Digest/Custom HTTP Auth.</li></ul><h3 id=\"let-there-be-users\">Let There be Users</h3><p>The main difference between this request and the last will be what we pass via the <em>data</em> argument. For example's sake we'll be creating a user named <em>bro</em> with the appropriate broiest details.</p><p>Take special note of <code>json.dumps(userdata)</code>. If an endpoint is expecting JSON (it probably is) we need to explicitly convert our dictionary of values to JSON before making this request.</p><pre><code class=\"language-python\">import requests\nimport json\nfrom creds import username\nfrom creds import password\n\nbase_url = &quot;https://examplejira.com/rest/api/2/&quot;\nheaders = {'Content-Type': 'application/json'}\nuserdata = {\n  'username': 'bro',\n  'name': 'Bro',\n  'password': '32456456',\n  'email': 'bro@broiest.com',\n  &quot;notification&quot; : &quot;true&quot;\n}\n\nreq = requests.post(base_url + 'user/', data=json.dumps(userdata), headers=headers, auth=(username, password))\n\nprint(req.content)\n\n</code></pre>\n<p>You just created a user. That's basically like giving birth to a child. Congratulations.</p><h2 id=\"advanced-post-requests\">Advanced POST Requests</h2><p>As fun as it is to create bro users in JIRA instances, one-off usage of APIs like this isn't really useful. We haven't done anything that we couldn't have just done ourselves via the UI.</p><p>To spice things up, here's a very real use case: importing a list of users via a CSV. As we speak, people in corporations around the world are manually adding thousands of users by hand to internal SaaS products. Don't be that person.</p><p>This request will do the following:</p><ul><li>Use <strong>pandas</strong> to open <em>users.csv</em>  (presumably this CSV should have columns for name, email, etc)  </li><li>Generate a random password using <strong>secrets</strong></li><li>Use the CSV to create accounts with each user's information</li><li>Output the result to <em>users_created.csv</em></li><li></li></ul><pre><code class=\"language-python\"># JIRA User Import\n\nimport pandas as pd\nimport requests\nimport secrets\nimport json\n\n# store credentials\nfrom creds import username\nfrom creds import password\n\n# dataframe from csv\nuser_df = pd.read_csv('users.csv')\n\n# store results of import\nrows_list = []\n\nheaders = {'Content-Type': 'application/json'}\nbase_url = &quot;https://examplejira.com/rest/api/2/&quot;\n\n# generate 20-character password\ndef generate_password():\n    alphabet = string.ascii_letters + string.digits\n    password = ''.join(secrets.choice(alphabet) for i in range(20))\n    return password\n\n# iterate and create users\nfor index, row in user_df.iterrows():\n    userdata = {\n        &quot;name&quot;: row['email'].split('@')[0],\n        &quot;password&quot;: generate_password(),\n        &quot;emailAddress&quot;: row['email'],\n        &quot;displayName&quot;: row['name'],\n        &quot;notification&quot; : &quot;true&quot;\n    }\n    req = requests.post(base_url + 'user/', data=json.dumps(userdata), headers=headers, auth=(jirauser, password))\n    rows_list.append(userdata) # adds row to array to be tracked\n    # create &amp; export results to a csv\n    users_imported_df = pd.DataFrame(rows_list)\n    users_imported_df.to_csv('users_created.csv')\n\n</code></pre>\n<p>If this worked for you, take a moment to put something in perspective: you just automated somebody's entire 9-5 job in a few minutes.</p><p>Also feel free to reflect on our purpose as a species. If automating this was so straightforward, why do so many of us choose not to automate more tasks? Is our entire economy a hoax created to grant the masses an illusion of free will? Are we running around in circles trying to solve problems we create ourselves, to pay the bills which come with being employed? Finally: if robots are clearly this superior, is there a purpose for the human race at all?</p><p>Now you're asking the real questions. Hail Megatron.</p>","url":"https://hackersandslackers.com/pythons-requests-library/","uuid":"9ee37ee8-83d1-452a-acb4-b90b96cf6725","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5ae8ec83ed09bd1cb7110e65"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673648","title":"MySQL on the Cloud with AWS RDS","slug":"setting-up-mysql-on-aws","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","excerpt":"Spinning up a standalone MySQL Database with Amazon.","custom_excerpt":"Spinning up a standalone MySQL Database with Amazon.","created_at_pretty":"30 April, 2018","published_at_pretty":"01 May, 2018","updated_at_pretty":"27 November, 2018","created_at":"2018-04-29T23:12:26.000-04:00","published_at":"2018-04-30T20:14:57.000-04:00","updated_at":"2018-11-27T03:54:21.000-05:00","meta_title":"Setting up MySQL on AWS | Hackers and Slackers","meta_description":"Spinning up a standalone MySQL Database with Amazon","og_description":"Spinning up a standalone MySQL Database with Amazon","og_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","og_title":"Setting up MySQL on AWS","twitter_description":"Spinning up a standalone MySQL Database with Amazon","twitter_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","twitter_title":"Setting up MySQL on AWS","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"Last time we became familiar with the handiwork of setting up MySQL locally,\nnavigating databases via command line, and exposing your database to external\naccess. While badass, it has come to my attention that most people don't bother\ndoing things this way. Unless you're getting deep into some heavy architecture,\nmost people opt to use cloud services such as AWS to set up databases which are\nintended to be interacted with by multiple services.\n\nA perfect example is one we ran into over the weekend while working on this very\nblog. We're running a Ghost instance, which is respectably complex\nproduction-ready app. For a bunch of guys just looking to make some stupid blog\nwidgets, it became obvious that reverse engineering the undocumented inner\nworkings of an open source node app was a rabbit hole of complexity.\n\nHosting on AWS\nIn our case, AWS is useful for enforcing separation of concerns. Instead of\nbuilding new logic into a live app, we can build that logic elsewhere in a way\nthat's reusable across multiple apps.\n\nThe end goal here is simply to read/write to a database. That said, there's\nstill a fair amount of complexity involved. We'll need to leverage the following\nAWS services:\n\n * RDS (Relational Database Service): A cloud hosted database\n * API Gateway: An interface for building APIs\n * Lambda: The necessary serverless connector between RDS and Gateway\n * IAM: Amazon's god-awful user and policy manager\n\nFor now, all we're going to worry about is RDS.\n\nData is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs\nHead to the AWS console and create a new RDS instance. Once prompted, go with\nMySQL:\n\nAs though some of these are real options. Please.Stick with MySQL Production  on the next screen.\n\nDo everything in production. AlwaysConfiguration Settings\nThis is where we set our configurations. You'll notice immediately how\nconvoluted AWS tends to be with their naming conventions. I personally hate how\nintentionally unintuitive all of AWS tends to be (what the hell is a \ndb.t2.medium)? This sort of absurdity is just something we need to deal with\nforever. Amazon is technically outside the realm of enforceable Monopoly laws,\nand there's no reason to believe their reign of mediocre products and talking\nrobots is ever going to end.\n\n * License: Select general-public-license\n * Version: Choose whichever, just don't do an old one\n * Instance class: Most of these instances are huge and unnecessary. Go with\n   something small: I would also advise looking at the pricing plan.\n * Multi AZ: Create a replica.\n * Storage type: General.\n * Allocated storage: Feel free to allocate more for latency.\n * Publicly Accessible: True.\n\nGod I love configuring esoteric shit.Once configuration is complete, it takes a\ngood amount of time for the database to be created. While we wait, let's move on\nto creating to a user to access this. We can do this with IAM: another AWS\nproduct with an even more terrible interface.\n\nAccess\nFair warning: user roles and permissions are the worst part of AWS. I could\nwrite an entire series on how deep this mess of a scheme goes, but quite\nhonestly I still barely understand what I'm doing most of the time.\n\nCreating a User\nCreate a new user that will access the database. Go to the Users panel  and\ncreate a user:\n\nModifying permission policies\nPermissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS\nhas some default policies that we can leverage for our purposes, so this should\nluckily be somewhat straightforward.\n\nPolicies can also be combined so that users have multiple policies across AWS\nproducts.\n\nNative Client\nOnce your DB pops up in AWS, we're going to need to get you a GUI to modify your\nDB. Don't even try to be a hotshot by setting up all your tables via command\nline. It sucks, it's slower, and nobody is impressed. Don't bother downloading\nthe AWS CLI either. Do not pass GO. Do not collect 500 dollars.\n\nIn case you need to install MySQL locally, an OSX download can be found here.\nCome to think of it, that step was probably unnecessary. I'm not sure why I did\nthat.\n\nI settled on Sequel Pro [https://www.sequelpro.com/]  as a client. It's good\nenough, and their logo looks like pancakes. That's really the only metric I\nneeded tbh.\n\nTo connect to your database, you'll need to retrieve the endpoint and port\nnumber from your RDS console:\n\nConnect to that ish:\n\nHopefully everything went well! If not, I'm sure the problem will be a quick and\neasy fix. Surely it won't involve mindlessly swapping permissions for an entire\nday. You defintely won't somehow end up corrupting your .bash_profile, making\nPython invisible to your OS, and effectively destroying your computer. Only an\nidiot would do something like that. Yesterday evening.\n\nGo ahead and get accustomed to the UI of Sequel Pro - it's pretty\nstraightforward, and ten thousand million times less effort than creating tables\nvia terminal. Create columns under the \"structure\" tab - the terminology should\nimmediately seem familiar if you've been following the series until this point.\n\nProtip: Issues with Security Groups\nIf you're running into an issue connecting to your DB externally, I happened to\nrun in to a nice little issue the other day with security groups. RDS instances\nlimit what kinds of connections they accept via \"security groups.\" This is yet\nanother layer of AWS security hassle where you'll need to specify which hosts\nare permitted to access your DB, by type of connection, port range, etc.\n\nIf you'd like to get this over with as soon as possible, this configuration will\nopen you up to the entire world:\n\nHappy Trails\nNext time we're going to sink deeper into this rabbit hole by exploring the\nwonderful world of serverless functions. Setting up AWS Lambda will allow us to\nconfigure endpoints which will allow us to read and write data to our brand new\ntable in the sky.\n\nWe'll still need to get into API Gateway after that, but let's not think about\nthat just yet. Let's not address the absurd amount of time and effort we're\nabout to spend just to make a god damn widget that shows Github commits.","html":"<p>Last time we became familiar with the handiwork of setting up MySQL locally, navigating databases via command line, and exposing your database to external access. While badass, it has come to my attention that most people don't bother doing things this way. Unless you're getting deep into some heavy architecture, most people opt to use cloud services such as AWS to set up databases which are intended to be interacted with by multiple services.</p><p>A perfect example is one we ran into over the weekend while working on this very blog. We're running a Ghost instance, which is respectably complex production-ready app. For a bunch of guys just looking to make some stupid blog widgets, it became obvious that reverse engineering the undocumented inner workings of an open source node app was a rabbit hole of complexity.</p><h2 id=\"hosting-on-aws\">Hosting on AWS</h2><p>In our case, AWS is useful for enforcing separation of concerns. Instead of building new logic into a live app, we can build that logic elsewhere in a way that's reusable across multiple apps.</p><p>The end goal here is simply to read/write to a database. That said, there's still a fair amount of complexity involved. We'll need to leverage the following AWS services:</p><ul><li>RDS (Relational Database Service): A cloud hosted database</li><li>API Gateway: An interface for building APIs</li><li>Lambda: The necessary serverless connector between RDS and Gateway</li><li>IAM: Amazon's god-awful user and policy manager</li></ul><p>For now, all we're going to worry about is RDS.</p><h2 id=\"data-is-the-new-kale-is-the-new-money-is-the-new-bitcoin-oil-gold-etfs\">Data is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs</h2><p>Head to the AWS console and create a new RDS instance. Once prompted, go with MySQL:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.13.png\" class=\"kg-image\" alt=\"Database Type\"><figcaption>As though some of these are real options. Please.</figcaption></figure><p>Stick with <strong>MySQL Production</strong> on the next screen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.22.png\" class=\"kg-image\" alt=\"Use case\"><figcaption>Do everything in production. Always</figcaption></figure><h3 id=\"configuration-settings\">Configuration Settings</h3><p>This is where we set our configurations. You'll notice immediately how convoluted AWS tends to be with their naming conventions. I personally hate how intentionally unintuitive all of AWS tends to be (what the hell is a <em>db.t2.medium</em>)? This sort of absurdity is just something we need to deal with forever. Amazon is technically outside the realm of enforceable Monopoly laws, and there's no reason to believe their reign of mediocre products and talking robots is ever going to end.</p><ul><li><strong>License</strong>: Select <em>general-public-license</em></li><li><strong>Version</strong>: Choose whichever, just don't do an old one</li><li><strong>Instance class</strong>: Most of these instances are huge and unnecessary. Go with something small: I would also advise looking at the pricing plan.</li><li><strong>Multi AZ</strong>: Create a replica.</li><li><strong>Storage type</strong>: General.</li><li><strong>Allocated storage</strong>: Feel free to allocate more for latency.</li><li><strong>Publicly Accessible</strong>: True.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.46.png\" class=\"kg-image\" alt=\"Configuration\"><figcaption>God I love configuring esoteric shit.</figcaption></figure><p>Once configuration is complete, it takes a good amount of time for the database to be created. While we wait, let's move on to creating to a user to access this. We can do this with IAM: another AWS product with an even more terrible interface.</p><h2 id=\"access\">Access</h2><p>Fair warning: user roles and permissions are the worst part of AWS. I could write an entire series on how deep this mess of a scheme goes, but quite honestly I still barely understand what I'm doing most of the time.</p><h3 id=\"creating-a-user\">Creating a User</h3><p>Create a new user that will access the database. Go to the <a href=\"https://console.aws.amazon.com/iam/home?region=us-east-1#/users\">Users panel</a> and create a user:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.37.20.png\" class=\"kg-image\" alt=\"Users\"></figure><h3 id=\"modifying-permission-policies\">Modifying permission policies</h3><p>Permissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS has some default policies that we can leverage for our purposes, so this should luckily be somewhat straightforward.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.39.15.png\" class=\"kg-image\" alt=\"Permissions\"></figure><p>Policies can also be combined so that users have multiple policies across AWS products.</p><h2 id=\"native-client\">Native Client</h2><p>Once your DB pops up in AWS, we're going to need to get you a GUI to modify your DB. Don't even try to be a hotshot by setting up all your tables via command line. It sucks, it's slower, and nobody is impressed. Don't bother downloading the AWS CLI either. Do not pass GO. Do not collect 500 dollars.</p><p>In case you need to install MySQL locally, an OSX download can be found <a href=\"https://dev.mysql.com/downloads/mysql/5.5.html#macosx-dmg\">here</a>. Come to think of it, that step was probably unnecessary. I'm not sure why I did that.</p><p>I settled on <a href=\"https://www.sequelpro.com/\">Sequel Pro</a> as a client. It's good enough, and their logo looks like pancakes. That's really the only metric I needed tbh.</p><p>To connect to your database, you'll need to retrieve the endpoint and port number from your RDS console:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.44.png\" class=\"kg-image\" alt=\"Endpoint\"></figure><p>Connect to that ish:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.28.png\" class=\"kg-image\" alt=\"Sequel Pro\"></figure><p>Hopefully everything went well! If not, I'm sure the problem will be a quick and easy fix. Surely it won't involve mindlessly swapping permissions for an entire day. You defintely won't somehow end up corrupting your .bash_profile, making Python invisible to your OS, and effectively destroying your computer. Only an idiot would do something like that. Yesterday evening.</p><p>Go ahead and get accustomed to the UI of Sequel Pro - it's pretty straightforward, and ten thousand million times less effort than creating tables via terminal. Create columns under the \"structure\" tab - the terminology should immediately seem familiar if you've been following the series until this point.</p><h2 id=\"protip-issues-with-security-groups\">Protip: Issues with Security Groups</h2><p>If you're running into an issue connecting to your DB externally, I happened to run in to a nice little issue the other day with security groups. RDS instances limit what kinds of connections they accept via \"security groups.\" This is yet another layer of AWS security hassle where you'll need to specify which hosts are permitted to access your DB, by type of connection, port range, etc.</p><p>If you'd like to get this over with as soon as possible, this configuration will open you up to the entire world:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-07.15.26.png\" class=\"kg-image\" alt=\"Security Groups\"></figure><h2 id=\"happy-trails\">Happy Trails</h2><p>Next time we're going to sink deeper into this rabbit hole by exploring the wonderful world of serverless functions. Setting up AWS Lambda will allow us to configure endpoints which will allow us to read and write data to our brand new table in the sky.</p><p>We'll still need to get into API Gateway after that, but let's not think about that just yet. Let's not address the absurd amount of time and effort we're about to spend just to make a god damn widget that shows Github commits.</p>","url":"https://hackersandslackers.com/setting-up-mysql-on-aws/","uuid":"bf9a9804-206a-4556-ade4-b7cbdd896ecc","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5ae6899aed09bd1cb7110e51"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673644","title":"Make Your First API Calls with JQuery AJAX","slug":"making-ajax-calls-with-jquery","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-2.jpg","excerpt":"Beginner's guide to consuming endpoints via Frontend Javascript.","custom_excerpt":"Beginner's guide to consuming endpoints via Frontend Javascript.","created_at_pretty":"24 April, 2018","published_at_pretty":"25 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-23T20:20:21.000-04:00","published_at":"2018-04-25T01:00:00.000-04:00","updated_at":"2019-03-28T09:44:00.000-04:00","meta_title":"Make Your First API Calls with JQuery AJAX | Hackers and Slackers","meta_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","og_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","og_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-2.jpg","og_title":"Make Your First API Calls with JQuery AJAX","twitter_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-1.jpg","twitter_title":"Make Your First API Calls with JQuery AJAX","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"The information age is over: we have all the information now. All of it. We're\nin a unique place in human history where we've somehow managed to mine more data\nthan we know what to do with... and a lot of that data is easily accessible via\nAPIs.\n\nWe're going to get our feet wet with REST APIs today, thus allowing us to\ninteract with meaningful information. Making Ajax GET calls with JQuery  is\nperhaps as basic as it gets: there's a good chance we already know all this\nstuff, but even I constantly forget the exact syntax of everyday functions.\nChances are I'm going to come back to this post at some point just to copy and\npaste the snippets below. \n\nIntroduction to REST APIs\nIf you're new to executing AJAX requests, chances are you may be new to REST\nAPIs in general. This crash course is going to be fast and rough around the\nedges, so strap in.\n\nIn the most simple sense, a REST API endpoint is a URL. It just so happens that\nthis URL probably expects more from you than simply visiting it, and as a\nresult, should output something useful for you. API Endpoints will almost always\noutput either JSON or XML; these responses will give you information varying\nfrom error codes to the actual data you seek.\n\nREST APIs expect requests to come in one of the following forms:\n\n * GET: A request looking for read-only data. Some GET requests simply need to\n   be copy and pasted into a browser window to receive results, but usually we\n   need to either authenticate or specify what we're looking for.\n * POST: A write  request to the target resource. Expects that new information\n   will come as a result of this request.\n * PUT: Updates pre-existing data somewhere, likely in some database.\n * PATCH: Somewhat similar to PUT, and in my experience rarely used at all.\n * DELETE: Expects that information will be deleted as a result of the request\n\nIf this all seems like new information, I'd highly recommend downloading Postman\n[https://www.getpostman.com/]  to become familiar with how API calls are\nstructured.\n\nFor now, we'll focus on working with a simple GET endpoint.\n\nLearning By Doing\nIf you've been checking out Snkia's roundup posts\n[https://hackersandslackers.com/tag/roundup/]  lately, you may have noticed\nnifty link previews being generated. To demonstrate how to make API calls via a\nfrontend client with JQuery, we'll be walking through how to create link\npreviews using the LinkPreview API [https://www.linkpreview.net/]. This service\nserves as a good tutorial because:\n\n * It's an example of a simple GET endpoint\n * There's a quick and immediately useful end result\n * It's free\n\nTell me That Ain't Insecurr\nI want to stress here that we're doing this for the sake of learning; while this\nis probably the fastest  way to start working with an API, it is most definitely\n not secure.\n\nMaking calls with private keys stored and passed via the client side exposes\nyour key publicly. In a production environment, this is like shoving your\npassword in people's faces. People will most definitely want to steal and\nexploit your private key: if what you were doing didn't have any value, it\nwouldn't require a key in the first place.\n\nHopefully this has scared you enough to consider passing credentials in the\nfuture. That said, there's another solid reason we selected LinkPreview as\ntoday's example. LinkPreview offers domain whitelisting for requests, so even if\nsomebody did steal your key, they'd only be able to use it from your domain ;).\n\nMake sure you whitelist the domain you'll be working from.Fetch Me Daddy\nGo get started with an API key over at LinkPreview  if you're following along.\nI'm going to assume you already have JQuery  readily available from here\nforward.\n\nTo get started, we'll wait for our document to load, and set two critical\nvariables: the API URL, and our API key.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n});\n\n\nIf you're following along what we've done with Lynx Roundups, our next step is\nto get all the relevant  <a>  tags on a page, loop through them, and replace\nthem with their respective link previews.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n  \n  $( \".content a\" ).each(function( index, element ) {\n      console.log($( this ).text());\n  }\n});\n\n\nThe JQuery  .each  method creates a loop which iterates over every element\nmatching the provided selector. In our example, we only want <a>  tags in the\ncontent of our page; otherwise we would get all  links, like navigation links\nand so forth.\n\nNow it's time to bring in that $.ajax()  thing we've been going off about.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( \".content a\" ).each(function( index, element ) {\n\n    $.ajax({\n        url: api_url + \"?key=\" + key + \" &q=\" + $( this ).text(),\n        contentType: \"application/json\",\n        dataType: 'json',\n        success: function(result){\n            console.log(result);\n        }\n    })\n  });\n});\n\n\nThis is how Ajax request are structured: the contents of $.ajax()  is\nessentially an object taking values it will use to construct the request. The\nabove example is about as simple as it gets for making a barebones GET call.\nWe're looping through each <a>  tag and passing its contents (the url) to the\nAPI, and receiving an object in response.\n\nAjax requests can take way more parameters than the ones we just specified. I\nrecommend reading over the JQuery Ajax documentation\n[http://api.jquery.com/jquery.ajax/]  closely; not only for the sake of these\nrequests, but understanding the potential items we can specify will solidify an\nunderstanding for REST APIs in general.\n\nThe line contentType: \"application/json\"  specifies that the content coming back\nto us will be in JSON format - this is a very common header when dealing with\nREST APIs. \n\nWith any luck, your response should come back looking like:\n\n{\n    \"title\":\"Google\",\n    \"description\":\"Search webpages, images, videos and more.\",\n    \"image\":\"https//:www.google.com/images/logo.png\",\n    \"url\":\"https://www.google.com/\"\n}\n\n\nIf you'd like to use this in a meaningful way, feel free to do something like\nthe mess I've put together below:\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( \".content a\" ).each(function( index, element ) {\n    $.ajax({\n        url: api_url + \"?key=\" + key + \" &q=\" + $( this ).text(),\n        contentType: \"application/json\",\n        dataType: 'json',\n        success: function(result){\n            $( element ).after(\n            '<a href=\"' + result.url + '\"> \\n ' +\n              '<div class=\"link-preview\"> \\n ' +\n                '<div class=\"preview-image\" style=\"background-image:url(' + result.image + ');\"></div> \\n ' + \n                '<div style=\"width:70%;\" class=\"link-info\"> \\n ' +\n                  '<h4>' + result.title +'</h4> \\n ' +\n                  '<p>' + result.description +'</p> \\n ' +\n                '</div><br> \\n ' +\n                  '<a href=\"' + result.url + '\" class=\"url-info\"><i class=\"far fa-link\"></i>' + result.url + '</a> \\n ' +\n                '</div></a>');\n            $( element ).remove();\n        }\n    })\n  });\n});\n\n\nThat template should serve you well for most GET API calls you're going to make\nvia JQuery. Go wild and see what you can do to leverage APIs and expose some\npeople's personal data or whatever.\n\nSee how I just created HTML by stringing together a bunch of ugly strings in\nJavascript? Don't do that; there are countless better ways to handle this, they\njust so happen to be out of scope for this post.If we were to truly complete\nthis example, we'd want to refine our logic to ensure we're not receiving\nnonsense. There's no validation on what's coming back in these calls, so there's\nnothing in place to protect us in the case that a page doesn't comply with our\nformat.","html":"<p>The information age is over: we have all the information now. All of it. We're in a unique place in human history where we've somehow managed to mine more data than we know what to do with... and a lot of that data is easily accessible via APIs.</p><p>We're going to get our feet wet with REST APIs today, thus allowing us to interact with meaningful information. Making Ajax GET calls with <strong>JQuery</strong> is perhaps as basic as it gets: there's a good chance we already know all this stuff, but even I constantly forget the exact syntax of everyday functions. Chances are I'm going to come back to this post at some point just to copy and paste the snippets below. </p><h2 id=\"introduction-to-rest-apis\">Introduction to REST APIs</h2><p>If you're new to executing AJAX requests, chances are you may be new to REST APIs in general. This crash course is going to be fast and rough around the edges, so strap in.</p><p>In the most simple sense, a REST API endpoint is a URL. It just so happens that this URL probably expects more from you than simply visiting it, and as a result, should output something useful for you. API Endpoints will almost always output either JSON or XML; these responses will give you information varying from error codes to the actual data you seek.</p><p>REST APIs expect requests to come in one of the following forms:</p><ul><li><strong>GET</strong>: A request looking for read-only data. Some GET requests simply need to be copy and pasted into a browser window to receive results, but usually we need to either authenticate or specify what we're looking for.</li><li><strong>POST</strong>: A <em>write</em> request to the target resource. Expects that new information will come as a result of this request.</li><li><strong>PUT</strong>: Updates pre-existing data somewhere, likely in some database.</li><li><strong>PATCH</strong>: Somewhat similar to PUT, and in my experience rarely used at all.</li><li><strong>DELETE: </strong>Expects that information will be deleted as a result of the request</li></ul><p>If this all seems like new information, I'd highly recommend downloading <a href=\"https://www.getpostman.com/\">Postman</a> to become familiar with how API calls are structured.</p><p>For now, we'll focus on working with a simple GET endpoint.</p><h2 id=\"learning-by-doing\">Learning By Doing</h2><p>If you've been checking out <a href=\"https://hackersandslackers.com/tag/roundup/\">Snkia's roundup posts</a> lately, you may have noticed nifty link previews being generated. To demonstrate how to make API calls via a frontend client with <strong>JQuery</strong>, we'll be walking through how to create link previews using the <a href=\"https://www.linkpreview.net/\">LinkPreview API</a>. This service serves as a good tutorial because:</p><ul><li>It's an example of a simple GET endpoint</li><li>There's a quick and immediately useful end result</li><li>It's free</li></ul><h3 id=\"tell-me-that-ain-t-insecurr\">Tell me That Ain't Insecurr</h3><p>I want to stress here that we're doing this for the sake of learning; while this is probably the <em>fastest</em> way to start working with an API, it is most definitely <strong>not secure</strong>.</p><p>Making calls with private keys stored and passed via the client side exposes your key publicly. In a production environment, this is like shoving your password in people's faces. People will most definitely want to steal and exploit your private key: if what you were doing didn't have any value, it wouldn't require a key in the first place.</p><p>Hopefully this has scared you enough to consider passing credentials in the future. That said, there's another solid reason we selected LinkPreview as today's example. LinkPreview offers domain whitelisting for requests, so even if somebody did steal your key, they'd only be able to use it from your domain ;).</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2018-11-25-at-6.06.44-AM.png\" class=\"kg-image\"><figcaption>Make sure you whitelist the domain you'll be working from.</figcaption></figure><h2 id=\"fetch-me-daddy\">Fetch Me Daddy</h2><p>Go get started with an API key over at <strong>LinkPreview</strong> if you're following along. I'm going to assume you already have <strong>JQuery</strong> readily available from here forward.</p><p>To get started, we'll wait for our document to load, and set two critical variables: the API URL, and our API key.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n});\n</code></pre>\n<p>If you're following along what we've done with Lynx Roundups, our next step is to get all the <em>relevant</em> <code>&lt;a&gt;</code> tags on a page, loop through them, and replace them with their respective link previews.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n  \n  $( &quot;.content a&quot; ).each(function( index, element ) {\n      console.log($( this ).text());\n  }\n});\n</code></pre>\n<p>The <strong>JQuery</strong> <code>.each</code> method creates a loop which iterates over every element matching the provided selector. In our example, we only want <code>&lt;a&gt;</code> tags in the content of our page; otherwise we would get <em>all</em> links, like navigation links and so forth.</p><p>Now it's time to bring in that <code>$.ajax()</code> thing we've been going off about.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( &quot;.content a&quot; ).each(function( index, element ) {\n\n    $.ajax({\n        url: api_url + &quot;?key=&quot; + key + &quot; &amp;q=&quot; + $( this ).text(),\n        contentType: &quot;application/json&quot;,\n        dataType: 'json',\n        success: function(result){\n            console.log(result);\n        }\n    })\n  });\n});\n</code></pre>\n<p>This is how Ajax request are structured: the contents of <code>$.ajax()</code> is essentially an object taking values it will use to construct the request. The above example is about as simple as it gets for making a barebones GET call. We're looping through each <code>&lt;a&gt;</code> tag and passing its contents (the url) to the API, and receiving an object in response.</p><p><strong>Ajax </strong>requests can take way more parameters than the ones we just specified. I recommend reading over the <a href=\"http://api.jquery.com/jquery.ajax/\">JQuery Ajax documentation</a> closely; not only for the sake of these requests, but understanding the potential items we can specify will solidify an understanding for REST APIs in general.</p><p>The line <code>contentType: \"application/json\"</code> specifies that the content coming back to us will be in JSON format - this is a very common header when dealing with REST APIs. </p><p>With any luck, your response should come back looking like:</p><pre><code class=\"language-json\">{\n    &quot;title&quot;:&quot;Google&quot;,\n    &quot;description&quot;:&quot;Search webpages, images, videos and more.&quot;,\n    &quot;image&quot;:&quot;https//:www.google.com/images/logo.png&quot;,\n    &quot;url&quot;:&quot;https://www.google.com/&quot;\n}\n</code></pre>\n<p>If you'd like to use this in a meaningful way, feel free to do something like the mess I've put together below:</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( &quot;.content a&quot; ).each(function( index, element ) {\n    $.ajax({\n        url: api_url + &quot;?key=&quot; + key + &quot; &amp;q=&quot; + $( this ).text(),\n        contentType: &quot;application/json&quot;,\n        dataType: 'json',\n        success: function(result){\n            $( element ).after(\n            '&lt;a href=&quot;' + result.url + '&quot;&gt; \\n ' +\n              '&lt;div class=&quot;link-preview&quot;&gt; \\n ' +\n                '&lt;div class=&quot;preview-image&quot; style=&quot;background-image:url(' + result.image + ');&quot;&gt;&lt;/div&gt; \\n ' + \n                '&lt;div style=&quot;width:70%;&quot; class=&quot;link-info&quot;&gt; \\n ' +\n                  '&lt;h4&gt;' + result.title +'&lt;/h4&gt; \\n ' +\n                  '&lt;p&gt;' + result.description +'&lt;/p&gt; \\n ' +\n                '&lt;/div&gt;&lt;br&gt; \\n ' +\n                  '&lt;a href=&quot;' + result.url + '&quot; class=&quot;url-info&quot;&gt;&lt;i class=&quot;far fa-link&quot;&gt;&lt;/i&gt;' + result.url + '&lt;/a&gt; \\n ' +\n                '&lt;/div&gt;&lt;/a&gt;');\n            $( element ).remove();\n        }\n    })\n  });\n});\n</code></pre>\n<p>That template should serve you well for most GET API calls you're going to make via <strong>JQuery</strong>. Go wild and see what you can do to leverage APIs and expose some people's personal data or whatever.</p><div class=\"protip\">\n    See how I just created HTML by stringing together a bunch of ugly strings in Javascript? Don't do that; there are countless better ways to handle this, they just so happen to be out of scope for this post.\n</div><p>If we were to truly complete this example, we'd want to refine our logic to ensure we're not receiving nonsense. There's no validation on what's coming back in these calls, so there's nothing in place to protect us in the case that a page doesn't comply with our format.</p>","url":"https://hackersandslackers.com/making-ajax-calls-with-jquery/","uuid":"1fbf30e7-7ab7-48bb-8976-f100fdced4e0","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ade784572a629364c5364c7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673641","title":"Accessing Self-Hosted MySQL  Externally","slug":"accessing-mysql-from-external-domains","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","excerpt":"Connecting to MySQL instances hosted on a VPS.","custom_excerpt":"Connecting to MySQL instances hosted on a VPS.","created_at_pretty":"22 April, 2018","published_at_pretty":"22 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-22T16:27:48.000-04:00","published_at":"2018-04-22T17:20:18.000-04:00","updated_at":"2019-03-28T04:54:42.000-04:00","meta_title":"Accessing MySQL Externally | Hackers and Slackers","meta_description":"How to configure a remote instance of MySQL to accept external connections.","og_description":"How to configure a remote instance of MySQL to accept external connections.","og_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","og_title":"Accessing MySQL Externally","twitter_description":"How to configure a remote instance of MySQL to accept external connections.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","twitter_title":"Accessing MySQL Externally","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"In the previous post [https://hackersandslackers.com/set-up-mysql-database/], we\ngot familiar with the basics of creating and navigating MySQL databases. This\nleads us to the next most logical thing to ask: how can I use this in any\nmeaningful way?\n\nMySQL installations default to refusing connections outside of the local\nmachine's IP address, as we should expect. That said, relational databases\naren't usually being used by a single person on a single machine forever (but if\nyou do, we should hang out). It goes without saying that our MySQL instance\nshould be focusing on uptime and accessibility, or in other terms, far away from\nour destructive personalities.\n\nI adore maintaining databases in the command line as much as the next\nself-hating masochist, but we'll need to accomplish work at some point. That\nmeans the remote database we just set up needs to be open-minded enough to allow\na connection from, say, the IP address of our personal local machine, which\nhappens to have a sexy GUI installed for this very purpose.\n\nMaking these kinds of configuration changes to any service or web server is\nalways a bit of fun. You think your day might suck until you cone home and a\npiece of software treats you like a cyber criminal, kicking and screaming while\nwe attempt the most basic out-of-the-box functionality.\n\nThe fine print here is that we wouldn't recommend messing with any of these\nsettings unless you know what you're doing. Then again, if you knew what you\nwere doing you probably wouldn't be reading this. The point is, if you mess up,\nit's your fault because we warned you.\n\nThe first thing we'll need to touch is the MySQL config found here on Ubuntu:\n\nvim /etc/mysql/mysql.conf.d/mysqld.cnf\n\n\nHere you can set various configurations for MySQL, such as the port number,\ndefault user, etc. The line we're interested in is bind-address.\n\n# The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - \"/etc/mysql/my.cnf\" to set global options,\n# - \"~/.my.cnf\" to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain \"#\" chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n\n\nBy default, bind-address is set to your local host. This is basically a\nwhitelist that allows changes only from the domains or IP addresses specified.\nYou can go ahead and add the address of the external domain you'd like to grant\naccess to here.\n\nCommenting out the line completely opens up MySQL to everybody. So there's that.\n\nNow we need to create a user with which to access the DBL:\n\nmysql -u root -p -h localhost -P 3306\n\n\nUse the CREATE USER  command to create a new homie. In the example below,\n'newuser' is the name of the new user, and '%' is from which location the user\nwill be permitted to make changes. This is usually 'localhost', for example. In\nthis case, we added '%' which means everywhere.\n\nmysql> CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n\n\nGrant all privileges to the new user, and always flush privileges  after making\nsuch modifications.\n\nmysql> GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql> FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nWith these changes made, restart MySQL.\n\nservice mysql restart\n\n\nAssuming this was done correctly, your DB should now be able to receive\nread/write queries from an external source, provided the correct username and\npassword are used.","html":"<p>In the <a href=\"https://hackersandslackers.com/set-up-mysql-database/\">previous post</a>, we got familiar with the basics of creating and navigating MySQL databases. This leads us to the next most logical thing to ask: how can I use this in any meaningful way?</p><p>MySQL installations default to refusing connections outside of the local machine's IP address, as we should expect. That said, relational databases aren't usually being used by a single person on a single machine forever (but if you do, we should hang out). It goes without saying that our MySQL instance should be focusing on uptime and accessibility, or in other terms, far away from our destructive personalities.</p><p>I adore maintaining databases in the command line as much as the next self-hating masochist, but we'll need to accomplish work at some point. That means the remote database we just set up needs to be open-minded enough to allow a connection from, say, the IP address of our personal local machine, which happens to have a sexy GUI installed for this very purpose.</p><p>Making these kinds of configuration changes to any service or web server is always a bit of fun. You think your day might suck until you cone home and a piece of software treats you like a cyber criminal, kicking and screaming while we attempt the most basic out-of-the-box functionality.</p><p>The fine print here is that we wouldn't recommend messing with any of these settings unless you know what you're doing. Then again, if you knew what you were doing you probably wouldn't be reading this. The point is, if you mess up, it's your fault because we warned you.</p><p>The first thing we'll need to touch is the MySQL config found here on Ubuntu:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">vim /etc/mysql/mysql.conf.d/mysqld.cnf\n</code></pre>\n<!--kg-card-end: markdown--><p>Here you can set various configurations for MySQL, such as the port number, default user, etc. The line we're interested in is <em>bind-address.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\"># The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - &quot;/etc/mysql/my.cnf&quot; to set global options,\n# - &quot;~/.my.cnf&quot; to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain &quot;#&quot; chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n</code></pre>\n<!--kg-card-end: markdown--><p>By default, bind-address is set to your local host. This is basically a whitelist that allows changes only from the domains or IP addresses specified. You can go ahead and add the address of the external domain you'd like to grant access to here.</p><p>Commenting out the line completely opens up MySQL to everybody. So there's that.</p><p>Now we need to create a user with which to access the DBL:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql -u root -p -h localhost -P 3306\n</code></pre>\n<!--kg-card-end: markdown--><p>Use the <em>CREATE USER</em> command to create a new homie. In the example below, 'newuser' is the name of the new user, and '%' is from which location the user will be permitted to make changes. This is usually 'localhost', for example. In this case, we added '%' which means <em>everywhere.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n</code></pre>\n<!--kg-card-end: markdown--><p>Grant all privileges to the new user, and always <code>flush privileges</code> after making such modifications.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n<!--kg-card-end: markdown--><p>With these changes made, restart MySQL.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">service mysql restart\n</code></pre>\n<!--kg-card-end: markdown--><p>Assuming this was done correctly, your DB should now be able to receive read/write queries from an external source, provided the correct username and password are used.</p>","url":"https://hackersandslackers.com/accessing-mysql-from-external-domains/","uuid":"ca2500b8-b307-4b1c-8ccd-d9cdf4f1e8eb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5adcf04441f6cf7b7a136a4a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673636","title":"Dropping Rows of Data Using Pandas","slug":"pandas-dataframe-drop","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","custom_excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","created_at_pretty":"22 November, 2017","published_at_pretty":"18 April, 2018","updated_at_pretty":"08 March, 2019","created_at":"2017-11-22T01:21:36.000-05:00","published_at":"2018-04-18T15:00:00.000-04:00","updated_at":"2019-03-08T14:23:37.000-05:00","meta_title":"Dropping Rows Using Pandas | Hackers and Slackers","meta_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","og_title":"Dropping Rows Using Pandas","twitter_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","twitter_title":"Dropping Rows Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"You've heard the cliché before: it is often cited that roughly %80~ of a data\nscientist's role is dedicated to cleaning data sets. I Personally haven't looked\nin to the papers or clinical trials which prove this number (that was a joke),\nbut the idea holds true: in the data profession, we find ourselves doing away\nwith blatantly corrupt or useless data. The simplistic approach is to discard\nsuch data entirely, thus here we are.\n\nWhat constitutes 'filthy' data is project-specific, and at times borderline\nsubjective. Occasionally, the offenders are more obvious: these might include\nchunks of data which are empty, poorly formatted, or simply irrelevant. While\n'bad' data can occasionally be fixed or salvaged via transforms, in many cases\nit's best to do away with rows entirely to ensure that only the fittest survive.\n\nDrop Empty Rows or Columns\nIf you're looking to drop rows (or columns) containing empty data, you're in\nluck: Pandas' dropna()  method is specifically for this. \n\nUsing dropna()  is a simple one-liner which accepts a number of useful\narguments:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\nTechnically you could run MyDataFrame.dropna()  without any parameters, and this\n would default to dropping all rows where are completely empty. If thats all you\nneeded, well, I guess you're done already. Otherwise, here are the parameters\nyou can include:\n\n * Axis: Specifies to drop by row  or column. 0  means row, 1  means column.\n * How: Accepts one of two possible values: any  or all. This will either drop\n   an axis which is completely empty (all), or an axis with even just a single\n   empty cell (any).\n * Thresh: Here's an interesting one: thresh  accepts an integer, and will drop\n   an axis only if that number threshold of empty cells is breached.\n * Subset: Accepts an array of which axis' to consider, as opposed to\n   considering all by default.\n * Inplace: If you haven't come across inplace  yet, learn this now: changes\n   will NOT be made to the DataFrame you're touching unless this is set to True.\n   It's False  by default.\n\nPandas' .drop() Method\nThe pandas .drop()  method is used to remove entire rows or columns based on\ntheir name. If we can see that our DataFrame contains extraneous information\n(perhaps for example, the HR team is storing a preferred_icecream_flavor  in\ntheir master records), we can destroy the column (or row) outright.\n\nUsing drop()  looks something like this:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n\nWe'll attempt to cover the usage of these parameters in plain English before\ninevitably falling into useless lingo which you have not yet learned.\n\n *   Axis: Similar to the above, setting the axis specifies if you're trying to\n   drop rows or columns. \n *   Labels: May refer to either the name (string) of the target axis, or its\n   index (int). Of course, whether this is referring to columns or rows in the\n   DataFrame is dependent on the value of the axis parameter. Labels are always\n   defined in the 0th axis of the target DataFrame, and may accept multiple\n   values in the form of an array when dropping multiple rows/columns at once. \n\nDrop by Index:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n\n\nDrop by Label:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n\n\n *   Index, Columns: An alternative method for specifying the same as the above.\n   Accepts single or multiple values. Setting columns=labels  is equivalent to \n   labels, axis=1.  index=0* is equivalent to *labels=0.  \n *   Levels: Used in sets of data which contain multiple hierarchical levels,\n   similar to that of nested arrays. A high-level few of Hierarchical indexing\n   can be found here\n   [https://pandas.pydata.org/pandas-docs/stable/advanced.html]. \n *   Inplace: Again, drop methods are not carried out on the target Dataframe\n   unless explicitly stated. The purpose of this is to presumably preserve the\n   original set of data during ad hoc manipulation.This adheres to the Python\n   style-guide which states that actions should not be performed on live sets of\n   data unless explicitly stated. Here\n   [https://www.youtube.com/watch?v=XaCSdr7pPmY]  is a video of some guy\n   describing this for some reason. \n *   Errors: Accepts either ignore  or raise, with 'raise' set as default. When \n   errors='ignore'  is set, no errors will be thrown and existing labels are\n   dropped. \n\nDrop by Criteria\nWe can also remove rows or columns based on whichever criteria your little heart\ndesires. For example, if you really hate people named Chad, you can drop all\nrows in your Customer database who have the name Chad. Screw Chad.\n\nUnlike previous methods, the popular way of handling this is simply by saving\nyour Dataframe over itself give a passed value. Here's how we'd get rid of Chad:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n\n\nThe syntax may seem a bit off-putting to newcomers (note the repetition of \nmy_dataframe  3 times). The format of my_dataframe[CONDITION]  simply returns a\nmodified version of my_dataframe, where only the data matching the given\ncondition is affected. \n\nSince we're purging this data altogether, statingmy_dataframe =\nmy_dataframe[CONDITION]  is an easy (albeit destructive) method for shedding\ndata and moving on with our lives.","html":"<p>You've heard the cliché before: it is often cited that roughly %80~ of a data scientist's role is dedicated to cleaning data sets. I Personally haven't looked in to the papers or clinical trials which prove this number (that was a joke), but the idea holds true: in the data profession, we find ourselves doing away with blatantly corrupt or useless data. The simplistic approach is to discard such data entirely, thus here we are.</p><p>What constitutes 'filthy' data is project-specific, and at times borderline subjective. Occasionally, the offenders are more obvious: these might include chunks of data which are empty, poorly formatted, or simply irrelevant. While 'bad' data can occasionally be fixed or salvaged via transforms, in many cases it's best to do away with rows entirely to ensure that only the fittest survive.</p><h2 id=\"drop-empty-rows-or-columns\">Drop Empty Rows or Columns</h2><p>If you're looking to drop rows (or columns) containing empty data, you're in luck: Pandas' <code>dropna()</code> method is specifically for this. </p><p>Using <code>dropna()</code> is a simple one-liner which accepts a number of useful arguments:</p><!--kg-card-begin: code--><pre><code>import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)</code></pre><!--kg-card-end: code--><p>Technically you could run <code>MyDataFrame.dropna()</code> without any parameters, and this  would default to dropping all rows where are completely empty. If thats all you needed, well, I guess you're done already. Otherwise, here are the parameters you can include:</p><ul><li><strong>Axis</strong>: Specifies to drop by <em>row</em> or <em>column</em>. <code>0</code> means <em>row</em>, <code>1</code> means <em>column</em>.</li><li><strong>How</strong>: Accepts one of two possible values: <em>any</em> or <em>all</em>. This will either drop an axis which is completely empty (all), or an axis with even just a single empty cell (any).</li><li><strong>Thresh</strong>: Here's an interesting one: <em>thresh</em> accepts an integer, and will drop an axis only if that number threshold of empty cells is breached.</li><li><strong>Subset</strong>: Accepts an array of which axis' to consider, as opposed to considering all by default.</li><li><strong>Inplace</strong>: If you haven't come across <code>inplace</code> yet, learn this now: changes will NOT be made to the DataFrame you're touching unless this is set to <code>True</code>. It's <code>False</code> by default.</li></ul><h2 id=\"pandas-drop-method\">Pandas' .drop() Method</h2><p>The pandas <code>.drop()</code> method is used to remove entire rows or columns based on their name. If we can see that our DataFrame contains extraneous information (perhaps for example, the HR team is storing a <strong>preferred_icecream_flavor</strong> in their master records), we can destroy the column (or row) outright.</p><p>Using <code>drop()</code> looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n</code></pre>\n<!--kg-card-end: markdown--><p>We'll attempt to cover the usage of these parameters in plain English before inevitably falling into useless lingo which you have not yet learned.</p><ul><li> <strong>Axis</strong>: Similar to the above, setting the axis specifies if you're trying to drop rows or columns. </li><li> <strong>Labels</strong>: May refer to either the name (string) of the target axis, or its index (int). Of course, whether this is referring to columns or rows in the DataFrame is dependent on the value of the axis parameter. Labels are always defined in the 0th axis of the target DataFrame, and may accept multiple values in the form of an array when dropping multiple rows/columns at once. </li></ul><h3 id=\"drop-by-index-\">Drop by Index:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"drop-by-label-\">Drop by Label:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n</code></pre>\n<!--kg-card-end: markdown--><ul><li> <strong>Index, Columns</strong>: An alternative method for specifying the same as the above. Accepts single or multiple values. Setting <em>columns=labels</em> is equivalent to <em>labels, axis=1.</em> <em>index=0</em>* is equivalent to *<em>labels=0.</em> </li><li> <strong>Levels</strong>: Used in sets of data which contain multiple hierarchical levels, similar to that of nested arrays. A high-level few of Hierarchical indexing can be found <a href=\"https://pandas.pydata.org/pandas-docs/stable/advanced.html\">here</a>. </li><li> <strong>Inplace</strong>: Again, drop methods are not carried out on the target Dataframe unless explicitly stated. The purpose of this is to presumably preserve the original set of data during ad hoc manipulation.This adheres to the Python style-guide which states that actions should not be performed on live sets of data unless explicitly stated. <a href=\"https://www.youtube.com/watch?v=XaCSdr7pPmY\">Here</a> is a video of some guy describing this for some reason. </li><li> <strong>Errors</strong>: Accepts either <em>ignore</em> or <em>raise</em>, with 'raise' set as default. When <em>errors='ignore'</em> is set, no errors will be thrown and existing labels are dropped. </li></ul><h2 id=\"drop-by-criteria\">Drop by Criteria</h2><p>We can also remove rows or columns based on whichever criteria your little heart desires. For example, if you really hate people named Chad, you can drop all rows in your Customer database who have the name Chad. Screw Chad.</p><p>Unlike previous methods, the popular way of handling this is simply by saving your Dataframe over itself give a passed value. Here's how we'd get rid of Chad:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n</code></pre>\n<!--kg-card-end: markdown--><p>The syntax may seem a bit off-putting to newcomers (note the repetition of <code>my_dataframe</code> 3 times). The format of <code>my_dataframe[CONDITION]</code> simply returns a modified version of <code>my_dataframe</code>, where only the data matching the given condition is affected. </p><p>Since we're purging this data altogether, stating  <code>my_dataframe = my_dataframe[CONDITION]</code> is an easy (albeit destructive) method for shedding data and moving on with our lives.</p>","url":"https://hackersandslackers.com/pandas-dataframe-drop/","uuid":"6f57d667-6bab-4d97-a62a-adfb2e887d6c","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a151770ade7aa41676efce7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867363b","title":"Setting up a MySQL Database on Ubuntu","slug":"set-up-mysql-database","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","excerpt":"Setting up MySQL the old-fashioned way: on a Linux server.","custom_excerpt":"Setting up MySQL the old-fashioned way: on a Linux server.","created_at_pretty":"17 April, 2018","published_at_pretty":"18 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-16T23:53:52.000-04:00","published_at":"2018-04-17T22:58:58.000-04:00","updated_at":"2019-03-28T04:52:03.000-04:00","meta_title":"Setting up a MySQL Database on Ubuntu | Hackers and Slackers","meta_description":"Setting up MySQL the old-fashioned way: on a linux server","og_description":"Setting up MySQL the old-fashioned way: on a Linux server","og_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","og_title":"Setting up a MySQL Database on Ubuntu","twitter_description":"Setting up MySQL the old-fashioned way: on a Linux server","twitter_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","twitter_title":"Setting up a MySQL Database on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"As frameworks and services evolve to remove us further away from boilerplate\ncode, the first casualty of saved time is the fundamental understanding of what\nwe're actually doing sometimes. This has good reason; one can only learn so much\nfrom repetitive command-line interactions with databases, thus making any\nservice's one-click-deploy  button all the more sensible.  If I  had to imagine\nthe least sexy title for a post in software development, it would be something\nalong the lines of How to Configure MySQL on a VPS, as opposed to like, a\ncloud-based solution, or Even a Docker Container, as Though we Live in the God\nDamn 90s or Something.\" And that's more or less the gist of this post.\n\nI'm not exactly crushing it in the MySQL shell every day- chances are a lot of\nus aren't considering we have plenty of tools to protect us from ever thinking\nabout doing so. That said, this is very much a real use-case for pretty much any\nself-hosted application running a database natively.\n\nSo here it goes: a crash course in MySQL, by An Idiot.\n\nInstallation\nInstalling MySQL server on Ubuntu is simple:\n\nsudo apt-get install mysql-server\n\n\nConfigure MySQL via the Shell\nCreating databases, users, and permissions all happens within the MySQL shell.\nThis can be accessed via:\n\nmysql -u root -p\n\n\nThis will log you in to MySQL as the root  user. In the future, the shell can be\naccessed as any other MySQL user you may create in the future.\n\nExplore your Databases\nSee which MySQL databases exit:\n\nmysql> SHOW DATABASES;\n\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| ghost_prod         |\n| hackers_prod       |\n| ind_prod           |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n\n\nNice databases bro. Notice the mysql  database. As you might imagine, there's\nprobably a lot of cool important shit in there that makes everything work. Let's\ncheck it out.\n\nGet in There\nTo access and start messing with your db, use the USE  query:\n\nmysql> USE mysql;\n\nDatabase changed\n\n\nLet's see which tables are chillin in here.\n\nmysql> SHOW tables;\n\n+---------------------------+\n| Tables_in_mysql           |\n+---------------------------+\n| columns_priv              |\n| db                        |\n| engine_cost               |\n| event                     |\n| func                      |\n| general_log               |\n| gtid_executed             |\n| help_category             |\n| help_keyword              |\n| help_relation             |\n| help_topic                |\n| innodb_index_stats        |\n| innodb_table_stats        |\n| ndb_binlog_index          |\n| plugin                    |\n| proc                      |\n| procs_priv                |\n| proxies_priv              |\n| server_cost               |\n| servers                   |\n| slave_master_info         |\n| slave_relay_log_info      |\n| slave_worker_info         |\n| slow_log                  |\n| tables_priv               |\n| time_zone                 |\n| time_zone_leap_second     |\n| time_zone_name            |\n| time_zone_transition      |\n| time_zone_transition_type |\n| user                      |\n+---------------------------+\n\n\nOh wow yeah, that looks pretty important. This is where configurations such as\nuser information exists. When we create users and grant them permissions, we'll\nbe doing so in mysql. We'll worry about that later, but let's see who's in there\nanyway for the hell of it:\n\nmysql> select user from user;\n\n+------------------+\n| user             |\n+------------------+\n| debian-sys-maint |\n| mysql.session    |\n| mysql.sys        |\n| root             |\n+------------------+\n\n\nSick. Without knowing much SQL at all, we can already see our databases, their\ntables, and get the values of whichever columns they might have. Now let's start\ndoing stuff.\n\nCreate a Database\nGo ahead and create a new database. In my case, I want to create a database\nwhich lists my Github repositories, so I'll create a db named github_repos:\n\nCREATE DATABASE github_repos;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> USE github_repos;\nDatabase changed\n\n\nCreating a table\nNow it's getting good: we're going to create a table in our database: to do\nthis, we're going to need to define our columns upfront, including the type of\ndata each column can accept as well as the restrictions on that column. I'm\nkeeping it simple and storing values of text for now.\n\nmysql> CREATE TABLE githubrepos (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, \n    -> full_name VARCHAR(100),\n    -> description VARCHAR(300),\n    -> name VARCHAR(200),\n    -> url VARCHAR(150));\n\n\nid  is a standard column which indicates the numerical index of each row. Here,\nwe're stating that our rows will count themselves.\n\nEach following line creates a column by [name]  [type of data]****[limit]. In\nthis example we're creating columns which accept alphanumeric characters, up to\na maximum of the the number specified.\n\nNOTE: you should really overestimate the number of characters each field can\naccept. I didn't. Its a waste of time and might drive you crazy down the line:\njust accept a large number and be done with it.\n\nFruits of your labor\nGo ahead and check out what you've done:\n\nmysql> SHOW tables;\n\n+------------------------+\n| Tables_in_github_repos |\n+------------------------+\n| githubrepos            |\n+------------------------+\n\n\nDamn dude, you did it. Let's take a look just to make sure:\n\n+-------------+--------------+------+-----+---------+----------------+\n| Field       | Type         | Null | Key | Default | Extra          |\n+-------------+--------------+------+-----+---------+----------------+\n| id          | int(11)      | NO   | PRI | NULL    | auto_increment |\n| full_name   | varchar(200) | YES  |     | NULL    |                |\n| description | varchar(300) | YES  |     | NULL    |                |\n| name        | varchar(200) | YES  |     | NULL    |                |\n| url         | varchar(200) | YES  |     | NULL    |                |\n+-------------+--------------+------+-----+---------+----------------+\n5 rows in set (0.01 sec)\n\n\nHoly shit it's literally a table.\n\nMaking Changes\nLet's make some changes to our table after the fact. We can use ALTER TABLE to\nadd, modify, or remove columns.\n\nmysql> ALTER TABLE githubrepos ADD homepage VARCHAR(255);\nQuery OK, 0 rows affected (0.09 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\n\nThis added column homepage  which accepts alphanumeric characters.\n\nNow that we've created our own database with our own defined structure, the\npossibilities are endless. The next step is to actually fill it with data, but\nlet's save that for next time.\n\nmysql> \\q\nBye","html":"<p>As frameworks and services evolve to remove us further away from boilerplate code, the first casualty of saved time is the fundamental understanding of what we're actually doing sometimes. This has good reason; one can only learn so much from repetitive command-line interactions with databases, thus making any service's <em>one-click-deploy</em> button all the more sensible.  If I  had to imagine the least sexy title for a post in software development, it would be something along the lines of <em><strong>How to Configure MySQL on a VPS, as opposed to like, a cloud-based solution, or Even a Docker Container, as Though we Live in the God Damn 90s or Something.\" </strong> </em>And that's more or less the gist of this post.</p><p>I'm not exactly crushing it in the MySQL shell every day- chances are a lot of us aren't considering we have plenty of tools to protect us from ever thinking about doing so. That said, this is very much a real use-case for pretty much any self-hosted application running a database natively.</p><p>So here it goes: a crash course in MySQL, by An Idiot.</p><h3 id=\"installation\">Installation</h3><p>Installing MySQL server on Ubuntu is simple:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">sudo apt-get install mysql-server\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"configure-mysql-via-the-shell\">Configure MySQL via the Shell</h3><p>Creating databases, users, and permissions all happens within the MySQL shell. This can be accessed via:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql -u root -p\n</code></pre>\n<!--kg-card-end: markdown--><p>This will log you in to MySQL as the <strong>root</strong> user. In the future, the shell can be accessed as any other MySQL user you may create in the future.</p><h3 id=\"explore-your-databases\">Explore your Databases</h3><p>See which MySQL databases exit:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW DATABASES;\n\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| ghost_prod         |\n| hackers_prod       |\n| ind_prod           |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice databases bro. Notice the <em>mysql</em> database. As you might imagine, there's probably a lot of cool important shit in there that makes everything work. Let's check it out.</p><h3 id=\"get-in-there\">Get in There</h3><p>To access and start messing with your db, use the <strong>USE</strong> query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; USE mysql;\n\nDatabase changed\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see which tables are chillin in here.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW tables;\n\n+---------------------------+\n| Tables_in_mysql           |\n+---------------------------+\n| columns_priv              |\n| db                        |\n| engine_cost               |\n| event                     |\n| func                      |\n| general_log               |\n| gtid_executed             |\n| help_category             |\n| help_keyword              |\n| help_relation             |\n| help_topic                |\n| innodb_index_stats        |\n| innodb_table_stats        |\n| ndb_binlog_index          |\n| plugin                    |\n| proc                      |\n| procs_priv                |\n| proxies_priv              |\n| server_cost               |\n| servers                   |\n| slave_master_info         |\n| slave_relay_log_info      |\n| slave_worker_info         |\n| slow_log                  |\n| tables_priv               |\n| time_zone                 |\n| time_zone_leap_second     |\n| time_zone_name            |\n| time_zone_transition      |\n| time_zone_transition_type |\n| user                      |\n+---------------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Oh wow yeah, that looks pretty important. This is where configurations such as user information exists. When we create users and grant them permissions, we'll be doing so in <em>mysql</em>. We'll worry about that later, but let's see who's in there anyway for the hell of it:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; select user from user;\n\n+------------------+\n| user             |\n+------------------+\n| debian-sys-maint |\n| mysql.session    |\n| mysql.sys        |\n| root             |\n+------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Sick. Without knowing much SQL at all, we can already see our databases, their tables, and get the values of whichever columns they might have. Now let's start doing stuff.</p><h3 id=\"create-a-database\">Create a Database</h3><p>Go ahead and create a new database. In my case, I want to create a database which lists my Github repositories, so I'll create a db named <em>github_repos</em>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">CREATE DATABASE github_repos;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql&gt; USE github_repos;\nDatabase changed\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"creating-a-table\">Creating a table</h3><p>Now it's getting good: we're going to create a table in our database: to do this, we're going to need to define our columns upfront, including the type of data each column can accept as well as the restrictions on that column. I'm keeping it simple and storing values of text for now.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; CREATE TABLE githubrepos (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, \n    -&gt; full_name VARCHAR(100),\n    -&gt; description VARCHAR(300),\n    -&gt; name VARCHAR(200),\n    -&gt; url VARCHAR(150));\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>id</strong> is a standard column which indicates the numerical index of each row. Here, we're stating that our rows will count themselves.</p><p>Each following line creates a column by <strong>[name]</strong> <strong>[type of data]****[limit]</strong>. In this example we're creating columns which accept alphanumeric characters, up to a maximum of the the number specified.</p><p><em><strong>NOTE: you should really overestimate the number of characters each field can accept. I didn't. Its a waste of time and might drive you crazy down the line: just accept a large number and be done with it.</strong></em></p><h3 id=\"fruits-of-your-labor\">Fruits of your labor</h3><p>Go ahead and check out what you've done:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW tables;\n\n+------------------------+\n| Tables_in_github_repos |\n+------------------------+\n| githubrepos            |\n+------------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Damn dude, you did it. Let's take a look just to make sure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">+-------------+--------------+------+-----+---------+----------------+\n| Field       | Type         | Null | Key | Default | Extra          |\n+-------------+--------------+------+-----+---------+----------------+\n| id          | int(11)      | NO   | PRI | NULL    | auto_increment |\n| full_name   | varchar(200) | YES  |     | NULL    |                |\n| description | varchar(300) | YES  |     | NULL    |                |\n| name        | varchar(200) | YES  |     | NULL    |                |\n| url         | varchar(200) | YES  |     | NULL    |                |\n+-------------+--------------+------+-----+---------+----------------+\n5 rows in set (0.01 sec)\n</code></pre>\n<!--kg-card-end: markdown--><p>Holy shit it's literally a table.</p><h3 id=\"making-changes\">Making Changes</h3><p>Let's make some changes to our table after the fact. We can use ALTER TABLE to add, modify, or remove columns.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; ALTER TABLE githubrepos ADD homepage VARCHAR(255);\nQuery OK, 0 rows affected (0.09 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n</code></pre>\n<!--kg-card-end: markdown--><p>This added column <em>homepage</em> which accepts alphanumeric characters.</p><p>Now that we've created our own database with our own defined structure, the possibilities are endless. The next step is to actually fill it with data, but let's save that for next time.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; \\q\nBye\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/set-up-mysql-database/","uuid":"697bc755-a833-43fe-b807-5668b8c284f4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ad56fd09bfe350c74e8a8cb"}},{"node":{"id":"Ghost__Post__5c64981a7c8ecc6ee30c6870","title":"Starting a Python Web App with Heroku","slug":"starting-a-python-web-app-with-heroku","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/flask2@2x.jpg","excerpt":"Pairing Flask with zero-effort container deployments is a deadly path to addiction.","custom_excerpt":"Pairing Flask with zero-effort container deployments is a deadly path to addiction.","created_at_pretty":"13 February, 2019","published_at_pretty":"13 February, 2018","updated_at_pretty":"13 February, 2019","created_at":"2019-02-13T17:20:10.000-05:00","published_at":"2018-02-13T17:20:00.000-05:00","updated_at":"2019-02-13T17:57:20.000-05:00","meta_title":"Starting a Python Application with Heroku | Hackers and Slackers","meta_description":"Flask is a gift to developers who value the act of development. Pairing Python with zero-effort container deployments is a deadly path to addiction.","og_description":"Flask is a gift to developers who value the act of development. Pairing Python with zero-effort container deployments is a deadly path to addiction.","og_image":"https://hackersandslackers.com/content/images/2019/02/flask2@2x.jpg","og_title":"Starting a Python Application with Heroku | Hackers and Slackers","twitter_description":"Flask is a gift to developers who value the act of development. Pairing Python with zero-effort container deployments is a deadly path to addiction.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/flask2@2x.jpg","twitter_title":"Starting a Python Application with Heroku | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"It's difficult to cover every cloud solution on the market without at least\nmentioning Heroku. Heroku contrasts nearly every cloud hosting solution by\noffering a clear purpose: make deploying apps of any kind as easy as possible.\nDeploying to a VPS requires knowledge of web servers and configurations.\nDeploying to containers requires knowledge of Docker or Kubernetes. Deploying to\nHeroku requires nearly no prior knowledge of anything.\n\nHeroku is great for getting MVPs out the door, or for devs who want to jump into\ndeveloping web applications with knowledge of a specific language. Even\ndevelopers with advanced knowledge of how to deploy production applications may\nwant to use Heroku for fast internal deployments, or as a platform for\n\"sketching out\" a quick prototype.\n\nIn this exploration, we'll be using Heroku to deploy a Python application using\nthe Flask framework.\n\nWhy Heroku?\nWe're on the topic of simplicity, so let's keep that theme going. Heroku's\ninfrastructure offering is unique in that Heroku obfuscates the DevOps aspect of\nweb development completely. That means that configuring web servers, managing\nLinux packages, and supplying SSL certs are entirely taken care of by Heroku. \n\nLet's consider Heroku's ease-of-use services to be luxuries which save us time.\nThey are NOT  a replacement for grasping these concepts.\n\nPipelines\nAside from VPS upkeep, Heroku obfuscates the process of moving an app through\ndevelopment and production environments by defining pipelines. That's right,\nCI/CD is built directly into Heroku's interface.\n\nAdd-ons\nThe most addictive aspect of Heroku is probably the Elements marketplace. This\nis a place to window-shop for set-it-and-forget-it plugins for your app, most of\nwhich are very easy to integrate with. \n\nMost add-ons fall under a few major categories: database resellers, analytics,\nand Redis, to name a few (interestingly enough, using the base Redis add-on in\nHeroku is free, while the equivalent instance would cost you 5 dollars from the\nsame provider had you used them directly. Add-ons are \"deployed\" after a single\nclick, and the ensuing configuration process varies from vendor-to-vendor after\nthat.\n\nSpeaking of single-click, they handle single-click deployments of popular build\npacks, too. You, the thing that made DigitalOcean a big deal way back. You get\nthe idea.\n\nCreating your Project\nLog in to the Heroku UI and create an app on a fresh Dyno. A Dyno is merely a\nfancy, overly branded word for \"container.\" Next, you'll be prompted to download\nthe Heroku CLI locally on your OS of choice, which is quick and painless. Now\nwe're cooking with gas.\n\nCreate an empty local directory and type the following command to be prompted\nfor your Heroku account credentials:\n\n$ heroku login\nEnter your Heroku credentials.\nEmail: python@example.com\nPassword:\n\n\nAt this point, Heroku has already magically created a git repository for your\napplication from which you'll be doing development from.\n\n$ git clone https://github.com/heroku/example-flask-project.git\n$ cd example-flask-project\n\n$ heroku create\nCreating example-flask-project in organization heroku... done, stack is cedar-14\nhttp://example-flask-project.herokuapp.com/ | https://git.heroku.com/example-flask-project.git\nGit remote heroku added\n\n\nWow, that sure looks a lot like we're working with Github huh? That's actually\nthe point: if you so chose, you can configure the Heroku CLI to recognize your\nGithub username with a simple heroku config:get GITHUB_USERNAME=yourname. With\nthis configured, Heroku will actually allow you to simply deploy to your\npersonal Github repo and mimic the changes on your Dyno. Now let's configure\nthis thing.\n\nA Project For Ants\nWe're going to get started by building you obligatory \"hello world\" app. The\nresulting file structure is going to end up looking like this:\n\nexample-flask-project\n├── app.py\n├── Procfile\n├── Pipfile\n├── Pipfile.lock\n├── runtime.txt\n├── requirements.txt\n├── Pipfile.lock\n└── setup.py\n\n\nNote the existence of two files you may not have seen before if you're new to\nHeroku: the Procfile  (no file extension) and requirements.txt. These are tiny\nfiles which specify which language we're using and how to start our app, but\nwe'll get to that in a moment.\n\nManaging Your Python Packages \nHeroku impressively supports Pipenv out-of-the-box for handling and installing\ndependencies. Every time you deploy your application, Heroku will install the\npackage version specified in Pipfile.lock to build your app from scratch. If\nyou're new to using Pipenv consider quickly picking up the basics from this\nquick tutorial\n[https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/]\n. If you're still using virtualenv, you should consider switching to Pipenv\nregardless.\n\nCreate a local folder for your project. In that folder, start a Pipenv shell:\n\n$ pip install pipenv\npipenv shell\n\n\nWith the shell activated, we can now install dependencies specific to our\nenvironment. At a bare minimum, we need to install two packages: Flask  as our\nframework, and Gunicorn  to run our app process.\n\n(my-project)$ pip3 install flask gunicorn\n\n\nGood job; now let's build out the files in our tree one-by-one.\n\nProcfile\nThe Procfile (no file extension) is a unique file to Heroku which is essentially\na build command. This will be a one-liner to tell Gunicorn  to startup our\napplication from our base app.py  file.\n\nweb: gunicorn app:app\n\nA quick breakdown here: web  is our process 'type'. other types exists, such as \nworker, urgentworker, and clock, but that's not important for now.\n\napp:app  signifies looking for the 'app' module in our app.py  file. If you'd\nlike to move app.py to . a different folder down the line, this can be adjusted\nas such:\n\nweb: gunicorn differentfolder app:app\n\nRuntime\nThe runtime.txt  file notifies Heroku of the language it's dealing with as well\nas the proper version. Heroku only supports up to a particular version of Python\nat any given moment (which is currently Python-3.7.1), but specifying a higher\nversion will default to the latest version Heroku supports.\n\nPython-3.7.1\n\nRequirements.txt\nEven though Heroku uses your Pipfile to build dependencies, it's still best\npractice to keep a requirements.txt  present for numerous reasons. For example,\nif you remove dependencies from your Pipfile without uninstalling them, \nrequirements.txt  is a useful way of identifying old packages in your\nenvironment that can be uninstalled.\n\n(my-project)$ pip freeze > requirements.txt\n\n\nAs I'm sure you know, pip freeze  will print all packages and their versions\ninto the designated file as such:\n\nasn1crypto==0.24.0\nbcrypt==3.1.4\nbeautifulsoup4==4.6.0\nblinker==1.4\ncffi==1.11.5\nclick==6.7\ncryptography==2.2.2\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Login==0.4.1\nFlask-Mail==0.9.1\nflask-mongoengine==0.9.5\nFlask-SQLAlchemy==2.3.2\nFlask-Static-Compress==1.0.2\nFlask-User==1.0.1.5\nFlask-WTF==0.14.2\ngunicorn==19.9.0\nidna==2.7\nitsdangerous==0.24\njac==0.17.1\nJinja2==2.10\nMarkupSafe==1.0\nmongoengine==0.15.0\nordereddict==1.1\npasslib==1.7.1\npycparser==2.18\npymongo==3.7.0\nrjsmin==1.0.12\nsix==1.11.0\nSQLAlchemy==1.2.9\nwebassets==0.12.1\nWerkzeug==0.14.1\nWTForms==2.2.1\n\n\nPipfile\nOur Pipfile is automatically generated by Pipenv by default, but be sure to call\nout packages which are essential to the build our app as. Packages which are\nrequired for your app to work belong under the [packages]  section.\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\ngunicorn = \"*\"\nflask = \"*\"\nrequests = \"*\"\nwtforms = \"*\"\nflask_assets = \"*\"\nflask_static_compress = \"*\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.7.1\"\n\n\nPipfile.lock\nHeroku looks at Pipfile.lock  every time our app builds to know which packages\nto install on the server side. Changing dependencies locally without updating\nthe Pipfile.lock  will not carry the changes over to your Dyno. Thus, be sure to\ngenerate this file when needed:\n\n(my-project)$ pipenv lock\n\n\nBetter yet, running the following will check your Pipfile for packages which can\nbe updated, will update those packages, and then  generate a lock file:\n\n(my-project)$ pipenv update\n\n\nSetup.py\nTechnically this file isn't required, but is a general best practice when\ncreating projects. Most of Setup.py's purpose comes in to play if you plan on\nsubmitting your project as a standalone package,\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='my-project',\n    version='1.0',\n    long_description=__doc__,\n    packages=find_packages(),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=['Flask'],\n)\n\n\n.env\nOkay, okay, just one last thing. Heroku will be upset unless there's a .env \nfile in its root directory at run time. .env  is where we would store sensitive\ninformation (such as secrets), but feel free to leave this empty for now. \n\nHeroku allows you to manage environment variables via their web UI as well.\nThese can then be conveniently saved to your local environment to run your app\nlocally, but let's stay focused on the task at hand: saying \"hello\" to the\nworld.\n\nDeployment\nRunning your app locally is as simple as two words: heroku local. This spins up\nan instance of your app on your machine at 0.0.0.0:5000.\n\nDeploying to your Heroku Dyno is much like deploying to Github (they can in fact\nbe the exact same if you configure it as such). Here's how deployment via the\nHeroku CLI looks:\n\ngit add .\ngit commit -am 'initial commit'\ngit push heroku master\n\n\nIf all went well, your app should be live at the URL Heroku generated for you\nwhen you created your project. Go ahead and checkout the Heroku UI to see how\nthings went. \n\nI highly suggest checking out the logs on the Heroku UI after each deploy. Often\ntimes issues which don't appear on your local environment will pop up on the\nserver:\n\nHeroku's logging system is surprisingly both helpful and aesthetically pleasing.\nWhat Do We Make Of This?\nThere are two general takeaways I suppose I'm getting at:\n\n * Heroku is easy and fun to use.\n * Flask is awesome. \n\nAs much as #1 is true, I think it's important to distinguish Heroku's place in a\ncrowded cloud market. Heroku is a platform best suited for dumping MVPs and side\nprojects... NOT production applications. While you certainly can host large apps\non Heroku, I consider it to highly unprofessional. Remember: Heroku is basically\na reseller. They host their containers on AWS, and sell add-ons from other\nvendors. If you depend too heavily on Heroku, you are essentially just adding a\nmiddle man to your billing cycle.\n\nOn the Flask side: Flask's development may not be as vast as the npm  packages\noffered by Node, there's more or less a package for anything you possibly need.\nI'd recommend checking out Flask's official list of packages\n[http://flask.pocoo.org/extensions/].\n\nWhile we may have set up our first Flask application, as it stands we've only\nbuilt something useless so far. Consider this to be the beginning of many, many\nFlask tips to come.","html":"<p>It's difficult to cover every cloud solution on the market without at least mentioning Heroku. Heroku contrasts nearly every cloud hosting solution by offering a clear purpose: make deploying apps of any kind as easy as possible. Deploying to a VPS requires knowledge of web servers and configurations. Deploying to containers requires knowledge of Docker or Kubernetes. Deploying to Heroku requires nearly no prior knowledge of anything.</p><p>Heroku is great for getting MVPs out the door, or for devs who want to jump into developing web applications with knowledge of a specific language. Even developers with advanced knowledge of how to deploy production applications may want to use Heroku for fast internal deployments, or as a platform for \"sketching out\" a quick prototype.</p><p>In this exploration, we'll be using Heroku to deploy a Python application using the Flask framework.</p><h2 id=\"why-heroku\">Why Heroku?</h2><p>We're on the topic of simplicity, so let's keep that theme going. Heroku's infrastructure offering is unique in that Heroku obfuscates the DevOps aspect of web development completely. That means that configuring web servers, managing Linux packages, and supplying SSL certs are entirely taken care of by Heroku. </p><p>Let's consider Heroku's ease-of-use services to be luxuries which save us time. They are <strong>NOT</strong> a replacement for grasping these concepts.</p><h3 id=\"pipelines\">Pipelines</h3><p>Aside from VPS upkeep, Heroku obfuscates the process of moving an app through development and production environments by defining <em>pipelines. </em>That's right, CI/CD is built directly into Heroku's interface.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-10-at-10.19.41-AM.png\" class=\"kg-image\"></figure><h3 id=\"add-ons\">Add-ons</h3><p>The most addictive aspect of Heroku is probably the Elements marketplace. This is a place to window-shop for set-it-and-forget-it plugins for your app, most of which are very easy to integrate with. </p><p>Most add-ons fall under a few major categories: database resellers, analytics, and Redis, to name a few (interestingly enough, using the base Redis add-on in Heroku is free, while the equivalent instance would cost you 5 dollars from the same provider had you used them directly. Add-ons are \"deployed\" after a single click, and the ensuing configuration process varies from vendor-to-vendor after that.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screenshot-2018-07-08-17.30.07.png\" class=\"kg-image\"></figure><p>Speaking of single-click, they handle single-click deployments of popular build packs, too. You, the thing that made DigitalOcean a big deal way back. You get the idea.</p><h2 id=\"creating-your-project\">Creating your Project</h2><p>Log in to the Heroku UI and create an app on a fresh Dyno. A Dyno is merely a fancy, overly branded word for \"container.\" Next, you'll be prompted to download the Heroku CLI locally on your OS of choice, which is quick and painless. Now we're cooking with gas.</p><p>Create an empty local directory and type the following command to be prompted for your Heroku account credentials:</p><pre><code class=\"language-bash\">$ heroku login\nEnter your Heroku credentials.\nEmail: python@example.com\nPassword:\n</code></pre>\n<p>At this point, Heroku has already magically created a git repository for your application from which you'll be doing development from.</p><pre><code class=\"language-bash\">$ git clone https://github.com/heroku/example-flask-project.git\n$ cd example-flask-project\n\n$ heroku create\nCreating example-flask-project in organization heroku... done, stack is cedar-14\nhttp://example-flask-project.herokuapp.com/ | https://git.heroku.com/example-flask-project.git\nGit remote heroku added\n</code></pre>\n<p>Wow, that sure looks a lot like we're working with Github huh? That's actually the point: if you so chose, you can configure the Heroku CLI to recognize your Github username with a simple <code>heroku config:get GITHUB_USERNAME=yourname</code>. With this configured, Heroku will actually allow you to simply deploy to your personal Github repo and mimic the changes on your Dyno. Now let's configure this thing.</p><h2 id=\"a-project-for-ants\">A Project For Ants</h2><p>We're going to get started by building you obligatory \"hello world\" app. The resulting file structure is going to end up looking like this:</p><pre><code class=\"language-bash\">example-flask-project\n├── app.py\n├── Procfile\n├── Pipfile\n├── Pipfile.lock\n├── runtime.txt\n├── requirements.txt\n├── Pipfile.lock\n└── setup.py\n</code></pre>\n<p>Note the existence of two files you may not have seen before if you're new to Heroku: the <strong>Procfile</strong> (no file extension) and <strong>requirements.txt</strong>. These are tiny files which specify which language we're using and how to start our app, but we'll get to that in a moment.</p><h3 id=\"managing-your-python-packages\">Managing Your Python Packages </h3><p>Heroku impressively supports Pipenv out-of-the-box for handling and installing dependencies. Every time you deploy your application, Heroku will install the package version specified in Pipfile.lock to build your app from scratch. If you're new to using Pipenv consider quickly picking up the basics from <a href=\"https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/\">this quick tutorial</a>. If you're still using virtualenv, you should consider switching to Pipenv regardless.</p><p>Create a local folder for your project. In that folder, start a Pipenv shell:</p><pre><code class=\"language-bash\">$ pip install pipenv\npipenv shell\n</code></pre>\n<p>With the shell activated, we can now install dependencies specific to our environment. At a bare minimum, we need to install two packages: <strong>Flask</strong> as our framework, and <strong>Gunicorn</strong> to run our app process.</p><pre><code class=\"language-bash\">(my-project)$ pip3 install flask gunicorn\n</code></pre>\n<p>Good job; now let's build out the files in our tree one-by-one.</p><h3 id=\"procfile\">Procfile</h3><p>The Procfile (no file extension) is a unique file to Heroku which is essentially a build command. This will be a one-liner to tell <strong>Gunicorn</strong> to startup our application from our base <code>app.py</code> file.</p><pre><code>web: gunicorn app:app</code></pre><p>A quick breakdown here: <code>web</code> is our process 'type'. other types exists, such as <code>worker</code>, <code>urgentworker</code>, and <code>clock</code>, but that's not important for now.</p><p><code>app:app</code> signifies looking for the 'app' module in our <em>app.py</em> file. If you'd like to move app.py to . a different folder down the line, this can be adjusted as such:</p><pre><code>web: gunicorn differentfolder app:app</code></pre><h3 id=\"runtime\">Runtime</h3><p>The runtime.txt  file notifies Heroku of the language it's dealing with as well as the proper version. Heroku only supports up to a particular version of Python at any given moment (which is currently <em>Python-3.7.1</em>), but specifying a higher version will default to the latest version Heroku supports.</p><pre><code>Python-3.7.1</code></pre><h3 id=\"requirements-txt\">Requirements.txt</h3><p>Even though Heroku uses your Pipfile to build dependencies, it's still best practice to keep a <code>requirements.txt</code> present for numerous reasons. For example, if you remove dependencies from your Pipfile without uninstalling them, <code>requirements.txt</code> is a useful way of identifying old packages in your environment that can be uninstalled.</p><pre><code class=\"language-bash\">(my-project)$ pip freeze &gt; requirements.txt\n</code></pre>\n<p>As I'm sure you know, <code>pip freeze</code> will print all packages and their versions into the designated file as such:</p><pre><code class=\"language-bash\">asn1crypto==0.24.0\nbcrypt==3.1.4\nbeautifulsoup4==4.6.0\nblinker==1.4\ncffi==1.11.5\nclick==6.7\ncryptography==2.2.2\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Login==0.4.1\nFlask-Mail==0.9.1\nflask-mongoengine==0.9.5\nFlask-SQLAlchemy==2.3.2\nFlask-Static-Compress==1.0.2\nFlask-User==1.0.1.5\nFlask-WTF==0.14.2\ngunicorn==19.9.0\nidna==2.7\nitsdangerous==0.24\njac==0.17.1\nJinja2==2.10\nMarkupSafe==1.0\nmongoengine==0.15.0\nordereddict==1.1\npasslib==1.7.1\npycparser==2.18\npymongo==3.7.0\nrjsmin==1.0.12\nsix==1.11.0\nSQLAlchemy==1.2.9\nwebassets==0.12.1\nWerkzeug==0.14.1\nWTForms==2.2.1\n</code></pre>\n<h3 id=\"pipfile\">Pipfile</h3><p>Our Pipfile is automatically generated by Pipenv by default, but be sure to call out packages which are essential to the build our app as. Packages which are required for your app to work belong under the <code>[packages]</code> section.</p><pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\ngunicorn = \"*\"\nflask = \"*\"\nrequests = \"*\"\nwtforms = \"*\"\nflask_assets = \"*\"\nflask_static_compress = \"*\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.7.1\"\n</code></pre><h3 id=\"pipfile-lock\">Pipfile.lock</h3><p>Heroku looks at <code>Pipfile.lock</code><em> </em>every time our app builds to know which packages to install on the server side. Changing dependencies locally without updating the <code>Pipfile.lock</code> will not carry the changes over to your Dyno. Thus, be sure to generate this file when needed:</p><pre><code class=\"language-bash\">(my-project)$ pipenv lock\n</code></pre>\n<p>Better yet, running the following will check your Pipfile for packages which can be updated, will update those packages, and <em>then</em> generate a lock file:</p><pre><code class=\"language-bash\">(my-project)$ pipenv update\n</code></pre>\n<h3 id=\"setup-py\">Setup.py</h3><p>Technically this file isn't required, but is a general best practice when creating projects. Most of <code>Setup.py</code>'s purpose comes in to play if you plan on submitting your project as a standalone package,</p><pre><code class=\"language-python\">from setuptools import setup, find_packages\n\nsetup(\n    name='my-project',\n    version='1.0',\n    long_description=__doc__,\n    packages=find_packages(),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=['Flask'],\n)\n</code></pre>\n<h2 id=\"-env\">.env</h2><p>Okay, okay, just one last thing. Heroku will be upset unless there's a <code>.env</code> file in its root directory at run time. <code>.env</code> is where we would store sensitive information (such as secrets), but feel free to leave this empty for now. </p><p>Heroku allows you to manage environment variables via their web UI as well. These can then be conveniently saved to your local environment to run your app locally, but let's stay focused on the task at hand: saying \"hello\" to the world.</p><h2 id=\"deployment\">Deployment</h2><p>Running your app locally is as simple as two words: <code>heroku local</code>. This spins up an instance of your app on your machine at <code>0.0.0.0:5000</code>.</p><p>Deploying to your Heroku Dyno is much like deploying to Github (they can in fact be the exact same if you configure it as such). Here's how deployment via the Heroku CLI looks:</p><pre><code class=\"language-bash\">git add .\ngit commit -am 'initial commit'\ngit push heroku master\n</code></pre>\n<p>If all went well, your app should be live at the URL Heroku generated for you when you created your project. Go ahead and checkout the Heroku UI to see how things went. </p><p>I highly suggest checking out the logs on the Heroku UI after each deploy. Often times issues which don't appear on your local environment will pop up on the server:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screenshot-2018-07-08-17.16.33.png\" class=\"kg-image\"><figcaption>Heroku's logging system is surprisingly both helpful and aesthetically pleasing.</figcaption></figure><h2 id=\"what-do-we-make-of-this\">What Do We Make Of This?</h2><p>There are two general takeaways I suppose I'm getting at:</p><ul><li>Heroku is easy and fun to use.</li><li>Flask is awesome. </li></ul><p>As much as #1 is true, I think it's important to distinguish Heroku's place in a crowded cloud market. Heroku is a platform best suited for dumping MVPs and side projects... NOT production applications. While you certainly can host large apps on Heroku, I consider it to highly unprofessional. Remember: Heroku is basically a reseller. They host their containers on AWS, and sell add-ons from other vendors. If you depend too heavily on Heroku, you are essentially just adding a middle man to your billing cycle.</p><p>On the Flask side: Flask's development may not be as vast as the <code>npm</code> packages offered by Node, there's more or less a package for anything you possibly need. I'd recommend checking out Flask's official list of <a href=\"http://flask.pocoo.org/extensions/\">packages</a>.</p><p>While we may have set up our first Flask application, as it stands we've only built something useless so far. Consider this to be the beginning of many, many Flask tips to come.</p>","url":"https://hackersandslackers.com/starting-a-python-web-app-with-heroku/","uuid":"c426aeae-5f78-405e-8452-57e8fc110b12","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c64981a7c8ecc6ee30c6870"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673635","title":"Installing Django CMS on Ubuntu","slug":"installing-django-cms","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/djangocms-2.jpg","excerpt":"How to install DjangoCMS: the largest of three major CMS products for Django.","custom_excerpt":"How to install DjangoCMS: the largest of three major CMS products for Django.","created_at_pretty":"19 November, 2017","published_at_pretty":"19 November, 2017","updated_at_pretty":"28 March, 2019","created_at":"2017-11-19T18:38:50.000-05:00","published_at":"2017-11-19T18:50:29.000-05:00","updated_at":"2019-03-28T09:28:54.000-04:00","meta_title":"Installing Django CMS on Ubuntu | Hackers and Slackers","meta_description":"Get the play-by-play on how to install DjangoCMS: the largest of three major CMS products for Python's Django framework.","og_description":"Get the play-by-play on how to install DjangoCMS: the largest of three major CMS products for Python's Django framework.","og_image":"https://hackersandslackers.com/content/images/2019/03/djangocms-2.jpg","og_title":"Installing Django CMS on Ubuntu","twitter_description":"Get the play-by-play on how to install DjangoCMS: the largest of three major CMS products for Python's Django framework.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/djangocms-1.jpg","twitter_title":"Installing Django CMS on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Getting into Django","slug":"starting-django","description":"Getting started with Django: the original daddy of Python frameworks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/django2.jpg","meta_description":"Getting started with Django: the original daddy of Python frameworks.","meta_title":"Setting up Django","visibility":"internal"}],"plaintext":"So you've selected Python as your language of choice for building a massive web\napp. Congratulations! While there are some that will point to Python's\nrelatively slow execution at runtime, you're brave enough to realize that saving\nchunks of your time is more important to the success of your project than\nbuilding your app in a marginally faster language.\n\n  There are a few options when it comes to picking an out-of-the-box CMS with\nDjango, but perhaps none are as popular as Django CMS\n[https://www.django-cms.org/en/].  DjangoCMS  is the biggest of the \"big three\"\nCMS choices for Django, with Mezzanine [http://mezzanine.jupo.org/]  and Wagtail\n[https://wagtail.io/]  following close behind (I personally like Wagtail, if I'm\ngonna be real here).\n\nAs it stands, there doesn't seem to be an existing  guide which walks through\nthe complete  installation of Django  + Django CMS. While is information exists\nin fragments, newcomers to Python could have trouble piecing much of this\ninformation together out of the gate. This guide hopes to serve as a reduction\nof friction to Django newcomers, thus ensuring Pythonic world domination.\n\nOur Stack\nTo get this party started, our VPS is going to run the following stack:\n\n * Ubuntu\n * Python 3\n * Virtualenv\n * PostgreSQL\n * Django\n * Django CMS\n\nUpdate All Dependencies\nFirst, we'll prep our VPS with all the necessary dependencies:\n\n$ apt-get update\n$ apt-get upgrade -y\n$ apt-get install python3 python3-pip python-dev libpq-dev postgresql postgresql-contrib\n\n\nCreate and Activate a Virtual Environment\nVirtualenv is included with Python3, so no installation needed here.\n\n$ virtualenv env\n$ source env/bin/activate\n\n\nInstall Django\nFind out what the latest distribution is by going here\n[https://www.djangoproject.com/download/].\n\n$ pip3 install Django==2.1.7\n\n\nInstall Django CMS\n$ pip3 install django-cms\n\n\nStart Project\nCreate a Django project in your desired directory.\n\n$ cd /home\n$ django-admin.py startproject yourproject\n\n\nUpdate Settings.py File\nWe'll need to update settings.py  with a few things.\n\n$ cd yourproject/yourproject\n$ vim settings.py\n\n\nThe resulting changes to settings.py  are shown in the block below. These\nchanges include:\n\n * Adding the first 4 lines in INSTALLED_APPS. These apps are specific to Django\n   CMS.\n * Adding the LANGUAGES  block.\n * Update the DATABASES  block with your preferred database. We'll set up the\n   corresponding database and database user later.\n * Add SITE_ID.\n\nINSTALLED_APPS = [\n    'django.contrib.sites',\n    'cms',\n    'menus',\n    'treebeard',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nLANGUAGES = [\n    ('en-us', 'English')\n]\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'yourdb',\n        'USER': 'youruser',\n        'PASSWORD': 'yourpass123',\n        'HOST': '127.0.0.1',\n        'PORT': '5432',\n    }\n}\n\nSITE_ID = 1\n\n\nSetting up PostgreSQL\nIn our case, we'll be using Postgres as our database. We already installed\nPostgreSQL earlier; the only thing left to install is Python's psycopg2  library\nfor connecting to PostgreSQL databases:\n\n$ pip3 install psycopg2\n\n\nActivate the psql shell as the Postgres user:\n\n$ su - postgres\n$ psql\n\n\nCreate the Database and Database User\nCreate a database with a user, matching the information you entered in \nsettings.py  earlier:\n\nCREATE DATABASE yourdb;\nCREATE USER youruser WITH PASSWORD 'yourpass123';\nALTER ROLE youruser SET client_encoding TO 'utf8';\nALTER ROLE youruser SET default_transaction_isolation TO 'read committed';\nALTER ROLE youruser SET timezone TO 'EST';\nGRANT ALL PRIVILEGES ON DATABASE yourdb TO youruser;\n\n\nExit the PostgreSQL user.\n\n\\q\nexit\n\n\nMigrate the Database Changes\nStart the migration.\n\n$ python3 manage.py migrate\n\n\nCreate a Superuser\nCreate a user for the CMS.\n\n$ python3 manage.py createsuperuser\n\n\nValidation \nMake sure everything is ok.\n\n$ python3 manage.py cms check\n\n\nAt this point, you might notice a few errors related to sekizai. We'll need to\nupdate a few more things.\n\nUpdate Settings.py Again\nWe'll need to update settings.py  with a few things.\n\nAdd 'sekizai' to INSTALLED_APPS.\n\nINSTALLED_APPS = [\n    'django.contrib.sites',\n    'cms',\n    'menus',\n    'treebeard',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'sekizai',\n]\n\n\nAdd an additional 5 lines to MIDDLEWARE:\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'cms.middleware.user.CurrentUserMiddleware',\n    'cms.middleware.page.CurrentPageMiddleware',\n    'cms.middleware.toolbar.ToolbarMiddleware',\n    'cms.middleware.language.LanguageCookieMiddleware',\n]\n\n\nAdd 'sekizai.context_processors.sekizai',  and \n'django.template.context_processors.i18n',  to TEMPLATES:\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n                'sekizai.context_processors.sekizai',\n                'cms.context_processors.cms_settings',\n                'django.template.context_processors.i18n',\n           ],\n       },\n    },\n]\n\n\nFinally, add your box's IP address to ALLOWED_HOSTS:\n\nALLOWED_HOSTS = [\n    '000.000.00.00',\n]\n\n\nUpdate urls.py\nUpdate urls.py  to look like the following:\n\nfrom django.conf.urls import url, include\nfrom django.contrib import admin\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r'^', include('cms.urls')),\n]\n\n\nGive it a Go\nEverything should be good! Start the app on port 8000 by entering the following:\n\n$ python3 manage.py runserver 0.0.0.0:8000\n\n\nThe app should now be accessible at your box's IP address.","html":"<p>So you've selected Python as your language of choice for building a massive web app. Congratulations! While there are some that will point to Python's relatively slow execution at runtime, you're brave enough to realize that saving chunks of your time is more important to the success of your project than building your app in a marginally faster language.</p><p> There are a few options when it comes to picking an out-of-the-box CMS with Django, but perhaps none are as popular as <a href=\"https://www.django-cms.org/en/\"><em>Django CMS</em></a><em>.</em> <strong>DjangoCMS</strong> is the biggest of the \"big three\" CMS choices for Django, with <a href=\"http://mezzanine.jupo.org/\">Mezzanine</a> and <a href=\"https://wagtail.io/\">Wagtail</a> following close behind (I personally like Wagtail, if I'm gonna be real here).</p><p>As it stands, there doesn't seem to be an existing  guide which walks through the <em>complete</em> installation of <strong>Django</strong> + <strong>Django CMS</strong>. While is information exists in fragments, newcomers to Python could have trouble piecing much of this information together out of the gate. This guide hopes to serve as a reduction of friction to Django newcomers, thus ensuring Pythonic world domination.</p><h2 id=\"our-stack\">Our Stack</h2><p>To get this party started, our VPS is going to run the following stack:</p><ul><li>Ubuntu</li><li>Python 3</li><li>Virtualenv</li><li>PostgreSQL</li><li>Django</li><li>Django CMS</li></ul><h2 id=\"update-all-dependencies\">Update All Dependencies</h2><p>First, we'll prep our VPS with all the necessary dependencies:</p><!--kg-card-begin: code--><pre><code>$ apt-get update\n$ apt-get upgrade -y\n$ apt-get install python3 python3-pip python-dev libpq-dev postgresql postgresql-contrib\n</code></pre><!--kg-card-end: code--><h2 id=\"create-and-activate-a-virtual-environment\">Create and Activate a Virtual Environment</h2><p>Virtualenv is included with Python3, so no installation needed here.</p><!--kg-card-begin: code--><pre><code>$ virtualenv env\n$ source env/bin/activate\n</code></pre><!--kg-card-end: code--><h2 id=\"install-django\">Install Django</h2><p>Find out what the latest distribution is by going <a href=\"https://www.djangoproject.com/download/\">here</a>.</p><!--kg-card-begin: code--><pre><code>$ pip3 install Django==2.1.7\n</code></pre><!--kg-card-end: code--><h3 id=\"install-django-cms\">Install Django CMS</h3><!--kg-card-begin: code--><pre><code>$ pip3 install django-cms\n</code></pre><!--kg-card-end: code--><h2 id=\"start-project\">Start Project</h2><p>Create a Django project in your desired directory.</p><!--kg-card-begin: code--><pre><code>$ cd /home\n$ django-admin.py startproject yourproject\n</code></pre><!--kg-card-end: code--><h2 id=\"update-settings-py-file\">Update Settings.py File</h2><p>We'll need to update <code>settings.py</code> with a few things.</p><!--kg-card-begin: code--><pre><code>$ cd yourproject/yourproject\n$ vim settings.py\n</code></pre><!--kg-card-end: code--><p>The resulting changes to <code>settings.py</code> are shown in the block below. These changes include:</p><ul><li>Adding the first 4 lines in <strong>INSTALLED_APPS</strong>. These apps are specific to Django CMS.</li><li>Adding the <strong>LANGUAGES</strong> block.</li><li>Update the <strong>DATABASES</strong> block with your preferred database. We'll set up the corresponding database and database user later.</li><li>Add <strong>SITE_ID</strong>.</li></ul><!--kg-card-begin: code--><pre><code>INSTALLED_APPS = [\n    'django.contrib.sites',\n    'cms',\n    'menus',\n    'treebeard',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nLANGUAGES = [\n    ('en-us', 'English')\n]\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': 'yourdb',\n        'USER': 'youruser',\n        'PASSWORD': 'yourpass123',\n        'HOST': '127.0.0.1',\n        'PORT': '5432',\n    }\n}\n\nSITE_ID = 1\n</code></pre><!--kg-card-end: code--><h2 id=\"setting-up-postgresql\">Setting up PostgreSQL</h2><p>In our case, we'll be using Postgres as our database. We already installed PostgreSQL earlier; the only thing left to install is Python's <strong>psycopg2</strong> library for connecting to PostgreSQL databases:</p><!--kg-card-begin: code--><pre><code>$ pip3 install psycopg2\n</code></pre><!--kg-card-end: code--><p>Activate the psql shell as the Postgres user:</p><!--kg-card-begin: code--><pre><code>$ su - postgres\n$ psql\n</code></pre><!--kg-card-end: code--><h3 id=\"create-the-database-and-database-user\">Create the Database and Database User</h3><p>Create a database with a user, matching the information you entered in <code>settings.py</code> earlier:</p><!--kg-card-begin: code--><pre><code>CREATE DATABASE yourdb;\nCREATE USER youruser WITH PASSWORD 'yourpass123';\nALTER ROLE youruser SET client_encoding TO 'utf8';\nALTER ROLE youruser SET default_transaction_isolation TO 'read committed';\nALTER ROLE youruser SET timezone TO 'EST';\nGRANT ALL PRIVILEGES ON DATABASE yourdb TO youruser;\n</code></pre><!--kg-card-end: code--><p>Exit the PostgreSQL user.</p><!--kg-card-begin: code--><pre><code>\\q\nexit\n</code></pre><!--kg-card-end: code--><h3 id=\"migrate-the-database-changes\">Migrate the Database Changes</h3><p>Start the migration.</p><!--kg-card-begin: code--><pre><code>$ python3 manage.py migrate\n</code></pre><!--kg-card-end: code--><h3 id=\"create-a-superuser\">Create a Superuser</h3><p>Create a user for the CMS.</p><!--kg-card-begin: code--><pre><code>$ python3 manage.py createsuperuser\n</code></pre><!--kg-card-end: code--><h3 id=\"validation\">Validation </h3><p>Make sure everything is ok.</p><!--kg-card-begin: code--><pre><code>$ python3 manage.py cms check\n</code></pre><!--kg-card-end: code--><p>At this point, you might notice a few errors related to <em>sekizai</em>. We'll need to update a few more things.</p><h2 id=\"update-settings-py-again\">Update Settings.py Again</h2><p>We'll need to update <code>settings.py</code> with a few things.</p><p>Add 'sekizai' to <strong>INSTALLED_APPS</strong>.</p><!--kg-card-begin: code--><pre><code>INSTALLED_APPS = [\n    'django.contrib.sites',\n    'cms',\n    'menus',\n    'treebeard',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'sekizai',\n]\n</code></pre><!--kg-card-end: code--><p>Add an additional 5 lines to <strong>MIDDLEWARE</strong>:</p><!--kg-card-begin: code--><pre><code>MIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.locale.LocaleMiddleware',\n    'cms.middleware.user.CurrentUserMiddleware',\n    'cms.middleware.page.CurrentPageMiddleware',\n    'cms.middleware.toolbar.ToolbarMiddleware',\n    'cms.middleware.language.LanguageCookieMiddleware',\n]\n</code></pre><!--kg-card-end: code--><p>Add <code>'sekizai.context_processors.sekizai',</code> and <code>'django.template.context_processors.i18n',</code> to <strong>TEMPLATES</strong>:</p><!--kg-card-begin: code--><pre><code>TEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n                'sekizai.context_processors.sekizai',\n                'cms.context_processors.cms_settings',\n                'django.template.context_processors.i18n',\n           ],\n       },\n    },\n]\n</code></pre><!--kg-card-end: code--><p>Finally, add your box's IP address to <strong>ALLOWED_HOSTS</strong>:</p><!--kg-card-begin: code--><pre><code>ALLOWED_HOSTS = [\n    '000.000.00.00',\n]\n</code></pre><!--kg-card-end: code--><h2 id=\"update-urls-py\">Update urls.py</h2><p>Update <strong>urls.py</strong> to look like the following:</p><!--kg-card-begin: code--><pre><code>from django.conf.urls import url, include\nfrom django.contrib import admin\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r'^', include('cms.urls')),\n]\n</code></pre><!--kg-card-end: code--><h2 id=\"give-it-a-go\">Give it a Go</h2><p>Everything should be good! Start the app on port 8000 by entering the following:</p><!--kg-card-begin: code--><pre><code>$ python3 manage.py runserver 0.0.0.0:8000\n</code></pre><!--kg-card-end: code--><p>The app should now be accessible at your box's IP address.</p>","url":"https://hackersandslackers.com/installing-django-cms/","uuid":"802f191e-c459-4bdc-86cc-088d3727a324","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a12160a2a6bec285f845812"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673634","title":"Starting an ExpressJS App","slug":"create-an-expressjs-app","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/expressjs@2x.jpg","excerpt":"Installation guide for ExpressJS with popular customization options.","custom_excerpt":"Installation guide for ExpressJS with popular customization options.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"07 March, 2019","created_at":"2017-11-18T08:44:44.000-05:00","published_at":"2017-11-18T08:54:54.000-05:00","updated_at":"2019-03-07T01:39:46.000-05:00","meta_title":"Starting an ExpressJS App | Hackers and Slackers","meta_description":"Installation guide for ExpressJS with popular customization options","og_description":"Installation guide for ExpressJS with popular customization options","og_image":"https://hackersandslackers.com/content/images/2017/11/expressjs@2x.jpg","og_title":"Starting an ExpressJS App","twitter_description":"Installation guide for ExpressJS with popular customization options","twitter_image":"https://hackersandslackers.com/content/images/2017/11/expressjs@2x.jpg","twitter_title":"Starting an ExpressJS App","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"}],"plaintext":"Over the past few months I've found myself spinning up tons of new servers and\nwalking through the process of pushing express apps live.\n\nBecause this process always comes at unexpected times, I've never bothered\ndocumenting the steps it takes to get a blank box running express. Surprisingly\nit seems as though few have bothered to walk through every step involved in a\nsingle place, and most express tutorials contain outdated information which\nlikely leads to headaches for newcomers.\n\nI'll be walking through an ExpressJS setup with the tech I always opt for.\n\nStack\n * NodeJS\n * NPM\n * Nginx\n * Express\n * Express-Generator\n * Sass\n * Handlebars\n * PM2\n * Grunt\n\nInstallation\nPrep your server by installing the latest updates:\n\n$ apt-get update\n$ apt-get upgrade -y\n\n\nNodeJS\nOf the things we'll be installing prior to development, NodeJS is the trickiest.\nUnlike other packages, we cannot simply use Ubuntu's apt-get install  command\nfor Node. NodeJS for Linux distributions is best installed via NodeSource, which\nis handled as such:\n\n$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n\n\nNote that this also hooks us up with the latest version of NPM, so we're all\ngood on that front.\n\nNginx\nLet's install Nginx before we forget. We'll come back to Nginx later to set up\nthe config.\n\napt-get install nginx\n\n\nExpress\nWe'll install express globally. This way we can support multiple apps running\nexpress, as well as use an express generator to easily create more projects down\nthe line.\n\nnpm install -g express\n\n\nExpress generator\nInstall express generator globally. Express generator is the easiest way to set\nup a new express app with the standard structure preconfigured.\n\nnpm install -g express-generator\n\n\nCreate an App\nNow we get to the fun stuff.\n\n$ cd  to the directory you'll be using to contain your apps, such as /home  or \n/var/www. When we use express-generator  in this directory, we'll be\ninitializing a new express project will most of the common boilerplate setup\npreconfigured.\n\nThis will create your Express app inside /home/myapp:\n\ncd /home\n\nexpress --view=hbs --css=less myapp\n\n\nExpress-generator prompts you to pass arguments to automatically configure your\nproject for you. We're able to specify which CSS preprocessor and templating\nsystem we prefer this way. I'm going to create a project using Handlebars and\nSass as my weapons of choice.\n\nThese are the options that come with express-generator in case you'd like to\ncustomize your installation:\n\n$ express -h\n\n  Usage: express [options] [dir]\n\n  Options:\n\n    -h, --help          output usage information\n        --version       output the version number\n    -e, --ejs           add ejs engine support\n        --hbs           add handlebars engine support\n        --pug           add pug engine support\n    -H, --hogan         add hogan.js engine support\n    -v, --view <engine> add view <engine> support (ejs|hbs|hjs|jade|pug|twig|vash) (defaults to jade)\n    -c, --css <engine>  add stylesheet <engine> support (less|stylus|compass|sass) (defaults to plain css)\n        --git           add .gitignore\n    -f, --force         force on non-empty directory\n\n\nWarning: common bug ahead\nIf your life sucks, running express generator may have given you this error:\n\n/usr/bin/env: ‘node’: No such file or directory\n\n\nThis is an issue with Debian distributions of linux which treat 'node' and\n'nodejs' as separate filepaths. To alleviate this, create the following symbolic\nlink:\n\nln -s /usr/bin/nodejs /usr/bin/node\n\n\nStart App\nInside your project directory run npm install  to install all dependancies for\nyour project. This will look at the package.json  file that express-generator\ncreated and will install the corresponding node modules.\n\ncd myapp\n\nnpm install\n\n\nHere's the result:\n\n.\n├── app.js\n├── bin\n│   └── www\n├── package.json\n├── public\n│   ├── images\n│   ├── javascripts\n│   └── stylesheets\n│       └── style.css\n├── routes\n│   ├── index.js\n│   └── users.js\n└── views\n    ├── error.pug\n    ├── index.pug\n    └── layout.pug\n\n7 directories, 9 files\n\n\nSet up Nginx Config\nBefore creating your Nginx config, it is best to verify which port express will\nbe running on. In most recent versions of express, this can be found in the www\nfile. The path to this looks like myapp/bin/www.\n\nCheck out that file and see what the value is for var port.\n\nvim bin/www\n\n\nYou should see something like this:\n\nvar port = normalizePort(process.env.PORT || '3000');\n\n\nThus, the port is 3000. Remember this.\n\nPreviously this information was stored in Express's app.js  file.\n\nNow, create a Nginx config in sites-available:\n\nvim /etc/nginx/sites-available/myapp\n\n\nFor a basic reverse proxy server configuration, use the configuration below.\n\nBe sure to replace the port with the port you found earlier.\n\nserver {\n    listen 80;\n\n    server_name example.com www.example.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:3000; #Replace port here\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n\n    location ~ /.well-known {\n        allow all;\n    }\n\n    client_max_body_size 50m;\n}\n\n\nSave this out. Now create a symbolic link to have this appear in sites enabled,\nand restart Nginx.\n\nln -s /etc/nginx/sites-available/myapp /etc/nginx/sites-enabled/myapp \n\nservice nginx restart\n\n\nRun your app\nNow your web server is pointing to the correct place, but your app isn't\nactually running. To keep your app running continuously, we'll use PM2.\n\nInstall pm2 globally:\n\nnpm install pm2 -g\n\n\nGo to your app's directory, and start your app using PM2:\n\ncd /home/myapp\n\npm2 start bin/www\n\n\nIf successful, PM2 should then list your app as running.\n\nYou can run as many express apps on one server as you like:\n\n┌──────────┬────┬──────┬───────┬────────┬─────────┬────────┬─────┬───────────┬──────┬──────────┐\n│ App name │ id │ mode │ pid   │ status │ restart │ uptime │ cpu │ mem       │ user │ watching │\n├──────────┼────┼──────┼───────┼────────┼─────────┼────────┼─────┼───────────┼──────┼──────────┤\n│ www      │ 0  │ fork │ 8953  │ online │ 79      │ 35h    │ 0%  │ 44.6 MB   │ root │ disabled │\n│ www      │ 1  │ fork │ 18195 │ online │ 0       │ 3D     │ 0%  │ 22.3 MB   │ root │ disabled │\n│ www      │ 2  │ fork │ 19990 │ online │ 0       │ 33h    │ 0%  │ 49.2 MB   │ root │ disabled │\n└──────────┴────┴──────┴───────┴────────┴─────────┴────────┴─────┴───────────┴──────┴──────────┘\n\n\nYour app should now be accessible via your domain.\n\nFinal items\nYour app is up and running, but you're not in the clear yet. To have things\nfully configured, you'll need to set up grunt or gulp to compress your source\nfor production.\n\nConfiguring gulp files is a tutorial in its own right. Otherwise, you're good to\ngo to serve static files directly out of the /public  folder.","html":"<p>Over the past few months I've found myself spinning up tons of new servers and walking through the process of pushing express apps live.</p><p>Because this process always comes at unexpected times, I've never bothered documenting the steps it takes to get a blank box running express. Surprisingly it seems as though few have bothered to walk through every step involved in a single place, and most express tutorials contain outdated information which likely leads to headaches for newcomers.</p><p>I'll be walking through an ExpressJS setup with the tech I always opt for.</p><h2 id=\"stack\">Stack</h2><ul><li>NodeJS</li><li>NPM</li><li>Nginx</li><li>Express</li><li>Express-Generator</li><li>Sass</li><li>Handlebars</li><li>PM2</li><li>Grunt</li></ul><h2 id=\"installation\">Installation</h2><p>Prep your server by installing the latest updates:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt-get update\n$ apt-get upgrade -y\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"nodejs\">NodeJS</h3><p>Of the things we'll be installing prior to development, NodeJS is the trickiest. Unlike other packages, we cannot simply use Ubuntu's <code>apt-get install</code> command for Node. NodeJS for Linux distributions is best installed via NodeSource, which is handled as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n</code></pre>\n<!--kg-card-end: markdown--><p>Note that this also hooks us up with the latest version of NPM, so we're all good on that front.</p><h3 id=\"nginx\">Nginx</h3><p>Let's install Nginx before we forget. We'll come back to Nginx later to set up the config.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">apt-get install nginx\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"express\">Express</h3><p>We'll install express globally. This way we can support multiple apps running express, as well as use an express generator to easily create more projects down the line.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install -g express\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"express-generator\">Express generator</h3><p>Install express generator globally. Express generator is the easiest way to set up a new express app with the standard structure preconfigured.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install -g express-generator\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"create-an-app\">Create an App</h2><p>Now we get to the fun stuff.</p><p><code>$ cd</code> to the directory you'll be using to contain your apps, such as <code>/home</code> or <code>/var/www</code>. When we use <code>express-generator</code> in this directory, we'll be initializing a new express project will most of the common boilerplate setup preconfigured.</p><p>This will create your Express app inside <code>/home/myapp</code>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">cd /home\n\nexpress --view=hbs --css=less myapp\n</code></pre>\n<!--kg-card-end: markdown--><p>Express-generator prompts you to pass arguments to automatically configure your project for you. We're able to specify which CSS preprocessor and templating system we prefer this way. I'm going to create a project using Handlebars and Sass as my weapons of choice.</p><p>These are the options that come with express-generator in case you'd like to customize your installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ express -h\n\n  Usage: express [options] [dir]\n\n  Options:\n\n    -h, --help          output usage information\n        --version       output the version number\n    -e, --ejs           add ejs engine support\n        --hbs           add handlebars engine support\n        --pug           add pug engine support\n    -H, --hogan         add hogan.js engine support\n    -v, --view &lt;engine&gt; add view &lt;engine&gt; support (ejs|hbs|hjs|jade|pug|twig|vash) (defaults to jade)\n    -c, --css &lt;engine&gt;  add stylesheet &lt;engine&gt; support (less|stylus|compass|sass) (defaults to plain css)\n        --git           add .gitignore\n    -f, --force         force on non-empty directory\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"warning-common-bug-ahead\">Warning: common bug ahead</h2><p>If your life sucks, running express generator may have given you this error:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">/usr/bin/env: ‘node’: No such file or directory\n</code></pre>\n<!--kg-card-end: markdown--><p>This is an issue with Debian distributions of linux which treat 'node' and 'nodejs' as separate filepaths. To alleviate this, create the following symbolic link:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">ln -s /usr/bin/nodejs /usr/bin/node\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"start-app\">Start App</h2><p>Inside your project directory run <strong>npm install</strong> to install all dependancies for your project. This will look at the <code>package.json</code> file that express-generator created and will install the corresponding node modules.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">cd myapp\n\nnpm install\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's the result:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">.\n├── app.js\n├── bin\n│   └── www\n├── package.json\n├── public\n│   ├── images\n│   ├── javascripts\n│   └── stylesheets\n│       └── style.css\n├── routes\n│   ├── index.js\n│   └── users.js\n└── views\n    ├── error.pug\n    ├── index.pug\n    └── layout.pug\n\n7 directories, 9 files\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"set-up-nginx-config\">Set up Nginx Config</h2><p>Before creating your Nginx config, it is best to verify which port express will be running on. In most recent versions of express, this can be found in the www file. The path to this looks like myapp/bin/www.</p><p>Check out that file and see what the value is for var port.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">vim bin/www\n</code></pre>\n<!--kg-card-end: markdown--><p>You should see something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var port = normalizePort(process.env.PORT || '3000');\n</code></pre>\n<!--kg-card-end: markdown--><p>Thus, the port is 3000. Remember this.</p><p>Previously this information was stored in Express's <code>app.js</code> file.</p><p>Now, create a Nginx config in sites-available:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">vim /etc/nginx/sites-available/myapp\n</code></pre>\n<!--kg-card-end: markdown--><p>For a basic reverse proxy server configuration, use the configuration below.</p><p>Be sure to replace the port with the port you found earlier.</p><!--kg-card-begin: markdown--><pre><code class=\"language-nginx\">server {\n    listen 80;\n\n    server_name example.com www.example.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:3000; #Replace port here\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n\n    location ~ /.well-known {\n        allow all;\n    }\n\n    client_max_body_size 50m;\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Save this out. Now create a symbolic link to have this appear in sites enabled, and restart Nginx.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">ln -s /etc/nginx/sites-available/myapp /etc/nginx/sites-enabled/myapp \n\nservice nginx restart\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"run-your-app\">Run your app</h2><p>Now your web server is pointing to the correct place, but your app isn't actually running. To keep your app running continuously, we'll use PM2.</p><p>Install pm2 globally:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install pm2 -g\n</code></pre>\n<!--kg-card-end: markdown--><p>Go to your app's directory, and start your app using PM2:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">cd /home/myapp\n\npm2 start bin/www\n</code></pre>\n<!--kg-card-end: markdown--><p>If successful, PM2 should then list your app as running.</p><p>You can run as many express apps on one server as you like:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">┌──────────┬────┬──────┬───────┬────────┬─────────┬────────┬─────┬───────────┬──────┬──────────┐\n│ App name │ id │ mode │ pid   │ status │ restart │ uptime │ cpu │ mem       │ user │ watching │\n├──────────┼────┼──────┼───────┼────────┼─────────┼────────┼─────┼───────────┼──────┼──────────┤\n│ www      │ 0  │ fork │ 8953  │ online │ 79      │ 35h    │ 0%  │ 44.6 MB   │ root │ disabled │\n│ www      │ 1  │ fork │ 18195 │ online │ 0       │ 3D     │ 0%  │ 22.3 MB   │ root │ disabled │\n│ www      │ 2  │ fork │ 19990 │ online │ 0       │ 33h    │ 0%  │ 49.2 MB   │ root │ disabled │\n└──────────┴────┴──────┴───────┴────────┴─────────┴────────┴─────┴───────────┴──────┴──────────┘\n</code></pre>\n<!--kg-card-end: markdown--><p>Your app should now be accessible via your domain.</p><h2 id=\"final-items\">Final items</h2><p>Your app is up and running, but you're not in the clear yet. To have things fully configured, you'll need to set up grunt or gulp to compress your source for production.</p><p>Configuring gulp files is a tutorial in its own right. Otherwise, you're good to go to serve static files directly out of the <code>/public</code> folder.</p>","url":"https://hackersandslackers.com/create-an-expressjs-app/","uuid":"048b6212-5dbf-429c-b86d-d3fc65238e06","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a10394c3858167c7082486e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673633","title":"Installing Django on Ubuntu","slug":"installing-django-on-a-linux-box","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/django2-1.jpg","excerpt":"Get started with the Python MVC framework that started it all.","custom_excerpt":"Get started with the Python MVC framework that started it all.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"28 March, 2019","created_at":"2017-11-18T06:07:00.000-05:00","published_at":"2017-11-18T06:12:07.000-05:00","updated_at":"2019-03-28T04:43:32.000-04:00","meta_title":"Installing Django on Ubuntu | Hackers and Slackers","meta_description":"Get started with the Python MVC framework that started it all","og_description":"Get started with the Python MVC framework that started it all","og_image":"https://hackersandslackers.com/content/images/2019/03/django2-1.jpg","og_title":"Installing Django on Ubuntu","twitter_description":"Get started with the Python MVC framework that started it all","twitter_image":"https://hackersandslackers.com/content/images/2019/03/django2-1.jpg","twitter_title":"Installing Django on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Getting into Django","slug":"starting-django","description":"Getting started with Django: the original daddy of Python frameworks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/django2.jpg","meta_description":"Getting started with Django: the original daddy of Python frameworks.","meta_title":"Setting up Django","visibility":"internal"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"Django is the OG Grandaddy of all Python frameworks: it's by far Python's most\nfully-featured MVC framework out of the box. Today we're going to look at the\nrelatively painless process of setting up Django on a Ubuntu server.\n\nStack\n * Ubuntu\n * Python 3\n * Pip 3\n * Nginx\n * Django (latest)\n\nInstall all Dependencies\nWe'll start with the obligatory update to ensure we're getting the latest\npackages.\n\n$ apt-get update\n$ apt-get upgrade\n\n\nLet's verify that the latest version of Python 3 is installed on your box:\n\n$ python3 --version\nPython 3.6.3\n\n\nRegardless, it's probably a good idea to download the latest anyway:\n\napt-get install python3 python3-pip python3-dev\n\n\nI highly recommend setting up a Python virtual environment before moving forward\nwith any installs. If you're living in the stone age, virtualenv  and \nvirtualenvwrapper  will probably server you fine. If you're a gentleman, use \npipenv  or poetry  instead.\n\nNow let's go ahead and install Django. We can find out what the latest\ndistribution is by checking Django's download page: \nhttps://www.djangoproject.com/download/\n\nWith the version number in hand, we'll install Django using pip:\n\npip3 install Django==2.1.7\n\n\nNote that we’re using pip3  here as opposed to pip, which explicitly downloads\nDjango under our Python3 installation. If we’re installing inside our virtual\nenvironment, specifying pip3  is redundant; pip  will work under the assumption\nthat we’re using the only Python version installed to our environment.\n\nLet's verify that you've installed Django correctly. Open your Python3 shell and\ninput the following:\n\n$ python3\n>>> import django\n>>> print(django.get_version())\n1.11\n\n\nIf you receive an error along the lines of ModuleNotFoundError: No module named\n'Django', Django was probably installed on Python2 as opposed to 3. Make sure\nthat you used pip3 instead of pip to install Django, and try again.\n\nWhy is This Somewhat Convoluted?\nAll major Linux distributions come with Python 2.7 pre-installed. Python2 is\nstill critical to the core functionality of most linux distributions, therefore\nPython2 must be left intact and cannot be deleted or modified without suffering\ndamage to the operating system.\n\nUnfortunately, the python and pip commands will forever refer to Python 2 as a\nresult, thus forcing Python 3 users to forever utilize the python3  and pip3 \ncommands. This isn't that big of a deal, but is a common pitfall for those\nswitching over to the light side.","html":"<p>Django is the OG Grandaddy of all Python frameworks: it's by far Python's most fully-featured MVC framework out of the box. Today we're going to look at the relatively painless process of setting up Django on a Ubuntu server.</p><h3 id=\"stack\">Stack</h3><ul><li>Ubuntu</li><li>Python 3</li><li>Pip 3</li><li>Nginx</li><li>Django (latest)</li></ul><h2 id=\"install-all-dependencies\">Install all Dependencies</h2><p>We'll start with the obligatory update to ensure we're getting the latest packages.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt-get update\n$ apt-get upgrade\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's verify that the latest version of Python 3 is installed on your box:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ python3 --version\nPython 3.6.3\n</code></pre>\n<!--kg-card-end: markdown--><p>Regardless, it's probably a good idea to download the latest anyway:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">apt-get install python3 python3-pip python3-dev\n</code></pre>\n<!--kg-card-end: markdown--><p>I highly recommend setting up a Python virtual environment before moving forward with any installs. If you're living in the stone age, <code>virtualenv</code> and <code>virtualenvwrapper</code> will probably server you fine. If you're a gentleman, use <code>pipenv</code> or <code>poetry</code> instead.</p><p>Now let's go ahead and install Django. We can find out what the latest distribution is by checking Django's download page: <a href=\"https://www.djangoproject.com/download/\">https://www.djangoproject.com/download/</a></p><p>With the version number in hand, we'll install Django using pip:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">pip3 install Django==2.1.7\n</code></pre>\n<!--kg-card-end: markdown--><p>Note that we’re using <strong>pip3</strong> here as opposed to <strong>pip</strong>, which explicitly downloads Django under our Python3 installation. If we’re installing inside our virtual environment, specifying <strong>pip3</strong> is redundant; <strong>pip</strong> will work under the assumption that we’re using the only Python version installed to our environment.</p><p>Let's verify that you've installed Django correctly. Open your Python3 shell and input the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ python3\n&gt;&gt;&gt; import django\n&gt;&gt;&gt; print(django.get_version())\n1.11\n</code></pre>\n<!--kg-card-end: markdown--><p>If you receive an error along the lines of <code>ModuleNotFoundError: No module named 'Django'</code>, Django was probably installed on Python2 as opposed to 3. Make sure that you used pip3 instead of pip to install Django, and try again.</p><h2 id=\"why-is-this-somewhat-convoluted\">Why is This Somewhat Convoluted?</h2><p>All major Linux distributions come with Python 2.7 pre-installed. Python2 is still critical to the core functionality of most linux distributions, therefore Python2 must be left intact and cannot be deleted or modified without suffering damage to the operating system.</p><p>Unfortunately, the python and pip commands will forever refer to Python 2 as a result, thus forcing Python 3 users to forever utilize the <em>python3</em> and <em>pip3</em> commands. This isn't that big of a deal, but is a common pitfall for those switching over to the light side.</p>","url":"https://hackersandslackers.com/installing-django-on-a-linux-box/","uuid":"77609409-5552-418d-b742-c549a2ccf01b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a101454d201b772c140d36e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673632","title":"Merge Sets of Data in Python Using Pandas","slug":"merge-dataframes-with-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","excerpt":"Perform SQL-like merges of data using Python's Pandas.","custom_excerpt":"Perform SQL-like merges of data using Python's Pandas.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"26 December, 2018","created_at":"2017-11-17T19:09:32.000-05:00","published_at":"2017-11-17T19:22:25.000-05:00","updated_at":"2018-12-26T04:29:22.000-05:00","meta_title":"Merging Dataframes with Pandas | Hackers and Slackers","meta_description":"Perform merges of data similar to SQL JOINs using Python's Pandas library: the essential library for data analysis in Oython. ","og_description":"Perform SQL-like merges of data using Python's Pandas","og_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","og_title":"Merging Dataframes with Pandas","twitter_description":"Perform SQL-like merges of data using Python's Pandas","twitter_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","twitter_title":"Merging Dataframes with Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you have two obscenely large sets of data. \n\nThese sets of data contain information on a similar topic, such as customers. \nDataset #1 might contain a high-level view of all customers of a business, while\n Datatset #2  contains a lifetime history of orders for a company.\nUnsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any\nbusiness' orders are made by customers.\n\nWelcome to Relational Databases\nWhat we just described is the core foundation for relational databases  which\nhave been running at the core of businesses since the 1970s. Starting with\nfamiliar names like MySQL,  Oracle, and Postgres,  the concept of maintaining\nmultiple -related- tables of data are the bare minimum technology stack for any\ncompany, regardless of what said company does.\n\nWhile our example of Datasets #1 and #2  can be thought of as isolated tables,\nthe process of 'joining' them (in SQL terms) or 'merging' them (in Pandas terms) \n is  trivial. What's more, we can do far more than with JOINS (or merges) than\nsimply combining our data into a single set.\n\nEnter The Panda\nPython happens to have an obscenely popular library for performing SQL-like\nlogic, dubbed Pandas. If it remains unclear as to what Pandas is, just remember:\n Databases are basically Excel spreadsheets are basically an interface for\nPandas. The technicality of that explanation may be horrendous to those who\nunderstand the differences, but the fundamental truth remains: we're dealing\nwith information, inside of cells, on a two-dimensional grid. When you hear the\nnext idiot spew a catch phrase like \"data is the new oil\", the \"data\" they're\nreferring to is akin to that sick Excel sheet you made at work.\n\nScenario: Finding Mismatches in Data\nThis scenario actually stems from a real-life example which, sure enough, was my\nfirst encounter with Pandas. One could argue I owe much 0f my data career to a\n3am Google Hangout with Snkia.\n\nIn our scenario, our company has signed up for a very expensive software product\nwhich charges by individual license. To our surprise, the number of licenses for\nthis software totaled over 1000  seats!  After giving this data a quick glance,\nhowever, it's clear that many of these employees have actually been terminated,\nthus resulting in unspeakable loss in revenue. \n\nThe good news is we have another dataset called active employees (aka: employees\nwhich have not been terminated... yet). So, how do we use these two sets of data\nto determine which software licenses are valid? First, let's look at the types \nof ways we can merge data in Pandas.\n\nTerminology\nMERGE\nSets of data can be merged in a number of ways. Merges can either be used to\nfind similarities in two Dataframes and merge associated information, or may be\nentirely non-destructive in the way that two sets of data are merged.\n\nKEY\nIn many cases (such as the one in this tutorial) you'd likely want to merge two\nDataframes based on the value of a key. A key is the authoritative column by\nwhich the Dataframes will be merged. When merging Dataframes in this way, keys\nwill stay in tact as an identifier while the values of columns in the same row\nassociated to that key.\n\nThis type of merge can be used when two Dataframes hold differing fields for\nsimilar rows. If Dataframe 1 contains the phone numbers of customers by name,\nand Dataframe 2 contains emails of a similar grouping of people, these two may\nbe merged to create a single collection of data with all of this information.\n\nAXIS\nA parameter of pandas functions which determines whether the function should be\nrun against a Dataframe's columns or rows. An axis of 0 determines that the\naction will be taken on a per-row basis, where an axis of 1 denotes column.\n\nFor example, performing a drop with axis 0 on key X will drop the row where\nvalue of a cell is equal to X.\n\nLEFT/RIGHT MERGE\nAn example of a left/right merge can be seen below:\n\nJoin on keys found in left Dataframe.The two data frames above hold similar keys\nwith different associated information per axis, thus the result is a combination\nof these two Dataframes where the keys remain intact.\n\n\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from\nboth Dataframes do not match 1-to-1, specifying a left/right merge determines\nwhich Dataframe's keys will be considered the authority to be preserved in the\nmerge.\n\nJoin on keys found in right Dataframe.INNER MERGE\nAn inner merge will merge two Dataframes based on overlap of values between keys\nin both Dataframes:\n\nJoin on keys found in right Dataframe.OUTER MERGE\nAn outer merge will preserve the most data by not dropping keys which are\nuncommon to both Dataframes. Keys which exist in a single Dataframe will be\nadded to the resulting Dataframe, with empty values populated for any columns\nbrought in by the other Dataframe:\n\nBack to our Scenario: Merging Two Dataframes via Left Merge\nLet's get it going. Enter the iPython shell.\n\nImport Pandas and read both of your CSV files.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"csv1.csv\")  \ndf2 = pd.read_csv(\"csv2.csv\")\n\n\nThe above opens the CSVs as Dataframes recognizable by pandas.\nNext, we'll merge the two CSV files.\n\nHow  specifies the type of merge, and on  specifies the column to merge by\n(key). The key must be present in both Dataframes.\n\nFor the purpose of this exercise we'll be merging left, as that is the CSV which\ncontains the keys we'd like to maintain.\n\nmergedDF = df2.merge(df, how=“left”, on=\"email\")\n\nprint(mergedDF)\n\n\nThis should return a dataset of all common rows, with columns from both CSVs\nincluded in the merge.\n\nScenario 2: Missing Data\nBefore we go, let's toss in another scenario for good measure.\n\nThis time around we have two datasets which should actually probably be a single\ndataset. Dataset #1  contains all customers once again, but for some reason, \nDataset #1  contains email address where set Dataset #2  does not. Similarly, \nDataset #2  contains addresses which are  missing in Dataset #1. We assume there\nis no reason to keep these sets of data isolated other than human error.\n\nIn the case where we are confident that employees exist in both datasets but\ncontain different information, performing an inner  merge will join these two\nsets by a key such as customer ID or email. If all goes well, the final dataset\nshould equal the same number of rows found in both Datasets #1 and #2.\n\nDocumentation\nFor more on merging, check out the official Pandas documentation here\n[https://pandas.pydata.org/pandas-docs/stable/merging.html].","html":"<style>\n    img {\n        border: 0 !important;\n    }\n </style>   <p>Let's say you have two obscenely large sets of data. </p><p>These sets of data contain information on a similar topic, such as customers. <strong>Dataset #1 </strong>might contain a high-level view of all customers of a business, while <strong>Datatset #2</strong> contains a lifetime history of orders for a company. Unsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any business' orders are made by customers.</p><h2 id=\"welcome-to-relational-databases\">Welcome to Relational Databases</h2><p>What we just described is the core foundation for <em>relational databases</em> which have been running at the core of businesses since the 1970s. Starting with familiar names like <strong>MySQL</strong>,<strong> Oracle</strong>, and <strong>Postgres,</strong> the concept of maintaining multiple -<em>related- </em>tables of data are the bare minimum technology stack for any company, regardless of what said company does.</p><p>While our example of <strong>Datasets #1 and #2</strong> can be thought of as isolated tables, the process of 'joining' them <em>(in SQL terms) </em>or 'merging' them <em>(in Pandas terms)</em> is  trivial. What's more, we can do far more than with JOINS (or merges) than simply combining our data into a single set.</p><h3 id=\"enter-the-panda\">Enter The Panda</h3><p>Python happens to have an obscenely popular library for performing SQL-like logic, dubbed <strong>Pandas. </strong>If it remains unclear as to what Pandas is, just remember: <em>Databases are basically Excel spreadsheets are basically an interface for Pandas</em>. The technicality of that explanation may be horrendous to those who understand the differences, but the fundamental truth remains: we're dealing with information, inside of cells, on a two-dimensional grid. When you hear the next idiot spew a catch phrase like <em>\"data is the new oil\"</em>, the \"data\" they're referring to is akin to that sick Excel sheet you made at work.</p><h3 id=\"scenario-finding-mismatches-in-data\">Scenario: Finding Mismatches in Data</h3><p>This scenario actually stems from a real-life example which, sure enough, was my first encounter with Pandas. One could argue I owe much 0f my data career to a 3am Google Hangout with Snkia.</p><p>In our scenario, our company has signed up for a very expensive software product which charges by individual license. To our surprise, the number of licenses for this software totaled <strong>over 1000</strong> seats!<strong> </strong>After giving this data a quick glance, however, it's clear that many of these employees have actually been terminated, thus resulting in unspeakable loss in revenue. </p><p>The good news is we have another dataset called <em>active employees </em>(aka: employees which have not been terminated... yet). So, how do we use these two sets of data to determine which software licenses are valid? First, let's look at the <em>types</em> of ways we can merge data in Pandas.</p><h2 id=\"terminology\">Terminology</h2><h3 id=\"merge\">MERGE</h3><p>Sets of data can be merged in a number of ways. Merges can either be used to find similarities in two Dataframes and merge associated information, or may be entirely non-destructive in the way that two sets of data are merged.</p><h3 id=\"key\">KEY</h3><p>In many cases (such as the one in this tutorial) you'd likely want to merge two Dataframes based on the value of a key. A key is the authoritative column by which the Dataframes will be merged. When merging Dataframes in this way, keys will stay in tact as an identifier while the values of columns in the same row associated to that key.</p><p>This type of merge can be used when two Dataframes hold differing fields for similar rows. If Dataframe 1 contains the phone numbers of customers by name, and Dataframe 2 contains emails of a similar grouping of people, these two may be merged to create a single collection of data with all of this information.</p><h3 id=\"axis\">AXIS</h3><p>A parameter of pandas functions which determines whether the function should be run against a Dataframe's columns or rows. An axis of 0 determines that the action will be taken on a per-row basis, where an axis of 1 denotes column.</p><p>For example, performing a drop with axis 0 on key X will drop the row where value of a cell is equal to X.</p><h3 id=\"left-right-merge\">LEFT/RIGHT MERGE</h3><p>An example of a left/right merge can be seen below:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasleftjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in left Dataframe.</figcaption></figure><p>The two data frames above hold similar keys with different associated information per axis, thus the result is a combination of these two Dataframes where the keys remain intact.</p><p>\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from both Dataframes do not match 1-to-1, specifying a left/right merge determines which Dataframe's keys will be considered the authority to be preserved in the merge.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasrightjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"inner-merge\">INNER MERGE</h3><p>An inner merge will merge two Dataframes based on overlap of values between keys in both Dataframes:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasinnerjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"outer-merge\">OUTER MERGE</h3><p>An outer merge will preserve the most data by not dropping keys which are uncommon to both Dataframes. Keys which exist in a single Dataframe will be added to the resulting Dataframe, with empty values populated for any columns brought in by the other Dataframe:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasouterjoin.png\" class=\"kg-image\"></figure><h2 id=\"back-to-our-scenario-merging-two-dataframes-via-left-merge\">Back to our Scenario: Merging Two Dataframes via Left Merge</h2><p>Let's get it going. Enter the iPython shell.</p><p>Import Pandas and read both of your CSV files.</p><pre><code class=\"language-python\">import pandas as pd\n\ndf = pd.read_csv(&quot;csv1.csv&quot;)  \ndf2 = pd.read_csv(&quot;csv2.csv&quot;)\n</code></pre>\n<p>The above opens the CSVs as Dataframes recognizable by pandas.<br>Next, we'll merge the two CSV files.</p><p><strong>How</strong> specifies the type of merge, and <strong>on</strong> specifies the column to merge by (key). The key must be present in both Dataframes.</p><p>For the purpose of this exercise we'll be merging left, as that is the CSV which contains the keys we'd like to maintain.</p><pre><code class=\"language-python\">mergedDF = df2.merge(df, how=“left”, on=&quot;email&quot;)\n\nprint(mergedDF)\n</code></pre>\n<p>This should return a dataset of all common rows, with columns from both CSVs included in the merge.</p><h2 id=\"scenario-2-missing-data\">Scenario 2: Missing Data</h2><p>Before we go, let's toss in another scenario for good measure.</p><p>This time around we have two datasets which should actually probably be a single dataset. <strong>Dataset #1</strong> contains all customers once again, but for some reason, <strong>Dataset #1</strong> contains email address where set <strong>Dataset #2</strong> does not. Similarly, <strong>Dataset #2</strong> contains addresses which are  missing in <strong>Dataset #1</strong>. We assume there is no reason to keep these sets of data isolated other than human error.</p><p>In the case where we are confident that employees exist in both datasets but contain different information, performing an <em>inner</em> merge will join these two sets by a key such as customer ID or email. If all goes well, the final dataset should equal the same number of rows found in both <strong>Datasets #1 and #2</strong>.</p><h2 id=\"documentation\">Documentation</h2><p>For more on merging, check out the official Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/merging.html\">here</a>.</p>","url":"https://hackersandslackers.com/merge-dataframes-with-pandas/","uuid":"d2c59476-d879-484c-b719-55f53b3d4980","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f7a3ce38d612cc8261316"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867362f","title":"Generating Tree Hierarchies with Treelib","slug":"creating-trees-in-treelib","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","excerpt":"Using Python to visualize file hierarchies as trees.","custom_excerpt":"Using Python to visualize file hierarchies as trees.","created_at_pretty":"17 November, 2017","published_at_pretty":"17 November, 2017","updated_at_pretty":"28 March, 2019","created_at":"2017-11-17T15:45:10.000-05:00","published_at":"2017-11-17T15:56:40.000-05:00","updated_at":"2019-03-28T05:02:39.000-04:00","meta_title":"Tree Hierarchies with Treelib | Hackers and Slackers","meta_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","og_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","og_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","og_title":"Tree Hierarchies with Treelib","twitter_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","twitter_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","twitter_title":"Tree Hierarchies with Treelib","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"The first part of understanding any type of software is taking a glance at its\nfile structure. It may seem like an outlandish and redundant statement to make\nto a generation who grew up on GUIs. GitHub is essentially no more than a GUI\nfor Git, so it’s unsurprisingly that one of the largest company to follow a\nsimilar business model recently bought Github for millions. \n\nAll that said, a question remains: how do we being to understand closed source\napplications? If we can’t see the structure behind an app, I suppose we’ll have\nto build this model ourselves.\n\nTreelib [https://treelib.readthedocs.io/en/latest/]  is a Python library that\nallows you to create a visual tree hierarchy: a simple plaintext representation\nof parent-child relationships.\n\nAside from scraping and mapping the intellectual property of others, Treelib\ncomes in handy in situations where we have access to flat information (like a\ndatabase table) where rows actually relate to one another (such as monolithic\ncontent-heavy site).\n\nTreelib prints results like this: \n\nHarry\n├── Bill\n│   └── n1\n│       ├── n2\n│       └── n3\n└── Jane\n    ├── Diane\n    │   └── Mary\n    └── Mark \n\n\nIt’s is a simple library, and only requires knowledge of a few lines of code in\norder to be used effectively. What’s more, we’re not simply spitting out flat\nuseless data; we're storing these node relationships in memory. If needed, the\ntrees we build can be modified or used for other the future.\n\nWhere da Treez At?\nInstall the Treelib package:\n\npip install treelib\n\n\nIn your project, import Treelib:\n\n# trees.py\nimport from treelib import Node, Tree\n\n\nCreate a Tree with a Parent Node\nThe first step in utilizing Treelib is to create a tree object. We need to give\nour tree a name - this is essentially creating the top-level node that all other\nnodes will stem from. \n\nIn createNode(x, y), X is the value which will be displayed in the node, while Y\nis the unique identifier for that node. Children will be added to this parent\nnode by referencing the unique identifier.\n\nNote that in trees created with TreeLib, unique identifiers may only occur once.\nTherefore it is good to follow a sort of GUI system for identifying nodes.\n\n# tree.py\n\n# Create tree object\ntree = Tree() \n\n# Create the base node\ntree.create_node(\"Confluence\", \"confluence\") \n\n\nCreate Child Nodes\nThe last necessary part of creating a tree is, of course, populating the\nresulting children.\n\nWe will once again use create_node to add additional nodes, but these nodes will\nbe associated with parents via parent=”x”. This will locate existing nodes in\nthe tree by ID and associate these new nodes to that parent. This is why IDs\nmust be unique for each node in the tree.\n\n# tree.py\ntree.create_node(spaceName, id, parent=\"confluence\")\n\n\nView the Tree\nFinally, you'll want to view the fruits of your labor:\n\nprint(tree.show())\n\n\nWay to go Johnny Appleseed, that’s pretty much the gist of it. There are\nadditional features in the way Trees can be parse, and the way that nodes store\nadditional data.\n\nCheck the official documentation [https://treelib.readthedocs.io/en/latest/] \nfor a full list of features.\n\nBonus Round\nIf all you care about is printing the file structure of a current directory with\nzero interest in working with the actual data, you’re in luck (at least on Mac,\nhell if I know anything about Windows).\n\nUnix systems come with a package named tree  which does just what we want. On\nMac OSX, we can install tree  using Homebrew:\n\n$ brew install tree\n\n\nGo ahead and explore the various features of tree, such as writing to files or\neven doing so on a schedule. For now, here's some basic usage:\n\n$ tree -v -L 1 --charset utf-8","html":"<p>The first part of understanding any type of software is taking a glance at its file structure. It may seem like an outlandish and redundant statement to make to a generation who grew up on GUIs. GitHub is essentially no more than a GUI for Git, so it’s unsurprisingly that one of the largest company to follow a similar business model recently bought Github for millions. </p><p>All that said, a question remains: how do we being to understand closed source applications? If we can’t see the structure behind an app, I suppose we’ll have to build this model ourselves.</p><p><strong><a href=\"https://treelib.readthedocs.io/en/latest/\">Treelib</a></strong> is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.</p><p>Aside from scraping and mapping the intellectual property of others, Treelib comes in handy in situations where we have access to flat information (like a database table) where rows actually relate to one another (such as monolithic content-heavy site).</p><p>Treelib prints results like this: </p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Harry\n├── Bill\n│   └── n1\n│       ├── n2\n│       └── n3\n└── Jane\n    ├── Diane\n    │   └── Mary\n    └── Mark \n</code></pre>\n<!--kg-card-end: markdown--><p>It’s is a simple library, and only requires knowledge of a few lines of code in order to be used effectively. What’s more, we’re not simply spitting out flat useless data; we're storing these node relationships in memory. If needed, the trees we build can be modified or used for other the future.</p><h2 id=\"where-da-treez-at\">Where da Treez At?</h2><p>Install the Treelib package:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">pip install treelib\n</code></pre>\n<!--kg-card-end: markdown--><p>In your project, import Treelib:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># trees.py\nimport from treelib import Node, Tree\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-tree-with-a-parent-node\">Create a Tree with a Parent Node</h3><p>The first step in utilizing Treelib is to create a tree object. We need to give our tree a name - this is essentially creating the top-level node that all other nodes will stem from. </p><p>In createNode(x, y), X is the value which will be displayed in the node, while Y is the unique identifier for that node. Children will be added to this parent node by referencing the unique identifier.</p><p>Note that in trees created with TreeLib, unique identifiers may only occur once. Therefore it is good to follow a sort of GUI system for identifying nodes.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># tree.py\n\n# Create tree object\ntree = Tree() \n\n# Create the base node\ntree.create_node(&quot;Confluence&quot;, &quot;confluence&quot;) \n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"create-child-nodes\">Create Child Nodes</h2><p>The last necessary part of creating a tree is, of course, populating the resulting children.</p><p>We will once again use create_node to add additional nodes, but these nodes will be associated with parents via parent=”x”. This will locate existing nodes in the tree by ID and associate these new nodes to that parent. This is why IDs must be unique for each node in the tree.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># tree.py\ntree.create_node(spaceName, id, parent=&quot;confluence&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"view-the-tree\">View the Tree</h3><p>Finally, you'll want to view the fruits of your labor:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">print(tree.show())\n</code></pre>\n<!--kg-card-end: markdown--><p>Way to go Johnny Appleseed, that’s pretty much the gist of it. There are additional features in the way Trees can be parse, and the way that nodes store additional data.</p><p>Check the <a href=\"https://treelib.readthedocs.io/en/latest/\">official documentation</a> for a full list of features.</p><h2 id=\"bonus-round\">Bonus Round</h2><p>If all you care about is printing the file structure of a current directory with zero interest in working with the actual data, you’re in luck (at least on Mac, hell if I know anything about Windows).</p><p>Unix systems come with a package named <strong>tree</strong> which does just what we want. On Mac OSX, we can install <strong>tree</strong> using Homebrew:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ brew install tree\n</code></pre>\n<!--kg-card-end: markdown--><p>Go ahead and explore the various features of tree, such as writing to files or even doing so on a schedule. For now, here's some basic usage:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ tree -v -L 1 --charset utf-8\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/creating-trees-in-treelib/","uuid":"f0c176ee-c88a-443c-a7b7-b5c5e7c5b9f7","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f4a56e38d612cc826130d"}}]}},"pageContext":{"slug":"todd","limit":12,"skip":72,"numberOfPages":8,"humanPageNumber":7,"prevPageNumber":6,"nextPageNumber":8,"previousPagePath":"/author/todd/page/6/","nextPagePath":"/author/todd/page/8/"}}