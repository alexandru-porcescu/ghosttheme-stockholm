{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369a","title":"Using Pandas and SQLAlchemy to Simplify Databases","slug":"using-pandas-to-make-dealing-with-dbs-less-of-a-hassle","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2.jpg","excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","custom_excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","created_at_pretty":"02 July, 2018","published_at_pretty":"03 July, 2018","updated_at_pretty":"04 April, 2019","created_at":"2018-07-02T14:54:34.000-04:00","published_at":"2018-07-03T07:00:00.000-04:00","updated_at":"2019-04-04T00:35:09.000-04:00","meta_title":"Using Pandas and SQLAlchemy to Simplify Databases | Hackers and Slackers","meta_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-2.jpg","og_title":"Using Pandas and SQLAlchemy to Simplify Databases","twitter_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","twitter_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-1.jpg","twitter_title":"Using Pandas and SQLAlchemy to Simplify Databases","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"Manually opening and closing cursors? Iterating through DB output by hand?\nRemembering which function is the actual one that matches the Python data\nstructure you're gonna be using?\n\nThere has to be a better way!\n\nThere totally is.\n\nOne of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an\nExcel file, or a database - Pandas gets you what you want painlessly. In\nfact,I'd say that even if you don't have the spare bandwidth at the moment to\nrewire your brain to learn all the wonderful ways Pandas lets you manipulate\ndata (array-based programming generally isn't the first paradigm people learn),\nthen it's STILL worth using just for the I/O. Grab your data with one of its\ndelicious one-liners, and once it's in a Dataframe there's have a method to\nconvert it to practically any combination of native Python data structures that\nyou want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've\ngot it!).\n\nHere we'll be looking at interacting with a database with Pandas, because that's\nwhat I was finishing up when I saw Todd texted me that a lot of the people who\nwind up on the site are searching for Pandas stuff.\n\nSo, for our purposes, let's say you're doing Python stuff with a MySQL database.\nI'm also assuming you're using Linux (including via Windows Subsystem for Linux,\nwhich is what I use in order to have Ubuntu on my Windows laptop), because I've\nnever done this with Windows.\n\nAnnoyingly, there's more than one potential connector - I generally use PyMySQL,\nfor no particular reason. Installing this involves an extra step - telling pip\nor Conda to install it won't do the whole thing. Annoying, but them's the\nbreaks.\n\nsudo apt-get install python3-pymysql -y\n\n\nShould get you there. I remember fussing with this every time I have to do it on\na new machine.\n\nYou also have to install a mysql-connector, for reasons I won't pretend to\nunderstand.\n\nconda install mysql-connector-python -y\n\n\nI've never actually done this with pip, but I assume it'd be\n\npip install mysql-connector\n\n\nIf that doesn't work, try\n\npip install --allow-external mysql-connector-python mysql-connector-python\n\n\nWE'RE ALMOST DONE INSTALLING THINGS!\nLast, but not least, is a lovely little thing called SQLAlchemy. Remember when I\nsaid I didn't actually know anything about PyMySQL and its relative advantages\nand disadvantages to other MySQL connectors for Python? Part of the reason for\nthat is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap\nPython SQL connectors at a higher level of abstraction and not have to deal with\nthe particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's\nactually a default Anaconda package. BUT, if you're in a fresh environment (or\nnot using Conda)...\n\n(Always remember the little -y   at the end. That ensures that you can walk away\nwhile it installs and it'll be done when you get back, instead of finishing the\nlong process of whatever's happening when you first tell it to install and then\nwaiting to start on the next long process that takes place after you hit y.)\n\nOR\n\npip install sqlalchemy   (that IS one of the nice things about pip! Doesn't need\nyou to confirm).\n\nSo, to review, before we start Pythoning...\n\n$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n\n\nOR\n\n$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n\n\nCool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!\n We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what\nPandas uses to connect to databases.\n\nAaaand one (possible) last step. If you're doing this with a locally-installed\ndb, you might have to sudo service mysql start.\n\n(Like, every time you do dev work on it. At least on my machine. Your mileage\nmay vary)\n\nOn to the Actual Python\nSo, what does all this do for us? Well, first off, it wasn't THAT much - and you\nonly have to do it once (per machine (really per environment per machine (you\nare  taking advantage of Conda envs or virtualenvs, right?))). But what it\nreally lets us skip all the implementation  details and focus on the actual\nlogic of what we're doing. That's really what both SQL and Pandas are all about-\nthey wed short, declarative  bits of code to highly optimized libraries\nunderneath the hood. Relational Database Management Systems sit on top of code\nthat's been optimized for decades, turning your short queries into highly\nefficient compiled code. Pandas takes advantage of scientific computing\nlibraries for doing matrix operations that have been around since before there\nwas an internet. Drill down deep enough in Pandas and you'll actually find\nFORTRAN code that was originally written to simulate nuclear explosions. Our\nwhole goal is to just focus on our data manipulation via SQL and Python's\nscientific computing family - and we don't want to add a bunch of additional\ncognitive load from worrying about cursors.\n\nOkay, I Lied - NOW on to the Actual Python\nimport pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n\n\nFrom all the database connector stuff we installed, we actually just need ONE\nfunction - and then it's pure blissful operations on data from here on out.\ncreate_engine()  will create a connection for us, and then we'll use that as an\nargument to Pandas functions that talk to the DB. You might have to reopen it\nsometimes. Whatever.\n\nFirst, let's make a string that describes our database. Note that normally you\nshouldn't actually store this directly in your code (IT will totally be mad at\nyou), but for our purposes let's include it.\n\nThe format is:\n\ndialect+driver://username:password@host:port/database\n\n\nSo, since we're using mysql, and the PyMySQL package, we'd start with:\n\nmysql+pymysql://username:password@host:port/database\n\n\nThe rest is fairly straightforward. So, if we've got the user \"analyst\"  with\nthe password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty\nsure you don't have to specify a port if it's localhost, but bear with me), and\nthe database itself is named terrible_purchases, it'd be:\n\nmysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n\n\nAnd now let's actually make our connection!\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n\n\nDone! Now Pandas can read and write from our database.\nIf you don't wanna have to see a whole bunch of extraneous stuff, I'd also add\nthe flag echo=False  (create_engine  has a ton of useful little flags, letting\nyou do stuff like automatically strip unicode that your database might not be\nable to read).\nBehold! We can write an SQL query and then get a Pandas Dataframe with those\nexact parameters. We can rapidly prototype without worrying that the reason our\nquery isn't working is that we forgot some fussy intermediary stage. We can get\nstuff wrong and experiment!\n\nReading a table is easy as:\n\ndf = pd.io.sql.sql_query(\"SELECT * FROM cursed_items\", cnx)\n\n\nDone in one line!\n\nWriting is just as easy (with a teeeensy little asterisk, that I'll probably\ntalk about in the future). Let's say we dusted off an old CSV with more cursed\nitems that aren't in our database. After we've loaded it (and maybe done some\ncleaning)\n\nnewCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n\n\nThe if_exists  flag adds a ton of flexibility. You can create a whole new DB\ntable from a Dataframe, and that's actually the default behavior. For our case,\nwe appended - but there's other times when we might want to replace, for\ninstance.\n\nFinal Addendum\nSo, this is a super useful workflow for doing interactive analytics work with\nstuff that's on a database. It's also quite useful for quick-and-dirty scripts,\nor for prototyping the logic of something that'll be expanded upon later.  And,\nah, bigger stuff too - especially if there's gonna be really complex\ntransformations that. As long as the data can fit in memory (RAM), Pandas is\npretty awesome.\n\n\nThat being said, there are  times when you'd actually want to deal with the\nlevels we bypassed here. Everything has tradeoffs. One example would be that all\nthose compiled Linear Algebra libraries that Pandas sit on top of have a massive\nmemory footprint, and AWS Lambda charges you according to memory usage. I'm\npretty sure there are also use cases where this method of handling the\nlower-level nitty-gritty of database interactions (ie, by mostly not  handling\nit) will cause problems. But at that point, we're dealing with the difference\nbetween Data Science & Data Engineering.","html":"<p>Manually opening and closing cursors? Iterating through DB output by hand? Remembering which function is the actual one that matches the Python data structure you're gonna be using?</p><p>There has to be a better way!</p><p>There totally is.</p><p>One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. In fact,I'd say that even if you don't have the spare bandwidth at the moment to rewire your brain to learn all the wonderful ways Pandas lets you manipulate data (array-based programming generally isn't the first paradigm people learn), then it's STILL worth using just for the I/O. Grab your data with one of its delicious one-liners, and once it's in a Dataframe there's have a method to convert it to practically any combination of native Python data structures that you want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've got it!).</p><p>Here we'll be looking at interacting with a database with Pandas, because that's what I was finishing up when I saw Todd texted me that a lot of the people who wind up on the site are searching for Pandas stuff.</p><p>So, for our purposes, let's say you're doing Python stuff with a MySQL database. I'm also assuming you're using Linux (including via Windows Subsystem for Linux, which is what I use in order to have Ubuntu on my Windows laptop), because I've never done this with Windows.</p><p>Annoyingly, there's more than one potential connector - I generally use PyMySQL, for no particular reason. Installing this involves an extra step - telling pip or Conda to install it won't do the whole thing. Annoying, but them's the breaks.</p><pre><code class=\"language-bash\">sudo apt-get install python3-pymysql -y\n</code></pre>\n<p>Should get you there. I remember fussing with this every time I have to do it on a new machine.</p><p>You also have to install a mysql-connector, for reasons I won't pretend to understand.</p><pre><code class=\"language-bash\">conda install mysql-connector-python -y\n</code></pre>\n<p>I've never actually done this with pip, but I assume it'd be</p><pre><code class=\"language-bash\">pip install mysql-connector\n</code></pre>\n<p>If that doesn't work, try</p><pre><code class=\"language-bash\">pip install --allow-external mysql-connector-python mysql-connector-python\n</code></pre>\n<h3 id=\"we-re-almost-done-installing-things-\">WE'RE ALMOST DONE INSTALLING THINGS!</h3><p>Last, but not least, is a lovely little thing called SQLAlchemy. Remember when I said I didn't actually know anything about PyMySQL and its relative advantages and disadvantages to other MySQL connectors for Python? Part of the reason for that is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap Python SQL connectors at a higher level of abstraction and not have to deal with the particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's actually a default Anaconda package. BUT, if you're in a fresh environment (or not using Conda)...</p><p>(Always remember the little <code>-y</code>  at the end. That ensures that you can walk away while it installs and it'll be done when you get back, instead of finishing the long process of whatever's happening when you first tell it to install and then waiting to start on the next long process that takes place after you hit y.)</p><p>OR</p><p><code>pip install sqlalchemy</code>  (that IS one of the nice things about pip! Doesn't need you to confirm).</p><p>So, to review, before we start Pythoning...</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n</code></pre>\n<p>OR</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n</code></pre>\n<p>Cool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!  We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what Pandas uses to connect to databases.</p><p>Aaaand one (possible) last step. If you're doing this with a locally-installed db, you might have to <code>sudo service mysql start</code>.</p><p>(Like, every time you do dev work on it. At least on my machine. Your mileage may vary)</p><h2 id=\"on-to-the-actual-python\">On to the Actual Python</h2><p>So, what does all this do for us? Well, first off, it wasn't THAT much - and you only have to do it once (per machine (really per environment per machine (you are  taking advantage of Conda envs or virtualenvs, right?))). But what it really lets us skip all the implementation  details and focus on the actual logic of what we're doing. That's really what both SQL and Pandas are all about- they wed short, declarative  bits of code to highly optimized libraries underneath the hood. Relational Database Management Systems sit on top of code that's been optimized for decades, turning your short queries into highly efficient compiled code. Pandas takes advantage of scientific computing libraries for doing matrix operations that have been around since before there was an internet. Drill down deep enough in Pandas and you'll actually find FORTRAN code that was originally written to simulate nuclear explosions. Our whole goal is to just focus on our data manipulation via SQL and Python's scientific computing family - and we don't want to add a bunch of additional cognitive load from worrying about cursors.</p><h2 id=\"okay-i-lied-now-on-to-the-actual-python\">Okay, I Lied - NOW on to the Actual Python</h2><pre><code class=\"language-python\">import pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n</code></pre>\n<p>From all the database connector stuff we installed, we actually just need ONE function - and then it's pure blissful operations on data from here on out. create_engine()  will create a connection for us, and then we'll use that as an argument to Pandas functions that talk to the DB. You might have to reopen it sometimes. Whatever.</p><p>First, let's make a string that describes our database. Note that normally you shouldn't actually store this directly in your code (IT will totally be mad at you), but for our purposes let's include it.</p><p>The format is:</p><pre><code class=\"language-bash\">dialect+driver://username:password@host:port/database\n</code></pre>\n<p>So, since we're using mysql, and the PyMySQL package, we'd start with:</p><pre><code class=\"language-bash\">mysql+pymysql://username:password@host:port/database\n</code></pre>\n<p>The rest is fairly straightforward. So, if we've got the user \"analyst\"  with the password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty sure you don't have to specify a port if it's localhost, but bear with me), and the database itself is named terrible_purchases, it'd be:</p><pre><code class=\"language-bash\">mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n</code></pre>\n<p>And now let's actually make our connection!</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n</code></pre>\n<p>Done! Now Pandas can read and write from our database.<br>If you don't wanna have to see a whole bunch of extraneous stuff, I'd also add the flag echo=False  (create_engine  has a ton of useful little flags, letting you do stuff like automatically strip unicode that your database might not be able to read).<br>Behold! We can write an SQL query and then get a Pandas Dataframe with those exact parameters. We can rapidly prototype without worrying that the reason our query isn't working is that we forgot some fussy intermediary stage. We can get stuff wrong and experiment!</p><p>Reading a table is easy as:</p><pre><code class=\"language-python\">df = pd.io.sql.sql_query(&quot;SELECT * FROM cursed_items&quot;, cnx)\n</code></pre>\n<p>Done in one line!</p><p>Writing is just as easy (with a teeeensy little asterisk, that I'll probably talk about in the future). Let's say we dusted off an old CSV with more cursed items that aren't in our database. After we've loaded it (and maybe done some cleaning)</p><pre><code class=\"language-python\">newCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n</code></pre>\n<p>The <code>if_exists</code> flag adds a ton of flexibility. You can create a whole new DB table from a Dataframe, and that's actually the default behavior. For our case, we appended - but there's other times when we might want to replace, for instance.</p><h2 id=\"final-addendum\">Final Addendum</h2><p>So, this is a super useful workflow for doing interactive analytics work with stuff that's on a database. It's also quite useful for quick-and-dirty scripts, or for prototyping the logic of something that'll be expanded upon later.  And, ah, bigger stuff too - especially if there's gonna be really complex transformations that. As long as the data can fit in memory (RAM), Pandas is pretty awesome.</p><p><br>That being said, there are  times when you'd actually want to deal with the levels we bypassed here. Everything has tradeoffs. One example would be that all those compiled Linear Algebra libraries that Pandas sit on top of have a massive memory footprint, and AWS Lambda charges you according to memory usage. I'm pretty sure there are also use cases where this method of handling the lower-level nitty-gritty of database interactions (ie, by mostly not  handling it) will cause problems. But at that point, we're dealing with the difference between Data Science &amp; Data Engineering.</p>","url":"https://hackersandslackers.com/using-pandas-to-make-dealing-with-dbs-less-of-a-hassle/","uuid":"859fc3cb-daff-4c4b-9b10-ee206c25bb9f","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3a74ead0ac8a143588f35a"}},"pageContext":{"slug":"using-pandas-to-make-dealing-with-dbs-less-of-a-hassle"}}