{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673661","title":"Dealing with Dirty Data in Excel","slug":"dealing-with-dirty-data-in-excel","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","excerpt":"Cleaning data Excel in the absence of tools designed to do so.","custom_excerpt":"Cleaning data Excel in the absence of tools designed to do so.","created_at_pretty":"30 May, 2018","published_at_pretty":"31 May, 2018","updated_at_pretty":"21 January, 2019","created_at":"2018-05-30T03:45:53.000-04:00","published_at":"2018-05-31T09:11:14.000-04:00","updated_at":"2019-01-21T13:41:02.000-05:00","meta_title":"Dealing with Dirty Data in Excel | Hackers and Slackers","meta_description":"Cleaning data Excel in the absence of tools designed to do so.","og_description":"Cleaning data Excel in the absence of tools designed to do so.","og_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","og_title":"Dealing with Dirty Data in Excel","twitter_description":"Cleaning data Excel in the absence of tools designed to do so.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","twitter_title":"Dealing with Dirty Data in Excel","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"In my last post, we discussed what separates a true  analyst (read: technical)\nfrom a project manager wearing the mask of an analyst like some Scott Snyder era\nJoker (I figure that there's a solid overlap between fans of comic books and\nfans of the real world application of data. Note that this is a study with an N\n= 1 so it bares no statistical significance, but I have a funny feeling...call\nit spidey sense).\n\nFull disclosure, this post comes mostly out of my inability to sleep in my hotel\nroom in Chicago following a grueling day of doing the very things I discuss in\nthis blog, and preceding a day where I'll have to literally explain my last post\nto the suits, but perhaps this is the best mindset to begin discussing the\nmyriad ways in which you may encounter dirty data in the wild, and how a savvy\nanalyst may pivot and match their way around it. However, if my prose isn't as\non point as you have grown accustomed...blame it on the 4AM haze.\n\nAlas, let's begin by discussing the organizational structure of the majority of\ncorporate entities that leverage data to some degree (note, this isn't all\ncorporations...and what does that say about the state of business?) and how, at\neach step of abstraction in this process that you are from the data, the data\ngets dirtier and dirtier.\n\nEssentially, there's always going to be a group of about 5-10\nfewer-than-necessary legitimately skilled data scientists and/or computer\nprogrammers/DBAs who are really solid at building and maintaining a database as\nwell as coding in some sort of compiling language (nowadays, that's probably\npython, but not exclusively, nor does this matter). However, depending on your\nindustry (unless of course your industry IS data), it's nearly impossible to\nrecruit people who have these skills to the level necessary AND have some\nfamiliarity with why this data is needed, and/or the ability to explain how the\ninternal products that they build can be used by an end user. As such, this team\nhas their own project manager(s) who's only job is to keep these guys from\ndeveloping a sentient AI that's sole goal is the annihilation of unfolded\nlaundry...when your industry is healthcare. This team should also have at least\none analyst who will take the raw code base and do the first step of translation \n to a more user friendly form. This generally takes shape as either dashboards\nin a system like Tableau, or  if your company has a group of particularly strong\ndata/business analysts (or particularly weak programmers) an interface written\nin plain(enough) English on a Business Intelligence platform such as Microsoft\nBI/SAP Business Objects or whatever other system your company utilizes. As a fun\nlittle note, this team ALMOST ALWAYS  is referred to by some sort of acronym\nsuch as QDAR! (Quality data and reporting!) or KMnR! (Knowledge management and\nreporting!) or Those Fucking Guys (who have something to do with reporting)\n(TFG(whstdwr)). On a less fun little note...neither you, nor seemingly ANYONE\nELSE will have contact with this team. In light of this information, how do the\nreports that they build get chosen and who decides how these databases are\nbuilt? The world may never know.\n\nSo let's assume the first type of reporting: the Dataratti  (which is how I will\nrefer to the acronym defined team described above moving forward) produces\ndashboards utilizing a tool such as Tableau or Crystal Reports. You may be\nthinking to yourself: \"hey, isn't my job taking the data and putting it in a\nform where people who are scared by more than two nested groups of parenthesis,\nand thus this renders my job unnecessary?\" The answer to the question is\ntwofold: Yes, and of course not! As mentioned previously, the decision to create\nthese dashboards, the data contained therein, and how you want them to look is\ndecided upon by a mythical creature who has full access and understanding of the\ndata warehouse, AND has full access to and understanding of the stakeholders\n(AKA, Those Who Sit Above in Shadow; that's a reference from a famous run of\nThor comics that refers to to a mysterious cabal of gods who perpetuate the\ncycle of Ragnarok in order to subsist upon the energies created by this\nstrife...which as I write this, is an almost disgustingly on-the-nose metaphor\nfor upper management). Now, if you believe that you may be this mythical\ncreature (as I do), I DARE  you to apply for a job with this job description,\nand once you clinch it with the advice from this blog,  rapidly realize that\nyour job will involve either one of these job duties or the other.\n\nWith that digression, even if somehow a useful dashboard for YOU is created, the\nlimitations inherent in these dashboarding tools make one CRUCIAL issue\nomnipresent: one can only effectively illustrate up to 16 different variables at\na time before the system breaks down (for example, Tableau's documentation\nspecifically warns against this). So even if you have the nicest, most\nillustrative dashboards on the planet from the Dataratti, there is a nearly 100%\nchance that the information that you actually need will be scattered across 2-3\ndifferent dashboards...rendering the nice looking dashboards essentially useless\nfor your purposes, and as previously stated, you have no contact with the\nDataratti, nor do you have access to the underlying data from which these\ndashboards are created. So pop quiz hot shot, what DO you do?\n\nWell, mercifully, all of these dashboard tools allow an end user to download a\n\"data dump\" (our parlance for \"a buncha numbers with headings\"). Using Tableau\nas an example, one can download either a \"crosstab\" or a text file of the data\nrepresented by the dashboard (in both \"summary\" and \"full data\" format). Now,\njust to get the truly gifted in Tableau off my back, yes, the functionality does\nexist to build in the ability to download the data in the exact format necessary\nfor your  needs through a specific combination of custom web server views and\nJavascript, but...\n\n 1. If the users of the dash are exclusively using this function, why do the\n    dashboard at all? And...\n 2. This forces the developers in the Dataratti to have decent web design skills\n    on top of really high level Tableau skills, and it requires someone to\n    anticipate exactly how the data will be used by the end user by the\n    Dataratti (which is incredibly hard as it's impossible to speak to this\n    department directly, and as previously stated, the lack of this  knowledge\n    on their end is the entire reason why my department exists).\n\nA few things to note before downloading data from Tableau:\n\n * You must highlight at least one element of the dashboard before downloading a\n   crosstab.\n * Depending on what kind of dashboard you're working with, you may need to\n   highlight the entirety of one column in order to capture the entirety of your\n   data (click the first element in any column  and then scroll down to the\n   bottom of the report...which may be enormously long, hit shift  and click the\n   last element in the report) before downloading either the data or the\n   crosstab.\n * If you are downloading a crosstab, be wary, Tableau web server caps how many\n   rows you can download in this method at a time, this can be avoided by\n   downloading the text version of the data (by clicking data as opposed to\n   crosstab). HOWEVER...\n * If you are going the data route, it defaults to summary view. Look over all\n   the headings, and ensure that this covers everything you need, otherwise\n   click \"full data\". Interestingly, this still isn't actually the entirety of\n   your data, and continue to check to make sure all of your headings are\n   covered, otherwise, click the display all columns  box, and then download all\n   the rows as a text file.\n\nNow, repeat these steps until all of the data that you need in your  report is\ncontained across these text files (.csv, AKA the Comma Separated Value file\ntype). With all that lunacy completed, you now have several sheets with some\ncommon columns, but all with different information; only some of which you need,\nso what do you think you do?\n\nSimple, you use the tools given to you in the previous posts: you lookup  on the\ncommon factors across the sheets and return the data that you want until you\nhave all the data you need, in the correct order, on one sheet, and then\ndepending on the ask, you may want to pivot that data out in order to summarize \nthe whole mess of data. THIS IS YOUR FINAL PRODUCT  well done. Another protip:\nif you want to reposition data that you've obtained via a lookup, highlight the\nwhole column, hit control+C  to copy the data and then hit control+V  pause a\nsecond (press NOTHING else) and then press control FOLLOWED by V. This takes the\nvalues generated by a formula and replaces them with the values obtained.\nFunctionally, this looks  exactly the same, but now you can move the data around\nwithout affecting or being affected by other data.\n\nAs explaining only one possible dirty data scenario took over 1500 words, next\ntime, we'll discuss the other most common form of taking the dirty data from the\nDataratti and making it useful to you: using business intelligence portals as\nopposed to dashboards in order to grab the data that you need. Also, if I don't\nget roasted on a spit for being half asleep for tomorrow's (today's?) meeting,\nI'll try and write up a companion post with an example of how this works out in\npractice.\n\nIn summary, in this post we've learned:\n\n 1. How data is generally siloed and sequestered within the corporate\n    environment, leading to a bevy of unnecessary steps on behalf of the analyst\n    in order to distill a functional report for the powers-that-be\n 2. Two major methods in which data comes from the data team (henceforth known\n    as the Dataratti) to your team: Dashboards and Business Intelligence\n    interfaces, and...\n 3. Assuming you get data in the form of dashboards, how to take these\n    dashboards, download the underlying data, recombine and manipulate the data,\n    and package it in a way acceptable for your needs.\n\nCongrats, you've just learned the crucial skill of the Slice n' Dice!\n\nQuite sleepily,\n\n-Snacks","html":"<p>In my last post, we discussed what separates a <em>true</em> analyst (read: technical) from a project manager wearing the mask of an analyst like some Scott Snyder era Joker (I figure that there's a solid overlap between fans of comic books and fans of the real world application of data. Note that this is a study with an N = 1 so it bares no statistical significance, but I have a funny feeling...call it spidey sense).</p><p>Full disclosure, this post comes mostly out of my inability to sleep in my hotel room in Chicago following a grueling day of doing the very things I discuss in this blog, and preceding a day where I'll have to literally explain my last post to the suits, but perhaps this is the best mindset to begin discussing the myriad ways in which you may encounter dirty data in the wild, and how a savvy analyst may pivot and match their way around it. However, if my prose isn't as on point as you have grown accustomed...blame it on the 4AM haze.</p><p>Alas, let's begin by discussing the organizational structure of the majority of corporate entities that leverage data to some degree (note, this isn't all corporations...and what does that say about the state of business?) and how, at each step of abstraction in this process that you are from the data, the data gets dirtier and dirtier.</p><p>Essentially, there's always going to be a group of about 5-10 fewer-than-necessary legitimately skilled data scientists and/or computer programmers/DBAs who are really solid at building and maintaining a database as well as coding in some sort of compiling language (nowadays, that's probably python, but not exclusively, nor does this matter). However, depending on your industry (unless of course your industry IS data), it's nearly impossible to recruit people who have these skills to the level necessary AND have some familiarity with why this data is needed, and/or the ability to explain how the internal products that they build can be used by an end user. As such, this team has their own project manager(s) who's only job is to keep these guys from developing a sentient AI that's sole goal is the annihilation of unfolded laundry...when your industry is healthcare. This team should also have at least one analyst who will take the raw code base and do the <strong>first step of translation</strong> to a more user friendly form. This generally takes shape as either dashboards in a system like Tableau, <strong>or</strong> if your company has a group of particularly strong data/business analysts (or particularly weak programmers) an interface written in plain(enough) English on a Business Intelligence platform such as Microsoft BI/SAP Business Objects or whatever other system your company utilizes. As a fun little note, this team <strong>ALMOST ALWAYS</strong> is referred to by some sort of acronym such as QDAR! (Quality data and reporting!) or KMnR! (Knowledge management and reporting!) or Those Fucking Guys (who have something to do with reporting) (TFG(whstdwr)). On a less fun little note...neither you, nor seemingly ANYONE ELSE will have contact with this team. In light of this information, how do the reports that they build get chosen and who decides how these databases are built? The world may never know.</p><p>So let's assume the first type of reporting: the Dataratti  (which is how I will refer to the acronym defined team described above moving forward) produces dashboards utilizing a tool such as Tableau or Crystal Reports. You may be thinking to yourself: \"hey, isn't my job taking the data and putting it in a form where people who are scared by more than two nested groups of parenthesis, and thus this renders my job unnecessary?\" The answer to the question is twofold: Yes, and of course not! As mentioned previously, the decision to create these dashboards, the data contained therein, and how you want them to look is decided upon by a mythical creature who has full access and understanding of the data warehouse, AND has full access to and understanding of the stakeholders (AKA, <strong>Those Who Sit Above in Shadow</strong>; that's a reference from a famous run of Thor comics that refers to to a mysterious cabal of gods who perpetuate the cycle of Ragnarok in order to subsist upon the energies created by this strife...which as I write this, is an almost disgustingly on-the-nose metaphor for upper management). Now, if you believe that you may be this mythical creature (as I do), I <em>DARE</em> you to apply for a job with this job description, and once you clinch it with the advice from this blog,  rapidly realize that your job will involve either one of these job duties or the other.</p><p>With that digression, even if somehow a useful dashboard for YOU is created, the limitations inherent in these dashboarding tools make one CRUCIAL issue omnipresent: one can only effectively illustrate up to 16 different variables at a time before the system breaks down (for example, Tableau's documentation specifically warns against this). So even if you have the nicest, most illustrative dashboards on the planet from the Dataratti, there is a nearly 100% chance that the information that you actually need will be scattered across 2-3 different dashboards...rendering the nice looking dashboards essentially useless for your purposes, and as previously stated, you have no contact with the Dataratti, nor do you have access to the underlying data from which these dashboards are created. So pop quiz hot shot, what DO you do?</p><p>Well, mercifully, all of these dashboard tools allow an end user to download a \"data dump\" (our parlance for \"a buncha numbers with headings\"). Using Tableau as an example, one can download either a \"crosstab\" or a text file of the data represented by the dashboard (in both \"summary\" and \"full data\" format). Now, just to get the truly gifted in Tableau off my back, yes, the functionality does exist to build in the ability to download the data in the exact format necessary for <em>your</em> needs through a specific combination of custom web server views and Javascript, but...</p><ol><li>If the users of the dash are exclusively using this function, why do the dashboard at all? And...</li><li>This forces the developers in the Dataratti to have decent web design skills on top of really high level Tableau skills, and it requires someone to anticipate exactly how the data will be used by the end user by the Dataratti (which is incredibly hard as it's impossible to speak to this department directly, and as previously stated, the lack of <em>this</em> knowledge on their end is the entire reason why my department exists).</li></ol><p>A few things to note before downloading data from Tableau:</p><ul><li>You must highlight at least one element of the dashboard before downloading a crosstab.</li><li>Depending on what kind of dashboard you're working with, you may need to highlight the entirety of one column in order to capture the entirety of your data (<strong>click the first element in any column</strong> and then scroll down to the bottom of the report...which may be enormously long, <strong>hit shift</strong> and click the last element in the report) before downloading either the data or the crosstab.</li><li>If you are downloading a crosstab, be wary, Tableau web server caps how many rows you can download in this method at a time, this can be avoided by downloading the text version of the data (by clicking data as opposed to crosstab). HOWEVER...</li><li>If you are going the data route, it defaults to summary view. Look over all the headings, and ensure that this covers everything you need, otherwise click <strong>\"full data\"</strong>. Interestingly, this still isn't actually the entirety of your data, and continue to check to make sure all of your headings are covered, otherwise, click the <strong>display all columns</strong> box, and then download all the rows as a text file.</li></ul><p>Now, repeat these steps until all of the data that you need in <em>your</em> report is contained across these text files (.csv, AKA the Comma Separated Value file type). With all that lunacy completed, you now have several sheets with some common columns, but all with different information; only some of which you need, so what do you think you do?</p><p>Simple, you use the tools given to you in the previous posts: you <em>lookup</em> on the common factors across the sheets and return the data that you want until you have all the data you need, in the correct order, on one sheet, and then depending on the ask, you may want to pivot that data out in order to <em>summarize</em> the whole mess of data. <strong>THIS IS YOUR FINAL PRODUCT</strong> well done. Another protip: if you want to reposition data that you've obtained via a lookup, highlight the whole column, hit <strong>control+C</strong> to copy the data and then hit <strong>control+V</strong> pause a second (press NOTHING else) and then press <strong>control FOLLOWED by V</strong>. This takes the values generated by a formula and replaces them with the values obtained. Functionally, this <em>looks</em> exactly the same, but now you can move the data around without affecting or being affected by other data.</p><p>As explaining only one possible dirty data scenario took over 1500 words, next time, we'll discuss the other most common form of taking the dirty data from the Dataratti and making it useful to you: using business intelligence portals as opposed to dashboards in order to grab the data that you need. Also, if I don't get roasted on a spit for being half asleep for tomorrow's (today's?) meeting, I'll try and write up a companion post with an example of how this works out in practice.</p><p>In summary, in this post we've learned:</p><ol><li>How data is generally siloed and sequestered within the corporate environment, leading to a bevy of unnecessary steps on behalf of the analyst in order to distill a functional report for the powers-that-be</li><li>Two major methods in which data comes from the data team (henceforth known as the Dataratti) to your team: Dashboards and Business Intelligence interfaces, and...</li><li>Assuming you get data in the form of dashboards, how to take these dashboards, download the underlying data, recombine and manipulate the data, and package it in a way acceptable for your needs.</li></ol><p>Congrats, you've just learned the crucial skill of the <em>Slice n' Dice</em>!</p><p>Quite sleepily,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/dealing-with-dirty-data-in-excel/","uuid":"4753f845-b70e-4159-9a45-90753b8620b4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b0e56b16cb7ee206e3e518a"}},"pageContext":{"slug":"dealing-with-dirty-data-in-excel"}}