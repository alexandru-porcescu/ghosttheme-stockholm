{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736fd","title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","slug":"tuning-random-forests-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","custom_excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","created_at_pretty":"09 September, 2018","published_at_pretty":"10 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-09T19:14:32.000-04:00","published_at":"2018-09-10T07:30:00.000-04:00","updated_at":"2019-02-19T03:46:39.000-05:00","meta_title":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","og_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","twitter_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","twitter_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Continued from here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\n\nNotebook for this post is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb]\n\nBinary search code itself is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nmax_depth  is an interesting parameter.  While n_estimators  has a tradeoff\nbetween speed & score, max_depth  has the possibility of improving both.  By\nlimiting the depth of your trees, you can reduce overfitting.\n\nUnfortunately, deciding on upper & lower bounds is less than straightforward.\n It'll depend on your dataset.  Luckily, I found a post on StackOverflow that\nhad a link to a blog post that had a promising methodology\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html].\n\nFirst, we build a tree with default arguments and fit it to our data. \n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\nNow, let's see how deep the trees get when we don't impose any sort of max_depth\n. We'll use the code from that wonderful blog post to crawl our Random Forest,\nand get the height of every tree.\n\n#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#> 2\nnp.hstack(allDepths).max()\n#> 9\n\nWe'll be searching between 2 and 9!\n\nLet's bring back our old make a helper function to easily return scores.\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\nmax_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)\n\nmax_depth\n score\n time\n 2\n 0.987707\n 0.145360\n 9\n 0.987029\n 0.147563\n 6\n 0.986247\n 0.140514\n 4\n 0.968316\n 0.140164\n \nmax_depth\n score\n time\n scoreTimeRatio\n 2\n 1.051571\n 0.837377\n 0.175986\n 9\n 1.016649\n 1.135158\n 0.103478\n 6\n 0.976311\n 0.182516\n 1.000000\n 4\n 0.051571\n 0.135158\n 0.000000\n So, for our purposes, 9 will function as our baseline since that was the\nbiggest depth that it built with default arguments.\n\nLooks like a max_depth  of 2 has a slightly higher score than 9, and is slightly\nfaster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that\nis.","html":"<p>Continued from <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a></p><p>Notebook for this post is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb\">here</a></p><p>Binary search code itself is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p><code>max_depth</code> is an interesting parameter.  While <code>n_estimators</code> has a tradeoff between speed &amp; score, <code>max_depth</code> has the possibility of improving both.  By limiting the depth of your trees, you can reduce overfitting.</p><p>Unfortunately, deciding on upper &amp; lower bounds is less than straightforward.  It'll depend on your dataset.  Luckily, I found a post on StackOverflow that had a link to a blog post that had a promising <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">methodology</a>.  </p><p>First, we build a tree with default arguments and fit it to our data. </p><pre><code>import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)</code></pre><p>Now, let's see how deep the trees get when we don't impose any sort of <code>max_depth</code>. We'll use the code from that wonderful blog post to crawl our Random Forest, and get the height of every tree.</p><pre><code>#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#&gt; 2\nnp.hstack(allDepths).max()\n#&gt; 9</code></pre><p>We'll be searching between 2 and 9!  </p><p>Let's bring back our old make a helper function to easily return scores.</p><pre><code>def getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)</code></pre><pre><code>max_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/09/max_depth.png\" class=\"kg-image\"></figure><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>0.987707</td>\n      <td>0.145360</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.987029</td>\n      <td>0.147563</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.986247</td>\n      <td>0.140514</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.968316</td>\n      <td>0.140164</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>1.051571</td>\n      <td>0.837377</td>\n      <td>0.175986</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.016649</td>\n      <td>1.135158</td>\n      <td>0.103478</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.976311</td>\n      <td>0.182516</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.051571</td>\n      <td>0.135158</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>So, for our purposes, 9 will function as our baseline since that was the biggest depth that it built with default arguments.  </p><p>Looks like a <code>max_depth</code> of 2 has a slightly higher score than 9, and is slightly faster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that is.</p>","url":"https://hackersandslackers.com/tuning-random-forests-hyperparameters-with-binary-search/","uuid":"3c92aed0-61ed-4c1a-b7d5-cc47c709764b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b95a9581fc1fc7d92b5c51f"}},"pageContext":{"slug":"tuning-random-forests-hyperparameters-with-binary-search"}}