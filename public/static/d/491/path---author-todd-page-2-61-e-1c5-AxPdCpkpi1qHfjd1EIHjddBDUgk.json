{"data":{"ghostAuthor":{"slug":"todd","name":"Todd Birchard","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","cover_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/fox_o_o.jpg","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","location":"New York City","website":"https://toddbirchard.com","twitter":"@ToddRBirchard","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c5bb0ec7999ff33f06876e1","title":"Welcome to SQL: Modifying Databases and Tables","slug":"welcome-to-sql-modifying-databases-and-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","custom_excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","created_at_pretty":"07 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-06T23:15:40.000-05:00","published_at":"2019-02-19T18:28:00.000-05:00","updated_at":"2019-02-27T23:16:44.000-05:00","meta_title":"Welcome to SQL: Modifying Databases and Tables | Hackers and Slackers","meta_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","og_title":"Welcome to SQL: Modifying Databases and Tables","twitter_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","twitter_title":"Welcome to SQL: Modifying Databases and Tables","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"SQL: we all pretend to be experts at it, and mostly get away with it thanks to\nStackOverflow. Paired with our vast experience of learning how to code in the\n90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go\nahead and chalk up a win for your resume.\n\nSQL has been around longer than our careers have, so why start a series on it \nnow?  Surely there’s sufficient enough documentation that we can Google the\nspecifics whenever the time comes for us to write a query? That, my friends, is\nprecisely the problem. Regardless of what tools we have at our disposable, some\nskills are better learned and practiced by heart. SQL is one of those skills.\n\nSure, SQLAlchemy or similar ORMs might protect us here-and-there from writing\nraw queries. Considering SQL is just one of many query languages we'll use\nregularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert\nreally that critical? In short, yes: relational databases are not only here to\nstay, but thinking  in queries as a second language solidifies one's\nunderstanding of the fine details of data. Marc Laforet\n[https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032] \n recently published a Medium post which drives home just how important leaning\non SQL is:\n\n> What’s even more interesting is that when these transformation scripts were\napplied to the 6.5 GB dataset, python completely failed. Out of 3 attempts,\npython crashed 2 times and my computer completely froze the 3rd time… while SQL\ntook 226 seconds.\n\n\nKeeping logic out of our apps and pipelines and in SQL results in exponentially\nfaster execution, while also being more readable and universally understood than\nwhatever we’d write in our language of choice. The lower down we can push\napplication logic in our stack, the better. This is why I’d much prefer to see\nthe datasphere saturated with SQL tutorials as opposed to Pandas tutorials.\n\nRelational Database Terminology\nI hate it when informational material kicks off with covering obvious\nterminology definitions. Under normal circumstances, I find this to be cliche,\nunhelpful, and damaging to an author's credibility; but these aren't normal\ncircumstances. In SQL, vocabulary commonly has multiple meanings depending on\ncontext, or even which flavor database you're using. Given this fact, it's\nentirely possible (and common) for individuals to rack up experience with\nrelational databases while completely misinterpreting fundamental concepts.\nLet's make sure that doesn't happen:\n\n * Databases: Every Database instance is separated at the highest level into \n   databases. Yes, a database is a collection of databases - we're already off\n   to a great start.\n * Schemas: In PostgreSQL (and other databases), a schema  is a grouping of\n   tables and other objects, including views, relations, etc. A schema is a way\n   of organizing data. Schemas imply that all the data belonging to it is at\n   some form related, even if only by concept. Note that the term schema  is\n   sometimes used to describe other concepts depending on the context.\n * Tables: The meat and potatos of relational databases. Tables consist of rows\n   and columns which hold our sweet, sweet data. Columns are best thought of as\n   'attributes', whereas rows are entries which consist of values for said\n   attributes. All values in a column must share the same data type. * Keys: Keys are used to help us organize and optimize data, as well as\n      place certain constraints on data coming in (for example, email addresses\n      of user accounts must be unique). Keys can also help us keep count of our\n      entries, ensure automatically unique values, and provide a bridge to link\n      multiple tables of data. * Primary keys: Identification tags for each row of data. The primary key\n         is different for every record in the relational database; values must\n         be provided, and they must be unique between rows.\n       * Foreign keys: Enable data searches and manipulation between the primary\n         database table and other related databases.\n      \n      \n   \n   \n * Objects: A blanket term for anything (including relations) that exist in a\n   schema (somewhat PostgreSQL-specific). * Views (PostgreSQL): Views display data in a fashion similar to tables,\n      with the difference that views do not store  data. Views are a snapshot of\n      data pulled from other tables in the form of a query; a good way to think\n      about views is to consider them to be 'virtual tables.'\n    * Functions (PostgreSQL): Logic for interacting with data saved for the\n      purpose of being reused.\n   \n   \n\nIn MySQL, a schema  is synonymous with a database. These keywords can even be\nswapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using CREATE\nSCHEMA  acheives the same effect as instead of CREATE DATABASE.Navigating and\nCreating Databases\nWe've got to start somewhere, so it might as well be with database management.\nAdmittedly, this will be the most useless of the things we'll cover. The act of\nnavigating databases is best suited for a GUI.\n\nShow Databases\nIf you access your database via command line shell (for some reason), the first\nlogical thing to do is to list the available databases:\n\nSHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\n\nUSE Database\nNow that we've listed the possible databases we can connect to, we can explore\nwhat each of these contains. To do this, we have to specify which database we\nwant to connect to, AKA \"use.\" \n\ndb> USE database_name;\nDatabase changed\n\n\nCreate Database\nCreating databases is straightforward. Be sure to pay attention to the character\nset  when creating a database: this will determine which types of characters\nyour database will be able to accept. For example, if we try to insert special\nencoded characters into a simple UTF-8 database, those characters won’t turn out\nas we’d expect.\n\nCREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n\n\nBonus: here's the shorthand for creating a database and then showing the result:\n\nSHOW CREATE DATABASE database_name;\n\n\nCreating and Modifying Tables\nCreating tables via SQL syntax can be critical when automating data imports.\nWhen creating a table, we also set the column names, types, and keys:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];\n\nWe can specify IF NOT EXISTS  when creating our table if we'd like to include\nvalidation in our query. When present, the table will only be created if a table\nof the specified name does not exist.\n\nWhen creating each of our columns, there are a number of things we can specify\nper-column:\n\n * Data Type (required):  The data which can be saved to cells of this column\n   (such as INTEGER, TEXT, etc).\n * Key Type:  Creates a key for the column.\n * Key Attributes:  Any key-related attributes, such as auto-incrementing.\n * Default:  If rows are created in the table without values passed to the\n   current column, the value specified as DEFAULT  \n * Primary Key:  Allows any of the previous specified columns to be set as the\n   table's primary key.\n\nMySQL tables can have a 'storage engine' specified via ENGINE=[engine_type],\nwhich determines the core logic of how the table will interpret data. Leaving\nthis blank defaults to InnoDB and is almost certainly fine to be left alone. In\ncase you're interested, you can find more about MySQL engines here\n[https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html].\n\nHere's an example of what an actual CREATE TABLE  query would look like:\n\nCREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nManaging Keys for Existing Tables\nIf we don't specify our keys at table creation time, we can always do so after\nthe fact. SQL tables can accept the following key types:\n\n * Primary Key:  One or more fields/columns that uniquely identify a record in\n   the table. It can not accept null, duplicate values.\n * Candidate Key:  Candidate keys are kind of like groups of non-committed\n   Primary Keys; these keys only accept unique values, and could potentially  be\n   used in the place of a Primary Key if need be, but are not actual Primary\n   Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.\n * Alternate Key:  Refers to a single Candidate Key (an alternative which can\n   satisfy the duty of a Primary Key id need be).\n * Composite/Compound Key:  Defined by combing the values of multiple columns;\n   the sum of which will always produce a unique value. There can be multiple\n   Candidate Keys in one table. Each Candidate Key can work as Primary Key.\n * Unique Key:  A set of one or more fields/columns of a table that uniquely\n   identify a record in a database table. Similar to Primary key, but it can\n   accept only one null value, and it can not have duplicate values.\n * Foreign Key: Foreign keys denote fields that serve as another table's \n   Primary key. Foreign keys are useful for building relationships between\n   tables. While a foreign key is required in the parent table where they are\n   primary, foreign keys can be null or empty in the tables intended to relate\n   to the other table.\n\nLet's look at an example query where we add a key to a table and dissect the\npieces:\n\nALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n\n\nALTER TABLE  is used to make any changes to a table's structure, whether that be\nmodifying columns or keys.\n\nIn this example, we ADD  a key which happens to be a FOREIGN KEY. While keys\nalways refer to columns, keys themselves must have names of their own to\ndistinguish the column's data and a key's conceptual logic. We name our key \nforeign_key_name  and specify which column the key will act on with \n(column_name). Because this is a foreign key, we need to specify which table's \nprimary key  we want this to be associated with. REFERENCES\nparent_table(primary_key_column)  is stating that the foreign key in this table\ncorresponds to values held in a column named primary_key_column, in a table\nnamed parent_table.\n\nThe statements ON DELETE  and ON UPDATE  are actions which take place if the\nparent table's primary key is deleted or updated, respectively. ON DELETE\nCASCADE  would result in our tables foreign key being deleted if the\ncorresponding primary key were to disappear.\n\nAdding Columns\nAdding columns follows the same syntax we used when creating tables. An\ninteresting additional feature is the ability to place the new column before or\nafter preexisting columns:\n\nALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n\n\nWhen referencing tables in PostgreSQL databases, we must specify the schema\nbelongs to. Thus, ALTER TABLE table_name  becomes ALTER TABLE\nschema_name.table_name. This applies to any time we reference tables, including\nwhen we create and delete tables.Pop Quiz\nThe below statement uses elements of everything we've learned about modifying\nand creating table structures thus far. Can you discern what is happening here?\n\nCREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n\n\nDropping Data\nDANGER ZONE: this is where we can start to mess things up. Dropping columns or\ntables results in a complete loss of data: whenever you see the word \"drop,\" be\nscared.\n\nIf you're sure you know what you're doing and would like to remove a table\ncolumn, this can be done as such:\n\nALTER TABLE table\nDROP column;\n\n\nDropping a table destroys the table structure as well as all data within it:\n\nDROP TABLE table_name;\n\n\nTruncating a table, on the other hand, will purge the table of data but retain\nthe table itself:\n\nTRUNCATE TABLE table_name;\n\n\nDrop Foreign Key\nLike tables and columns, we can drop keys as well:\n\nALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n\n\nThis can also be handed by dropping CONSTRAINT:\n\nALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n\n\nWorking with Views (Specific to PostgreSQL)\nLastly, let's explore the act of creating views. There are three types of views\nPostgreSQL can handle:\n\n * Simple Views: Virtual tables which represent data of underlying tables.\n   Simple views are automatically updatable: the system will allow INSERT,\n   UPDATE and DELETE statements to be used on the view in the same way as on a\n   regular table.\n * Materialized Views: PostgreSQL extends the view concept to a next level that\n   allows views to store data 'physically', and we call those views are\n   materialized views. A materialized view caches the result of a complex query\n   and then allow you to refresh the result periodically.\n * Recursive Views: Recursive views are a bit difficult to explain without\n   delving deep into the complicated (but cool!) functionality of recursive\n   reporting. I won't get into the details, but these views are able to\n   represent relationships which go multiple layers deep. Here's a quick taste,\n   if you;re curious:\n\nSample RECURSIVE  query:\n\nWITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' > ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n\n\nOutput:\n\n employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North > Megan Berry\n           3 | Michael North > Sarah Berry\n           4 | Michael North > Zoe Black\n           5 | Michael North > Tim James\n           6 | Michael North > Megan Berry > Bella Tucker\n           7 | Michael North > Megan Berry > Ryan Metcalfe\n           8 | Michael North > Megan Berry > Max Mills\n           9 | Michael North > Megan Berry > Benjamin Glover\n          10 | Michael North > Sarah Berry > Carolyn Henderson\n          11 | Michael North > Sarah Berry > Nicola Kelly\n          12 | Michael North > Sarah Berry > Alexandra Climo\n          13 | Michael North > Sarah Berry > Dominic King\n          14 | Michael North > Zoe Black > Leonard Gray\n          15 | Michael North > Zoe Black > Eric Rampling\n          16 | Michael North > Megan Berry > Ryan Metcalfe > Piers Paige\n          17 | Michael North > Megan Berry > Ryan Metcalfe > Ryan Henderson\n          18 | Michael North > Megan Berry > Max Mills > Frank Tucker\n          19 | Michael North > Megan Berry > Max Mills > Nathan Ferguson\n          20 | Michael North > Megan Berry > Max Mills > Kevin Rampling\n(20 rows)\n\n\nCreating a View\nCreating a simple view is as simple as writing a standard query! All that is\nrequired is the addition of CREATE VIEW view_name AS  before the query, and this\nwill create a saved place for us to always come back and reference the results\nof this query:\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n\n\nGet Out There and Start SQLing\nI highly encourage anybody to get in the habit of always writing SQL queries by\nhand. With the right GUI, autocompletion can be your best friend.\n\nExplicitly forcing one's self to write queries instead of copy & pasting\nanything forces us to come to realizations, such as SQL's order of operations.\nIndeed, this query holds the correct syntax...\n\nSELECT *\nFROM table_name\nWHERE column_name = 'Value';\n\n\n...Whereas this one does not:\n\nSELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n\n\nGrasping the subtleties of SQL is the difference between being blazing fast and\nmostly clueless. The good news is, you’ll start to find that these concepts\naren’t nearly as daunting as they may have once seemed, so the track from ‘bad\ndata engineer’ to ‘expert’ is an easy win that would be foolish not to take.\n\nStick around for next time where we actually work with data in SQL: The Sequel,\nrated PG-13.","html":"<p>SQL: we all pretend to be experts at it, and mostly get away with it thanks to StackOverflow. Paired with our vast experience of learning how to code in the 90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go ahead and chalk up a win for your resume.</p><p>SQL has been around longer than our careers have, so why start a series on it <em>now?</em> Surely there’s sufficient enough documentation that we can Google the specifics whenever the time comes for us to write a query? That, my friends, is precisely the problem. Regardless of what tools we have at our disposable, some skills are better learned and practiced by heart. SQL is one of those skills.</p><p>Sure, SQLAlchemy or similar ORMs might protect us here-and-there from writing raw queries. Considering SQL is just one of many query languages we'll use regularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert really that critical? In short, yes: relational databases are not only here to stay, but <em>thinking</em> in queries as a second language solidifies one's understanding of the fine details of data. <a href=\"https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032\">Marc Laforet</a> recently published a Medium post which drives home just how important leaning on SQL is:</p><blockquote>\n<p>What’s even more interesting is that when these transformation scripts were applied to the 6.5 GB dataset, python completely failed. Out of 3 attempts, python crashed 2 times and my computer completely froze the 3rd time… while SQL took 226 seconds.</p>\n</blockquote>\n<p>Keeping logic out of our apps and pipelines and in SQL results in exponentially faster execution, while also being more readable and universally understood than whatever we’d write in our language of choice. The lower down we can push application logic in our stack, the better. This is why I’d much prefer to see the datasphere saturated with SQL tutorials as opposed to Pandas tutorials.</p><h2 id=\"relational-database-terminology\">Relational Database Terminology</h2><p>I hate it when informational material kicks off with covering obvious terminology definitions. Under normal circumstances, I find this to be cliche, unhelpful, and damaging to an author's credibility; but these aren't normal circumstances. In SQL, vocabulary commonly has multiple meanings depending on context, or even which flavor database you're using. Given this fact, it's entirely possible (and common) for individuals to rack up experience with relational databases while completely misinterpreting fundamental concepts. Let's make sure that doesn't happen:</p><ul>\n<li><strong>Databases</strong>: Every Database instance is separated at the highest level into <em>databases</em>. Yes, a database is a collection of databases - we're already off to a great start.</li>\n<li><strong>Schemas</strong>: In PostgreSQL (and other databases), a <em>schema</em> is a grouping of tables and other objects, including views, relations, etc. A schema is a way of organizing data. Schemas imply that all the data belonging to it is at some form related, even if only by concept. Note that the term <em>schema</em> is sometimes used to describe other concepts depending on the context.</li>\n<li><strong>Tables</strong>: The meat and potatos of relational databases. Tables consist of rows and columns which hold our sweet, sweet data. Columns are best thought of as 'attributes', whereas rows are entries which consist of values for said attributes. All values in a column must share the same data type.\n<ul>\n<li><strong>Keys</strong>: Keys are used to help us organize and optimize data, as well as place certain constraints on data coming in (for example, email addresses of user accounts must be <em>unique</em>). Keys can also help us keep count of our entries, ensure automatically unique values, and provide a bridge to link multiple tables of data.\n<ul>\n<li><strong>Primary keys</strong>:  Identification tags for each row of data. The primary key is different for every record in the relational database; values must be provided, and they must be unique between rows.</li>\n<li><strong>Foreign keys</strong>: Enable data searches and manipulation between the primary database table and other related databases.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objects</strong>: A blanket term for anything (including relations) that exist in a schema (somewhat PostgreSQL-specific).\n<ul>\n<li><strong>Views (PostgreSQL)</strong>: Views display data in a fashion similar to tables, with the difference that views do not <em>store</em> data. Views are a snapshot of data pulled from other tables in the form of a query; a good way to think about views is to consider them to be 'virtual tables.'</li>\n<li><strong>Functions  (PostgreSQL)</strong>: Logic for interacting with data saved for the purpose of being reused.</li>\n</ul>\n</li>\n</ul>\n<div class=\"protip\">\nIn MySQL, a <strong>schema</strong> is synonymous with a <strong>database</strong>. These keywords can even be swapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using <code>CREATE SCHEMA</code> acheives the same effect as instead of <code>CREATE DATABASE</code>.   \n</div><h2 id=\"navigating-and-creating-databases\">Navigating and Creating Databases</h2><p>We've got to start somewhere, so it might as well be with database management. Admittedly, this will be the most useless of the things we'll cover. The act of navigating databases is best suited for a GUI.</p><h3 id=\"show-databases\">Show Databases</h3><p>If you access your database via command line shell (for some reason), the first logical thing to do is to list the available databases:</p><pre><code class=\"language-sql\">SHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n</code></pre>\n<h3 id=\"use-database\">USE Database</h3><p>Now that we've listed the possible databases we can connect to, we can explore what each of these contains. To do this, we have to specify which database we want to connect to, AKA \"use.\" </p><pre><code class=\"language-sql\">db&gt; USE database_name;\nDatabase changed\n</code></pre>\n<h3 id=\"create-database\">Create Database</h3><p>Creating databases is straightforward. Be sure to pay attention to the <em>character set</em> when creating a database: this will determine which types of characters your database will be able to accept. For example, if we try to insert special encoded characters into a simple UTF-8 database, those characters won’t turn out as we’d expect.</p><pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n</code></pre>\n<p>Bonus: here's the shorthand for creating a database and then showing the result:</p><pre><code class=\"language-sql\">SHOW CREATE DATABASE database_name;\n</code></pre>\n<h2 id=\"creating-and-modifying-tables\">Creating and Modifying Tables</h2><p>Creating tables via SQL syntax can be critical when automating data imports. When creating a table, we also set the column names, types, and keys:</p><pre><code>CREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];</code></pre><p>We can specify <code>IF NOT EXISTS</code> when creating our table if we'd like to include validation in our query. When present, the table will only be created if a table of the specified name does not exist.</p><p>When creating each of our columns, there are a number of things we can specify per-column:</p><ul><li><strong>Data Type (required):</strong> The data which can be saved to cells of this column (such as INTEGER, TEXT, etc).</li><li><strong>Key Type:</strong> Creates a key for the column.</li><li><strong>Key Attributes:</strong> Any key-related attributes, such as auto-incrementing.</li><li><strong>Default:</strong> If rows are created in the table without values passed to the current column, the value specified as <code>DEFAULT</code> </li><li><strong>Primary Key:</strong> Allows any of the previous specified columns to be set as the table's primary key.</li></ul><p>MySQL tables can have a 'storage engine' specified via <code>ENGINE=[engine_type]</code>, which determines the core logic of how the table will interpret data. Leaving this blank defaults to InnoDB and is almost certainly fine to be left alone. In case you're interested, you can find more about MySQL engines <a href=\"https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html\">here</a>.</p><p>Here's an example of what an actual <code>CREATE TABLE</code> query would look like:</p><pre><code>CREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;</code></pre><h3 id=\"managing-keys-for-existing-tables\">Managing Keys for Existing Tables</h3><p>If we don't specify our keys at table creation time, we can always do so after the fact. SQL tables can accept the following key types:</p><ul><li><strong>Primary Key:</strong> One or more fields/columns that uniquely identify a record in the table. It can not accept null, duplicate values.</li><li><strong>Candidate Key:</strong> Candidate keys are kind of like groups of non-committed Primary Keys; these keys only accept unique values, and <em>could potentially</em> be used in the place of a Primary Key if need be, but are not actual Primary Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.</li><li><strong>Alternate Key:</strong> Refers to a single Candidate Key (an alternative which can satisfy the duty of a Primary Key id need be).</li><li><strong>Composite/Compound Key:</strong> Defined by combing the values of multiple columns; the sum of which will always produce a unique value. There can be multiple Candidate Keys in one table. Each Candidate Key can work as Primary Key.</li><li><strong>Unique Key:</strong> A set of one or more fields/columns of a table that uniquely identify a record in a database table. Similar to Primary key, but it can accept only one null value, and it can not have duplicate values.</li><li><strong>Foreign Key: </strong>Foreign keys denote fields that serve as <em>another table's</em> Primary key. Foreign keys are useful for building relationships between tables. While a foreign key is required in the parent table where they are primary, foreign keys can be null or empty in the tables intended to relate to the other table.</li></ul><p>Let's look at an example query where we add a key to a table and dissect the pieces:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n</code></pre>\n<p><code>ALTER TABLE</code> is used to make any changes to a table's structure, whether that be modifying columns or keys.</p><p>In this example, we <code>ADD</code> a key which happens to be a <code>FOREIGN KEY</code>. While keys always refer to columns, keys themselves must have names of their own to distinguish the column's data and a key's conceptual logic. We name our key <code>foreign_key_name</code> and specify which column the key will act on with <code>(column_name)</code>. Because this is a foreign key, we need to specify which table's <em>primary key</em> we want this to be associated with. <code>REFERENCES parent_table(primary_key_column)</code> is stating that the foreign key in this table corresponds to values held in a column named <code>primary_key_column</code>, in a table named <code>parent_table</code>.</p><p>The statements <code>ON DELETE</code> and <code>ON UPDATE</code> are actions which take place if the parent table's primary key is deleted or updated, respectively. <code>ON DELETE CASCADE</code> would result in our tables foreign key being deleted if the corresponding primary key were to disappear.</p><h3 id=\"adding-columns\">Adding Columns</h3><p>Adding columns follows the same syntax we used when creating tables. An interesting additional feature is the ability to place the new column before or after preexisting columns:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n</code></pre>\n<div class=\"protip\">\nWhen referencing tables in PostgreSQL databases, we must specify the schema belongs to. Thus, <code>ALTER TABLE table_name</code> becomes <code>ALTER TABLE schema_name.table_name</code>. This applies to any time we reference tables, including when we create and delete tables.\n</div><h3 id=\"pop-quiz\">Pop Quiz</h3><p>The below statement uses elements of everything we've learned about modifying and creating table structures thus far. Can you discern what is happening here?</p><pre><code class=\"language-sql\">CREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n</code></pre>\n<h2 id=\"dropping-data\">Dropping Data</h2><p>DANGER ZONE: this is where we can start to mess things up. Dropping columns or tables results in a complete loss of data: whenever you see the word \"drop,\" be scared.</p><p>If you're sure you know what you're doing and would like to remove a table column, this can be done as such:</p><pre><code class=\"language-sql\">ALTER TABLE table\nDROP column;\n</code></pre>\n<p>Dropping a table destroys the table structure as well as all data within it:</p><pre><code class=\"language-sql\">DROP TABLE table_name;\n</code></pre>\n<p>Truncating a table, on the other hand, will purge the table of data but retain the table itself:</p><pre><code class=\"language-sql\">TRUNCATE TABLE table_name;\n</code></pre>\n<h3 id=\"drop-foreign-key\">Drop Foreign Key</h3><p>Like tables and columns, we can drop keys as well:</p><pre><code class=\"language-sql\">ALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n</code></pre>\n<p>This can also be handed by dropping CONSTRAINT:</p><pre><code class=\"language-sql\">ALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n</code></pre>\n<h2 id=\"working-with-views-specific-to-postgresql-\">Working with Views (Specific to PostgreSQL)</h2><p>Lastly, let's explore the act of creating views. There are three types of views PostgreSQL can handle:</p><ul>\n<li><strong>Simple Views</strong>: Virtual tables which represent data of underlying tables. Simple views are automatically updatable: the system will allow INSERT, UPDATE and DELETE statements to be used on the view in the same way as on a regular table.</li>\n<li><strong>Materialized Views</strong>: PostgreSQL extends the view concept to a next level that allows views to store data 'physically', and we call those views are materialized views. A materialized view caches the result of a complex query and then allow you to refresh the result periodically.</li>\n<li><strong>Recursive Views</strong>: Recursive views are a bit difficult to explain without delving deep into the complicated (but cool!) functionality of recursive reporting. I won't get into the details, but these views are able to represent relationships which go multiple layers deep. Here's a quick taste, if you;re curious:</li>\n</ul>\n<p><strong>Sample </strong><code>RECURSIVE</code> <strong>query:</strong></p><pre><code class=\"language-sql\">WITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' &gt; ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n</code></pre>\n<p><strong>Output:</strong></p><pre><code class=\"language-shell\"> employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North &gt; Megan Berry\n           3 | Michael North &gt; Sarah Berry\n           4 | Michael North &gt; Zoe Black\n           5 | Michael North &gt; Tim James\n           6 | Michael North &gt; Megan Berry &gt; Bella Tucker\n           7 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe\n           8 | Michael North &gt; Megan Berry &gt; Max Mills\n           9 | Michael North &gt; Megan Berry &gt; Benjamin Glover\n          10 | Michael North &gt; Sarah Berry &gt; Carolyn Henderson\n          11 | Michael North &gt; Sarah Berry &gt; Nicola Kelly\n          12 | Michael North &gt; Sarah Berry &gt; Alexandra Climo\n          13 | Michael North &gt; Sarah Berry &gt; Dominic King\n          14 | Michael North &gt; Zoe Black &gt; Leonard Gray\n          15 | Michael North &gt; Zoe Black &gt; Eric Rampling\n          16 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Piers Paige\n          17 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Ryan Henderson\n          18 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Frank Tucker\n          19 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Nathan Ferguson\n          20 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Kevin Rampling\n(20 rows)\n</code></pre>\n<h3 id=\"creating-a-view\">Creating a View</h3><p>Creating a simple view is as simple as writing a standard query! All that is required is the addition of <code>CREATE VIEW view_name AS</code> before the query, and this will create a saved place for us to always come back and reference the results of this query:</p><pre><code class=\"language-sql\">CREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n</code></pre>\n<h2 id=\"get-out-there-and-start-sqling\">Get Out There and Start SQLing</h2><p>I highly encourage anybody to get in the habit of <em>always </em>writing SQL queries by hand. With the right GUI, autocompletion can be your best friend.</p><p>Explicitly forcing one's self to write queries instead of copy &amp; pasting anything forces us to come to realizations, such as SQL's order of operations. Indeed, this query holds the correct syntax...</p><pre><code class=\"language-sql\">SELECT *\nFROM table_name\nWHERE column_name = 'Value';\n</code></pre>\n<p>...Whereas this one does not:</p><pre><code class=\"language-sql\">SELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n</code></pre>\n<p>Grasping the subtleties of SQL is the difference between being blazing fast and mostly clueless. The good news is, you’ll start to find that these concepts aren’t nearly as daunting as they may have once seemed, so the track from ‘bad data engineer’ to ‘expert’ is an easy win that would be foolish not to take.</p><p>Stick around for next time where we actually work with data in <strong>SQL: The Sequel</strong>, rated PG-13.</p>","url":"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/","uuid":"fe99e822-f21a-432c-8bbf-4d399e575570","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5bb0ec7999ff33f06876e1"}},{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},{"node":{"id":"Ghost__Post__5c5a3e362c71af62216fd45e","title":"Manage Database Models with Flask-SQLAlchemy","slug":"manage-database-models-with-flask-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","custom_excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","created_at_pretty":"06 February, 2019","published_at_pretty":"06 February, 2019","updated_at_pretty":"03 April, 2019","created_at":"2019-02-05T20:53:58.000-05:00","published_at":"2019-02-06T08:00:00.000-05:00","updated_at":"2019-04-03T11:38:02.000-04:00","meta_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","meta_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","og_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","twitter_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","twitter_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Python’s fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Python’s fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"By now you're surely familiar with the benefits of Python's core SQLAlchemy\nlibrary\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/]:\nthe all-in-one solution for basically anything database related. Like most major\nPython libraries, SQLAlchemy has been ported into a version specifically\ncompatible with Flask, aptly named Flask-SQLAlchemy.\n\nSimilar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us\nto modify application data by easily creating defined models. Regardless of what\nyour database of choice might be, Flask-SQLAlchemy will ensure that the models\nwe create in Python will translate to the syntax of our chosen database. Given\nthe ease-of-use and one-size-fits-all  nature of Flask-SQLAlchemy, it's no\nwonder that the library has been the de facto database library of choice for\nFlask since the very beginning (seriously, is there even another option?)\n\nConfiguring Flask-SQLAlchemy For Your Application\nThere are a few essential configuration variables we need to set upfront before\ninteracting with our database. As is standard, we'll be using a class defined in\n config.py  to handle our Flask config:\n\nimport os\n\n\nclass Config:\n    \"\"\"Set Flask configuration vars from .env file.\"\"\"\n    \n    # General\n    TESTING = os.environ[\"TESTING\"]\n    FLASK_DEBUG = os.environ[\"FLASK_DEBUG\"]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(\"SQLALCHEMY_DATABASE_URI\")\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(\"SQLALCHEMY_TRACK_MODIFICATIONS\")\n\n\nLet's break these down:\n\n * SQLALCHEMY_DATABASE_URI: the connection string we need to connect to our\n   database. This follows the standard convention: \n   [db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]\n * SQLALCHEMY_ECHO: When set to 'True', Flask-SQLAlchemy will log all database\n   activity to Python's stderr for debugging purposes.\n * SQLALCHEMY_TRACK_MODIFICATIONS: Honestly, I just always set this to 'False,'\n   otherwise an obnoxious warning appears every time you run your app reminding\n   you that this option takes a lot of system resources.\n\nThose are the big ones we should worry about. If you're into some next-level\ndatabase shit, there are a few other pro-mode configuration variables which you\ncan find here [http://flask-sqlalchemy.pocoo.org/2.3/config/].\n\nBy using the exact naming conventions for the variables above, simply having\nthem in our config file will automatically configure our database connections\nfor us. We will never have to create engines, sessions, or connections.\nInitiating Flask-SQLAlchemy With Our App\nAs always, we're going to use the Flask Application Factory method\n[https://hackersandslackers.com/structuring-your-flask-app/]  for initiating our\napp. If you're unfamiliar with the term, you're going to find this tutorial to\nbe confusing and pretty much useless.\n\nThe most basic __init__.py  file for Flask applications using Flask-SQLAlchemy\nshould look like this:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n\n\nNote the presence of db  and its location: this our database object being set as\na global  variable outside of create_app(). Inside of create_app(), on the other\nhand, contains the line db.init_app(app). Even though we've set our db object\nglobally, this means nothing until we initialize it after creating our\napplication. We accomplish this by calling init_app()  within create_app(), and\npassing our app as the parameter. Within the actual 'application context' is\nwhere we'll call create_all(), which we'll cover in a bit.\n\nIf that last paragraph sounded like total gibberish to you, you are not alone. \nThe Flask Application Factory is perhaps one of the most odd and poorly\nexplained concepts in Python software development- my best advice is to not\nbecome frustrated, take the copy + paste code above, and blindly accept the\nspoon-fed nonsense enough times until it becomes second nature. That's what I\ndid, and even as I worked through this tutorial, I still  came across obnoxious\nquirks that caught me off-guard.\n\nTake note of import we make inside of the application context called routes.\nThis is one of two files we haven't written just yet: once we create them, our\napplication file structure will look something like this:\n\nmy-app\n├── /application\n│   ├── __init__.py\n│   ├── routes.py\n│   ├── models.py\n├── .env\n├── config.py\n└── wsgi.py\n\n\nCreating Database Models\nCreate a models.py  file in our application directory. Here we'll import the db \nobject that we created in __init__.py. Now we can create database models by\ndefining classes in this file.\n\nA common example would be to start with a User  model. The first variable we\ncreate is __tablename__, which will correspond to the name of the SQL table new\nusers will be saved. Each additional variable we create within this model class\nwill correspond a column in the database:\n\nfrom . import db\n\n\nclass User(db.Model):\n    \"\"\"Model for user accounts.\"\"\"\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n\nEach \"column\" accepts the following attributes:\n\n * Data Type:  Accepts one of the following: String(size), Text, DateTime, Float\n   , Boolean, PickleType, or LargeBinary.\n * primary_key: Whether or not the column should serve as the primary key.\n * unique: Whether or not to enforce unique values for the column.\n * nullable: Denotes required fields.\n\nWith our first model created, you're already way closer to interacting with your\ndatabase than you might think.\n\nCreating Our First Entry\nLet's create a user in our routes.py  file.\n\nfrom flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(\"User created!\")\n\n\nCheck out how easy this is! All it takes to create a user is create an instance\nof the User  class from models.py, add it to our session via \ndb.session.add(new_user), and commit the changes with db.session.commit()! Let's\nsee what happens when we run this app:\n\nUser Created!\n\n\nThat's what we like to see! If we access our database at this point, we can see\nthat this exact record was created in our users  table.\n\nQuerying Our New Data\nCreating information is dope, but how can we confirm it exists? I've added a few\nthings to routes.py  to show us what's up:\n\nfrom flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=\"Show Users\")\n\n\n  The statement User.query.all()  will return all instances of User  in our\ndatabase. I created a Jinja template to show us all records nicely:\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n  {% for user in users %}\n    <ul id=\"user.username\">\n      <li>Username: {{ user.username }}</li>\n      <li>Email: {{ user.email }}</li>\n      <li>Created: {{ user.created }}</li>\n      <li>Bio: {{ user.bio }}</li>\n      <li>Admin: {{ user.admin }}</li>\n    </ul>\n  {% endfor %}\n{% endblock %}\n\n\nThus, our app gives us:\n\nWe have liftoff!So we can get a single user, but what about a whole table full\nof users? Well, all we need to do is keep changing the username and email\naddress (our unique keys, to avoid a clash) when firing up the app, and each\ntime it runs, it'll create a new user. Here's what comes back after running the\napp a few times with different values:\n\nI Can't Believe It's Not Error Messages.™Here, Take All My Stuff\nSure, Flask-SQLAlchemy is great once you get going, but as we've already seen\n\"getting set up\" isn't always a walk in the park. This is one of those things\nthat always seems to be wrong no matter how many times you've done it from\nmemory.\n\nAs a parting gift, I've put the source for this tutorial up on Github\n[https://github.com/toddbirchard/flasksqlalchemy-tutorial]  for you to treasure\nand enjoy. No seriously, take it. Get it away from me. I'm done with Flask for\ntoday. I need to go play some Rocket League.\n\nPS: Add me on PSN","html":"<p>By now you're surely familiar with the benefits of Python's <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">core SQLAlchemy library</a>: the all-in-one solution for basically anything database related. Like most major Python libraries, SQLAlchemy has been ported into a version specifically compatible with Flask, aptly named <strong>Flask-SQLAlchemy</strong>.</p><p>Similar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us to modify application data by easily creating defined models. Regardless of what your database of choice might be, Flask-SQLAlchemy will ensure that the models we create in Python will translate to the syntax of our chosen database. Given the ease-of-use and one-size-fits-all  nature of Flask-SQLAlchemy, it's no wonder that the library has been the de facto database library of choice for Flask since the very beginning (seriously, is there even another option?)</p><h2 id=\"configuring-flask-sqlalchemy-for-your-application\">Configuring Flask-SQLAlchemy For Your Application</h2><p>There are a few essential configuration variables we need to set upfront before interacting with our database. As is standard, we'll be using a class defined in <code>config.py</code> to handle our Flask config:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\n\n\nclass Config:\n    &quot;&quot;&quot;Set Flask configuration vars from .env file.&quot;&quot;&quot;\n    \n    # General\n    TESTING = os.environ[&quot;TESTING&quot;]\n    FLASK_DEBUG = os.environ[&quot;FLASK_DEBUG&quot;]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(&quot;SQLALCHEMY_DATABASE_URI&quot;)\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(&quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's break these down:</p><ul><li><code>SQLALCHEMY_DATABASE_URI</code>: the connection string we need to connect to our database. This follows the standard convention: <code>[db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]</code></li><li><code>SQLALCHEMY_ECHO</code>: When set to 'True', Flask-SQLAlchemy will log all database activity to Python's stderr for debugging purposes.</li><li><code>SQLALCHEMY_TRACK_MODIFICATIONS</code>: Honestly, I just always set this to 'False,' otherwise an obnoxious warning appears every time you run your app reminding you that this option takes a lot of system resources.</li></ul><p>Those are the big ones we should worry about. If you're into some next-level database shit, there are a few other pro-mode configuration variables which you can find <a href=\"http://flask-sqlalchemy.pocoo.org/2.3/config/\">here</a>.</p><!--kg-card-begin: html--><div class=\"protip\">By using the exact naming conventions for the variables above, simply having them in our config file will automatically configure our database connections for us. We will never have to create engines, sessions, or connections.</div><!--kg-card-end: html--><h2 id=\"initiating-flask-sqlalchemy-with-our-app\">Initiating Flask-SQLAlchemy With Our App</h2><p>As always, we're going to use the <a href=\"https://hackersandslackers.com/structuring-your-flask-app/\">Flask Application Factory method</a> for initiating our app. If you're unfamiliar with the term, you're going to find this tutorial to be confusing and pretty much useless.</p><p>The most basic <code>__init__.py</code> file for Flask applications using Flask-SQLAlchemy should look like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>Note the presence of <code>db</code> and its location: this our database object being set as a <em>global</em> variable outside of <code>create_app()</code>. Inside of <code>create_app()</code>, on the other hand, contains the line <code>db.init_app(app)</code>. Even though we've set our db object globally, this means nothing until we initialize it after creating our application. We accomplish this by calling <code>init_app()</code> within <code>create_app()</code>, and passing our app as the parameter. Within the actual 'application context' is where we'll call <code>create_all()</code>, which we'll cover in a bit.</p><p>If that last paragraph sounded like total gibberish to you, <em>you are not alone.</em> The Flask Application Factory is perhaps one of the most odd and poorly explained concepts in Python software development- my best advice is to not become frustrated, take the copy + paste code above, and blindly accept the spoon-fed nonsense enough times until it becomes second nature. That's what I did, and even as I worked through this tutorial, I <em>still</em> came across obnoxious quirks that caught me off-guard.</p><p>Take note of import we make inside of the application context called <strong>routes</strong>. This is one of two files we haven't written just yet: once we create them, our application file structure will look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-app\n├── /application\n│   ├── __init__.py\n│   ├── routes.py\n│   ├── models.py\n├── .env\n├── config.py\n└── wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-database-models\">Creating Database Models</h2><p>Create a <code>models.py</code> file in our application directory. Here we'll import the <code>db</code> object that we created in <code>__init__.py</code>. Now we can create database models by defining classes in this file.</p><p>A common example would be to start with a <strong>User</strong> model. The first variable we create is <code>__tablename__</code>, which will correspond to the name of the SQL table new users will be saved. Each additional variable we create within this model class will correspond a column in the database:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from . import db\n\n\nclass User(db.Model):\n    &quot;&quot;&quot;Model for user accounts.&quot;&quot;&quot;\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '&lt;User {}&gt;'.format(self.username)\n</code></pre>\n<!--kg-card-end: markdown--><p>Each \"column\" accepts the following attributes:</p><ul><li><strong>Data Type:</strong> Accepts one of the following: <code>String(size)</code>, <code>Text</code>, <code>DateTime</code>, <code>Float</code>, <code>Boolean</code>, <code>PickleType</code>, or <code>LargeBinary</code>.</li><li><strong>primary_key</strong>: Whether or not the column should serve as the primary key.</li><li><strong>unique</strong>: Whether or not to enforce unique values for the column.</li><li><strong>nullable: </strong>Denotes required fields.</li></ul><p>With our first model created, you're already way closer to interacting with your database than you might think.</p><h2 id=\"creating-our-first-entry\">Creating Our First Entry</h2><p>Let's create a user in our <code>routes.py</code> file.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(&quot;User created!&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out how easy this is! All it takes to create a user is create an instance of the <code>User</code> class from <code>models.py</code>, add it to our session via <code>db.session.add(new_user)</code>, and commit the changes with <code>db.session.commit()</code>! Let's see what happens when we run this app:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">User Created!\n</code></pre>\n<!--kg-card-end: markdown--><p>That's what we like to see! If we access our database at this point, we can see that this exact record was created in our <em>users</em> table.</p><h2 id=\"querying-our-new-data\">Querying Our New Data</h2><p>Creating information is dope, but how can we confirm it exists? I've added a few things to <code>routes.py</code> to show us what's up:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=&quot;Show Users&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p> The statement <code>User.query.all()</code> will return all instances of <code>User</code> in our database. I created a Jinja template to show us all records nicely:</p><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n  {% for user in users %}\n    &lt;ul id=&quot;user.username&quot;&gt;\n      &lt;li&gt;Username: {{ user.username }}&lt;/li&gt;\n      &lt;li&gt;Email: {{ user.email }}&lt;/li&gt;\n      &lt;li&gt;Created: {{ user.created }}&lt;/li&gt;\n      &lt;li&gt;Bio: {{ user.bio }}&lt;/li&gt;\n      &lt;li&gt;Admin: {{ user.admin }}&lt;/li&gt;\n    &lt;/ul&gt;\n  {% endfor %}\n{% endblock %}\n</code></pre>\n<!--kg-card-end: markdown--><p>Thus, our app gives us:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/flasksqlalchemy-test.png\" class=\"kg-image\"><figcaption>We have liftoff!</figcaption></figure><!--kg-card-end: image--><p>So we can get a single user, but what about a whole table full of users? Well, all we need to do is keep changing the username and email address (our unique keys, to avoid a clash) when firing up the app, and each time it runs, it'll create a new user. Here's what comes back after running the app a few times with different values:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-06-at-12.39.07-AM.png\" class=\"kg-image\"><figcaption>I Can't Believe It's Not Error Messages.<b>™</b></figcaption></figure><!--kg-card-end: image--><h3 id=\"here-take-all-my-stuff\">Here, Take All My Stuff</h3><p>Sure, Flask-SQLAlchemy is great once you get going, but as we've already seen \"getting set up\" isn't always a walk in the park. This is one of those things that always seems to be wrong no matter how many times you've done it from memory.</p><p>As a parting gift, I've put the source for this tutorial up <a href=\"https://github.com/toddbirchard/flasksqlalchemy-tutorial\">on Github</a> for you to treasure and enjoy. No seriously, take it. Get it away from me. I'm done with Flask for today. I need to go play some Rocket League.</p><!--kg-card-begin: html--><span style=\"color: #a7a7a7;font-style: italic;font-size:.9em;\">PS: Add me on PSN</span><!--kg-card-end: html-->","url":"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/","uuid":"e9fdcb15-3289-472f-8892-2e01cdaced9d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5a3e362c71af62216fd45e"}},{"node":{"id":"Ghost__Post__5c570ae30b20340296f57709","title":"Easily Build GraphQL APIs with Prisma","slug":"easily-build-graphql-apis-with-prisma","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","custom_excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","created_at_pretty":"03 February, 2019","published_at_pretty":"03 February, 2019","updated_at_pretty":"29 March, 2019","created_at":"2019-02-03T10:38:11.000-05:00","published_at":"2019-02-03T16:33:15.000-05:00","updated_at":"2019-03-29T14:47:01.000-04:00","meta_title":"Build GraphQL APIs with Prisma | Hackers and Slackers","meta_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","og_title":"Build GraphQL APIs with Prisma","twitter_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","twitter_title":"Build GraphQL APIs with Prisma","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"The technology sector is reeling after an official statement was released by the\nUN's International Council of Coolness last week. The statement clearly states\nwhat status-quo developers have feared for months: if you haven't shifted from\nREST to GraphQL by now, you are officially recognized by the international\ncommunity to hold \"uncool\" status. A humanitarian crisis is already unfolding as\nrefugees of coolness are threatening to overtake borders, sparking fears of an\ninflux of Thinkpad Laptops, IntelliJ, and other Class A  uncool narcotics.\n\nHold up: is GraphQL That Dramatic of an Improvement over REST?\nIn all honesty, I've found that the only way to properly answer this question is\nto first utter \"kinda,\" then mull back and forth for a little while, and then\nfinishing with a weak statement like \"so pretty much, yeah.\"\n\nLet’s put it this way. When you’re first familiarizing yourself with a set of\ndata, what do you do? Do you read extensive documentation about the SQL table\nyou’re about to check out? Do you read the entire spec for your version\nPostgreSQL to see if it contains the functionality that might be missing for\nsome reason? I’m going to guess you do neither of these- chances are you just\nlook at the data. \n\nUsing any REST API is inherently a context-switch. No matter how many APIs\nyou’ve worked with in the past, you’ll never be able to know a new API’s\nendpoints, quirks, or the awful manner in which the creator has abandoned any\ndistinction between GET, POST, or PUT methods altogether. GraphQL is not\nnecessarily more technologically impressive than REST, but it does  provide us a\nsyntax and workflow comparable to working directly with databases with which\nwe're already familiar.\n\nRemember when us young guys justified replacing older devs when we came out of\nthe gate with NodeJS, arguing that context-switching changes everything? GraphQL\nis just that: a \"better\" technology with less mental context-switching, which\nconveniently serves a double-purpose for enterprises looking to fire anybody\nthey perceive to be dead weight over the age of 30. Good luck finding a better\nsynopsis than that.\n\nWhat’s this Prisma Nonsense? \nPrisma [https://www.prisma.io/]  is a free (FREE!) service that provides with\nthe tools to create an API client, as well as an Admin panel to manage it.\nWithout any prior knowledge of GraphQL needed, Prisma provides us with:\n\n * A CLI which’s stand up a web server which will serve as our API: either cloud\n   or self-hosted.\n * Automatic integration with your database of choice (including cloud DBs, such\n   as RDS).\n * A clever obfuscation of data models via a simple config file. No classes, no\n   code, no bullshit.\n * A \"playground\" interface which allows us to mess around in GraphQL syntax\n   against our models without breaking everything.\n * A web GUI which displays the relationships between all of these things and\n   their usage.\n\nIn short, Prisma does our jobs for us. Now that tasks associated with building\nAPIs, creating ORMs, and managing databases have all been trivialized, we can\nfinally cut some more of that dead weight we mentioned earlier- specifically\nBob, the asshole coming up on his 35th birthday sitting on his high-horse just\nbecause he has an obligation to feed 3 beautiful children. Sorry Bob, it just\nwasn't working out.\n\nPrisma does  provide the option to set up a test environment on their cloud, but\nlet's do something useful with our lives for once and build something\nproduction-ready. In this case, that means standing up a 5-dollar Digital Ocean\nDroplet.\n\nCreate a Prisma Account\nGet over to the sexually appealing Prisma Cloud landing page\n[https://www.prisma.io/cloud]  and make yourself an account. When prompted, make\nsure you select Deploy a new Prisma Service.\n\nExample services are for sissys.You should then be prompted with the following\nscreen. It will instruct you to install an NPM package, but there are a few\nthings we need to do first.\n\nWhen she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my\nlove.\"Installing Prisma Dependencies on a Fresh VPS\nSSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this\ntutorial. If you happen to be using Ubuntu as well, feel free to copy + paste\nall the stuff I'm sure you've done a million times already. First, we need to\ninstall Node:\n\n$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n\n\nBefore you do anything crazy like copy & paste those two lines from Prisma,\nyou're going to need to set up Docker a few steps later, so you might as well do\nthat now: \n\n1. Install Docker Dependencies\n$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n\n2. Add Docker Key\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n\n\n3. Get Docker Repository\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\n\n4. Finally Install Docker\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGood job, you're doing great.\n\nInstall & Activate The Prisma CLI\nCool, now we can carry on with Prisma's demands. Install the Prisma CLI\nglobally, and then use said CLI to log in to Prisma.\n\n$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n\n\nWait a couple of seconds after entering the login prompt, and you'll notice your\nbrowser window will have changed to indicate that you're now logged in. \n\nThe next step will create the local files which serve as the heart and soul of\nour API. Make sure you init Prisma  in whichever directory you like to keep\nthings in:\n\n$ cd /my/desired/directory/\n$ prisma init my-prisma\n\n\nInitiating the project will kickstart a quick and painless interrogation\nprocess. Keep in mind that it's recommended to use Prisma with a fresh database\ninstance; in my case, I spun up a cloud PostgreSQL instance.\n\n? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n\n\nCompleting this will result in the following structure:\n\nmy-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n\n\nWe're almost there cowboy and/or cowgirl. \n\nSet Phasers to \"Deploy\"\nPrisma is going to stand itself up on port 4466, which is closed by default on\nmost servers. Make sure you have this port open:\n\n$ ufw allow 4466\n\n\nFinally, we need to set a secret  in order to connect to Prisma cloud. Open the \ndocker-compose.yml  file and uncomment the managementApiSecret  line. Replace\nthe value with some sort of deep dark personal secret of yours.\n\n$ vim docker-compose.yml\n\n\nversion: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - \"4466:4466\"\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n\n\nFor some reason, Prisma does not automatically specify your SSL preferences in\ndocker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your\ndatabase requires SSL, be sure to add ssl: true  to the end of your\ndocker-compose config. Otherwise, your deployment will fail.As per the comment\nin our yml  file, we need to export the secret we specify with \nmanagementApiSecret: my-secret  as an environment variable. Back in your Prisma\ndirectory, export your secret as such:\n\n$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n\n\nThis secret is used to generate a token to secure our endpoint. Skipping this\nstep would result in exposing your database to the world with full read/write\naccess to anybody.\n\nIt's Game Time\nIt's time to deploy, baby! Do it, push the button! DO IT NOW!\n\n$ docker-compose up -d\n$ prisma deploy\n\n\nDeploying for the first time does a few things. It'll stand up a 'playground'\ninterface on your local server (localhost:4466), as well as automatically get\nyou set up with Prisma Cloud, which is essentially an admin interface for your\ndeployment hosted on Prisma's site.\n\nCheck Out Your Workspace \nVisit [Your Server's IP]:4466 to see what you've done:\n\nA playground for children of all agesCheck it out! Along with documentation of\nthe generic data models Prisma shipped with, you can test queries or mutations\non the left side of the UI, and receive responses on the right. Sure beats\nPostman imho.\n\nDon't Look Down: You're in the Cloud\nYou can now add your server to Prisma Cloud to get the benefits of their admin\npanel. From here, you can modify information directly, review usage metrics, and\nmanage multiple instances:\n\nBreaking News: Prisma is Too Cool For School.Working With Prisma And GraphQL\nNow that we've spun up this shiny new toy, let's be sure we know how to drive\nit.\n\nOn your VPS, take a look at the datamodels.prisma  file:\n\n$ vim datamodels.prisma\n\n\nYou should see a data model called User (everybody has this model). To add or\nmodify data models, all we need to do is change the fields as we see fit, set\ntheir data type, and specify whether or not we'd like the field to be unique.\nBelow I've added a couple of new 'fields.'\n\ntype User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n\n\nDeploying Prisma again with these changes will modify our database's table\nstructure to match the new model:\n\n$ prisma deploy\n\n\nThere you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to\ncompletely falsify the existence of a GraphQL certification and throw that on\nthere, too. If you're the kind of person who enjoys reading technical posts like\nthis in your free time, chances are you're already qualified for the job. Unless\nyou're Bob.","html":"<p>The technology sector is reeling after an official statement was released by the UN's International Council of Coolness last week. The statement clearly states what status-quo developers have feared for months: if you haven't shifted from REST to GraphQL by now, you are officially recognized by the international community to hold \"uncool\" status. A humanitarian crisis is already unfolding as refugees of coolness are threatening to overtake borders, sparking fears of an influx of Thinkpad Laptops, IntelliJ, and other <em>Class A</em> uncool narcotics.</p><h3 id=\"hold-up-is-graphql-that-dramatic-of-an-improvement-over-rest\">Hold up: is GraphQL That Dramatic of an Improvement over REST?</h3><p>In all honesty, I've found that the only way to properly answer this question is to first utter \"kinda,\" then mull back and forth for a little while, and then finishing with a weak statement like \"so pretty much, yeah.\"</p><p>Let’s put it this way. When you’re first familiarizing yourself with a set of data, what do you do? Do you read extensive documentation about the SQL table you’re about to check out? Do you read the entire spec for your version PostgreSQL to see if it contains the functionality that might be missing for some reason? I’m going to guess you do neither of these- chances are you <em>just look at the data. </em></p><p>Using any REST API is inherently a context-switch. No matter how many APIs you’ve worked with in the past, you’ll never be able to know a new API’s endpoints, quirks, or the awful manner in which the creator has abandoned any distinction between GET, POST, or PUT methods altogether. GraphQL is not necessarily more technologically impressive than REST, but it <em>does</em> provide us a syntax and workflow comparable to working directly with databases with which we're already familiar.</p><p>Remember when us young guys justified replacing older devs when we came out of the gate with NodeJS, arguing that context-switching <em>changes everything</em>? GraphQL is just that: a \"better\" technology with less mental context-switching, which conveniently serves a double-purpose for enterprises looking to fire anybody they perceive to be dead weight over the age of 30. Good luck finding a better synopsis than that.</p><h2 id=\"what-s-this-prisma-nonsense\">What’s this Prisma Nonsense? </h2><p><a href=\"https://www.prisma.io/\">Prisma</a> is a free (FREE!) service that provides with the tools to create an API client, as well as an Admin panel to manage it. Without any prior knowledge of GraphQL needed, Prisma provides us with:</p><ul><li>A CLI which’s stand up a web server which will serve as our API: either cloud or self-hosted.</li><li>Automatic integration with your database of choice (including cloud DBs, such as RDS).</li><li>A clever obfuscation of data models via a simple config file. No classes, no code, no bullshit.</li><li>A \"playground\" interface which allows us to mess around in GraphQL syntax against our models without breaking everything.</li><li>A web GUI which displays the relationships between all of these things and their usage.</li></ul><p>In short, Prisma does our jobs for us. Now that tasks associated with building APIs, creating ORMs, and managing databases have all been trivialized, we can finally cut some more of that dead weight we mentioned earlier- specifically Bob, the asshole coming up on his 35th birthday sitting on his high-horse just because he has an obligation to feed 3 beautiful children. Sorry Bob, it just wasn't working out.</p><p>Prisma <em>does</em> provide the option to set up a test environment on their cloud, but let's do something useful with our lives for once and build something production-ready. In this case, that means standing up a 5-dollar Digital Ocean Droplet.</p><h3 id=\"create-a-prisma-account\">Create a Prisma Account</h3><p>Get over to the sexually appealing <a href=\"https://www.prisma.io/cloud\">Prisma Cloud landing page</a> and make yourself an account. When prompted, make sure you select <strong>Deploy a new Prisma Service</strong>.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step1.png\" class=\"kg-image\"><figcaption>Example services are for sissys.</figcaption></figure><!--kg-card-end: image--><p>You should then be prompted with the following screen. It will instruct you to install an NPM package, but there are a few things we need to do first.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step2.png\" class=\"kg-image\"><figcaption>When she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my love.\"</figcaption></figure><!--kg-card-end: image--><h2 id=\"installing-prisma-dependencies-on-a-fresh-vps\">Installing Prisma Dependencies on a Fresh VPS</h2><p>SSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this tutorial. If you happen to be using Ubuntu as well, feel free to copy + paste all the stuff I'm sure you've done a million times already. First, we need to install Node:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n</code></pre>\n<!--kg-card-end: markdown--><p>Before you do anything crazy like copy &amp; paste those two lines from Prisma, you're going to need to set up Docker a few steps later, so you might as well do that now: </p><h3 id=\"1-install-docker-dependencies\">1. Install Docker Dependencies</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"2-add-docker-key\">2. Add Docker Key</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"3-get-docker-repository\">3. Get Docker Repository</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo add-apt-repository \\\n   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable&quot;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"4-finally-install-docker\">4. Finally Install Docker</h3><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre>\n<!--kg-card-end: markdown--><p>Good job, you're doing great.</p><h2 id=\"install-activate-the-prisma-cli\">Install &amp; Activate The Prisma CLI</h2><p>Cool, now we can carry on with Prisma's demands. Install the Prisma CLI globally, and then use said CLI to log in to Prisma.  </p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n</code></pre>\n<!--kg-card-end: markdown--><p>Wait a couple of seconds after entering the login prompt, and you'll notice your browser window will have changed to indicate that you're now logged in. </p><p>The next step will create the local files which serve as the heart and soul of our API. Make sure you init <code>Prisma</code> in whichever directory you like to keep things in:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd /my/desired/directory/\n$ prisma init my-prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>Initiating the project will kickstart a quick and painless interrogation process. Keep in mind that it's recommended to use Prisma with a fresh database instance; in my case, I spun up a cloud PostgreSQL instance.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n</code></pre>\n<!--kg-card-end: markdown--><p>Completing this will result in the following structure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n</code></pre>\n<!--kg-card-end: markdown--><p>We're almost there cowboy and/or cowgirl. </p><h2 id=\"set-phasers-to-deploy\">Set Phasers to \"Deploy\"</h2><p>Prisma is going to stand itself up on port <strong>4466</strong>, which is closed by default on most servers. Make sure you have this port open:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ ufw allow 4466\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we need to set a <em>secret</em> in order to connect to Prisma cloud. Open the <code>docker-compose.yml</code> file and uncomment the <code>managementApiSecret</code> line. Replace the value with some sort of deep dark personal secret of yours.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim docker-compose.yml\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">version: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - &quot;4466:4466&quot;\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"proptip\">\nFor some reason, Prisma does not automatically specify your SSL preferences in docker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your database requires SSL, be sure to add <code>ssl: true</code> to the end of your docker-compose config. Otherwise, your deployment will fail. \n</div><!--kg-card-end: html--><p>As per the comment in our <code>yml</code> file, we need to export the secret we specify with <code>managementApiSecret: my-secret</code> as an environment variable. Back in your Prisma directory, export your secret as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n</code></pre>\n<!--kg-card-end: markdown--><p>This secret is used to generate a token to secure our endpoint. Skipping this step would result in exposing your database to the world with full read/write access to anybody.</p><h2 id=\"it-s-game-time\">It's Game Time</h2><p>It's time to deploy, baby! Do it, push the button! DO IT NOW!</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ docker-compose up -d\n$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying for the first time does a few things. It'll stand up a 'playground' interface on your local server (localhost:4466), as well as automatically get you set up with Prisma Cloud, which is essentially an admin interface for your deployment hosted on Prisma's site.</p><h3 id=\"check-out-your-workspace\">Check Out Your Workspace </h3><p>Visit [Your Server's IP]:4466 to see what you've done:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/playground.png\" class=\"kg-image\"><figcaption>A playground for children of all ages</figcaption></figure><!--kg-card-end: image--><p>Check it out! Along with documentation of the generic data models Prisma shipped with, you can test queries or mutations on the left side of the UI, and receive responses on the right. Sure beats Postman imho.</p><h3 id=\"don-t-look-down-you-re-in-the-cloud\">Don't Look Down: You're in the Cloud</h3><p>You can now add your server to Prisma Cloud to get the benefits of their admin panel. From here, you can modify information directly, review usage metrics, and manage multiple instances:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prismacloud.gif\" class=\"kg-image\"><figcaption>Breaking News: Prisma is Too Cool For School.</figcaption></figure><!--kg-card-end: image--><h2 id=\"working-with-prisma-and-graphql\">Working With Prisma And GraphQL</h2><p>Now that we've spun up this shiny new toy, let's be sure we know how to drive it.</p><p>On your VPS, take a look at the <code>datamodels.prisma</code> file:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim datamodels.prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>You should see a data model called User (everybody has this model). To add or modify data models, all we need to do is change the fields as we see fit, set their data type, and specify whether or not we'd like the field to be unique. Below I've added a couple of new 'fields.'</p><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">type User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying Prisma again with these changes will modify our database's table structure to match the new model:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>There you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to completely falsify the existence of a GraphQL certification and throw that on there, too. If you're the kind of person who enjoys reading technical posts like this in your free time, chances are you're already qualified for the job. Unless you're Bob.</p>","url":"https://hackersandslackers.com/easily-build-graphql-apis-with-prisma/","uuid":"86286c72-478c-4108-8bef-89ca01caf043","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c570ae30b20340296f57709"}},{"node":{"id":"Ghost__Post__5c47584f4f3823107c9e8f23","title":"Google BigQuery's Python SDK: Creating Tables Programmatically","slug":"getting-started-google-big-query-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","custom_excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","created_at_pretty":"22 January, 2019","published_at_pretty":"02 February, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-22T12:52:15.000-05:00","published_at":"2019-02-02T09:24:00.000-05:00","updated_at":"2019-03-28T17:06:20.000-04:00","meta_title":"Google BigQuery's Python SDK: Creating Tables | Hackers and Slackers","meta_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","og_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","twitter_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","twitter_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"GCP is on the rise, and it's getting harder and harder to have conversations\naround data without addressing the 500-pound gorilla in the room: Google\nBigQuery. With most enterprises comfortably settled into their Apache-based Big\nData stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers\nAnd Slackers is no such enterprise. Thus, we aren't afraid to ask the Big\nquestion: how much easier would life be with BigQuery?\n\nBig Data, BigQuery\nIn short, BigQuery trivializes the act of querying against multiple,\nunpredictable data sources. To better understand when this is useful, it would\nbetter serve us to identify the types of questions BigQuery can answer. Such as:\n\n * What are our users doing across our multiple systems? How do we leverage log\n   files outputted by multiple systems to find out?\n * How can we consolidate information about employee information, payroll, and\n   benefits, when these all live in isolated systems?\n * What the hell am I supposed to do with all these spreadsheets?\n\nUnlike previous solutions, BigQuery solves these problems in a single product\nand does so with SQL-like query syntax,  a web interface, and 7 native Client\nLibraries.  There are plenty of reasons to love BigQuery, but let's start with\none we've recently already talked about: the auto-generation of table schemas. \n\nMatt has demonstrated how to approach this problem manually with the help of\nPandas\n[https://hackersandslackers.com/downcast-numerical-columns-python-pandas/]. I\nprovided a more gimmicky approach by leveraging the Python table-schema library\n[https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/]. With\nBigQuery, we find yet another alternative which is neither manual or gimmicky:\nperfect for those who are lazy, rich, and demand perfection (AKA: your clients,\nprobably).\n\nFirst, we'll need to get our data into BigQuery\n\nUploading Data into Google Cloud Storage via the Python SDK\nBigQuery requires us to go through Google Cloud Storage as a buffer before\ninputting data into tables. No big deal, we'll write a script!\n\nWe're assuming that you have a basic knowledge of Google Cloud, Google Cloud\nStorage, and how to download a JSON Service Account key\n[https://cloud.google.com/bigquery/docs/reference/libraries]  to store locally\n(hint: click the link).\n\nfrom google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n\n\nThe above is nearly a copy + paste of Google Cloud's sample code for the Google\nCloud Storage Python SDK:\n\n * bucket_uri  is found by inspecting any bucket's information on Google Cloud.\n * bucket_name  is... well, you know.\n * bucket_target  represents the resulting file structure representing the saved\n   CSV when completed.\n * local_dataset  is the path to a CSV we've stored locally: we can assume that\n   we've grabbed some data from somewhere, like an API, and tossed into a local\n   file temporarily.\n\nSuccessfully executing the above results in the following message:\n\nFile data/test.csv uploaded to datasets/data_upload.csv.\n\n\nInserting Data from Cloud Storage to BigQuery\nThat was the easy part. Let's move on to the good stuff:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n\n\nWe've added the function insert_bigquery()  to handle creating a BigQuery table\nout of a CSV.\n\nAfter we set our client, we create a dataset reference. In BigQuery, tables can\nbelong to a 'dataset,' which is a grouping of tables. Compare this concept to\nMongoDB's collections, or PostgreSQL's schemas. Note that this process is made\nmuch easier by the fact that we stored our project key locally: otherwise, we'd\nhave to specify which Google Cloud project we're looking for, etc.\n\nWith the dataset specified, we begin to build our \"job\" object with \nLoadJobConfig. This is like loading a gun before unleashing a shotgun blast into\nthe face of our problems. Alternatively, a more relevant comparison could be\nwith the Python requests  library and the act of prepping an API request before\nexecution.\n\nWe set job_config.autodetect  to be True, obviously. \njob_config.skip_leading_rows  reserves our header row from screwing things up.\n\nload_job  puts our request together, and load_job.result()  executes said job.\nThe .result()  method graciously puts the rest of our script on hold until the\nspecified job is completed. In our case, we want this happen: it simplifies our\nscript so that we don't need to verify this manually before moving on.\n\nLet's see what running that job with our fake data looks like in the BigQuery\nUI:\n\nAll my fake friends are here!Getting Our Flawlessly Inferred Table Schema\nBigQuery surely gets table schemas wrong some of the time. That said, I have yet\nto see it happen. Let's wrap this script up:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    \"\"\"Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n\n\nWith the addition of get_bigquery_schema(), our script is complete!\n\nTableReference()  is similar to the dataset reference we went over earlier, only\nfor tables (duh). This allows us to call upon get_table(), which returns a Table\nclass representing the table we just created. Amongst the methods of that class,\nwe can call .schema(), which gives us precisely what we want: a beautiful\nrepresentation of a Table schema, generated from raw CSV information, where\nthere previously was none.\n\nBehold the fruits of your labor:\n\n[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n\n\nThere you have it; a correctly inferred schema, from data which wasn't entirely\nclean in the first place (our dates are in MM/DD/YY  format as opposed to \nMM/DD/YYYY, but Google still gets it right. How? Because Google).\n\nIt Doesn't End Here\nI hope it goes without saying that abusing Google BigQuery's API to generate\nschemas for you is only a small, obscure use case of what Google BigQuery is\nintended to do, and what it can do for you. That said, I need to stop this\nfanboying post before anybody realizes I'll promote their products for free\nforever (I think I may have passed that point).\n\nIn case you're interested, the source code for this script has been uploaded as\na Gist here\n[https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e]. Have at\nit, and remember to think Big™*.\n\n*Not a real trademark, I'm making things up again.","html":"<p>GCP is on the rise, and it's getting harder and harder to have conversations around data without addressing the 500-pound gorilla in the room: Google BigQuery. With most enterprises comfortably settled into their Apache-based Big Data stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers And Slackers is no such enterprise. Thus, we aren't afraid to ask the Big question: how much easier would life be with BigQuery?</p><h2 id=\"big-data-bigquery\">Big Data, BigQuery</h2><p>In short, BigQuery trivializes the act of querying against multiple, unpredictable data sources. To better understand when this is useful, it would better serve us to identify the types of questions BigQuery can answer. Such as:</p><ul><li>What are our users doing across our multiple systems? How do we leverage log files outputted by multiple systems to find out?</li><li>How can we consolidate information about employee information, payroll, and benefits, when these all live in isolated systems?</li><li>What the hell am I supposed to do with all these spreadsheets?</li></ul><p>Unlike previous solutions, BigQuery solves these problems in a single product and does so with <strong>SQL-like query syntax,</strong> a <strong>web interface</strong>, and <strong>7 native Client Libraries.</strong> There are plenty of reasons to love BigQuery, but let's start with one we've recently already talked about: the <em>auto-generation of table schemas</em>. </p><p>Matt has demonstrated how to approach this problem <a href=\"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/\">manually with the help of Pandas</a>. I provided a more gimmicky approach by leveraging the <a href=\"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/\">Python table-schema library</a>. With BigQuery, we find yet another alternative which is neither manual or gimmicky: perfect for those who are lazy, rich, and demand perfection (AKA: your clients, probably).</p><p>First, we'll need to get our data into BigQuery</p><h2 id=\"uploading-data-into-google-cloud-storage-via-the-python-sdk\">Uploading Data into Google Cloud Storage via the Python SDK</h2><p>BigQuery requires us to go through Google Cloud Storage as a buffer before inputting data into tables. No big deal, we'll write a script!</p><p>We're assuming that you have a basic knowledge of Google Cloud, Google Cloud Storage, and how to download a <a href=\"https://cloud.google.com/bigquery/docs/reference/libraries\">JSON Service Account key</a> to store locally (hint: click the link).</p><pre><code class=\"language-python\">from google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n</code></pre>\n<p>The above is nearly a copy + paste of Google Cloud's sample code for the Google Cloud Storage Python SDK:</p><ul><li><code>bucket_uri</code> is found by inspecting any bucket's information on Google Cloud.</li><li><code>bucket_name</code> is... well, you know.</li><li><code>bucket_target</code><strong> </strong>represents the resulting file structure representing the saved CSV when completed.</li><li><code>local_dataset</code> is the path to a CSV we've stored locally: we can assume that we've grabbed some data from somewhere, like an API, and tossed into a local file temporarily.</li></ul><p>Successfully executing the above results in the following message:</p><pre><code class=\"language-shell\">File data/test.csv uploaded to datasets/data_upload.csv.\n</code></pre>\n<h2 id=\"inserting-data-from-cloud-storage-to-bigquery\">Inserting Data from Cloud Storage to BigQuery</h2><p>That was the easy part. Let's move on to the good stuff:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n</code></pre>\n<p>We've added the function <code>insert_bigquery()</code> to handle creating a BigQuery table out of a CSV.</p><p>After we set our client, we create a <strong>dataset reference</strong>. In BigQuery, tables can belong to a 'dataset,' which is a grouping of tables. Compare this concept to MongoDB's <strong>collections, </strong>or PostgreSQL's <strong>schemas</strong>. Note that this process is made much easier by the fact that we stored our project key locally: otherwise, we'd have to specify which Google Cloud project we're looking for, etc.</p><p>With the dataset specified, we begin to build our \"job\" object with <code>LoadJobConfig</code>. This is like loading a gun before unleashing a shotgun blast into the face of our problems. Alternatively, a more relevant comparison could be with the Python <code>requests</code> library and the act of prepping an API request before execution.</p><p>We set <code>job_config.autodetect</code> to be <code>True</code>, obviously. <code>job_config.skip_leading_rows</code> reserves our header row from screwing things up.</p><p><code>load_job</code> puts our request together, and <code>load_job.result()</code> executes said job. The <code>.result()</code> method graciously puts the rest of our script on hold until the specified job is completed. In our case, we want this happen: it simplifies our script so that we don't need to verify this manually before moving on.</p><p>Let's see what running that job with our fake data looks like in the BigQuery UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-01-at-7.42.52-PM.png\" class=\"kg-image\"><figcaption>All my fake friends are here!</figcaption></figure><h2 id=\"getting-our-flawlessly-inferred-table-schema\">Getting Our Flawlessly Inferred Table Schema</h2><p>BigQuery surely gets table schemas wrong <em>some </em>of the time. That said, I have yet to see it happen. Let's wrap this script up:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    &quot;&quot;&quot;Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n</code></pre>\n<p>With the addition of <code>get_bigquery_schema()</code>, our script is complete!</p><p><code>TableReference()</code> is similar to the dataset reference we went over earlier, only for tables (duh). This allows us to call upon <code>get_table()</code>, which returns a Table class representing the table we just created. Amongst the methods of that class, we can call <code>.schema()</code>, which gives us precisely what we want: a beautiful representation of a Table schema, generated from raw CSV information, where there previously was none.</p><p>Behold the fruits of your labor:</p><pre><code class=\"language-python\">[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n</code></pre>\n<p>There you have it; a correctly inferred schema, from data which wasn't entirely clean in the first place (our dates are in <strong>MM/DD/YY</strong> format as opposed to <strong>MM/DD/YYYY</strong>, but Google still gets it right. How? Because Google).</p><h3 id=\"it-doesn-t-end-here\">It Doesn't End Here</h3><p>I hope it goes without saying that abusing Google BigQuery's API to generate schemas for you is only a small, obscure use case of what Google BigQuery is intended to do, and what it can do for you. That said, I need to stop this fanboying post before anybody realizes I'll promote their products for free forever (I think I may have passed that point).</p><p>In case you're interested, the source code for this script has been uploaded as a Gist <a href=\"https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e\">here</a>. Have at it, and remember to think Big<strong>™*</strong>.</p><span style=\"color: #a6a6a6;font-style: italic; font-size: .8em;\">*Not a real trademark, I'm making things up again.</span>","url":"https://hackersandslackers.com/getting-started-google-big-query-python/","uuid":"30051eb2-7fa7-4a09-91fd-c3f11966b398","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47584f4f3823107c9e8f23"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apache’s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c3fc99b89c81d4ccc3f64b1","title":"The Hostile Extraction of Tableau Server Data","slug":"hostile-extraction-of-tableau-server-data","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-2.jpg","excerpt":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","custom_excerpt":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","created_at_pretty":"17 January, 2019","published_at_pretty":"17 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-16T19:17:31.000-05:00","published_at":"2019-01-17T07:43:00.000-05:00","updated_at":"2019-03-28T11:06:50.000-04:00","meta_title":"The Hostile Extraction of Tableau Server Data | Hackers and Slackers","meta_description":"How to create a Python application to take back your Tableau data. Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","og_description":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","og_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-2.jpg","og_title":"The Hostile Extraction of Tableau Server Data","twitter_description":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-1.jpg","twitter_title":"The Hostile Extraction of Tableau Server Data","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"I try my best not to hate on Tableau. It was the software’s combination of power\nand ease-of-use that drove me to purchase a license in the first place. Ever\nsince then, I’m finding new and exciting ways Tableau intentionally locks users\nout of their data. \n\nI gave the Tableau Server Client Python library\n[https://tableau.github.io/server-client-python/docs/]  a spin recently in hopes\nof finding something useful. I decided to (sigh, once more) allow Tableau the\nbenefit of the doubt: after pushing four updates in a single month, maybe things\nhad changed. On the contrary, the Tableau business strategy stands strong: to be\na raging, flaming turd pile. A perfect example of this is the View  object\nTableau allows you to interact with on your server. Those familiar know that \nviews  are slang for sheets  of workbooks  stored on Tableau server. \n\nConnecting to your Tableau instance via Python to retrieve your view objects is\na piece of cake:\n\nimport tableauserverclient as TSC\ntableau_auth = TSC.TableauAuth('username', 'password')\nserver = TSC.Server('http://servername')\n\nwith server.auth.sign_in(tableau_auth):\n  all_views, pagination_item = server.views.get()\n  print([view.name for view in all_views])\n\n\nThis simple snippet lists every view object on your server. Wow! Think of what\nwe can do with all that tabular data we worked so hard to transform, rig- WRONG.\nLook at what Tableau's Python 'View Object' actually contains:\n\n * id  The identifier of the view item.\n * name  The name of the view.\n * owner_id  The id for the owner of the view.\n * preview_image  The thumbnail image for the view.\n * total_views  The usage statistics for the view. Indicates the total number of\n   times the view has been accessed.\n * workbook_id  The id of the workbook associated with the view.\n\nHOLY MOSES STOP THE PRESSES, we can get a thumbnail image  of our data?! THANK\nYOU GENEROUS TABLEAU OVERLORDS!\n\nNotice how there's no mention of, you know, the actual data.\n\nWe're going to play a game. In the wake of my time has been wasted, I feel that\nwarm tickling feeling which seems to say \"Viciously dismantle the ambitions of\nan establishment!\"  May I remind you, we're talking about the kind of\nestablishment that bills customer licenses based on the number of CPUs being\nutilized by their server infrastructure.  This is effectively recognizing the\nhorrifying and inefficient codebase behind Tableau server, and leveraging this\nflaw for monetization. Yes, you're paying more money to incentivize worst\npractices.\n\nLet's Make a Flask App. An Angry One.\nIn our last post I shared a little script to help you get started stealing data\n[https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/] \n off your own Tableau Server. That doesn't quite scratch my itch anymore. I'm\ngoing to build an interface. I want to make it easy as possible for anybody to\nsystemically rob Tableau Server of every penny its got. That's a lot of pennies\nwhen we consider the equation: data = oil + new.\n\nBefore I bore you, here's a quick demo of the MVP we're building:\n\nEach table is a view being pulled from Tableau Server.This POC demonstrates that\nit is very  possible to automate the extraction of Tableau views from Tableau\nServer. The success  message is signaling that we've successfully taken a\nTableau view and created a corresponding table in an external database. Any data\nwe manipulate in Tableau is now truly ours: we can now leverage the transforms\nwe've applied in workbooks, use this data in other applications, and utilize an\nextract scheduler to keep the data coming. We've turned a BI tool into an ETL\ntool. In other words, you can kindly take those thumbnail previews and shove it.\n\nI'll be open sourcing all of this, as is my civic duty. Let us be clear to\nenterprises: withholding freedom to one's own data is an act of war. Pricing\nmodels which reward poor craftsmanship are an insult to our intellect. For every\narrogant atrocity committed against consumers, the war will wage twice as hard.\nI should probably mention these opinions are my own.\n\nThe Proletariat Strikes Back\nGet a feel for where we're heading with the obligatory project-file-structure\ntree:\n\ntableau-exporter\n├── application\n│   ├── __init__.py\n│   ├── database.py\n│   ├── tableau.py\n│   ├── routes.py\n│   ├── static\n│   │   ├── data\n│   │   │   └── view.csv\n│   │   ├── dist\n│   │   │   ├── all.css\n│   │   │   ├── packed.js\n│   │   ├── img\n│   │   │   └── tableaugithub.jpg\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── scss\n│   │       └── main.scss\n│   └── templates\n│       ├── export.html\n│       ├── index.html\n│       ├── layout.html\n│       └── view.html\n├── config.ini\n├── config.py\n├── app.yaml\n├── start.sh\n├── wsgi.py\n├── Pipfile\n├── README.md\n└── requirements.txt\n\n\nAs usual, we're using a classic Flask application factory  set up here.\n\nWeapons Of Choice\nLet's have a look at our core arsenal:\n\n * requests: We're achieving our goal by exploiting some loopholes exposed in\n   the Tableau REST API.\n * pandas: Will handle everything from extracting comma-separated data into a\n   CSV, render HTML tables, and output SQL.\n * flask_sqlalchemy: Used in tandem with pandas  to handle shipping our data off\n   elsewhere.\n * flask_redis: To handle session variables.\n\nInitiating our Application\nHere's how we construct our app:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_redis import FlaskRedis\n\n# Set global entities\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Initiate globals\n        db.init_app(app)\n        r.init_app(app, charset=\"utf-8\", decode_responses=True)\n\n        # Set global contexts\n        r.set('uri', app.config['SQLALCHEMY_DATABASE_URI'])\n        r.set('baseurl',  app.config['BASE_URL'])\n        r.set('username',  app.config['USERNAME'])\n        r.set('password', app.config['PASSWORD'])\n\n        # Import our modules\n        from . import routes\n        from . import tableau\n        app.register_blueprint(routes.home_blueprint)\n\n        return app\n\n\nThis should all feel like business-as-usual. The core of our application is\nsplit between routes.py, which handles views, and tableau.py, which handles the\nanti-establishment logic. Let's begin with the latter.\n\nLife, Liberty, and The Pursuit of Sick Data Pipelines\nOur good friend tableau.py  might look familiar to those who joined us last time\n[https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/]\n. tableau.py  has been busy hitting the gym since then and is looking sharp for\nprimetime:\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    \"\"\"Class for working in a Tableau instance.\"\"\"\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        \"\"\"Extract contents of a single view.\"\"\"\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        \"\"\"List all views belonging to a Tableau Site.\"\"\"\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        \"\"\"Receive Auth token to perform API requests.\"\"\"\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        \"\"\"Retrieve ID of Tableau 'site' instance.\"\"\"\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        \"\"\"Retrieve core XML for interacting with Tableau.\"\"\"\n        headers = {'Content-Type': 'application/xml'}\n        body = '<tsRequest><credentials name=\"' + cls.__username + '\" password=\"' + cls.__password + '\" ><site contentUrl=\"' + cls.__contenturl + '\" /></credentials></tsRequest>'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n\n\nI wish I could take full credit for what a shit show this class appears to be at\nfirst glance, but I assure you we've been left with no choice. For example: have\nI mentioned that Tableau's REST API returns XML so malformed that it breaks XML\nparsers? I can't tell incompetence from malicious intent at this point.\n\nHere's a method breakdown of our class:\n\n * initialize_tableau_request(): Handles initial auth and returns valuable\n   information such as site ID and API Token to be used thereafter.\n * get_site(): Extracts the site ID from XML returned by the above.\n * get_token(): Similarly extracts our token.\n * list_views(): Compiles a list of all views within a Tableau site, giving us a\n   chance to select ones for extraction.\n * get_view(): Takes a view of our choice and creates a DataFrame, which is to\n   be shipped off to a foreign database.\n\nOur Routing Logic\nMoving on we have routes.py  building the views and associated logic for our\napp:\n\nfrom flask import current_app as app\nfrom flask import render_template, Blueprint, request, Markup\nfrom flask_assets import Bundle, Environment\nfrom . import tableau\nfrom . import database\nimport pandas as pd\n\nhome_blueprint = Blueprint('home', __name__, template_folder='templates', static_folder='static')\n\nassets = Environment(app)\njs = Bundle('js/*.js', filters='jsmin', output='dist/packed.js')\nscss = Bundle('scss/*.scss', filters='libsass', output='dist/all.css')\nassets.register('scss_all', scss)\nassets.register('js_all', js)\nscss.build()\njs.build()\n\n\n@home_blueprint.route('/', methods=['GET', 'POST'])\ndef entry():\n    \"\"\"Homepage which lists all available views.\"\"\"\n    tableau_view_extractor = tableau.ExtractTableauView()\n    xml = tableau_view_extractor.initialize_tableau_request()\n    token = tableau_view_extractor.get_token(xml)\n    site = tableau_view_extractor.get_site(xml)\n    views = tableau_view_extractor.list_views(site, xml, token)\n    return render_template(\n        'index.html',\n        title=\"Here are your views.\",\n        template=\"home-template\",\n        views=views,\n        token=token,\n        xml=xml,\n        site=site\n    )\n\n\n@home_blueprint.route('/view', methods=['GET', 'POST'])\ndef view():\n    \"\"\"Displays a preview of a selected view.\"\"\"\n    site = request.args.get('site')\n    xml = request.args.get('xml')\n    view = request.args.get('view')\n    token = request.args.get('token')\n    tableau_view_extractor = tableau.ExtractTableauView()\n    view_df = tableau_view_extractor.get_view(site, xml, view, token)\n    view_df.to_csv('application/static/data/view.csv')\n    return render_template(\n        'view.html',\n        title='Your View',\n        template=\"home-template\",\n        view=view,\n        token=token,\n        xml=xml,\n        site=site,\n        view_df=Markup(view_df.to_html(index=False))\n    )\n\n\n@home_blueprint.route('/export', methods=['GET', 'POST'])\ndef export():\n    \"\"\"Exports view to external database.\"\"\"\n    view_df = pd.read_csv('application/static/data/view.csv')\n    view_df.to_sql(name='temp', con=database.engine, if_exists='replace', chunksize=50, index=True)\n    return render_template(\n        'export.html',\n        title='Success!',\n        template=\"success-template\",\n    )\n\n\nWe only have 3 pages to our application. They include our list of views, a\npreview of a single view, and a success page for when said view is exported.\nThis is all core Flask logic.\n\nPutting it On Display\nWe build our pages dynamically based on the values we pass our Jinja templates.\nThe homepage utilizes some nested loops to list the views we returned from \ntableau.py, and also makes use of query strings to pass values on to other\ntemplates.\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n<div class=\"extended-container {{template}}\">\n  <div class=\"container\">\n    <div class=\"row\">\n      <div class=\"col s12\">\n        <h1>{{title}}</h1>\n      </div>\n      <div class=\"col s12 flex-container\">\n        {% for view in views %}\n        <div class=\"download\">\n          <a href=\"{{ url_for('home.view') }}?token={{token}}&site={{site}}&view={{view.id}}&xml={{xml}}\">\n            <ul>\n              {% for key, value in view.items() %}\n              <li><span class=\"key {{key}}\">{{key}}</span> {{ value }}</li>\n              {% endfor %}\n            </ul>\n          </a>\n        </div>\n        {% endfor %}\n      </div>\n    </div>\n  </div>\n  {% endblock %}\n\n\nMoving on: our humble view.html  page has two purposes: display the selected\nview, and export it in the name of justice.\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n<div class=\"extended-container {{template}}\">\n  <div class=\"container\">\n    <div class=\"row\">\n      <div class=\"col s12\">\n        <h1>{{title}}</h1>\n        <a href=\"{{ url_for('home.export') }}\" class=\"export\"><i class=\"far fa-file-export\"></i></a>\n        {{view_df}}\n      </div>\n    </div>\n  </div>\n  {% endblock %}\n\n\nThe War is Not Over\nThis repository is open to the public and can be found here\n[https://github.com/toddbirchard/tableau-extraction]. There are still crusades\nleft ahead of us: for instance, building out this interface to accept\ncredentials via login as opposed to a config file, and the scheduling of view\nexports, as opposed to on-demand.\n\nWhere we go from here depends on what we the people decide. For all I know, I\ncould be shouting to an empty room here (I'm almost positive anybody who pays\nfor enterprise software prefers the blind eye of denial). If the opposite holds\ntrue, I dare say the revolution is only getting started.","html":"<p>I try my best not to hate on Tableau. It was the software’s combination of power and ease-of-use that drove me to purchase a license in the first place. Ever since then, I’m finding new and exciting ways Tableau intentionally locks users out of their data. </p><p>I gave the <a href=\"https://tableau.github.io/server-client-python/docs/\"><strong>Tableau Server Client</strong> Python library</a> a spin recently in hopes of finding something useful. I decided to (sigh, <em>once more</em>) allow Tableau the benefit of the doubt: after pushing <strong>four updates in a single month</strong>, maybe things had changed. On the contrary, the Tableau business strategy stands strong: to be a raging, flaming turd pile. A perfect example of this is the <strong>View</strong> object Tableau allows you to interact with on your server. Those familiar know that <strong>views</strong> are slang for <em>sheets</em> of <em>workbooks</em> stored on Tableau server. </p><p>Connecting to your Tableau instance via Python to retrieve your view objects is a piece of cake:</p><pre><code class=\"language-python\">import tableauserverclient as TSC\ntableau_auth = TSC.TableauAuth('username', 'password')\nserver = TSC.Server('http://servername')\n\nwith server.auth.sign_in(tableau_auth):\n  all_views, pagination_item = server.views.get()\n  print([view.name for view in all_views])\n</code></pre>\n<p>This simple snippet lists every view object on your server. Wow! Think of what we can do with all that tabular data we worked so hard to transform, rig- <strong>WRONG</strong>. Look at what Tableau's Python 'View Object' actually contains:</p><ul><li><code>id</code> The identifier of the view item.</li><li><code>name</code> The name of the view.</li><li><code>owner_id</code> The id for the owner of the view.</li><li><code>preview_image</code> The thumbnail image for the view.</li><li><code>total_views</code> The usage statistics for the view. Indicates the total number of times the view has been accessed.</li><li><code>workbook_id</code> The id of the workbook associated with the view.</li></ul><p>HOLY MOSES STOP THE PRESSES, we can get a <strong><em>thumbnail image</em></strong> of our data?! THANK YOU GENEROUS TABLEAU OVERLORDS!</p><p>Notice how there's no mention of, you know, the <em>actual data</em>.</p><p>We're going to play a game. In the wake of my time has been wasted, I feel that warm tickling feeling which seems to say <em>\"Viciously dismantle the ambitions of an establishment!\"</em> May I remind you, we're talking about the kind of establishment that bills customer licenses based on the <strong><em>number of CPUs being utilized by their server infrastructure.</em></strong> This is effectively recognizing the horrifying and inefficient codebase behind Tableau server, and leveraging this flaw for monetization. Yes, you're paying more money to incentivize worst practices.</p><h2 id=\"let-s-make-a-flask-app-an-angry-one-\">Let's Make a Flask App. An Angry One.</h2><p>In our last post I shared <a href=\"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/\">a little script to help you get started stealing data</a> off your own Tableau Server. That doesn't quite scratch my itch anymore. I'm going to build an interface. I want to make it easy as possible for anybody to systemically rob Tableau Server of every penny its got. That's a lot of pennies when we consider the equation: <strong>data = oil + new</strong>.</p><p>Before I bore you, here's a quick demo of the MVP we're building:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/tableau.gif\" class=\"kg-image\"><figcaption>Each table is a view being pulled from Tableau Server.</figcaption></figure><p>This POC demonstrates that it is <em>very</em> possible to automate the extraction of Tableau views from Tableau Server. The <em>success</em> message is signaling that we've successfully taken a Tableau view and <strong>created a corresponding table in an external database</strong>. Any data we manipulate in Tableau is now truly ours: we can now leverage the transforms we've applied in workbooks, use this data in other applications, and utilize an extract scheduler to keep the data coming. We've turned a BI tool into an ETL tool. In other words, you can kindly take those thumbnail previews and shove it.</p><p>I'll be open sourcing all of this, as is my civic duty. Let us be clear to enterprises: withholding freedom to one's own data is an act of war. Pricing models which reward poor craftsmanship are an insult to our intellect. For every arrogant atrocity committed against consumers, the war will wage twice as hard. I should probably mention these opinions are my own.</p><h2 id=\"the-proletariat-strikes-back\">The Proletariat Strikes Back</h2><p>Get a feel for where we're heading with the obligatory project-file-structure tree:</p><pre><code class=\"language-bash\">tableau-exporter\n├── application\n│   ├── __init__.py\n│   ├── database.py\n│   ├── tableau.py\n│   ├── routes.py\n│   ├── static\n│   │   ├── data\n│   │   │   └── view.csv\n│   │   ├── dist\n│   │   │   ├── all.css\n│   │   │   ├── packed.js\n│   │   ├── img\n│   │   │   └── tableaugithub.jpg\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── scss\n│   │       └── main.scss\n│   └── templates\n│       ├── export.html\n│       ├── index.html\n│       ├── layout.html\n│       └── view.html\n├── config.ini\n├── config.py\n├── app.yaml\n├── start.sh\n├── wsgi.py\n├── Pipfile\n├── README.md\n└── requirements.txt\n</code></pre>\n<p>As usual, we're using a classic Flask <em>application factory</em> set up here.</p><h3 id=\"weapons-of-choice\">Weapons Of Choice</h3><p>Let's have a look at our core arsenal:</p><ul><li><code>requests</code>: We're achieving our goal by exploiting some loopholes exposed in the Tableau REST API.</li><li><code>pandas</code>: Will handle everything from extracting comma-separated data into a CSV, render HTML tables, and output SQL.</li><li><code>flask_sqlalchemy</code>: Used in tandem with <em>pandas</em> to handle shipping our data off elsewhere.</li><li><code>flask_redis</code>: To handle session variables.</li></ul><h3 id=\"initiating-our-application\">Initiating our Application</h3><p>Here's how we construct our app:</p><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_redis import FlaskRedis\n\n# Set global entities\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Initiate globals\n        db.init_app(app)\n        r.init_app(app, charset=&quot;utf-8&quot;, decode_responses=True)\n\n        # Set global contexts\n        r.set('uri', app.config['SQLALCHEMY_DATABASE_URI'])\n        r.set('baseurl',  app.config['BASE_URL'])\n        r.set('username',  app.config['USERNAME'])\n        r.set('password', app.config['PASSWORD'])\n\n        # Import our modules\n        from . import routes\n        from . import tableau\n        app.register_blueprint(routes.home_blueprint)\n\n        return app\n</code></pre>\n<p>This should all feel like business-as-usual. The core of our application is split between <code>routes.py</code>, which handles views, and <code>tableau.py</code>, which handles the anti-establishment logic. Let's begin with the latter.</p><h2 id=\"life-liberty-and-the-pursuit-of-sick-data-pipelines\">Life, Liberty, and The Pursuit of Sick Data Pipelines</h2><p>Our good friend <code>tableau.py</code> might look familiar to those who joined us <a href=\"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/\">last time</a>. <code>tableau.py</code> has been busy hitting the gym since then and is looking sharp for primetime:</p><pre><code class=\"language-python\">import requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    &quot;&quot;&quot;Class for working in a Tableau instance.&quot;&quot;&quot;\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        &quot;&quot;&quot;Extract contents of a single view.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        &quot;&quot;&quot;List all views belonging to a Tableau Site.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        &quot;&quot;&quot;Receive Auth token to perform API requests.&quot;&quot;&quot;\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        &quot;&quot;&quot;Retrieve ID of Tableau 'site' instance.&quot;&quot;&quot;\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        &quot;&quot;&quot;Retrieve core XML for interacting with Tableau.&quot;&quot;&quot;\n        headers = {'Content-Type': 'application/xml'}\n        body = '&lt;tsRequest&gt;&lt;credentials name=&quot;' + cls.__username + '&quot; password=&quot;' + cls.__password + '&quot; &gt;&lt;site contentUrl=&quot;' + cls.__contenturl + '&quot; /&gt;&lt;/credentials&gt;&lt;/tsRequest&gt;'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n</code></pre>\n<p>I wish I could take full credit for what a shit show this class appears to be at first glance, but I assure you we've been left with no choice. For example: have I mentioned that Tableau's REST API returns XML so malformed that it breaks XML parsers? I can't tell incompetence from malicious intent at this point.</p><p>Here's a method breakdown of our class:</p><ul><li><code>initialize_tableau_request()</code>: Handles initial auth and returns valuable information such as site ID and API Token to be used thereafter.</li><li><code>get_site()</code>: Extracts the site ID from XML returned by the above.</li><li><code>get_token()</code>: Similarly extracts our token.</li><li><code>list_views()</code>: Compiles a list of all views within a Tableau site, giving us a chance to select ones for extraction.</li><li><code>get_view()</code>: Takes a view of our choice and creates a DataFrame, which is to be shipped off to a foreign database.</li></ul><h2 id=\"our-routing-logic\">Our Routing Logic</h2><p>Moving on we have <code>routes.py</code> building the views and associated logic for our app:</p><pre><code class=\"language-python\">from flask import current_app as app\nfrom flask import render_template, Blueprint, request, Markup\nfrom flask_assets import Bundle, Environment\nfrom . import tableau\nfrom . import database\nimport pandas as pd\n\nhome_blueprint = Blueprint('home', __name__, template_folder='templates', static_folder='static')\n\nassets = Environment(app)\njs = Bundle('js/*.js', filters='jsmin', output='dist/packed.js')\nscss = Bundle('scss/*.scss', filters='libsass', output='dist/all.css')\nassets.register('scss_all', scss)\nassets.register('js_all', js)\nscss.build()\njs.build()\n\n\n@home_blueprint.route('/', methods=['GET', 'POST'])\ndef entry():\n    &quot;&quot;&quot;Homepage which lists all available views.&quot;&quot;&quot;\n    tableau_view_extractor = tableau.ExtractTableauView()\n    xml = tableau_view_extractor.initialize_tableau_request()\n    token = tableau_view_extractor.get_token(xml)\n    site = tableau_view_extractor.get_site(xml)\n    views = tableau_view_extractor.list_views(site, xml, token)\n    return render_template(\n        'index.html',\n        title=&quot;Here are your views.&quot;,\n        template=&quot;home-template&quot;,\n        views=views,\n        token=token,\n        xml=xml,\n        site=site\n    )\n\n\n@home_blueprint.route('/view', methods=['GET', 'POST'])\ndef view():\n    &quot;&quot;&quot;Displays a preview of a selected view.&quot;&quot;&quot;\n    site = request.args.get('site')\n    xml = request.args.get('xml')\n    view = request.args.get('view')\n    token = request.args.get('token')\n    tableau_view_extractor = tableau.ExtractTableauView()\n    view_df = tableau_view_extractor.get_view(site, xml, view, token)\n    view_df.to_csv('application/static/data/view.csv')\n    return render_template(\n        'view.html',\n        title='Your View',\n        template=&quot;home-template&quot;,\n        view=view,\n        token=token,\n        xml=xml,\n        site=site,\n        view_df=Markup(view_df.to_html(index=False))\n    )\n\n\n@home_blueprint.route('/export', methods=['GET', 'POST'])\ndef export():\n    &quot;&quot;&quot;Exports view to external database.&quot;&quot;&quot;\n    view_df = pd.read_csv('application/static/data/view.csv')\n    view_df.to_sql(name='temp', con=database.engine, if_exists='replace', chunksize=50, index=True)\n    return render_template(\n        'export.html',\n        title='Success!',\n        template=&quot;success-template&quot;,\n    )\n</code></pre>\n<p>We only have 3 pages to our application. They include our list of views, a preview of a single view, and a success page for when said view is exported. This is all core Flask logic.</p><h2 id=\"putting-it-on-display\">Putting it On Display</h2><p>We build our pages dynamically based on the values we pass our Jinja templates. The homepage utilizes some nested loops to list the views we returned from <code>tableau.py</code>, and also makes use of query strings to pass values on to other templates.</p><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n&lt;div class=&quot;extended-container {{template}}&quot;&gt;\n  &lt;div class=&quot;container&quot;&gt;\n    &lt;div class=&quot;row&quot;&gt;\n      &lt;div class=&quot;col s12&quot;&gt;\n        &lt;h1&gt;{{title}}&lt;/h1&gt;\n      &lt;/div&gt;\n      &lt;div class=&quot;col s12 flex-container&quot;&gt;\n        {% for view in views %}\n        &lt;div class=&quot;download&quot;&gt;\n          &lt;a href=&quot;{{ url_for('home.view') }}?token={{token}}&amp;site={{site}}&amp;view={{view.id}}&amp;xml={{xml}}&quot;&gt;\n            &lt;ul&gt;\n              {% for key, value in view.items() %}\n              &lt;li&gt;&lt;span class=&quot;key {{key}}&quot;&gt;{{key}}&lt;/span&gt; {{ value }}&lt;/li&gt;\n              {% endfor %}\n            &lt;/ul&gt;\n          &lt;/a&gt;\n        &lt;/div&gt;\n        {% endfor %}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  {% endblock %}\n</code></pre>\n<p>Moving on: our humble <code>view.html</code> page has two purposes: display the selected view, and export it in the name of justice.</p><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n&lt;div class=&quot;extended-container {{template}}&quot;&gt;\n  &lt;div class=&quot;container&quot;&gt;\n    &lt;div class=&quot;row&quot;&gt;\n      &lt;div class=&quot;col s12&quot;&gt;\n        &lt;h1&gt;{{title}}&lt;/h1&gt;\n        &lt;a href=&quot;{{ url_for('home.export') }}&quot; class=&quot;export&quot;&gt;&lt;i class=&quot;far fa-file-export&quot;&gt;&lt;/i&gt;&lt;/a&gt;\n        {{view_df}}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  {% endblock %}\n</code></pre>\n<h2 id=\"the-war-is-not-over\">The War is Not Over</h2><p>This repository is open to the public and can be found <a href=\"https://github.com/toddbirchard/tableau-extraction\">here</a>. There are still crusades left ahead of us: for instance, building out this interface to accept credentials via login as opposed to a config file, and the scheduling of view exports, as opposed to on-demand.</p><p>Where we go from here depends on what we the people decide. For all I know, I could be shouting to an empty room here (I'm almost positive anybody who pays for enterprise software prefers the blind eye of denial). If the opposite holds true, I dare say the revolution is only getting started.</p>","url":"https://hackersandslackers.com/hostile-extraction-of-tableau-server-data/","uuid":"23914fde-b90e-4496-9a7d-56d6ae3765d9","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3fc99b89c81d4ccc3f64b1"}},{"node":{"id":"Ghost__Post__5c3d0b441719dc6b38ee53b6","title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","slug":"psycopg2-postgres-python-the-old-fashioned-way","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","custom_excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","created_at_pretty":"14 January, 2019","published_at_pretty":"15 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-14T17:20:52.000-05:00","published_at":"2019-01-15T15:57:34.000-05:00","updated_at":"2019-03-28T14:46:14.000-04:00","meta_title":"Psycopg2: PostgreSQL & Python the Old Way | Hackers and Slackers","meta_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","og_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","twitter_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","twitter_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Last time we met, we joyfully shared a little tirade\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/] \nabout missing out on functionality provided to us by libraries such as \nSQLAlchemy, and the advantages of interacting with databases where ORMs are\ninvolved. I stand by that sentiment, but I’ll now directly contradict myself by\nsharing some tips on using vanilla Psycopg2  to interact with databases. \n\nWe never know when we’ll be stranded on a desert island without access to\nSQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps\nyou’re part of a development team stuck in a certain way of doing things which\ndoesn't include utilize SQLAlchemy. Whatever the situation may be, we’re here\nfor you. \n\nThe Quintessential Boilerplate\nNo matter the type of database or the library, the boilerplate code for\nconnecting to databases remains mostly the same. To some extent, this even holds\ntrue across programming languages. Let's look at a barebones example while\nignoring the library at hand:\n\nimport SomeDatabaseLibrary\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nIn the above example, we could swap SomeDatabaseLibrary  with either Psycopg2 \nor PyMySQL  just the same. If we compare this to our example with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], it's easy to see that the\nbasics of utilizing connections, cursors, and the methods to close them\ntranscend libraries. If you know the basics of one, you know them all.\n\nIf you'd like to keep your connection logic separate (as I do), we can cleanly\nbreak the logic of handling connections out to a separate function. This time,\nwe'll replace SomeDatabaseLibrary  with Psycopg2  to produce some working code:\n\nimport psycopg2\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        \"\"\"Encapsulated connection management logic.\"\"\"\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(\"ERROR: Could not connect to Postgres.\")\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nPsycopg2 Extras\nPsycopg2 has many useful features via a library called psycopg2.extras\n[http://initd.org/psycopg/docs/extras.html]. My personal favorite of these\nextras is the DictCursor, which renders the rows being returned by our query as\nPython dictionaries  as opposed to lists. \n\nUsing DictCursor to Return More Useful Results\nWhen using a DictCursor, the key  is always the column name, and the value is\nthe value of that column in that particular row.\n\nTo use extras, we import psycopg2.extras.\n\nThen, we turn our attention to the following line:\n\nself.conn.cursor() as cur:\n\n\nWithin cursor, we can pass an attribute named cursor_factory   and set it as\nsuch:\n\nconn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n\n\nWhile our cursor is open, all rows returned by the query will be returned as\ndictionaries. For example, the row  in the above example will be returned as a\ndict. To demonstrate, here's what a query on this exact post  you're reading now\nlooks like when returned as a Dict:\n\n{\n    title: \"Psycopg2: Postgres & Python the Old Fashioned Way\",\n    slug: \"psycopg2-postgres-python-the-old-fashioned-way\",\n    feature_image: \"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n    status: \"draft\",\n    created_at: \"2019-01-14 22:20:52\",\n    custom_excerpt: \"Managing Postgres Database connections with Psycopg2\"\n}\n\n\nCompare this to what we would've seen had we not used DictCursor:\n\n[\"Psycopg2: Postgres & Python the Old Fashioned Way\",\n\"psycopg2-postgres-python-the-old-fashioned-way\",\n\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n\"draft\",\n\"2019-01-14 22:20:52\",\n\"Managing Postgres Database connections with Psycopg2\"]\n\n\nYes, it's a list, and thereby much less useful. Even from a readability\nstandpoint, I (the human user) have no idea what these values represent unless\ncomparing them to the table schema. Even worse would be compiling CSVs or even\nPandas Dataframes this way. When building a table made of lists, you set your\nheaders and hope that every row to come matches the number of header columns\none-to-one. Otherwise, it's entirely unclear as to which value belongs to which\ncolumn.\n\nOther Psycopg2 Extras\nThere are plenty more Psycopg2 extras where that came from; it's mostly up to\nyou to decide which are worth your while.\n\nFor example, another extra which might be of interest could be \npsycopg2.extras.LoggingConnection, useful for debugging connection statuses and\nerrors as you work through your program.\n\nThere's even a JSON Adaptation  extra, which provides support for leveraging\nJSON data in building queries:\n\ncur.execute(\"insert into mytable (jsondata) values (%s)\",\n    [Json({'a': 100})])\n\n\nI don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike\nextras I'm missing, feel free to call them out in the COMMENTS BELOW!  (Hah!\nI've always wanted to say that).\n\nA Few More Fundamental Useful Things\nSomething worth visiting is the ability to upload CSVs into Postgres to create\ntables. We can accomplish this via the built-in method copy_expert.\n\nFrom CSV to Postgres Table\nTo save a CSV to Postgres table, we need to begin with a basic SQL query saved\nin our project as a variable:\n\nCOPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n\n\nAs should be familiar, %s  represents a value we can pass in later. With this\nraw query, we're only missing two more values:\n\n * The path of our CSV file to be uploaded\n * The name of the table we'd like to upload to in Postgres\n\nCheck out how we use copy_expert  here to put it all together:\n\nsql = \"COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','\"\nfile = open('files/myfile.csv', \"r\")\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(\"truncate \" + table + \";\")\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n\n\nNotice that I opt to truncate the existing table before uploading the new data,\nas seen by cur.execute(\"truncate \" + table + \";\"). Without doing this, we would\nbe uploading the same CSV to the same table forever, creating duplicate rows\nover and over.\n\nWhat if The Table Doesn't Exist?\nUgh, of course  this would come up. The truth is (to the best of my knowledge),\nthere aren't many native things Psycopg2 has to offer to make this process easy.\n \n\nRecall that creating a table has a syntax similar to this:\n\nCREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n\n\nIt's not impossible to build this string yourself in Python. It just entails a\nlot of iterating over whichever dynamic data structure you have coming through,\ndetermining the correct data type per column, and then the unavoidable task of\nsetting your Primary  and Unique  keys if applicable. This is where my patience\nends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey,\nit's possible! I just don't feel like writing about it. :).\n\nGodspeed to You, Brave Warrior\nFor those about to Psycopg2, we salute you. That is unless the choice is\nself-inflicted. In that case, perhaps it's best we don't work together any time\nsoon.","html":"<p>Last time we met, we joyfully <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">shared a little tirade</a> about missing out on functionality provided to us by libraries such as <strong>SQLAlchemy</strong>, and the advantages of interacting with databases where ORMs are involved. I stand by that sentiment, but I’ll now directly contradict myself by sharing some tips on using vanilla <strong>Psycopg2</strong> to interact with databases. </p><p>We never know when we’ll be stranded on a desert island without access to SQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps you’re part of a development team stuck in a certain way of doing things which doesn't include utilize SQLAlchemy. Whatever the situation may be, we’re here for you. </p><h2 id=\"the-quintessential-boilerplate\">The Quintessential Boilerplate</h2><p>No matter the type of database or the library, the boilerplate code for connecting to databases remains mostly the same. To some extent, this even holds true across programming languages. Let's look at a barebones example while ignoring the library at hand:</p><pre><code class=\"language-python\">import SomeDatabaseLibrary\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<p>In the above example, we could swap <code>SomeDatabaseLibrary</code> with either <code>Psycopg2</code> or <code>PyMySQL</code> just the same. If we compare this to <a href=\"https://hackersandslackers.com/using-pymysql/\">our example with PyMySQL</a>, it's easy to see that the basics of utilizing <strong>connections</strong>, <strong>cursors</strong>, and the methods to close them transcend libraries. If you know the basics of one, you know them all.</p><p>If you'd like to keep your connection logic separate (as I do), we can cleanly break the logic of handling connections out to a separate function. This time, we'll replace <code>SomeDatabaseLibrary</code> with <code>Psycopg2</code> to produce some working code:</p><pre><code class=\"language-python\">import psycopg2\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        &quot;&quot;&quot;Encapsulated connection management logic.&quot;&quot;&quot;\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(&quot;ERROR: Could not connect to Postgres.&quot;)\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<h2 id=\"psycopg2-extras\">Psycopg2 Extras</h2><p>Psycopg2 has many useful features via a library called <a href=\"http://initd.org/psycopg/docs/extras.html\">psycopg2.extras</a>. My personal favorite of these extras is the <code>DictCursor</code>, which renders the rows being returned by our query as Python <em>dictionaries</em> as opposed to <em>lists. </em></p><h3 id=\"using-dictcursor-to-return-more-useful-results\">Using DictCursor to Return More Useful Results</h3><p>When using a DictCursor, the <em>key</em> is always the column name, and the <em>value </em>is the value of that column in that particular row.</p><p>To use extras, we <code>import psycopg2.extras</code>.</p><p>Then, we turn our attention to the following line:</p><pre><code class=\"language-python\">self.conn.cursor() as cur:\n</code></pre>\n<p>Within <code>cursor</code>, we can pass an attribute named <code>cursor_factory</code>  and set it as such:</p><pre><code class=\"language-python\">conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n</code></pre>\n<p>While our cursor is open, all rows returned by the query will be returned as dictionaries. For example, the <strong>row</strong> in the above example will be returned as a dict. To demonstrate, here's what a query on this <em>exact post</em> you're reading now looks like when returned as a Dict:</p><pre><code class=\"language-python\">{\n    title: &quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n    slug: &quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n    feature_image: &quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n    status: &quot;draft&quot;,\n    created_at: &quot;2019-01-14 22:20:52&quot;,\n    custom_excerpt: &quot;Managing Postgres Database connections with Psycopg2&quot;\n}\n</code></pre>\n<p>Compare this to what we would've seen had we not used <code>DictCursor</code>:</p><pre><code class=\"language-python\">[&quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n&quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n&quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n&quot;draft&quot;,\n&quot;2019-01-14 22:20:52&quot;,\n&quot;Managing Postgres Database connections with Psycopg2&quot;]\n</code></pre>\n<p>Yes, it's a list, and thereby much less useful. Even from a readability standpoint, I (the human user) have no idea what these values represent unless comparing them to the table schema. Even worse would be compiling CSVs or even Pandas Dataframes this way. When building a table made of lists, you set your headers and hope that every row to come matches the number of header columns one-to-one. Otherwise, it's entirely unclear as to which value belongs to which column.</p><h3 id=\"other-psycopg2-extras\">Other Psycopg2 Extras</h3><p>There are plenty more Psycopg2 extras where that came from; it's mostly up to you to decide which are worth your while.</p><p>For example, another extra which might be of interest could be <code>psycopg2.extras.LoggingConnection</code>, useful for debugging connection statuses and errors as you work through your program.</p><p>There's even a <strong>JSON Adaptation</strong> extra, which provides support for leveraging JSON data in building queries:</p><pre><code class=\"language-python\">cur.execute(&quot;insert into mytable (jsondata) values (%s)&quot;,\n    [Json({'a': 100})])\n</code></pre>\n<p>I don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike extras I'm missing, feel free to call them out in the <strong><em>COMMENTS BELOW!</em></strong> (Hah! I've always wanted to say that).</p><h2 id=\"a-few-more-fundamental-useful-things\">A Few More Fundamental Useful Things</h2><p>Something worth visiting is the ability to upload CSVs into Postgres to create tables. We can accomplish this via the built-in method <code>copy_expert</code>.</p><h3 id=\"from-csv-to-postgres-table\">From CSV to Postgres Table</h3><p>To save a CSV to Postgres table, we need to begin with a basic SQL query saved in our project as a variable:</p><pre><code class=\"language-sql\">COPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n</code></pre>\n<p>As should be familiar, <code>%s</code> represents a value we can pass in later. With this raw query, we're only missing two more values:</p><ul><li>The path of our CSV file to be uploaded</li><li>The name of the table we'd like to upload to in Postgres</li></ul><p>Check out how we use <code>copy_expert</code> here to put it all together:</p><pre><code class=\"language-python\">sql = &quot;COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','&quot;\nfile = open('files/myfile.csv', &quot;r&quot;)\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(&quot;truncate &quot; + table + &quot;;&quot;)\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n</code></pre>\n<p>Notice that I opt to truncate the existing table before uploading the new data, as seen by <code>cur.execute(\"truncate \" + table + \";\")</code>. Without doing this, we would be uploading the same CSV to the same table forever, creating duplicate rows over and over.</p><h3 id=\"what-if-the-table-doesn-t-exist\">What if The Table Doesn't Exist?</h3><p>Ugh, of <em>course</em> this would come up. The truth is (to the best of my knowledge), there aren't many native things Psycopg2 has to offer to make this process easy. </p><p>Recall that creating a table has a syntax similar to this:</p><pre><code class=\"language-sql\">CREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n</code></pre>\n<p>It's not impossible to build this string yourself in Python. It just entails a lot of iterating over whichever dynamic data structure you have coming through, determining the correct data type per column, and then the unavoidable task of setting your <strong>Primary</strong> and <strong>Unique</strong> keys if applicable. This is where my patience ends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey, it's possible! I just don't feel like writing about it. :).</p><h2 id=\"godspeed-to-you-brave-warrior\">Godspeed to You, Brave Warrior</h2><p>For those about to Psycopg2, we salute you. That is unless the choice is self-inflicted. In that case, perhaps it's best we don't work together any time soon.</p>","url":"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/","uuid":"f07736c5-c167-4fe9-b932-1b6b4d95e3ff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3d0b441719dc6b38ee53b6"}},{"node":{"id":"Ghost__Post__5c3409a094d3e847951adf44","title":"Pythonic Database Management with SQLAlchemy","slug":"pythonic-database-management-with-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","excerpt":"The iconic Python library for handling any conceivable database interaction.","custom_excerpt":"The iconic Python library for handling any conceivable database interaction.","created_at_pretty":"08 January, 2019","published_at_pretty":"09 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-07T21:23:28.000-05:00","published_at":"2019-01-09T08:00:00.000-05:00","updated_at":"2019-03-28T11:17:45.000-04:00","meta_title":"Pythonic Database Management with SQLAlchemy | Hackers and Slackers","meta_description":"The iconic Python library for handling any conceivable database interaction.","og_description":"The iconic Python library for handling any conceivable database interaction.","og_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","og_title":"Pythonic Database Management with SQLAlchemy","twitter_description":"The iconic Python library for handling any conceivable database interaction.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-1.jpg","twitter_title":"Pythonic Database Management with SQLAlchemy","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we've taken for granted thus far on Hackers and Slackers is a library\nmost data professionals have accepted as an undisputed standard: SQLAlchemy\n[https://www.sqlalchemy.org/].\n\nIn the past, we've covered database connection management and querying using\nlibraries such as PyMySQL [https://hackersandslackers.com/using-pymysql/]  and \nPsycopg2\n[https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/]\n, both of which do an excellent job of interacting with databases just as we'd\nexpect them to. The nature of opening/closing DB connections and working with\ncursors hasn't changed much in the past few decades (nearly the lifespan of\nrelational databases themselves). While boilerplate is boring, at least it has\nremained consistent, one might figure. That may  have been the case, but the\nphilosophical boom of MVC frameworks nearly a decade ago sparked the emergence\nof popularity for ORMs. While the world was singing praises of object-oriented\nprogramming, containing database-oriented functionality within objects must have\nbeen a wet dream.\n\nThe only thing shocking about SQLAlchemy's popularity is its flip side: the\ncontingency of those functioning without  SQLAlchemy as a part of their regular\nstack. Whether this stems from unawareness or active reluctance to change, data\nteams using Python without a proper ORM are surprisingly prevalent. It's easy to\nforget the reality of the workforce when our interactions with other\nprofessionals come mostly from blogs published by those at the top of their\nfield.\n\nI realize the \"this is how we've always done it\" attitude is a cliché with no\nshortage of commentary. Tales of adopting new (relatively speaking) practices\ndominate Silicon Valley blogs every day- it's the manner in which this is\nmanifested, however, that catches me off guard. In this case, resistance to a\nsingle Python library can shed light on a frightening mental model that has\nimplications up and down a corporation's stack.\n\nPutting The 'M' In MVC\nFrameworks which enforce a Model-View-Controller have held undisputed consensus\nfor long enough: none of us need to recap why creating apps this way is\nunequivocally correct. To understand why side-stepping an ORM is so significant,\nlet's recall what ORM stands for:\n\n> Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a\ntechnique that connects the rich objects of an application to tables in a\nrelational database management system. Using ORM, the properties and\nrelationships of the objects in an application can be easily stored and\nretrieved from a database without writing SQL statements directly and with less\noverall database access code. - Active Record\n[https://guides.rubyonrails.org/active_record_basics.html]\nORMs allow us to interact with databases simply by modifying objects in code\n(such as classes) as opposed to generating SQL queries by hand for each database\ninteraction. Bouncing from application code to SQL is a major context switch,\nand the more interactions we introduce, the more out of control our app becomes.\n \n\nTo illustrate the alternative to this using models, I'll use an example offered\nby Flask-SQLAlchemy. Let's say we have a table of users which contains columns\nfor id, username,  and email. A model for such a table would look as such:\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '<User %r>' % self.username\n\n\nThe 'model' is an object representing the structure of a single entry in our\ntable. Once our model exists, this is all it takes to create an entry:\n\nnewuser = User(username='admin', email='admin@example.com')\n\n\nThat's a single readable line of code without writing a single line of SQL.\nCompare this to the alternative, which would be to use Psycopg2:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\ndef query_function(query):\n  \"\"\"Runs a database query.\"\"\"\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n\n\nSure, query_function()  only needs to be set once, but compare the readability\nof using a model to the following:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\nquery_function(query)\n\n\nDespite achieving the same effect, the latter is much less readable or\nmaintainable by human beings. Building an application around raw string queries\ncan quickly become a nightmare.\n\nIntegration With Other Data Libraries\nWhen it comes to golden standards of Python libraries, there is none more\nquintessential to data analysis than Pandas. The pairing of Pandas and\nSQLAlchemy is standard to the point where Pandas has built-in integrations to\ninteract with data from SQLAlchemy. Here's what it takes to turn a database\ntable into a Pandas dataframe with SQLAlchemy as our connector:\n\ndf = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n\n\nOnce again, a single line of Python code!\n\nWriting Queries Purely in Python\nSo far by using SQLAlchemy, we haven't needed to write a single line of SQL: how\nfar could we take this? As far as we want, in fact. SQLAlchemy contains what\nthey've dubbed as function-based query construction, which is to say we can\nconstruct nearly any conceivable SQL query purely in Python by using the methods\noffered to us. For example, here's an update query:\n\nstmt = users.update().values(fullname=\"Fullname: \" + users.c.name)\nconn.execute(stmt)\n\n\nCheck the  full reference to see what I mean\n[https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates].\nEvery query you've ever needed to write: it's all there. All of it.\n\nSimple Connection Management\nSeeing as how we all now agree that SQLAlchemy is beneficial to our workflow,\nlet's visit square one and see how simple it is to manage connections. The two\nkey words to remember here are engines  and sessions.\n\nThe Engine\nAn engine in SQLAlchemy is merely a bare-bones object representing our database.\nMaking SQLAlchemy aware of our database is as simple as these two lines:\n\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\nThe Engine can interact with our database by accepting a simple URI. Once engine \n exists, we could in theory use engine exclusively via functions such as \nengine.connect()  and engine.execute().\n\nSessions\nTo interact with our database in a Pythonic manner via the ORM, we'll need to\ncreate a session  from the engine we just declared. Thus our code expands:\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n\n\nThat's all it takes! Now just as before, we can use SQLAlchemy's ORM and\nbuilt-in functions to make simple interacts:\n\nnew_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n\n\nTakeaway Goodies\nIt's worth mentioning that SQLAlchemy works with nearly every type of database,\nand does so by leveraging the base Python library for the respective type of\ndatabase. For example, it probably seems to the outsider that we've spent some\ntime shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a\nPostgres database, it is using the Psycopg2 library under the hood to manage the\nboilerplate for us. The same goes for every other type of relational database\n[https://docs.sqlalchemy.org/en/latest/core/engines.html]  along with their\nstandard libraries.\n\nThere are plenty of more reasons [https://www.sqlalchemy.org/features.html]  why\nSQLAlchemy is beneficial to the point where it is arguably critical to data\nanalysis workflows. The critical point to be made here is that leaving\nSQLAlchemy out of any data workflow only hurts the person writing the code, or\nmore importantly, all those who come after.","html":"<p>Something we've taken for granted thus far on Hackers and Slackers is a library most data professionals have accepted as an undisputed standard: <strong><a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a></strong>.</p><p>In the past, we've covered database connection management and querying using libraries such as <a href=\"https://hackersandslackers.com/using-pymysql/\"><strong>PyMySQL</strong></a> and <strong><a href=\"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/\">Psycopg2</a></strong>, both of which do an excellent job of interacting with databases just as we'd expect them to. The nature of opening/closing DB connections and working with cursors hasn't changed much in the past few decades (nearly the lifespan of relational databases themselves). While boilerplate is boring, at least it has remained consistent, one might figure. That <strong><em>may</em></strong> have been the case, but the philosophical boom of MVC frameworks nearly a decade ago sparked the emergence of popularity for ORMs. While the world was singing praises of object-oriented programming, containing database-oriented functionality within objects must have been a wet dream.</p><p>The only thing shocking about SQLAlchemy's popularity is its flip side: the contingency of those functioning <em>without</em> SQLAlchemy as a part of their regular stack. Whether this stems from unawareness or active reluctance to change, data teams using Python without a proper ORM are surprisingly prevalent. It's easy to forget the reality of the workforce when our interactions with other professionals come mostly from blogs published by those at the top of their field.</p><p>I realize the \"this is how we've always done it\" attitude is a cliché with no shortage of commentary. Tales of adopting new (relatively speaking) practices dominate Silicon Valley blogs every day- it's the manner in which this is manifested, however, that catches me off guard. In this case, resistance to a single Python library can shed light on a frightening mental model that has implications up and down a corporation's stack.</p><h2 id=\"putting-the-m-in-mvc\">Putting The 'M' In MVC</h2><p>Frameworks which enforce a Model-View-Controller have held undisputed consensus for long enough: none of us need to recap why creating apps this way is unequivocally correct. To understand why side-stepping an ORM is so significant, let's recall what ORM stands for:</p><blockquote><em>Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a technique that connects the rich objects of an application to tables in a relational database management system. Using ORM, the properties and relationships of the objects in an application can be easily stored and retrieved from a database without writing SQL statements directly and with less overall database access code. <strong>- <a href=\"https://guides.rubyonrails.org/active_record_basics.html\">Active Record</a></strong></em></blockquote><p>ORMs allow us to interact with databases simply by modifying objects in code (such as classes) as opposed to generating SQL queries by hand for each database interaction. Bouncing from application code to SQL is a <em>major context switch</em>, and the more interactions we introduce, the more out of control our app becomes. </p><p>To illustrate the alternative to this using models, I'll use an example offered by <strong>Flask-SQLAlchemy</strong>. Let's say we have a table of users which contains columns for <strong>id, username,</strong> and <strong>email. </strong>A model for such a table would look as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">class User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '&lt;User %r&gt;' % self.username\n</code></pre>\n<!--kg-card-end: markdown--><p>The 'model' is an object representing the structure of a single entry in our table. Once our model exists, this is all it takes to create an entry:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">newuser = User(username='admin', email='admin@example.com')\n</code></pre>\n<!--kg-card-end: markdown--><p>That's a single readable line of code without writing a single line of SQL. Compare this to the alternative, which would be to use Psycopg2:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\ndef query_function(query):\n  &quot;&quot;&quot;Runs a database query.&quot;&quot;&quot;\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Sure, <code>query_function()</code> only needs to be set once, but compare the readability of using a model to the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Despite achieving the same effect, the latter is much less readable or maintainable by human beings. Building an application around raw string queries can quickly become a nightmare.</p><h2 id=\"integration-with-other-data-libraries\">Integration With Other Data Libraries</h2><p>When it comes to golden standards of Python libraries, there is none more quintessential to data analysis than Pandas. The pairing of Pandas and SQLAlchemy is standard to the point where Pandas has built-in integrations to interact with data from SQLAlchemy. Here's what it takes to turn a database table into a Pandas dataframe with SQLAlchemy as our connector:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n</code></pre>\n<!--kg-card-end: markdown--><p>Once again, a single line of Python code!</p><h2 id=\"writing-queries-purely-in-python\">Writing Queries Purely in Python</h2><p>So far by using SQLAlchemy, we haven't needed to write a single line of SQL: how far could we take this? As far as we want, in fact. SQLAlchemy contains what they've dubbed as <strong>function-based query construction, </strong>which is to say we can construct nearly any conceivable SQL query purely in Python by using the methods offered to us. For example, here's an update query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">stmt = users.update().values(fullname=&quot;Fullname: &quot; + users.c.name)\nconn.execute(stmt)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check the<a href=\"https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates\"> full reference to see what I mean</a>. Every query you've ever needed to write: it's all there. All of it.</p><h2 id=\"simple-connection-management\">Simple Connection Management</h2><p>Seeing as how we all now agree that SQLAlchemy is beneficial to our workflow, let's visit square one and see how simple it is to manage connections. The two key words to remember here are <strong>engines</strong> and <strong>sessions</strong>.</p><h3 id=\"the-engine\">The Engine</h3><p>An engine in SQLAlchemy is merely a bare-bones object representing our database. Making SQLAlchemy aware of our database is as simple as these two lines:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>The Engine can interact with our database by accepting a simple URI. Once <code>engine</code> exists, we could in theory use engine exclusively via functions such as <code>engine.connect()</code> and <code>engine.execute()</code>.</p><h3 id=\"sessions\">Sessions</h3><p>To interact with our database in a Pythonic manner via the ORM, we'll need to create a <strong>session</strong> from the <strong>engine </strong>we just declared. Thus our code expands:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n</code></pre>\n<!--kg-card-end: markdown--><p>That's all it takes! Now just as before, we can use SQLAlchemy's ORM and built-in functions to make simple interacts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">new_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"takeaway-goodies\">Takeaway Goodies</h2><p>It's worth mentioning that SQLAlchemy works with nearly every type of database, and does so by leveraging the base Python library for the respective type of database. For example, it probably seems to the outsider that we've spent some time shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a Postgres database, it is using the Psycopg2 library under the hood to manage the boilerplate for us. The same goes for <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\">every other type of relational database</a> along with their standard libraries.</p><p>There are <a href=\"https://www.sqlalchemy.org/features.html\">plenty of more reasons</a> why SQLAlchemy is beneficial to the point where it is arguably critical to data analysis workflows. The critical point to be made here is that leaving SQLAlchemy out of any data workflow only hurts the person writing the code, or more importantly, all those who come after.</p>","url":"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/","uuid":"7246d9db-39cb-44aa-9da8-cf87df00eeff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3409a094d3e847951adf44"}},{"node":{"id":"Ghost__Post__5c34086694d3e847951adf3e","title":"Poetically Packaging Your Python Project","slug":"poetic-python-project-packaging","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","excerpt":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","custom_excerpt":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","created_at_pretty":"08 January, 2019","published_at_pretty":"08 January, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-01-07T21:18:14.000-05:00","published_at":"2019-01-08T10:16:00.000-05:00","updated_at":"2019-04-09T17:59:40.000-04:00","meta_title":"Poetically Packaging Your Python Project | Hackers and Slackers","meta_description":"Manage your projects with Poetry: a dependency manager and project packager all in one. Handle your environment and project data in a single file.","og_description":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","og_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","og_title":"Poetically Packaging Your Python Project","twitter_description":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","twitter_title":"Poetically Packaging Your Python Project","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn't long ago that we Hackers were singing the praises of Pipenv\n[https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/]\n: Python's seemingly superior dependency manager at the time. While we hold much\nlove in hearts, sometimes there is love to go around. We just so happen to be\nfair weather fans, which reminds me: what has Pipenv done for me lately?\n\nAs you've probably guessed (considering its a piece of software), nothing much.\nWell, there was that time when pip upgraded from v.18  to v.18.1, which broke\nPipenv entirely with almost minimal acknowledgment (for all I know this might\nstill be broken). As our lives seemed to fade, a miracle emerged from the ashes:\na young, smart, attractive alternative to Pipenv that's been whispering in my\near, and promising the world. Her name is Poetry [https://poetry.eustace.io/].\n\nWhat Light Through Yonder GitHub Breaks?\nPoetry stems from the genuine frustration that comes with not only managing\nenvironments and dependencies in Python, but the fact that even solving this\nproblem (albeit poorly) still doesn't solve the related tasks needing\nfulfillment when creating respectable Python projects. Consider Node's \npackage.json: a single file which contains a project's metadata, prod\ndependencies, dev dependencies, contact information, etc. Instead, Python\nprojects usually come with the following:\n\nSetup.py\nIf you've never bothered to publish a package to PyPI before, there's a decent\nchance you may not be very familiar with some of the nuances that come with \nsetup.py  or why you'd bother creating one. This is a losing mentality: we\nshould assume that most (or some) of the things we build might become useful\nenough to distribute some day.\n\nThus, we get this monstrosity:\n\nfrom setuptools import setup, find_packages, tests_require, packages, name\n\nwith open(\"README\", 'r') as f:\n    long_description = f.read()\n\nsetup = (\n    name='Fake Project',\n    version='1.0',\n    description='A fake project used for example purposes.',\n    long_description=long_description,\n    author='Todd Birchard',\n    author_email='todd@hackersandslackers.com',\n    maintainer='Some Loser',\n    maintainer_email='some.loser@example.com,\n    url=\"https://github.com/toddbirchard/fake-project\",\n    license='MIT',\n    include_package_data=True,\n    package_dir={'application'}\n    packages=['distutils', 'modules'],\n    tests_require=[\"pytest\"],\n    cmdclass={\"pytest\": PyTest},\n    classifiers=[\n          'Development Status :: 2 - Beta',\n          'Environment :: Console',\n          'Environment :: Web Environment',\n          'Intended Audience :: End Users/Desktop',\n          'Intended Audience :: Developers',\n          'Intended Audience :: System Administrators',\n          'License :: OSI Approved :: Python Software Foundation License',\n          'Operating System :: MacOS :: MacOS X',\n          'Operating System :: Microsoft :: Windows',\n          'Operating System :: POSIX',\n          'Programming Language :: Python',\n          'Topic :: Communications :: Email',\n          'Topic :: Office/Business',\n          'Topic :: Software Development :: Bug Tracking',\n          ],\n)\n\n\nMany of the metadata fields are rather self-explanatory. But what about the\nfields related to package dependencies, such as package_dir or packages? Wasn't\nthis already handled in our Pipfile? On top of that, we need to specify then the\ntest suite we're using via tests_require  and cmdclass? Short answer: pretty\nmuch.\n\nSetup.cfg\nThe real joke with setup.py  is that it needs its own configuration file: yes, a\nconfiguration file for your configuration file. setup.cfg, as the name\nsuggestions, sets even more granular configurations for the things mentioned in \nsetup.py, such as how pytest  should be handled, etc. Let's not get into it, but\nhere's an example:\n\n[coverage:run]\nomit = */test/*\n\n[flake8]\nexclude = *.egg*,.env,.git,.tox,_*,build*,dist*,venv*,python2/,python3/\nignore = E261,W503\nmax-line-length = 121\n\n[tool:pytest]\nminversion = 3.2\naddopts =\n  # --fulltrace\n  # -n auto\n  --cov-config=setup.cfg\n  --cov=httplib2\n  --noconftest\n  --showlocals\n  --strict\n  --tb=short\n  --timeout=17\n  --verbose\n  -ra\n\n\nPipfile and Pipfile.lock\nIf you have been using Pipenv, you'll recognize these files as being responsible\nfor setting your Python version and dependencies. But wait- didn't we also need\nto specify dependencies in setup.py?  Yes, we did. There is no God, but if there\nwere, he'd probably hate you. Here's all the work you'd need to do creating an\nacceptable Pipfile:\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nFlask-SQLAlchemy = \"*\"\npsycopg2 = \"*\"\npsycopg2-binary = \"*\"\nrequests = \"*\"\nconfigparser=\"*\"\nmapbox=\"*\"\nflask=\"*\"\npandas=\"*\"\nFlask-Assets=\"*\"\nlibsass=\"*\"\njsmin=\"*\"\ndash_core_components=\"*\"\ndash-table=\"*\"\ndash_html_components=\"*\"\ndash=\"*\"\nflask-session=\"*\"\nflask-redis=\"*\"\ngunicorn=\"*\"\npytest-flask=\"*\"\n\n\n[dev-packages]\n\n[requires]\npython_version = \"3.7.1\"\n\n\n\nBut wait, there's more!\n\nRequirements.txt\nBecause the Pipfile format has not been adopted as a standard for dependency\nmanagement, we still  need to create a requirements.txt file if we want to\ndeploy our application to respectable hosts such as Google App Engine  or\nwhat-have-you. So now we have this ugly son of a bitch from the stone age to\ndeal with as well:\n\natomicwrites==1.2.1\nattrs==18.2.0\nboto3==1.9.75\nbotocore==1.12.75\nCacheControl==0.12.5\ncertifi==2018.11.29\nchardet==3.0.4\nClick==7.0\nconfigparser==3.5.0\ndash==0.35.1\ndash-core-components==0.42.0\ndash-html-components==0.13.4\ndash-renderer==0.16.1\ndash-table==3.1.11\ndecorator==4.3.0\ndocutils==0.14\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Compress==1.4.0\nFlask-Redis==0.3.0\nFlask-Session==0.3.1\nFlask-SQLAlchemy==2.3.2\ngunicorn==19.9.0\nidna==2.8\nipython-genutils==0.2.0\niso3166==0.9\nitsdangerous==1.1.0\nJinja2==2.10\njmespath==0.9.3\njsmin==2.2.2\njsonschema==2.6.0\njupyter-core==4.4.0\nlibsass==0.17.0\nmapbox==0.17.2\nMarkupSafe==1.1.0\nmore-itertools==5.0.0\nmsgpack==0.6.0\nnbformat==4.4.0\nnumpy==1.15.4\npandas==0.23.4\nplotly==3.5.0\npluggy==0.8.0\npolyline==1.3.2\npsycopg2==2.7.6.1\npsycopg2-binary==2.7.6.1\npy==1.7.0\npytest==4.1.0\npytest-flask==0.14.0\npython-dateutil==2.7.5\npytz==2018.9\nredis==3.0.1\nrequests==2.21.0\nretrying==1.3.3\ns3transfer==0.1.13\nsix==1.12.0\nSQLAlchemy==1.2.15\ntraitlets==4.3.2\nuritemplate==3.0.0\nurllib3==1.24.1\nwebassets==0.12.1\nWerkzeug==0.14.1\n\n\nMANIFEST.in\nYES, THERE'S MORE. If you're not bothered by now, please leave this blog\nimmediately. The job market is ripe for neckbeards who take pleasure in\nunnecessary complexity. Until the robots take over, this blog is for humans.\n\nAnyway, there's an entire file dedicated to including files in your project\nwhich aren't code. We're entering comically ridiculous territory:\n\ninclude README.rst\ninclude docs/*.txt\ninclude funniest/data.json\n\n\nIt's a Bird! It's a Plane! Its... A Single, Sophisticated Config File?\nI hope you're thoroughly pissed off after looking back at all the things we've\nlet slide by year after year, telling ourselves that this patchwork of standards\nis just fine. Cue our hero: the creator of Poetry:\n\n> Packaging systems and dependency management in Python are rather convoluted and\nhard to understand for newcomers. Even for seasoned developers it might be\ncumbersome at times to create all files needed in a Python project: setup.py,\nrequirements.txt, setup.cfg, MANIFEST.in  and the newly added Pipfile. So I\nwanted a tool that would limit everything to a single configuration file to do:\ndependency management, packaging and publishing.\nOh God yes, but HOW?!?!\n\nIntroducing pyproject.toml\nPoetry is built around a single configuration dubbed pyproject.toml  which has\nbecome an accepted standard in the Python community\n[https://www.python.org/dev/peps/pep-0518/]  by way of PEP 518.  With the weight\nof the Python development community itself, it's safe to say this isn't another\nfad and is worth using.\n\nHere's an example .toml file from the Poetry Github repository\n[https://github.com/sdispater/poetry]:\n\n[tool.poetry]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"The description of the package\"\n\nlicense = \"MIT\"\n\nauthors = [\n    \"Sébastien Eustace <sebastien@eustace.io>\"\n]\n\nreadme = 'README.md'  # Markdown files are supported\n\nrepository = \"https://github.com/sdispater/poetry\"\nhomepage = \"https://github.com/sdispater/poetry\"\n\nkeywords = ['packaging', 'poetry']\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.2\"  # Compatible python versions must be declared here\ntoml = \"^0.9\"\n# Dependencies with extras\nrequests = { version = \"^2.13\", extras = [ \"security\" ] }\n# Python specific dependencies with prereleases allowed\npathlib2 = { version = \"^2.2\", python = \"~2.7\", allows-prereleases = true }\n# Git dependencies\ncleo = { git = \"https://github.com/sdispater/cleo.git\", branch = \"master\" }\n\n# Optional dependencies (extras)\npendulum = { version = \"^1.4\", optional = true }\n\n[tool.poetry.dev-dependencies]\npytest = \"^3.0\"\npytest-cov = \"^2.4\"\n\n[tool.poetry.scripts]\nmy-script = 'my_package:main'\n\n\nIn addition to covering the scope of all previously mentioned files, using \npyproject.toml  with Poetry also covers:\n\n * Auto-populating the exclude  section from values found in .gitignore\n * The addition of a keywords  section to be included with the resulting PyPi\n   package\n * Support for version numbers using any syntax, such as wildcard (*)  or carrot\n   (^1.0.0)  syntax\n * Auto-detection for virtual environments, thus a global install that can be\n   used within envs\n\nCreating Poetic Art\nAre we all fired up yet? Right: let's change our workflow forever.\n\nInstallation\n  To install Poetry on OSX, use the following:\n\n$ curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python\n\n\nThis will create an addition to your ~/.bash_profile. Restart your terminal and\nverify the installation:\n\n$ poetry --version\nPoetry 0.12.10\n\n\nCreating a New Python Project\nNavigate to whichever file path you'd like your new project to call home. To get\nstarted, all we need next is the following command:\n\npoetry new my-package\n\n\nReady for a breath of fresh air? This command generates a basic project\nstructure for you- something that's been missing from Python for a long time\nwhen compared to similar generators for Node or otherwise. The resulting project\nstructure looks as such:\n\nmy-package\n├── pyproject.toml\n├── README.rst\n├── my_package\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_my_package\n\n\nOf the beautiful things happening here, the only one we haven't touched on yet\nis Poetry's built-in integration with pytest. Oh, happy day!\n\nAlternative Interactive Installation Method\nIf you'd prefer a bit more handholding, feel free to use poetry init  in an\nempty directory (or a directory without the existing .toml  file) to be walked\nthrough the creation process:\n\n$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [my-package]: Great Package\nVersion [0.1.0]:\nDescription []: Great package for great people.\nAuthor [Todd Birchard <todd@hackersandslackers.com>, n to skip]:\nLicense []: MIT\nCompatible Python versions [^2.7]: ^3.7\n\nWould you like to define your dependencies (require) interactively? (yes/no) [yes] no\n\n\n\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\n\nGenerated file\n\n[tool.poetry]\nname = \"Great Package\"\nversion = \"0.1.0\"\ndescription = \"Great package for great people.\"\nauthors = [\"Todd Birchard <todd@hackersandslackers.com>\"]\nlicense = \"MIT\"\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n\n\nDo you confirm generation? (yes/no) [yes] yes\n\n\nManaging Dependencies in pyproject.toml\nIf you're familiar with Pipfiles, pyproject.toml handles dependencies the same\nway. Just remember that poetry install  installs your listed dependencies, and \npoetry update  will update dependencies in poetry.lock to their latest versions.\n\nCarry on my Wayward Son\nI could spend all day copy-pasting general usage from the Poetry Github page,\nbut I think my work here is done. Do yourself a favor and  take a look at the\nGithub repo [https://github.com/sdispater/poetry]  to make your life easier\nforever. Or at least until the next replacement solution comes along.","html":"<p>It wasn't long ago that we Hackers were <a href=\"https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/\">singing the praises of Pipenv</a>: Python's seemingly superior dependency manager at the time. While we hold much love in hearts, sometimes there is love to go around. We just so happen to be fair weather fans, which reminds me: what has Pipenv done for me <em>lately</em>?</p><p>As you've probably guessed (considering its a piece of software), nothing much. Well, there was that time when pip upgraded from <code>v.18</code> to <code>v.18.1</code>, which broke Pipenv entirely with almost minimal acknowledgment (for all I know this might still be broken). As our lives seemed to fade, a miracle emerged from the ashes: a young, smart, attractive alternative to Pipenv that's been whispering in my ear, and promising the world. Her name is <a href=\"https://poetry.eustace.io/\"><strong>Poetry</strong></a>.</p><h2 id=\"what-light-through-yonder-github-breaks\">What Light Through Yonder GitHub Breaks?</h2><p>Poetry stems from the genuine frustration that comes with not only managing environments and dependencies in Python, but the fact that even solving this problem (albeit poorly) still doesn't solve the related tasks needing fulfillment when creating respectable Python projects. Consider Node's <code>package.json</code>: a single file which contains a project's metadata, prod dependencies, dev dependencies, contact information, etc. Instead, Python projects usually come with the following:</p><h3 id=\"setup-py\">Setup.py</h3><p>If you've never bothered to publish a package to PyPI before, there's a decent chance you may not be very familiar with some of the nuances that come with <code>setup.py</code> or why you'd bother creating one. This is a losing mentality: we should assume that most (or some) of the things we build might become useful enough to distribute some day.</p><p>Thus, we get this monstrosity:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from setuptools import setup, find_packages, tests_require, packages, name\n\nwith open(&quot;README&quot;, 'r') as f:\n    long_description = f.read()\n\nsetup = (\n    name='Fake Project',\n    version='1.0',\n    description='A fake project used for example purposes.',\n    long_description=long_description,\n    author='Todd Birchard',\n    author_email='todd@hackersandslackers.com',\n    maintainer='Some Loser',\n    maintainer_email='some.loser@example.com,\n    url=&quot;https://github.com/toddbirchard/fake-project&quot;,\n    license='MIT',\n    include_package_data=True,\n    package_dir={'application'}\n    packages=['distutils', 'modules'],\n    tests_require=[&quot;pytest&quot;],\n    cmdclass={&quot;pytest&quot;: PyTest},\n    classifiers=[\n          'Development Status :: 2 - Beta',\n          'Environment :: Console',\n          'Environment :: Web Environment',\n          'Intended Audience :: End Users/Desktop',\n          'Intended Audience :: Developers',\n          'Intended Audience :: System Administrators',\n          'License :: OSI Approved :: Python Software Foundation License',\n          'Operating System :: MacOS :: MacOS X',\n          'Operating System :: Microsoft :: Windows',\n          'Operating System :: POSIX',\n          'Programming Language :: Python',\n          'Topic :: Communications :: Email',\n          'Topic :: Office/Business',\n          'Topic :: Software Development :: Bug Tracking',\n          ],\n)\n</code></pre>\n<!--kg-card-end: markdown--><p>Many of the metadata fields are rather self-explanatory. But what about the fields related to package dependencies, such as package_dir or packages? Wasn't this already handled in our Pipfile? On top of that, we need to specify then the test suite we're using via <strong>tests_require</strong> and <strong>cmdclass</strong>? Short answer: pretty much.</p><h3 id=\"setup-cfg\">Setup.cfg</h3><p>The real joke with <code>setup.py</code> is that it needs its own configuration file: yes, a configuration file for your configuration file. <code>setup.cfg</code>, as the name suggestions, sets even more granular configurations for the things mentioned in <code>setup.py</code>, such as how <strong>pytest</strong> should be handled, etc. Let's not get into it, but here's an example:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">[coverage:run]\nomit = */test/*\n\n[flake8]\nexclude = *.egg*,.env,.git,.tox,_*,build*,dist*,venv*,python2/,python3/\nignore = E261,W503\nmax-line-length = 121\n\n[tool:pytest]\nminversion = 3.2\naddopts =\n  # --fulltrace\n  # -n auto\n  --cov-config=setup.cfg\n  --cov=httplib2\n  --noconftest\n  --showlocals\n  --strict\n  --tb=short\n  --timeout=17\n  --verbose\n  -ra\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"pipfile-and-pipfile-lock\">Pipfile and Pipfile.lock</h3><p>If you have been using Pipenv, you'll recognize these files as being responsible for setting your Python version and dependencies. <em>But wait- didn't we also need to specify dependencies in setup.py?</em> Yes, we did. There is no God, but if there were, he'd probably hate you. Here's all the work you'd need to do creating an acceptable Pipfile:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">[[source]]\nurl = &quot;https://pypi.org/simple&quot;\nverify_ssl = true\nname = &quot;pypi&quot;\n\n[packages]\nFlask-SQLAlchemy = &quot;*&quot;\npsycopg2 = &quot;*&quot;\npsycopg2-binary = &quot;*&quot;\nrequests = &quot;*&quot;\nconfigparser=&quot;*&quot;\nmapbox=&quot;*&quot;\nflask=&quot;*&quot;\npandas=&quot;*&quot;\nFlask-Assets=&quot;*&quot;\nlibsass=&quot;*&quot;\njsmin=&quot;*&quot;\ndash_core_components=&quot;*&quot;\ndash-table=&quot;*&quot;\ndash_html_components=&quot;*&quot;\ndash=&quot;*&quot;\nflask-session=&quot;*&quot;\nflask-redis=&quot;*&quot;\ngunicorn=&quot;*&quot;\npytest-flask=&quot;*&quot;\n\n\n[dev-packages]\n\n[requires]\npython_version = &quot;3.7.1&quot;\n\n</code></pre>\n<!--kg-card-end: markdown--><p>But wait, there's more!</p><h3 id=\"requirements-txt\">Requirements.txt</h3><p>Because the Pipfile format has not been adopted as a standard for dependency management, we <em>still</em> need to create a requirements.txt file if we want to deploy our application to respectable hosts such as <strong>Google App Engine</strong> or what-have-you. So now we have this ugly son of a bitch from the stone age to deal with as well:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">atomicwrites==1.2.1\nattrs==18.2.0\nboto3==1.9.75\nbotocore==1.12.75\nCacheControl==0.12.5\ncertifi==2018.11.29\nchardet==3.0.4\nClick==7.0\nconfigparser==3.5.0\ndash==0.35.1\ndash-core-components==0.42.0\ndash-html-components==0.13.4\ndash-renderer==0.16.1\ndash-table==3.1.11\ndecorator==4.3.0\ndocutils==0.14\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Compress==1.4.0\nFlask-Redis==0.3.0\nFlask-Session==0.3.1\nFlask-SQLAlchemy==2.3.2\ngunicorn==19.9.0\nidna==2.8\nipython-genutils==0.2.0\niso3166==0.9\nitsdangerous==1.1.0\nJinja2==2.10\njmespath==0.9.3\njsmin==2.2.2\njsonschema==2.6.0\njupyter-core==4.4.0\nlibsass==0.17.0\nmapbox==0.17.2\nMarkupSafe==1.1.0\nmore-itertools==5.0.0\nmsgpack==0.6.0\nnbformat==4.4.0\nnumpy==1.15.4\npandas==0.23.4\nplotly==3.5.0\npluggy==0.8.0\npolyline==1.3.2\npsycopg2==2.7.6.1\npsycopg2-binary==2.7.6.1\npy==1.7.0\npytest==4.1.0\npytest-flask==0.14.0\npython-dateutil==2.7.5\npytz==2018.9\nredis==3.0.1\nrequests==2.21.0\nretrying==1.3.3\ns3transfer==0.1.13\nsix==1.12.0\nSQLAlchemy==1.2.15\ntraitlets==4.3.2\nuritemplate==3.0.0\nurllib3==1.24.1\nwebassets==0.12.1\nWerkzeug==0.14.1\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"manifest-in\">MANIFEST.in</h3><p>YES, THERE'S MORE. If you're not bothered by now, please leave this blog immediately. The job market is ripe for neckbeards who take pleasure in unnecessary complexity. Until the robots take over, this blog is for humans.</p><p>Anyway, there's an entire file dedicated to including files in your project which aren't code. We're entering comically ridiculous territory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-ini\">include README.rst\ninclude docs/*.txt\ninclude funniest/data.json\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"it-s-a-bird-it-s-a-plane-its-a-single-sophisticated-config-file\">It's a Bird! It's a Plane! Its... A Single, Sophisticated Config File?</h2><p>I hope you're thoroughly pissed off after looking back at all the things we've let slide by year after year, telling ourselves that this patchwork of standards is just fine. Cue our hero: the creator of Poetry:</p><blockquote>Packaging systems and dependency management in Python are rather convoluted and hard to understand for newcomers. Even for seasoned developers it might be cumbersome at times to create all files needed in a Python project: <code>setup.py</code>,<code>requirements.txt</code>, <code>setup.cfg</code>, <code>MANIFEST.in</code> and the newly added <code>Pipfile</code>. So I wanted a tool that would limit everything to a single configuration file to do: dependency management, packaging and publishing.</blockquote><p>Oh God yes, but HOW?!?!</p><h3 id=\"introducing-pyproject-toml\">Introducing pyproject.toml</h3><p>Poetry is built around a single configuration dubbed <code>pyproject.toml</code> which has become an <a href=\"https://www.python.org/dev/peps/pep-0518/\">accepted standard in the Python community</a> by way of <strong>PEP 518.</strong> With the weight of the Python development community itself, it's safe to say this isn't another fad and is worth using.</p><p>Here's an example <strong>.toml </strong>file from the <a href=\"https://github.com/sdispater/poetry\">Poetry Github repository</a>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-toml\">[tool.poetry]\nname = &quot;my-package&quot;\nversion = &quot;0.1.0&quot;\ndescription = &quot;The description of the package&quot;\n\nlicense = &quot;MIT&quot;\n\nauthors = [\n    &quot;Sébastien Eustace &lt;sebastien@eustace.io&gt;&quot;\n]\n\nreadme = 'README.md'  # Markdown files are supported\n\nrepository = &quot;https://github.com/sdispater/poetry&quot;\nhomepage = &quot;https://github.com/sdispater/poetry&quot;\n\nkeywords = ['packaging', 'poetry']\n\n[tool.poetry.dependencies]\npython = &quot;~2.7 || ^3.2&quot;  # Compatible python versions must be declared here\ntoml = &quot;^0.9&quot;\n# Dependencies with extras\nrequests = { version = &quot;^2.13&quot;, extras = [ &quot;security&quot; ] }\n# Python specific dependencies with prereleases allowed\npathlib2 = { version = &quot;^2.2&quot;, python = &quot;~2.7&quot;, allows-prereleases = true }\n# Git dependencies\ncleo = { git = &quot;https://github.com/sdispater/cleo.git&quot;, branch = &quot;master&quot; }\n\n# Optional dependencies (extras)\npendulum = { version = &quot;^1.4&quot;, optional = true }\n\n[tool.poetry.dev-dependencies]\npytest = &quot;^3.0&quot;\npytest-cov = &quot;^2.4&quot;\n\n[tool.poetry.scripts]\nmy-script = 'my_package:main'\n</code></pre>\n<!--kg-card-end: markdown--><p>In addition to covering the scope of all previously mentioned files, using <strong>pyproject.toml</strong> with Poetry also covers:</p><ul><li>Auto-populating the <strong>exclude</strong> section from values found in <code>.gitignore</code></li><li>The addition of a <strong>keywords</strong> section to be included with the resulting PyPi package</li><li>Support for version numbers using any syntax, such as <strong>wildcard (*)</strong> or <strong>carrot (^1.0.0)</strong> syntax</li><li>Auto-detection for virtual environments, thus a global install that can be used within envs</li></ul><h2 id=\"creating-poetic-art\">Creating Poetic Art</h2><p>Are we all fired up yet? Right: let's change our workflow forever.</p><h3 id=\"installation\">Installation</h3><p> To install Poetry on OSX, use the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python\n</code></pre>\n<!--kg-card-end: markdown--><p>This will create an addition to your <code>~/.bash_profile</code>. Restart your terminal and verify the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ poetry --version\nPoetry 0.12.10\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"creating-a-new-python-project\">Creating a New Python Project</h3><p>Navigate to whichever file path you'd like your new project to call home. To get started, all we need next is the following command:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">poetry new my-package\n</code></pre>\n<!--kg-card-end: markdown--><p>Ready for a breath of fresh air? This command generates a basic project structure for you- something that's been missing from Python for a long time when compared to similar generators for Node or otherwise. The resulting project structure looks as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">my-package\n├── pyproject.toml\n├── README.rst\n├── my_package\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_my_package\n</code></pre>\n<!--kg-card-end: markdown--><p>Of the beautiful things happening here, the only one we haven't touched on yet is Poetry's built-in integration with <strong>pytest</strong>. Oh, happy day!</p><h4 id=\"alternative-interactive-installation-method\">Alternative Interactive Installation Method</h4><p>If you'd prefer a bit more handholding, feel free to use <code>poetry init</code> in an empty directory (or a directory without the existing <strong>.toml</strong> file) to be walked through the creation process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [my-package]: Great Package\nVersion [0.1.0]:\nDescription []: Great package for great people.\nAuthor [Todd Birchard &lt;todd@hackersandslackers.com&gt;, n to skip]:\nLicense []: MIT\nCompatible Python versions [^2.7]: ^3.7\n\nWould you like to define your dependencies (require) interactively? (yes/no) [yes] no\n\n\n\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\n\nGenerated file\n\n[tool.poetry]\nname = &quot;Great Package&quot;\nversion = &quot;0.1.0&quot;\ndescription = &quot;Great package for great people.&quot;\nauthors = [&quot;Todd Birchard &lt;todd@hackersandslackers.com&gt;&quot;]\nlicense = &quot;MIT&quot;\n\n[tool.poetry.dependencies]\npython = &quot;^3.7&quot;\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [&quot;poetry&gt;=0.12&quot;]\nbuild-backend = &quot;poetry.masonry.api&quot;\n\n\nDo you confirm generation? (yes/no) [yes] yes\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"managing-dependencies-in-pyproject-toml\">Managing Dependencies in pyproject.toml</h2><p>If you're familiar with Pipfiles, pyproject.toml handles dependencies the same way. Just remember that <code>poetry install</code> installs your listed dependencies, and <code>poetry update</code> will update dependencies in <em>poetry.lock </em>to their latest versions.</p><h3 id=\"carry-on-my-wayward-son\">Carry on my Wayward Son</h3><p>I could spend all day copy-pasting general usage from the Poetry Github page, but I think my work here is done. Do yourself a favor and<a href=\"https://github.com/sdispater/poetry\"> take a look at the Github repo</a> to make your life easier forever. Or at least until the next replacement solution comes along.</p>","url":"https://hackersandslackers.com/poetic-python-project-packaging/","uuid":"10ddf06a-b12f-40b4-9849-2b057d3fe2f4","page":false,"codeinjection_foot":null,"codeinjection_head":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@glorious/demo/dist/gdemo.min.css\">\n<script src=\"https://cdn.jsdelivr.net/npm/@glorious/demo/dist/gdemo.min.js\"></script>\n","comment_id":"5c34086694d3e847951adf3e"}},{"node":{"id":"Ghost__Post__5c307c9493bed0776a0a3d80","title":"Using Redis to Store Information in Python Applications","slug":"using-redis-with-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-2.jpg","excerpt":"A temporary data store for everything from session variables to chat queues.","custom_excerpt":"A temporary data store for everything from session variables to chat queues.","created_at_pretty":"05 January, 2019","published_at_pretty":"05 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-05T04:44:52.000-05:00","published_at":"2019-01-05T08:21:00.000-05:00","updated_at":"2019-03-28T05:41:12.000-04:00","meta_title":"Using Redis to Store Information in Python Apps | Hackers and Slackers","meta_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","og_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","og_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-2.jpg","og_title":"Using Redis to Store Information in Python Applications","twitter_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-1.jpg","twitter_title":"Using Redis to Store Information in Python Applications","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"We’re hacking into the new year here at Hackers and Slackers, and in the\nprocess, we’ve received plenty of new gifts to play with. Nevermind how Santa\nmanages to fit physically non-existent SaaS products under the Christmas tree.\nWe ask for abstract enterprise software every year, and this time we happened to\nget a little red box.\n\nIf you've never personally used Redis, the name probably sounds familiar as\nyou've been bombarded with obscure technology brand names in places like the\nHeroku marketplace, or your unacceptably nerdy Twitter account (I assure you,\nmine is worse). So what is Redis, you ask? Well, it's a NoSQL datasto- wait,\nwhere are you... NO! Don't leave! It's not like THAT, I swear!\n\nWhat Redis is and When to Use It\nRedis stores information in the familiar key/value pair format, but the term\n‘NoSQL’ is more of a technicality than an indicator of use cases synonymous with\nNoSQL databases of the past. Redis looks the part for the very purpose it\nserves: a box that you fill with crap which may or may not be important down the\nline. It’s the perfect place to put a Starbucks gift card or the clothes you’ve\nalready worn which aren’t quite ready to be washed yet.\n\nAll Users go to Heaven: Cloud Storage for User Sessions\nPerhaps the most common use case is a glorified session cache. Similar to the\nway users might store temporary app information in cookies, Redis holds on to\ninformation which is fleeting. The difference is we now own this information\ninside our very own box, thus the Redis motto: “your box, your rules.”* \n\n* I made this up: it holds zero truth.Because temporary user information is in\nour hands as opposed to a fickle browser, we can decide just how  temporary our\n“cache” is, having it persist across sessions or even devices. While local\nmemory storage may as well be a place for throwaway information, and databases\nfor persistent or eternal information, Redis is somewhere in between. As users\ninteract and the information they create within our app evolves, we may choose\nat any point to promote information stored in Redis to a database, or perhaps\nhave it stick around a little while longer. They’ll be thrilled to see their\nshopping cart still filled with the stupid things they almost bought while they\nwere drunk yesterday.\n\nWhen Variables are on a Bagel, You can Have Variables Any Time \nIn other words, Redis is great for solving the need of globally accessible\nvariables throughout an entire application, on a per-user basis. Users who\naccidentally quit your app, move to a new context, or merely use your app for\nlonger than your QA team are easier to manage when their temporary information\nis in a safe and global environment. Compare this to saving a user’s Pac-Man\nscore to a global variable:  the moment an app like Pac-Man crashes or restarts,\nthat session is gone forever. Thus dies another three-letter app obscenity\nbelonging to a leaderboard.\n\nSpeaking of Leaderboards...\nRedis is great at counting in increments. This is probably made evident by the\nfact that it is a computer, and these are the things computers do. Something\nelse that’s made great by counting: queues! Cues of tasks, notifications, chats,\ndisappearing naked photos, etc: all of these things are ideally suited for our\nred box.\n\nGetting a Red Box of Your Own\nPlaying around with a cloud-hosted Redis box will cost you maybe 5 bucks\n(monthly if you forget to cancel). Redis is open source so there are plenty of\nvendors to choose from with little differentiation between them. I’ll consider\nrecommending whichever vendor offers to bribe me the most, but in the meantime\nI’ll leave the window shopping to you.\n\nSetting up Redis should feel like setting up a cloud SQL database, except\nsmaller and cuter. You’ll be able to pick adorable features for your box of\npossibly-but-not-surely-worthless stuff, such as hosted region, etc. Once you're\nset up you should have a host URL for reaching your instance:\n\nredis-1738.c62.us-east-1-4.ec2.cloud.fakeredisprovider.com:42069\n\nNow we’re cooking with gas.\n\nUsing the redis-py Python Library\nThe main Python Redis library is typed as redis, as in pip install Redis. The\neasiest way to connect in any case is via a URI connection string, like such:\n\nr = redis.Redis( url='rediss://:password@hostname:port/0')\n\n\nNote the unique structure of the URI above:\n\n * rediss://: precedes all Redis URIs; NOTE THE TRAILING COLON.\n * password  comes next, with the interesting choice to bypass usernames.\n * hostname  is the instance's URL... almost always a thinly veiled repurposed\n   EC2 instance. That's right, we're being sold simple open source software\n   hosted on AWS. Don't think about it.\n * port is your preferred port of call after pillaging British trade ships. Just\n   making sure you're still here.\n * /database brings up the rear, which is the name of your database.\n\nAs with regular databases, other connection methods exist such as via SSL\ncertificates, etc.\n\nStoring and Getting Values\nThis is your bread and butter for interacting with Redis:\n\n * .set():  Set a key/value pair by either overwriting or creating a new value\n * .get():  Retrieve a value by naming the associated key\n * hmget():  Accepts a variable number of keys, and will return values for each\n   if they exist\n * hmset():  Set multiple values to a single key.\n * hgetall():  Get all values for a key where a key has been assigned multiple\n   values.\n\nIt’s important to note that Redis by default returns bytes as opposed to\nstrings. As a result, it is important to remember the encoding/decoding of\nvalues in order to retrieve them properly. For example:\n\n# Setting a Value\nr.set('uri', str(app.config['SQLALCHEMY_DATABASE_URI']).encode('utf-8'))\n\n# Getting a Value\nr.get('uri').decode('utf-8')\n\n\nIf you happen to be remotely sane, you probably don't want to deal with encoding\nand decoding values over and again. Luckily we can ensure that responses are\nalways decoded for us by setting the decode_responses  parameter to True  when\nsetting up our Redis instance:\n\nredis.StrictRedis(host=\"localhost\", port=6379, charset=\"utf-8\", decode_responses=True)\n\n\nThe redis-py documentation [https://redis-py.readthedocs.io/en/latest/] \nactually goes wayyy deeper than the 5 methods listed above. If you ever somehow\nmanage to cover all of it, I have many questions about the type of person you\nare.\n\nMore Redis Libraries for Python\nIf the above encoding/decoding seems annoying, you aren’t the first. That’s why\nlibraries like Redisworks [https://github.com/seperman/redisworks]  exist.\nRedisworks allows for the seamless exchange of Python data types to and from\nRedis. Want to shove a Python dict down your box’s throat? Go ahead! You won’t\neven have to think about it very hard. There are plenty of similar libraries all\naimed to make sad lives easier.\n\nWant more? How about Asyncio’s very own asynchronous Redis library\n[https://asyncio-redis.readthedocs.io/en/latest/]?  Or how about the similar \naioredis [aioredis.readthedocs.org], another Asyncio Redis plug-in, which also\nincludes pure Python parsing, clustering support, and things I don’t even\nunderstand! There are truly more Python libraries for Redis\n[https://redis.io/clients#python]  than you could need.\n\nFinally, how could we ever forget Flask-Redis? We’ve already covered this\n[https://hackersandslackers.com/demystifying-flasks-application-context/], but\nis easily the first and last Redis library any Flask developer will use.\n\nYour Box, Your Treasure, Your World™\nNow that we’ve uncovered this niche between cached data and stored data, the\npossibilities are endless. The world is your oyster full of things which you may\nor may not choose to shove in your box.\n\nOk, fine. Perhaps this whole concept feels like a bit of an obscure niche hardly\nworthy of the words on this page. Just remember that feeling when the time comes\nthat you too need a little red cube, and it will be waiting with love and\ncompassion. A companion cube, if you will.","html":"<p>We’re hacking into the new year here at Hackers and Slackers, and in the process, we’ve received plenty of new gifts to play with. Nevermind how Santa manages to fit physically non-existent SaaS products under the Christmas tree. We ask for abstract enterprise software every year, and this time we happened to get a little red box.</p><p>If you've never personally used Redis, the name probably sounds familiar as you've been bombarded with obscure technology brand names in places like the Heroku marketplace, or your unacceptably nerdy Twitter account (I assure you, mine is worse). So what is Redis, you ask? Well, it's a NoSQL datasto- wait, where are you... NO! Don't leave! It's not like THAT, I swear!</p><h2 id=\"what-redis-is-and-when-to-use-it\">What Redis is and When to Use It</h2><p>Redis stores information in the familiar key/value pair format, but the term ‘NoSQL’ is more of a technicality than an indicator of use cases synonymous with NoSQL databases of the past. Redis looks the part for the very purpose it serves: a box that you fill with crap which may or may not be important down the line. It’s the perfect place to put a Starbucks gift card or the clothes you’ve already worn which aren’t quite ready to be washed yet.</p><h3 id=\"all-users-go-to-heaven-cloud-storage-for-user-sessions\">All Users go to Heaven: Cloud Storage for User Sessions</h3><p>Perhaps the most common use case is a glorified <strong>session cache</strong>. Similar to the way users might store temporary app information in cookies, Redis holds on to information which is fleeting. The difference is we now own this information inside our very own box, thus the Redis motto: “<em>your box, your rules</em>.”* </p><!--kg-card-begin: html--><span style=\"color:#9DA0A0;font-style:italic;margin-bottom:30px;display:block;text-align:right;width:100%;\">* I made this up: it holds zero truth.</span><!--kg-card-end: html--><p>Because temporary user information is in our hands as opposed to a fickle browser, we can decide just <em>how</em> temporary our “cache” is, having it persist across sessions or even devices. While local memory storage may as well be a place for throwaway information, and databases for persistent or eternal information, Redis is somewhere in between. As users interact and the information they create within our app evolves, we may choose at any point to promote information stored in Redis to a database, or perhaps have it stick around a little while longer. They’ll be thrilled to see their shopping cart still filled with the stupid things they almost bought while they were drunk yesterday.</p><h3 id=\"when-variables-are-on-a-bagel-you-can-have-variables-any-time\">When Variables are on a Bagel, You can Have Variables Any Time </h3><p>In other words, Redis is great for solving the need of globally accessible variables throughout an entire application, on a per-user basis. Users who accidentally quit your app, move to a new context, or merely use your app for longer than your QA team are easier to manage when their temporary information is in a safe and global environment. Compare this to saving a user’s Pac-Man score to a global variable:  the moment an app like Pac-Man crashes or restarts, that session is gone forever. Thus dies another three-letter app obscenity belonging to a leaderboard.</p><h3 id=\"speaking-of-leaderboards-\">Speaking of Leaderboards...</h3><p>Redis is great at counting in increments. This is probably made evident by the fact that it is a computer, and these are the things computers do. Something else that’s made great by counting: queues! Cues of tasks, notifications, chats, disappearing naked photos, etc: all of these things are ideally suited for our red box.</p><h2 id=\"getting-a-red-box-of-your-own\">Getting a Red Box of Your Own</h2><p>Playing around with a cloud-hosted Redis box will cost you maybe 5 bucks (monthly if you forget to cancel). Redis is open source so there are plenty of vendors to choose from with little differentiation between them. I’ll consider recommending whichever vendor offers to bribe me the most, but in the meantime I’ll leave the window shopping to you.</p><p>Setting up Redis should feel like setting up a cloud SQL database, except smaller and cuter. You’ll be able to pick adorable features for your box of possibly-but-not-surely-worthless stuff, such as hosted region, etc. Once you're set up you should have a host URL for reaching your instance:</p><!--kg-card-begin: code--><pre><code>redis-1738.c62.us-east-1-4.ec2.cloud.fakeredisprovider.com:42069</code></pre><!--kg-card-end: code--><p>Now we’re cooking with gas.</p><h2 id=\"using-the-redis-py-python-library\">Using the redis-py Python Library</h2><p>The main Python Redis library is typed as <code>redis</code>, as in <code>pip install Redis</code>. The easiest way to connect in any case is via a URI connection string, like such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">r = redis.Redis( url='rediss://:password@hostname:port/0')\n</code></pre>\n<!--kg-card-end: markdown--><p>Note the unique structure of the URI above:</p><ul><li><strong>rediss://: </strong>precedes all Redis URIs; <em>NOTE THE TRAILING COLON.</em></li><li><strong>password</strong> comes next, with the interesting choice to bypass usernames.</li><li><strong>hostname</strong> is the instance's URL... almost always a thinly veiled repurposed EC2 instance. That's right, we're being sold simple open source software hosted on AWS. Don't think about it.</li><li><strong>port </strong>is your preferred port of call after pillaging British trade ships. Just making sure you're still here.</li><li><strong>/database </strong>brings up the rear, which is the name of your database.</li></ul><p>As with regular databases, other connection methods exist such as via SSL certificates, etc.</p><h3 id=\"storing-and-getting-values\">Storing and Getting Values</h3><p>This is your bread and butter for interacting with Redis:</p><ul><li><strong>.set():</strong> Set a key/value pair by either overwriting or creating a new value</li><li><strong>.get():</strong> Retrieve a value by naming the associated key</li><li><strong>hmget():</strong> Accepts a variable number of keys, and will return values for each if they exist</li><li><strong>hmset():</strong> Set multiple values to a single key.</li><li><strong>hgetall():</strong> Get all values for a key where a key has been assigned multiple values.</li></ul><p>It’s important to note that Redis by default returns bytes as opposed to strings. As a result, it is important to remember the encoding/decoding of values in order to retrieve them properly. For example:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># Setting a Value\nr.set('uri', str(app.config['SQLALCHEMY_DATABASE_URI']).encode('utf-8'))\n\n# Getting a Value\nr.get('uri').decode('utf-8')\n</code></pre>\n<!--kg-card-end: markdown--><p>If you happen to be remotely sane, you probably don't want to deal with encoding and decoding values over and again. Luckily we can ensure that responses are always decoded for us by setting the <code>decode_responses</code> parameter to <code>True</code> when setting up our Redis instance:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">redis.StrictRedis(host=&quot;localhost&quot;, port=6379, charset=&quot;utf-8&quot;, decode_responses=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>The <a href=\"https://redis-py.readthedocs.io/en/latest/\" rel=\"noopener\"><strong>redis-py</strong> documentation</a> actually goes wayyy deeper than the 5 methods listed above. If you ever somehow manage to cover all of it, I have many questions about the type of person you are.</p><h2 id=\"more-redis-libraries-for-python\">More Redis Libraries for Python</h2><p>If the above encoding/decoding seems annoying, you aren’t the first. That’s why libraries like <a href=\"https://github.com/seperman/redisworks\" rel=\"noopener\"><strong>Redisworks</strong></a> exist. Redisworks allows for the seamless exchange of Python data types to and from Redis. Want to shove a Python dict down your box’s throat? Go ahead! You won’t even have to think about it very hard. There are plenty of similar libraries all aimed to make sad lives easier.</p><p>Want more? How about Asyncio’s very own <a href=\"https://asyncio-redis.readthedocs.io/en/latest/\">asynchronous Redis library</a>?  Or how about the similar <strong><a href=\"aioredis.readthedocs.org\">aioredis</a></strong>, another Asyncio Redis plug-in, which also includes pure Python parsing, clustering support, and things I don’t even understand! There are truly <a href=\"https://redis.io/clients#python\">more Python libraries for Redis</a> than you could need.</p><p>Finally, how could we ever forget <strong>Flask-Redis</strong>? We’ve <a href=\"https://hackersandslackers.com/demystifying-flasks-application-context/\" rel=\"noopener\">already covered this</a>, but is easily the first and last Redis library any Flask developer will use.</p><h2 id=\"your-box-your-treasure-your-world-\">Your Box, Your Treasure, Your World<strong>™</strong></h2><p>Now that we’ve uncovered this niche between cached data and stored data, the possibilities are endless. The world is your oyster full of things which you may or may not choose to shove in your box.</p><p>Ok, fine. Perhaps this whole concept feels like a bit of an obscure niche hardly worthy of the words on this page. Just remember that feeling when the time comes that you too need a little red cube, and it will be waiting with love and compassion. A companion cube, if you will.</p>","url":"https://hackersandslackers.com/using-redis-with-python/","uuid":"fcf41325-f7d3-4f3f-b43f-8609e5dc6b07","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c307c9493bed0776a0a3d80"}},{"node":{"id":"Ghost__Post__5c27630bda392c696eab97de","title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","slug":"tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","excerpt":"Organizing a heist on Tableau Server to reclaim workbook data.","custom_excerpt":"Organizing a heist on Tableau Server to reclaim workbook data.","created_at_pretty":"29 December, 2018","published_at_pretty":"29 December, 2018","updated_at_pretty":"13 March, 2019","created_at":"2018-12-29T07:05:31.000-05:00","published_at":"2018-12-29T07:18:53.000-05:00","updated_at":"2019-03-13T05:53:25.000-04:00","meta_title":"Tableau's View Extraction REST API | Hackers and Slackers","meta_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","og_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","og_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","og_title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","twitter_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","twitter_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","twitter_title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"}],"plaintext":"There's nothing I love more than exposing expensive enterprise software. \n\nIt may not seem obvious, but most SaaS products have an underlying core goal:\nshackle businesses to depend on proprietary, closed-source, costly software.\nWhen you pair a surplus of money with a reluctance to work, you've arrived at\nCorporate America: a prime victim yearning to marry itself to any vendor with a\nnice pitch deck and a vague promise.\n\nIn the case of Tableau, this becomes obvious when you attempt to do anything\nbesides create visuals. I don't like spending hours of my time cleaning data to\nbe rewarded with a shitty iframe embed: I want my data. As we've already seen by\nexposing Tableau's hidden Superadmin access\n[\thttps://hackersandslackers.com/hacking-linux-tableu-server/], it's pretty\nclear Tableau doesn't want you to do this. \n\nI realize Tableau is a BI tool, and some might argue we're barking up the wrong\ntree, and all data should be clean before reaching Tableau. My sentiment is\nthis: fuck that. If a single license costs one thousand dollars, and we have the\npower to manipulate data faster  as we visualize it, we should at least be able\nto own  that data: and by \"own,\" I don't mean a CSV export. I want it in my own \ndatabase of choice, not a locked down and hidden Postgres database living on a\nVPS filled with Tableau stuff.\n\nHere's how we'd do that.\n\n\"You Expect us to Just Walk out the Casino with Millions of Dollars on us?\"\nYou're looking at a Boeski, a Jim Brown, a Miss Daisy, two Jethros and a Leon\nSpinks, not to mention the biggest Ella Fitzgerald ever.You're here because\nyou're the best of the best. If you're feeling scared, feel free to back out\nnow.\n\nThis tutorial assumes you have a Tableau Server instance, with a workbook\npublished to a site within said instance. We're going to take a page out of that\nworkbook and turn the raw data into a database table. FAIR  WARNING: We're about\nto dive deep into the obscure world of the Tableau Server REST API\n[https://onlinehelp.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref.htm]\n. It's clunky, it's ugly, and it returns XML. Strap yourself in. \n\nWe're going to be working with 3 core endpoints. Let's walk through them, and\nI'll show you how to exploit said endpoints to create a ruthless data mining\nmachine in Python.\n\n'Tableau Authorization' Endpoint\nLike all obnoxious (aka useful) APIs, we need to authorize each API call with a\ntemporary token. Of course, we'll just have Python generate said token for every\ncall we make.\n\nPOST: http://[MyTaleauServerURL]/api/3.0/auth/signin\n\nHitting this endpoint successfully will result in an XML response (ugh). The\nresponse should look something like this:\n\n<?xml version='1.0' encoding='UTF-8'?>\n<tsResponse xmlns=\"http://tableau.com/api\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd\">\n    <credentials token=\"KBIuvu6FTViuyivuTUR^yfvgTUycvjGubgc\">\n        <site id=\"09Hiugv-345-45d0-b48b-34543giuyvg\" contentUrl=\"hackers\"/>\n        <user id=\"Uohiiyu-3455-8675-9b42-bugvdr876gv\"/>\n    </credentials>\n</tsResponse>\n\n\nThere are a number of things going on here that we should take note of. The\nfirst being a marvel of modern technology: this is perhaps the shittiest\nresponse to a token API call in modern history. Other than that, we need two\nthings from this response:\n\n * The token  is required for every API call from here on out. It is intended to\n   be passed as a header value with the key X-Tableau-Auth.\n * The site ID  is what we'll be using to look up the location of our workbooks\n   in our server instance. This is added to the URL of future API calls (again,\n   impressively shitty design here).\n\n'List All Views by Site' Endpoint\nThere are actually a number of methods we could use to retrieve views, but we're\nspecifically settling on listing our views by 'site,' in the Tableau sense of\nthe word. If you're unfamiliar, a Tableau site  is not a site at all: it's more\nof project within a greater Tableau instance. They probably should've named them\nthat.\n\nGET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views\n\nAs mentioned, we use the site ID  from step 1 to construct this endpoint. In my\nparticular instance, I've only saved a single workbook for simplicity's sake.\nThe response for such a case is as follows:\n\n<?xml version='1.0' encoding='UTF-8'?>\n<tsResponse xmlns=\"http://tableau.com/api\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd\">\n    <pagination pageNumber=\"1\" pageSize=\"100\" totalAvailable=\"1\"/>\n    <views>\n        <view id=\"9a4a1de9-b7af-4a4a-8556-fd5ac82f92bd\" name=\"Jira\" contentUrl=\"JiraIssues/sheets/Jira\" createdAt=\"2018-12-21T09:11:39Z\" updatedAt=\"2018-12-21T09:11:39Z\">\n            <workbook id=\"208a0c4e-e1d9-4852-9d19-7a2fe2717191\"/>\n            <owner id=\"Uohiiyu-3455-8675-9b42-bugvdr876gv\"/>\n            <project id=\"4d1ca337-20b4-442c-aa7b-1dfd470b68bd\"/>\n            <tags/>\n        </view>\n    </views>\n</tsResponse>\n\n\nCheck out the views  node: when we make this API call, <views>  will contain a\nlist of every view saved to the specified site. Keep in mind that a view is\nequivalent to a \"sheet\" of a workbook: in almost any case, you will have many\nviews listed here. \n\nMy sheet happens to be called \"Jira,\" as stated by name=\"Jira\". The thing we\nreally need however is the view id attribute: this will be used in our third and\nfinal API call.\n\n'Get View Data' Endpoint\nNow let's get the raw data from a view of our choice.\n\nGET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views/[MyViewID]/data\n\n\nHere's where we hit pay dirt. This request will result in an output of\ncomma-separated values; I don't need to tell you what we can do with\ncomma-separated values. Here's what my response looks like after formatting it\nas a table:\n\nCurrent AssigneeCurrent StatusDay of Updatedepic_colorepic_nameIssue Type\nissuetype_colorissuetype_urlkeyPriorityprojectsummaryTodd BirchardDoneJune 7,\n2018#42526EWidgetsBug#db5d5dhttps://hackers.nyc3.digitaloceanspaces.com/bug.png\nHACK-96LowestHackers and Slackers\"Recent Posts\" widget does not have link\nrolloverTodd BirchardBacklogJune 15, 2018#57D9A3Page TemplatesTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-32LowestHackers and\nSlackers“Join” pageTodd BirchardDoneNovember 13, 2018#42526EWidgetsTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-543MediumHackers and\nSlackersAdd “pro tip” boxTodd BirchardTo DoDecember 14, 2018#679EEFSEOMajor\nFunctionality#93d171https://hackers.nyc3.digitaloceanspaces.com/story.png\nHACK-656LowHackers and SlackersAdd alt attributes to images vis clarifai Todd\nBirchardBacklogOctober 16, 2018#FDDA3EAccountsMajor Functionality#93d171\nhttps://hackers.nyc3.digitaloceanspaces.com/story.pngHACK-473MediumHackers and\nSlackersAdd avatar selection to signupTodd BirchardDoneNovember 13, 2018#57D9A3\nPage TemplatesSub-task#92BFE5\nhttps://hackers.nyc3.digitaloceanspaces.com/subtask.pngHACK-231MediumHackers and\nSlackersAdd blurb to each post page explaining what these areTodd BirchardDone\nDecember 10, 2018#291BA9Code snippetsTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-452MediumHackers and\nSlackersAdd color styles for json snippetsThat's right, a table.  Databases are comprised of tables. Perhaps you see where\nI'm going with this.\n\n\"There's a Ninety-five Pound Chinese Man with a Hundred Sixty Million Dollars\nBehind this Door.\"\nLet's get him out.We've got the goods, but calling all these individual\nendpoints manually does nothing for us. We don't want to steal a single view, we\nwant to systematically rob Tableau of it's views on a scheduler and Shanghai\nthem off to a database of our choosing.\n\nIt would be a crime not to automate this, so I've created a class containing all\nthe relevant methods we'd want when it comes to interacting with Tableau's REST\nAPI:\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    \"\"\"Class for with the Tableau server API.\"\"\"\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        \"\"\"Extract contents of a single view.\"\"\"\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        \"\"\"List all views belonging to a Tableau Site.\"\"\"\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        \"\"\"Receive Auth token to perform API requests.\"\"\"\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        \"\"\"Retrieve ID of Tableau 'site' instance.\"\"\"\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        \"\"\"Retrieve core XML for interacting with Tableau.\"\"\"\n        headers = {'Content-Type': 'application/xml'}\n        body = '<tsRequest><credentials name=\"' + cls.__username + '\" password=\"' + cls.__password + '\" ><site contentUrl=\"' + cls.__contenturl + '\" /></credentials></tsRequest>'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n\n\nThe above snippet is a Python class utilizing all the API endpoints we explored\nin a mostly effortless manner. Instantiating the class immediately covers the\ngrunt work of:\n\n *   Generating a token\n * Getting your (unfriendly) site ID\n * Listing all views belonging to the provided site\n * Retrieving data from a worksheet of choice\n\nGet a list of views in your Tableau site by using the list_views()  method. When\nyou see the view you want, pass the view ID  to the .get_view()  method. This\nwill result in response of all raw data in the view in the form of a CSV. \n\nHow to Pull a Heist (Final Chapter): Storing in Offshore Accounts\nTo earn your title as a true con artist, I'm leaving the final step up to you.\nYou've escaped with the loot, but you'll need to put all that data somewhere.\nThis should be a trivial matter of automating a simple database query, but the\nspecifics are up to you.\n\nIf you're ready to liberate your data, feel free to grab the source off of\nGithub [https://gist.github.com/toddbirchard/ad1386b4334e7b22b2f7b38edca3bd5c] \nand go nuts.","html":"<p>There's nothing I love more than exposing expensive enterprise software. </p><p>It may not seem obvious, but most SaaS products have an underlying core goal: shackle businesses to depend on proprietary, closed-source, costly software. When you pair a surplus of money with a reluctance to work, you've arrived at Corporate America: a prime victim yearning to marry itself to any vendor with a nice pitch deck and a vague promise.</p><p>In the case of Tableau, this becomes obvious when you attempt to do anything besides create visuals. I don't like spending hours of my time cleaning data to be rewarded with a shitty iframe embed: I want my <em>data</em>. As we've already seen by exposing Tableau's hidden <a href=\"\thttps://hackersandslackers.com/hacking-linux-tableu-server/\">Superadmin access</a>, it's pretty clear Tableau doesn't want you to do this. </p><p>I realize Tableau is a BI tool, and some might argue we're barking up the wrong tree, and all data should be clean before reaching Tableau. My sentiment is this: <em>fuck that</em>. If a single license costs <em><strong>one thousand dollars</strong></em>, and we have the power to manipulate data <em>faster</em> as we visualize it, we should at least be able to <em>own</em> that data: and by \"own,\" I don't mean a CSV export. I want it in my <em>own</em> database of choice, not a locked down and hidden Postgres database living on a VPS filled with Tableau stuff.</p><p>Here's how we'd do that.</p><h2 id=\"you-expect-us-to-just-walk-out-the-casino-with-millions-of-dollars-on-us\">\"You Expect us to Just Walk out the Casino with Millions of Dollars on us?\"</h2><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/oceans.gif\" class=\"kg-image\"><figcaption>You're looking at a Boeski, a Jim Brown, a Miss Daisy, two Jethros and a Leon Spinks, not to mention the biggest Ella Fitzgerald ever.</figcaption></figure><!--kg-card-end: image--><p>You're here because you're the best of the best. If you're feeling scared, feel free to back out now.</p><p>This tutorial assumes you have a Tableau Server instance, with a workbook published to a site within said instance. We're going to take a page out of that workbook and turn the raw data into a database table. <strong>FAIR</strong> <strong>WARNING</strong>: We're about to dive deep into the obscure world of the <a href=\"https://onlinehelp.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref.htm\">Tableau Server REST API</a>. It's clunky, it's ugly, and it returns XML. Strap yourself in. </p><p>We're going to be working with 3 core endpoints. Let's walk through them, and I'll show you how to exploit said endpoints to create a ruthless data mining machine in Python.</p><h3 id=\"-tableau-authorization-endpoint\">'Tableau Authorization' Endpoint</h3><p>Like all obnoxious (aka useful) APIs, we need to authorize each API call with a temporary token. Of course, we'll just have Python generate said token for every call we make.</p><!--kg-card-begin: code--><pre><code>POST: http://[MyTaleauServerURL]/api/3.0/auth/signin</code></pre><!--kg-card-end: code--><p>Hitting this endpoint successfully will result in an XML response (ugh). The response should look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-xml\">&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;tsResponse xmlns=&quot;http://tableau.com/api&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd&quot;&gt;\n    &lt;credentials token=&quot;KBIuvu6FTViuyivuTUR^yfvgTUycvjGubgc&quot;&gt;\n        &lt;site id=&quot;09Hiugv-345-45d0-b48b-34543giuyvg&quot; contentUrl=&quot;hackers&quot;/&gt;\n        &lt;user id=&quot;Uohiiyu-3455-8675-9b42-bugvdr876gv&quot;/&gt;\n    &lt;/credentials&gt;\n&lt;/tsResponse&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>There are a number of things going on here that we should take note of. The first being a marvel of modern technology: this is perhaps the shittiest response to a token API call in modern history. Other than that, we need two things from this response:</p><ul><li>The <strong>token</strong> is required for every API call from here on out. It is intended to be passed as a header value with the key <code>X-Tableau-Auth</code>.</li><li>The <strong>site ID</strong> is what we'll be using to look up the location of our workbooks in our server instance. This is added to the URL of future API calls (again, impressively shitty design here).</li></ul><h3 id=\"-list-all-views-by-site-endpoint\">'List All Views by Site' Endpoint</h3><p>There are actually a number of methods we could use to retrieve views, but we're specifically settling on listing our views by '<em>site,' </em>in the Tableau sense of the word<em>. </em>If you're unfamiliar, a Tableau <em>site</em> is not a site at all: it's more of project within a greater Tableau instance. They probably should've named them that.</p><!--kg-card-begin: code--><pre><code>GET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views</code></pre><!--kg-card-end: code--><p>As mentioned, we use the <strong>site ID</strong> from step 1 to construct this endpoint. In my particular instance, I've only saved a single workbook for simplicity's sake. The response for such a case is as follows:</p><!--kg-card-begin: markdown--><pre><code class=\"language-xml\">&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;tsResponse xmlns=&quot;http://tableau.com/api&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd&quot;&gt;\n    &lt;pagination pageNumber=&quot;1&quot; pageSize=&quot;100&quot; totalAvailable=&quot;1&quot;/&gt;\n    &lt;views&gt;\n        &lt;view id=&quot;9a4a1de9-b7af-4a4a-8556-fd5ac82f92bd&quot; name=&quot;Jira&quot; contentUrl=&quot;JiraIssues/sheets/Jira&quot; createdAt=&quot;2018-12-21T09:11:39Z&quot; updatedAt=&quot;2018-12-21T09:11:39Z&quot;&gt;\n            &lt;workbook id=&quot;208a0c4e-e1d9-4852-9d19-7a2fe2717191&quot;/&gt;\n            &lt;owner id=&quot;Uohiiyu-3455-8675-9b42-bugvdr876gv&quot;/&gt;\n            &lt;project id=&quot;4d1ca337-20b4-442c-aa7b-1dfd470b68bd&quot;/&gt;\n            &lt;tags/&gt;\n        &lt;/view&gt;\n    &lt;/views&gt;\n&lt;/tsResponse&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out the <strong>views</strong> node: when we make this API call, <code>&lt;views&gt;</code> will contain a list of every view saved to the specified site. Keep in mind that a view is equivalent to a \"sheet\" of a workbook: in almost any case, you will have many views listed here. </p><p>My sheet happens to be called \"Jira,\" as stated by <code>name=\"Jira\"</code>. The thing we really need however is the <strong>view id </strong>attribute: this will be used in our third and final API call.</p><h3 id=\"-get-view-data-endpoint\">'Get View Data' Endpoint</h3><p>Now let's get the raw data from a view of our choice.</p><!--kg-card-begin: code--><pre><code>GET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views/[MyViewID]/data\n</code></pre><!--kg-card-end: code--><p>Here's where we hit pay dirt. This request will result in an output of comma-separated values; I don't need to tell you what we can do with comma-separated values. Here's what my response looks like after formatting it as a table:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"table table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">Current Assignee</th>\n<th title=\"Field #2\">Current Status</th>\n<th title=\"Field #3\">Day of Updated</th>\n<th title=\"Field #4\">epic_color</th>\n<th title=\"Field #5\">epic_name</th>\n<th title=\"Field #6\">Issue Type</th>\n<th title=\"Field #7\">issuetype_color</th>\n<th title=\"Field #8\">issuetype_url</th>\n<th title=\"Field #9\">key</th>\n<th title=\"Field #10\">Priority</th>\n<th title=\"Field #11\">project</th>\n<th title=\"Field #12\">summary</th>\n</tr></thead>\n<tbody><tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>June 7, 2018</td>\n<td>#42526E</td>\n<td>Widgets</td>\n<td>Bug</td>\n<td>#db5d5d</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/bug.png</td>\n<td>HACK-96</td>\n<td>Lowest</td>\n<td>Hackers and Slackers</td>\n<td>&quot;Recent Posts&quot; widget does not have link rollover</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Backlog</td>\n<td>June 15, 2018</td>\n<td>#57D9A3</td>\n<td>Page Templates</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-32</td>\n<td>Lowest</td>\n<td>Hackers and Slackers</td>\n<td>“Join” page</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>November 13, 2018</td>\n<td>#42526E</td>\n<td>Widgets</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-543</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add “pro tip” box</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>To Do</td>\n<td>December 14, 2018</td>\n<td>#679EEF</td>\n<td>SEO</td>\n<td>Major Functionality</td>\n<td>#93d171</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/story.png</td>\n<td>HACK-656</td>\n<td>Low</td>\n<td>Hackers and Slackers</td>\n<td>Add alt attributes to images vis clarifai </td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Backlog</td>\n<td>October 16, 2018</td>\n<td>#FDDA3E</td>\n<td>Accounts</td>\n<td>Major Functionality</td>\n<td>#93d171</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/story.png</td>\n<td>HACK-473</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add avatar selection to signup</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>November 13, 2018</td>\n<td>#57D9A3</td>\n<td>Page Templates</td>\n<td>Sub-task</td>\n<td>#92BFE5</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/subtask.png</td>\n<td>HACK-231</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add blurb to each post page explaining what these are</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>December 10, 2018</td>\n<td>#291BA9</td>\n<td>Code snippets</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-452</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add color styles for json snippets</td>\n</tr>\n</tbody></table>\n</div><!--kg-card-end: html--><p>That's right, a <em>table.</em> Databases are comprised of tables. Perhaps you see where I'm going with this.</p><h2 id=\"there-s-a-ninety-five-pound-chinese-man-with-a-hundred-sixty-million-dollars-behind-this-door-\">\"There's a Ninety-five Pound Chinese Man with a Hundred Sixty Million Dollars Behind this Door.\"</h2><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/1379892761308689767.jpg\" class=\"kg-image\"><figcaption>Let's get him out.</figcaption></figure><!--kg-card-end: image--><p>We've got the goods, but calling all these individual endpoints manually does nothing for us. We don't want to steal a single view, we want to systematically rob Tableau of it's views on a scheduler and Shanghai them off to a database of our choosing.</p><p>It would be a crime not to automate this, so I've created a class containing all the relevant methods we'd want when it comes to interacting with Tableau's REST API:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    &quot;&quot;&quot;Class for with the Tableau server API.&quot;&quot;&quot;\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        &quot;&quot;&quot;Extract contents of a single view.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        &quot;&quot;&quot;List all views belonging to a Tableau Site.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        &quot;&quot;&quot;Receive Auth token to perform API requests.&quot;&quot;&quot;\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        &quot;&quot;&quot;Retrieve ID of Tableau 'site' instance.&quot;&quot;&quot;\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        &quot;&quot;&quot;Retrieve core XML for interacting with Tableau.&quot;&quot;&quot;\n        headers = {'Content-Type': 'application/xml'}\n        body = '&lt;tsRequest&gt;&lt;credentials name=&quot;' + cls.__username + '&quot; password=&quot;' + cls.__password + '&quot; &gt;&lt;site contentUrl=&quot;' + cls.__contenturl + '&quot; /&gt;&lt;/credentials&gt;&lt;/tsRequest&gt;'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n</code></pre>\n<!--kg-card-end: markdown--><p>The above snippet is a Python class utilizing all the API endpoints we explored in a mostly effortless manner. Instantiating the class immediately covers the grunt work of:</p><ul><li> Generating a token</li><li>Getting your (unfriendly) site ID</li><li>Listing all views belonging to the provided site</li><li>Retrieving data from a worksheet of choice</li></ul><p>Get a list of views in your Tableau site by using the <code>list_views()</code> method. When you see the view you want, pass the <strong>view ID</strong> to the <code>.get_view()</code> method. This will result in response of all raw data in the view in the form of a CSV. </p><h3 id=\"how-to-pull-a-heist-final-chapter-storing-in-offshore-accounts\">How to Pull a Heist (Final Chapter): Storing in Offshore Accounts</h3><p>To earn your title as a true con artist, I'm leaving the final step up to you. You've escaped with the loot, but you'll need to put all that data somewhere. This should be a trivial matter of automating a simple database query, but the specifics are up to you.</p><p>If you're ready to liberate your data, feel free to <a href=\"https://gist.github.com/toddbirchard/ad1386b4334e7b22b2f7b38edca3bd5c\">grab the source off of Github</a> and go nuts.</p>","url":"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/","uuid":"77d21a34-e5c1-4582-aade-ff92d8596387","page":false,"codeinjection_foot":null,"codeinjection_head":"<style>\n  .post-template .post-content img {\n    width: 100% !important;\n  }\n\n  figcaption {\n    width: -webkit-fill-available !important;\n    width: -moz-available !important;\n    margin: 0 auto 0;\n    margin-top: -10px;\n    padding: 10px !important;\n    background-color: rgba(33, 69, 138, .04);\n    color: #5e6167;\n    font-size: .8em !important;\n    font-style: italic;\n    text-align: center !important;\n    white-space: normal !important;\n    line-height: 1.5 !important;\n  }\n\n  .language-xml::before {\n    content: \"XML\" !important;\n  }\n\n  .language-html::before {\n    content: \"XML\" !important;\n  }\n\n  td {\n    display: table-cell;\n    padding: 15px 10px !important;\n    font-size: .7em !important;\n    line-height: 1.2 !important;\n    text-align: left !important;\n    text-align: center !important;\n    vertical-align: middle !important;\n    max-width: 150px !important;\n    overflow: hidden !important;\n    white-space: nowrap !important;\n  }\n\n  th {\n    padding: 10px !important;\n    font-size: .7em !important;\n    text-align: center !important;\n    min-width: none !important;\n  }\n\n  .tableContainer {\n    margin-top: 30px;\n    overflow: hidden;\n  }\n</style>","comment_id":"5c27630bda392c696eab97de"}}]}},"pageContext":{"slug":"todd","limit":12,"skip":12,"numberOfPages":8,"humanPageNumber":2,"prevPageNumber":1,"nextPageNumber":3,"previousPagePath":"/author/todd/","nextPagePath":"/author/todd/page/3/"}}