{"data":{"ghostTag":{"slug":"data-analysis","name":"Data Analysis","visibility":"public","feature_image":null,"description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c5bb0ec7999ff33f06876e1","title":"Welcome to SQL: Modifying Databases and Tables","slug":"welcome-to-sql-modifying-databases-and-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","custom_excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","created_at_pretty":"07 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-06T23:15:40.000-05:00","published_at":"2019-02-19T18:28:00.000-05:00","updated_at":"2019-02-27T23:16:44.000-05:00","meta_title":"Welcome to SQL: Modifying Databases and Tables | Hackers and Slackers","meta_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","og_title":"Welcome to SQL: Modifying Databases and Tables","twitter_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","twitter_title":"Welcome to SQL: Modifying Databases and Tables","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"SQL: we all pretend to be experts at it, and mostly get away with it thanks to\nStackOverflow. Paired with our vast experience of learning how to code in the\n90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go\nahead and chalk up a win for your resume.\n\nSQL has been around longer than our careers have, so why start a series on it \nnow?  Surely there’s sufficient enough documentation that we can Google the\nspecifics whenever the time comes for us to write a query? That, my friends, is\nprecisely the problem. Regardless of what tools we have at our disposable, some\nskills are better learned and practiced by heart. SQL is one of those skills.\n\nSure, SQLAlchemy or similar ORMs might protect us here-and-there from writing\nraw queries. Considering SQL is just one of many query languages we'll use\nregularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert\nreally that critical? In short, yes: relational databases are not only here to\nstay, but thinking  in queries as a second language solidifies one's\nunderstanding of the fine details of data. Marc Laforet\n[https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032] \n recently published a Medium post which drives home just how important leaning\non SQL is:\n\n> What’s even more interesting is that when these transformation scripts were\napplied to the 6.5 GB dataset, python completely failed. Out of 3 attempts,\npython crashed 2 times and my computer completely froze the 3rd time… while SQL\ntook 226 seconds.\n\n\nKeeping logic out of our apps and pipelines and in SQL results in exponentially\nfaster execution, while also being more readable and universally understood than\nwhatever we’d write in our language of choice. The lower down we can push\napplication logic in our stack, the better. This is why I’d much prefer to see\nthe datasphere saturated with SQL tutorials as opposed to Pandas tutorials.\n\nRelational Database Terminology\nI hate it when informational material kicks off with covering obvious\nterminology definitions. Under normal circumstances, I find this to be cliche,\nunhelpful, and damaging to an author's credibility; but these aren't normal\ncircumstances. In SQL, vocabulary commonly has multiple meanings depending on\ncontext, or even which flavor database you're using. Given this fact, it's\nentirely possible (and common) for individuals to rack up experience with\nrelational databases while completely misinterpreting fundamental concepts.\nLet's make sure that doesn't happen:\n\n * Databases: Every Database instance is separated at the highest level into \n   databases. Yes, a database is a collection of databases - we're already off\n   to a great start.\n * Schemas: In PostgreSQL (and other databases), a schema  is a grouping of\n   tables and other objects, including views, relations, etc. A schema is a way\n   of organizing data. Schemas imply that all the data belonging to it is at\n   some form related, even if only by concept. Note that the term schema  is\n   sometimes used to describe other concepts depending on the context.\n * Tables: The meat and potatos of relational databases. Tables consist of rows\n   and columns which hold our sweet, sweet data. Columns are best thought of as\n   'attributes', whereas rows are entries which consist of values for said\n   attributes. All values in a column must share the same data type. * Keys: Keys are used to help us organize and optimize data, as well as\n      place certain constraints on data coming in (for example, email addresses\n      of user accounts must be unique). Keys can also help us keep count of our\n      entries, ensure automatically unique values, and provide a bridge to link\n      multiple tables of data. * Primary keys: Identification tags for each row of data. The primary key\n         is different for every record in the relational database; values must\n         be provided, and they must be unique between rows.\n       * Foreign keys: Enable data searches and manipulation between the primary\n         database table and other related databases.\n      \n      \n   \n   \n * Objects: A blanket term for anything (including relations) that exist in a\n   schema (somewhat PostgreSQL-specific). * Views (PostgreSQL): Views display data in a fashion similar to tables,\n      with the difference that views do not store  data. Views are a snapshot of\n      data pulled from other tables in the form of a query; a good way to think\n      about views is to consider them to be 'virtual tables.'\n    * Functions (PostgreSQL): Logic for interacting with data saved for the\n      purpose of being reused.\n   \n   \n\nIn MySQL, a schema  is synonymous with a database. These keywords can even be\nswapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using CREATE\nSCHEMA  acheives the same effect as instead of CREATE DATABASE.Navigating and\nCreating Databases\nWe've got to start somewhere, so it might as well be with database management.\nAdmittedly, this will be the most useless of the things we'll cover. The act of\nnavigating databases is best suited for a GUI.\n\nShow Databases\nIf you access your database via command line shell (for some reason), the first\nlogical thing to do is to list the available databases:\n\nSHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\n\nUSE Database\nNow that we've listed the possible databases we can connect to, we can explore\nwhat each of these contains. To do this, we have to specify which database we\nwant to connect to, AKA \"use.\" \n\ndb> USE database_name;\nDatabase changed\n\n\nCreate Database\nCreating databases is straightforward. Be sure to pay attention to the character\nset  when creating a database: this will determine which types of characters\nyour database will be able to accept. For example, if we try to insert special\nencoded characters into a simple UTF-8 database, those characters won’t turn out\nas we’d expect.\n\nCREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n\n\nBonus: here's the shorthand for creating a database and then showing the result:\n\nSHOW CREATE DATABASE database_name;\n\n\nCreating and Modifying Tables\nCreating tables via SQL syntax can be critical when automating data imports.\nWhen creating a table, we also set the column names, types, and keys:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];\n\nWe can specify IF NOT EXISTS  when creating our table if we'd like to include\nvalidation in our query. When present, the table will only be created if a table\nof the specified name does not exist.\n\nWhen creating each of our columns, there are a number of things we can specify\nper-column:\n\n * Data Type (required):  The data which can be saved to cells of this column\n   (such as INTEGER, TEXT, etc).\n * Key Type:  Creates a key for the column.\n * Key Attributes:  Any key-related attributes, such as auto-incrementing.\n * Default:  If rows are created in the table without values passed to the\n   current column, the value specified as DEFAULT  \n * Primary Key:  Allows any of the previous specified columns to be set as the\n   table's primary key.\n\nMySQL tables can have a 'storage engine' specified via ENGINE=[engine_type],\nwhich determines the core logic of how the table will interpret data. Leaving\nthis blank defaults to InnoDB and is almost certainly fine to be left alone. In\ncase you're interested, you can find more about MySQL engines here\n[https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html].\n\nHere's an example of what an actual CREATE TABLE  query would look like:\n\nCREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nManaging Keys for Existing Tables\nIf we don't specify our keys at table creation time, we can always do so after\nthe fact. SQL tables can accept the following key types:\n\n * Primary Key:  One or more fields/columns that uniquely identify a record in\n   the table. It can not accept null, duplicate values.\n * Candidate Key:  Candidate keys are kind of like groups of non-committed\n   Primary Keys; these keys only accept unique values, and could potentially  be\n   used in the place of a Primary Key if need be, but are not actual Primary\n   Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.\n * Alternate Key:  Refers to a single Candidate Key (an alternative which can\n   satisfy the duty of a Primary Key id need be).\n * Composite/Compound Key:  Defined by combing the values of multiple columns;\n   the sum of which will always produce a unique value. There can be multiple\n   Candidate Keys in one table. Each Candidate Key can work as Primary Key.\n * Unique Key:  A set of one or more fields/columns of a table that uniquely\n   identify a record in a database table. Similar to Primary key, but it can\n   accept only one null value, and it can not have duplicate values.\n * Foreign Key: Foreign keys denote fields that serve as another table's \n   Primary key. Foreign keys are useful for building relationships between\n   tables. While a foreign key is required in the parent table where they are\n   primary, foreign keys can be null or empty in the tables intended to relate\n   to the other table.\n\nLet's look at an example query where we add a key to a table and dissect the\npieces:\n\nALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n\n\nALTER TABLE  is used to make any changes to a table's structure, whether that be\nmodifying columns or keys.\n\nIn this example, we ADD  a key which happens to be a FOREIGN KEY. While keys\nalways refer to columns, keys themselves must have names of their own to\ndistinguish the column's data and a key's conceptual logic. We name our key \nforeign_key_name  and specify which column the key will act on with \n(column_name). Because this is a foreign key, we need to specify which table's \nprimary key  we want this to be associated with. REFERENCES\nparent_table(primary_key_column)  is stating that the foreign key in this table\ncorresponds to values held in a column named primary_key_column, in a table\nnamed parent_table.\n\nThe statements ON DELETE  and ON UPDATE  are actions which take place if the\nparent table's primary key is deleted or updated, respectively. ON DELETE\nCASCADE  would result in our tables foreign key being deleted if the\ncorresponding primary key were to disappear.\n\nAdding Columns\nAdding columns follows the same syntax we used when creating tables. An\ninteresting additional feature is the ability to place the new column before or\nafter preexisting columns:\n\nALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n\n\nWhen referencing tables in PostgreSQL databases, we must specify the schema\nbelongs to. Thus, ALTER TABLE table_name  becomes ALTER TABLE\nschema_name.table_name. This applies to any time we reference tables, including\nwhen we create and delete tables.Pop Quiz\nThe below statement uses elements of everything we've learned about modifying\nand creating table structures thus far. Can you discern what is happening here?\n\nCREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n\n\nDropping Data\nDANGER ZONE: this is where we can start to mess things up. Dropping columns or\ntables results in a complete loss of data: whenever you see the word \"drop,\" be\nscared.\n\nIf you're sure you know what you're doing and would like to remove a table\ncolumn, this can be done as such:\n\nALTER TABLE table\nDROP column;\n\n\nDropping a table destroys the table structure as well as all data within it:\n\nDROP TABLE table_name;\n\n\nTruncating a table, on the other hand, will purge the table of data but retain\nthe table itself:\n\nTRUNCATE TABLE table_name;\n\n\nDrop Foreign Key\nLike tables and columns, we can drop keys as well:\n\nALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n\n\nThis can also be handed by dropping CONSTRAINT:\n\nALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n\n\nWorking with Views (Specific to PostgreSQL)\nLastly, let's explore the act of creating views. There are three types of views\nPostgreSQL can handle:\n\n * Simple Views: Virtual tables which represent data of underlying tables.\n   Simple views are automatically updatable: the system will allow INSERT,\n   UPDATE and DELETE statements to be used on the view in the same way as on a\n   regular table.\n * Materialized Views: PostgreSQL extends the view concept to a next level that\n   allows views to store data 'physically', and we call those views are\n   materialized views. A materialized view caches the result of a complex query\n   and then allow you to refresh the result periodically.\n * Recursive Views: Recursive views are a bit difficult to explain without\n   delving deep into the complicated (but cool!) functionality of recursive\n   reporting. I won't get into the details, but these views are able to\n   represent relationships which go multiple layers deep. Here's a quick taste,\n   if you;re curious:\n\nSample RECURSIVE  query:\n\nWITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' > ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n\n\nOutput:\n\n employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North > Megan Berry\n           3 | Michael North > Sarah Berry\n           4 | Michael North > Zoe Black\n           5 | Michael North > Tim James\n           6 | Michael North > Megan Berry > Bella Tucker\n           7 | Michael North > Megan Berry > Ryan Metcalfe\n           8 | Michael North > Megan Berry > Max Mills\n           9 | Michael North > Megan Berry > Benjamin Glover\n          10 | Michael North > Sarah Berry > Carolyn Henderson\n          11 | Michael North > Sarah Berry > Nicola Kelly\n          12 | Michael North > Sarah Berry > Alexandra Climo\n          13 | Michael North > Sarah Berry > Dominic King\n          14 | Michael North > Zoe Black > Leonard Gray\n          15 | Michael North > Zoe Black > Eric Rampling\n          16 | Michael North > Megan Berry > Ryan Metcalfe > Piers Paige\n          17 | Michael North > Megan Berry > Ryan Metcalfe > Ryan Henderson\n          18 | Michael North > Megan Berry > Max Mills > Frank Tucker\n          19 | Michael North > Megan Berry > Max Mills > Nathan Ferguson\n          20 | Michael North > Megan Berry > Max Mills > Kevin Rampling\n(20 rows)\n\n\nCreating a View\nCreating a simple view is as simple as writing a standard query! All that is\nrequired is the addition of CREATE VIEW view_name AS  before the query, and this\nwill create a saved place for us to always come back and reference the results\nof this query:\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n\n\nGet Out There and Start SQLing\nI highly encourage anybody to get in the habit of always writing SQL queries by\nhand. With the right GUI, autocompletion can be your best friend.\n\nExplicitly forcing one's self to write queries instead of copy & pasting\nanything forces us to come to realizations, such as SQL's order of operations.\nIndeed, this query holds the correct syntax...\n\nSELECT *\nFROM table_name\nWHERE column_name = 'Value';\n\n\n...Whereas this one does not:\n\nSELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n\n\nGrasping the subtleties of SQL is the difference between being blazing fast and\nmostly clueless. The good news is, you’ll start to find that these concepts\naren’t nearly as daunting as they may have once seemed, so the track from ‘bad\ndata engineer’ to ‘expert’ is an easy win that would be foolish not to take.\n\nStick around for next time where we actually work with data in SQL: The Sequel,\nrated PG-13.","html":"<p>SQL: we all pretend to be experts at it, and mostly get away with it thanks to StackOverflow. Paired with our vast experience of learning how to code in the 90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go ahead and chalk up a win for your resume.</p><p>SQL has been around longer than our careers have, so why start a series on it <em>now?</em> Surely there’s sufficient enough documentation that we can Google the specifics whenever the time comes for us to write a query? That, my friends, is precisely the problem. Regardless of what tools we have at our disposable, some skills are better learned and practiced by heart. SQL is one of those skills.</p><p>Sure, SQLAlchemy or similar ORMs might protect us here-and-there from writing raw queries. Considering SQL is just one of many query languages we'll use regularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert really that critical? In short, yes: relational databases are not only here to stay, but <em>thinking</em> in queries as a second language solidifies one's understanding of the fine details of data. <a href=\"https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032\">Marc Laforet</a> recently published a Medium post which drives home just how important leaning on SQL is:</p><blockquote>\n<p>What’s even more interesting is that when these transformation scripts were applied to the 6.5 GB dataset, python completely failed. Out of 3 attempts, python crashed 2 times and my computer completely froze the 3rd time… while SQL took 226 seconds.</p>\n</blockquote>\n<p>Keeping logic out of our apps and pipelines and in SQL results in exponentially faster execution, while also being more readable and universally understood than whatever we’d write in our language of choice. The lower down we can push application logic in our stack, the better. This is why I’d much prefer to see the datasphere saturated with SQL tutorials as opposed to Pandas tutorials.</p><h2 id=\"relational-database-terminology\">Relational Database Terminology</h2><p>I hate it when informational material kicks off with covering obvious terminology definitions. Under normal circumstances, I find this to be cliche, unhelpful, and damaging to an author's credibility; but these aren't normal circumstances. In SQL, vocabulary commonly has multiple meanings depending on context, or even which flavor database you're using. Given this fact, it's entirely possible (and common) for individuals to rack up experience with relational databases while completely misinterpreting fundamental concepts. Let's make sure that doesn't happen:</p><ul>\n<li><strong>Databases</strong>: Every Database instance is separated at the highest level into <em>databases</em>. Yes, a database is a collection of databases - we're already off to a great start.</li>\n<li><strong>Schemas</strong>: In PostgreSQL (and other databases), a <em>schema</em> is a grouping of tables and other objects, including views, relations, etc. A schema is a way of organizing data. Schemas imply that all the data belonging to it is at some form related, even if only by concept. Note that the term <em>schema</em> is sometimes used to describe other concepts depending on the context.</li>\n<li><strong>Tables</strong>: The meat and potatos of relational databases. Tables consist of rows and columns which hold our sweet, sweet data. Columns are best thought of as 'attributes', whereas rows are entries which consist of values for said attributes. All values in a column must share the same data type.\n<ul>\n<li><strong>Keys</strong>: Keys are used to help us organize and optimize data, as well as place certain constraints on data coming in (for example, email addresses of user accounts must be <em>unique</em>). Keys can also help us keep count of our entries, ensure automatically unique values, and provide a bridge to link multiple tables of data.\n<ul>\n<li><strong>Primary keys</strong>:  Identification tags for each row of data. The primary key is different for every record in the relational database; values must be provided, and they must be unique between rows.</li>\n<li><strong>Foreign keys</strong>: Enable data searches and manipulation between the primary database table and other related databases.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objects</strong>: A blanket term for anything (including relations) that exist in a schema (somewhat PostgreSQL-specific).\n<ul>\n<li><strong>Views (PostgreSQL)</strong>: Views display data in a fashion similar to tables, with the difference that views do not <em>store</em> data. Views are a snapshot of data pulled from other tables in the form of a query; a good way to think about views is to consider them to be 'virtual tables.'</li>\n<li><strong>Functions  (PostgreSQL)</strong>: Logic for interacting with data saved for the purpose of being reused.</li>\n</ul>\n</li>\n</ul>\n<div class=\"protip\">\nIn MySQL, a <strong>schema</strong> is synonymous with a <strong>database</strong>. These keywords can even be swapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using <code>CREATE SCHEMA</code> acheives the same effect as instead of <code>CREATE DATABASE</code>.   \n</div><h2 id=\"navigating-and-creating-databases\">Navigating and Creating Databases</h2><p>We've got to start somewhere, so it might as well be with database management. Admittedly, this will be the most useless of the things we'll cover. The act of navigating databases is best suited for a GUI.</p><h3 id=\"show-databases\">Show Databases</h3><p>If you access your database via command line shell (for some reason), the first logical thing to do is to list the available databases:</p><pre><code class=\"language-sql\">SHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n</code></pre>\n<h3 id=\"use-database\">USE Database</h3><p>Now that we've listed the possible databases we can connect to, we can explore what each of these contains. To do this, we have to specify which database we want to connect to, AKA \"use.\" </p><pre><code class=\"language-sql\">db&gt; USE database_name;\nDatabase changed\n</code></pre>\n<h3 id=\"create-database\">Create Database</h3><p>Creating databases is straightforward. Be sure to pay attention to the <em>character set</em> when creating a database: this will determine which types of characters your database will be able to accept. For example, if we try to insert special encoded characters into a simple UTF-8 database, those characters won’t turn out as we’d expect.</p><pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n</code></pre>\n<p>Bonus: here's the shorthand for creating a database and then showing the result:</p><pre><code class=\"language-sql\">SHOW CREATE DATABASE database_name;\n</code></pre>\n<h2 id=\"creating-and-modifying-tables\">Creating and Modifying Tables</h2><p>Creating tables via SQL syntax can be critical when automating data imports. When creating a table, we also set the column names, types, and keys:</p><pre><code>CREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];</code></pre><p>We can specify <code>IF NOT EXISTS</code> when creating our table if we'd like to include validation in our query. When present, the table will only be created if a table of the specified name does not exist.</p><p>When creating each of our columns, there are a number of things we can specify per-column:</p><ul><li><strong>Data Type (required):</strong> The data which can be saved to cells of this column (such as INTEGER, TEXT, etc).</li><li><strong>Key Type:</strong> Creates a key for the column.</li><li><strong>Key Attributes:</strong> Any key-related attributes, such as auto-incrementing.</li><li><strong>Default:</strong> If rows are created in the table without values passed to the current column, the value specified as <code>DEFAULT</code> </li><li><strong>Primary Key:</strong> Allows any of the previous specified columns to be set as the table's primary key.</li></ul><p>MySQL tables can have a 'storage engine' specified via <code>ENGINE=[engine_type]</code>, which determines the core logic of how the table will interpret data. Leaving this blank defaults to InnoDB and is almost certainly fine to be left alone. In case you're interested, you can find more about MySQL engines <a href=\"https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html\">here</a>.</p><p>Here's an example of what an actual <code>CREATE TABLE</code> query would look like:</p><pre><code>CREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;</code></pre><h3 id=\"managing-keys-for-existing-tables\">Managing Keys for Existing Tables</h3><p>If we don't specify our keys at table creation time, we can always do so after the fact. SQL tables can accept the following key types:</p><ul><li><strong>Primary Key:</strong> One or more fields/columns that uniquely identify a record in the table. It can not accept null, duplicate values.</li><li><strong>Candidate Key:</strong> Candidate keys are kind of like groups of non-committed Primary Keys; these keys only accept unique values, and <em>could potentially</em> be used in the place of a Primary Key if need be, but are not actual Primary Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.</li><li><strong>Alternate Key:</strong> Refers to a single Candidate Key (an alternative which can satisfy the duty of a Primary Key id need be).</li><li><strong>Composite/Compound Key:</strong> Defined by combing the values of multiple columns; the sum of which will always produce a unique value. There can be multiple Candidate Keys in one table. Each Candidate Key can work as Primary Key.</li><li><strong>Unique Key:</strong> A set of one or more fields/columns of a table that uniquely identify a record in a database table. Similar to Primary key, but it can accept only one null value, and it can not have duplicate values.</li><li><strong>Foreign Key: </strong>Foreign keys denote fields that serve as <em>another table's</em> Primary key. Foreign keys are useful for building relationships between tables. While a foreign key is required in the parent table where they are primary, foreign keys can be null or empty in the tables intended to relate to the other table.</li></ul><p>Let's look at an example query where we add a key to a table and dissect the pieces:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n</code></pre>\n<p><code>ALTER TABLE</code> is used to make any changes to a table's structure, whether that be modifying columns or keys.</p><p>In this example, we <code>ADD</code> a key which happens to be a <code>FOREIGN KEY</code>. While keys always refer to columns, keys themselves must have names of their own to distinguish the column's data and a key's conceptual logic. We name our key <code>foreign_key_name</code> and specify which column the key will act on with <code>(column_name)</code>. Because this is a foreign key, we need to specify which table's <em>primary key</em> we want this to be associated with. <code>REFERENCES parent_table(primary_key_column)</code> is stating that the foreign key in this table corresponds to values held in a column named <code>primary_key_column</code>, in a table named <code>parent_table</code>.</p><p>The statements <code>ON DELETE</code> and <code>ON UPDATE</code> are actions which take place if the parent table's primary key is deleted or updated, respectively. <code>ON DELETE CASCADE</code> would result in our tables foreign key being deleted if the corresponding primary key were to disappear.</p><h3 id=\"adding-columns\">Adding Columns</h3><p>Adding columns follows the same syntax we used when creating tables. An interesting additional feature is the ability to place the new column before or after preexisting columns:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n</code></pre>\n<div class=\"protip\">\nWhen referencing tables in PostgreSQL databases, we must specify the schema belongs to. Thus, <code>ALTER TABLE table_name</code> becomes <code>ALTER TABLE schema_name.table_name</code>. This applies to any time we reference tables, including when we create and delete tables.\n</div><h3 id=\"pop-quiz\">Pop Quiz</h3><p>The below statement uses elements of everything we've learned about modifying and creating table structures thus far. Can you discern what is happening here?</p><pre><code class=\"language-sql\">CREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n</code></pre>\n<h2 id=\"dropping-data\">Dropping Data</h2><p>DANGER ZONE: this is where we can start to mess things up. Dropping columns or tables results in a complete loss of data: whenever you see the word \"drop,\" be scared.</p><p>If you're sure you know what you're doing and would like to remove a table column, this can be done as such:</p><pre><code class=\"language-sql\">ALTER TABLE table\nDROP column;\n</code></pre>\n<p>Dropping a table destroys the table structure as well as all data within it:</p><pre><code class=\"language-sql\">DROP TABLE table_name;\n</code></pre>\n<p>Truncating a table, on the other hand, will purge the table of data but retain the table itself:</p><pre><code class=\"language-sql\">TRUNCATE TABLE table_name;\n</code></pre>\n<h3 id=\"drop-foreign-key\">Drop Foreign Key</h3><p>Like tables and columns, we can drop keys as well:</p><pre><code class=\"language-sql\">ALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n</code></pre>\n<p>This can also be handed by dropping CONSTRAINT:</p><pre><code class=\"language-sql\">ALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n</code></pre>\n<h2 id=\"working-with-views-specific-to-postgresql-\">Working with Views (Specific to PostgreSQL)</h2><p>Lastly, let's explore the act of creating views. There are three types of views PostgreSQL can handle:</p><ul>\n<li><strong>Simple Views</strong>: Virtual tables which represent data of underlying tables. Simple views are automatically updatable: the system will allow INSERT, UPDATE and DELETE statements to be used on the view in the same way as on a regular table.</li>\n<li><strong>Materialized Views</strong>: PostgreSQL extends the view concept to a next level that allows views to store data 'physically', and we call those views are materialized views. A materialized view caches the result of a complex query and then allow you to refresh the result periodically.</li>\n<li><strong>Recursive Views</strong>: Recursive views are a bit difficult to explain without delving deep into the complicated (but cool!) functionality of recursive reporting. I won't get into the details, but these views are able to represent relationships which go multiple layers deep. Here's a quick taste, if you;re curious:</li>\n</ul>\n<p><strong>Sample </strong><code>RECURSIVE</code> <strong>query:</strong></p><pre><code class=\"language-sql\">WITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' &gt; ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n</code></pre>\n<p><strong>Output:</strong></p><pre><code class=\"language-shell\"> employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North &gt; Megan Berry\n           3 | Michael North &gt; Sarah Berry\n           4 | Michael North &gt; Zoe Black\n           5 | Michael North &gt; Tim James\n           6 | Michael North &gt; Megan Berry &gt; Bella Tucker\n           7 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe\n           8 | Michael North &gt; Megan Berry &gt; Max Mills\n           9 | Michael North &gt; Megan Berry &gt; Benjamin Glover\n          10 | Michael North &gt; Sarah Berry &gt; Carolyn Henderson\n          11 | Michael North &gt; Sarah Berry &gt; Nicola Kelly\n          12 | Michael North &gt; Sarah Berry &gt; Alexandra Climo\n          13 | Michael North &gt; Sarah Berry &gt; Dominic King\n          14 | Michael North &gt; Zoe Black &gt; Leonard Gray\n          15 | Michael North &gt; Zoe Black &gt; Eric Rampling\n          16 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Piers Paige\n          17 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Ryan Henderson\n          18 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Frank Tucker\n          19 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Nathan Ferguson\n          20 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Kevin Rampling\n(20 rows)\n</code></pre>\n<h3 id=\"creating-a-view\">Creating a View</h3><p>Creating a simple view is as simple as writing a standard query! All that is required is the addition of <code>CREATE VIEW view_name AS</code> before the query, and this will create a saved place for us to always come back and reference the results of this query:</p><pre><code class=\"language-sql\">CREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n</code></pre>\n<h2 id=\"get-out-there-and-start-sqling\">Get Out There and Start SQLing</h2><p>I highly encourage anybody to get in the habit of <em>always </em>writing SQL queries by hand. With the right GUI, autocompletion can be your best friend.</p><p>Explicitly forcing one's self to write queries instead of copy &amp; pasting anything forces us to come to realizations, such as SQL's order of operations. Indeed, this query holds the correct syntax...</p><pre><code class=\"language-sql\">SELECT *\nFROM table_name\nWHERE column_name = 'Value';\n</code></pre>\n<p>...Whereas this one does not:</p><pre><code class=\"language-sql\">SELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n</code></pre>\n<p>Grasping the subtleties of SQL is the difference between being blazing fast and mostly clueless. The good news is, you’ll start to find that these concepts aren’t nearly as daunting as they may have once seemed, so the track from ‘bad data engineer’ to ‘expert’ is an easy win that would be foolish not to take.</p><p>Stick around for next time where we actually work with data in <strong>SQL: The Sequel</strong>, rated PG-13.</p>","url":"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/","uuid":"fe99e822-f21a-432c-8bbf-4d399e575570","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5bb0ec7999ff33f06876e1"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673756","title":"Geocoding Raw Datasets for Mapbox","slug":"preparing-data-for-mapbox-geocoding","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","excerpt":"Make sense of unstructured data with enough precision to put it on a map.","custom_excerpt":"Make sense of unstructured data with enough precision to put it on a map.","created_at_pretty":"10 December, 2018","published_at_pretty":"18 December, 2018","updated_at_pretty":"02 April, 2019","created_at":"2018-12-10T17:16:29.000-05:00","published_at":"2018-12-18T08:00:00.000-05:00","updated_at":"2019-04-01T20:27:23.000-04:00","meta_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","meta_description":"Make sense of unstructured data with enough precision to put it on a map.","og_description":"Make sense of unstructured data with enough precision to put it on a map.","og_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","og_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","twitter_description":"Make sense of unstructured data with enough precision to put it on a map.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","twitter_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"This wouldn't be a proper data blog unless we spend a vast majority of our time\ntalking about cleaning data. Chances are if you're pursuing analysis that's\ngroundbreaking (or worthwhile), we're probably starting with some ugly, untapped\ninformation. It turns out Mapbox has an API specifically for this purpose: the \nMapbox Geocoding API [https://www.mapbox.com/api-documentation/#geocoding].\n\nGeocoding  is a blanket term for turning vague information into specific\nLat/Long coordinates. How vague, you ask? The API covers:\n\n * Pinpointing exact location via street address.\n * Locating regions or cities by recognizable name (ie: Rio de Janeiro).\n * Locating cities by highly unspecific name (Geocoding for \"Springfield\" will\n   return results for 41 American cities)\n * Locating cities or venues by name within a given region (such as searching\n   for Ray's Pizza in NYC).\n\nWe can also use Geocoding to do the reverse of this, where passing in\ncoordinates will return location names. If you find this useful, I'm assuming\nyou're a spy.\n\nChipping Away at a Real Use Case\nIn a real-life example, I have two sets of data: one represents general places\nof residence for a particular sample group. The goal is to see how they interact\nwith the second dataset: a list of locations they will be traveling to. I'd love\nto go into more detail, but:\n\nI get one cliche meme per year.So how can we use the Mapbox Geocoding API to\nsystematically extract coordinates for thousands of addresses, from multiple\ndatasets? With Pandas, of course!\n\nI'm Just Happy to be Writing About Pandas Right Now\nPardon my excitement; I've been far overdue for posting anything Pandas-related.\nIt's been killing me on the inside.\n\nWe need to make sense of some vague data. As seen in our Citibike example\n[https://hackersandslackers.com/map-data-visualization-with-mapbox/], New York\nhas plenty of public datasets with information like Taxi pickup/dropoffs, public\ntransit, etc. These start and end points are typically too fluid to have\nLat/Long coordinates associated with them, so we'll add them in ourselves. Given\nthat we're about to pass hundreds or thousands of addresses and locations, we'll\nuse Pandas .apply()  to fill out the missing Lat/Long columns in our dataset. \n\nInstead of using Mapbox's Python SDK, I'll actually be using requests  to hit\nthe Mapbox REST API. For some reason, the Python SDK was a bit unpredictable on\nmy last run.*\n\n*UPDATE: the Python SDK \"wasn't working\" because I apparently don't know the\ndifference between longitude and latitude. Awesome, so I'm a moron.\n\nimport sys\nimport os\nimport pandas as pd\nimport requests\nimport json\n\n\nclass GeocodeAddresses:\n    \"\"\"Add missing lat/long information to exisiting dataset.\"\"\"\n\n    def __init__(self, address_data):\n        self.data = address_data\n        self.address_df = pd.read_csv(self.data)\n        self.complete_data = self.get_coords(self.address_df)\n\n\n    @classmethod\n    def get_coords(self, employee_address_df):\n        \"\"\"Fill Dataframe lat/long columns.\"\"\"\n\n        def fill_coords(row):\n            \"\"\"Create a route object by passing GeoJSON start/end objects.\"\"\"\n            base_url = 'https://api.mapbox.com/geocoding/v5/mapbox.places/'\n            address = str(row.home_address)\n            format = '.json'\n            endpoint = base_url + address + format\n            params = {\n                'access_token':  'pk.eyJ1IjNoYXJkd2VthisisreallolaXdyNHQ3OTUifQ.VTAUrmzD91Ppxr1AJww'\n            }\n            headers = {\n                'Content-Type': 'application/json'\n            }\n            r = requests.get(endpoint, params=params, headers=headers)\n            try:\n                Lat = r.json()['features'][0]['geometry']['coordinates'][0]\n                Long = r.json()['features'][0]['geometry']['coordinates'][1]\n                print(pd.Series([Lat, Long]))\n                return pd.Series([Lat, Long])\n            except IndexError:\n                pass\n\n        address_df[['Lat', 'Long']] = address_df.head(100).apply(fill_coords, axis=1)\n        address_df.to_csv('geocoded.csv')\n\n\nIn the above example, we're using .apply()  against an empty series (our\nLat/Long columns) as opposed to our entire Dataframe. When get_coords()  returns\ntwo values, these values will fill the empty columns on a per-row basis.\n\nFor the scope of this tutorial, we'll simply focus on getting these points\nplotted. Don't worry, this is only part 2 of our Mapbox series! Yes, an entire\nseries!\n\nTurning Your Datasets into Tilesets\nIn Mapbox terms, a Tileset  is essentially a layer of data we can overlay on top\nof our blank map. The map style  we created last time was really just the\naesthetic unpinning of all the interesting data we can pile on time.\n\nTilesets can be stacked on one another, thus giving us infinite possibilities of\nthe types of data we can communicate: especially when you consider that Mapbox\nsupports Heatmaps and topology - far more than just plotted points.\n\nFirst, we'll plot our origins. I've put together a dataset of completely\nfalsified names (with presumably real addresses?) to demonstrate how we'd plot\nthese points. Here's a sample of the garbage I'll be feeding into Mapbox:\n\naddressnamelonglat761 ST ANNS AVE NY NY 10451Royal Hiett40.754466-73.97945525\nCOLUMBUS CIR NY NY 10019Yolanda Antonio40.8201997-73.91103241145 LENOX RD NY NY\n11212Marguerita Autry40.7667595-73.98157042800 VICTORY BLVD NY NY 10314Alyse\nPeranio40.6597804-73.9183181750 LEXINGTON AVE NY NY 10022Sina Walberg40.6080557\n-74.153241829 BAY RIDGE AVE NY NY 11220Ignacia Frasher40.7625148-73.9685564550\nRIVERSIDE DR NY NY 10027Marta Haymond40.6386587-74.034633808 W END AVE NY NY\n10025Angie Tseng40.8159612-73.96031841-03 69 ST NY NY 11377Marcella Weinstock\n40.797233-73.971324550 PARK AVE NY NY 10016Filiberto Everett40.7444514\n-73.8956728739 BROOK AVE NY NY 10451Vernia Mcgregor40.7492656-73.9803386777 W\nEND AVE NY NY 10025Michelina Althoff40.8199675-73.9122757866 E 165 ST NY NY\n10459Dave Tauber40.7965956-73.9726135130 E 37 ST NY NY 10016Tandra Gowen\n40.8237011-73.8990202797 ST ANNS AVE NY NY 10451Toby Philbrick40.7482336\n-73.97856641 AARON LN NY NY 10309Aisha Grief40.82089-73.9109118641 LEXINGTON AVE\nNY NY 10022Tarah Sinkler40.5541368-74.21266534201 4 AVE NY NY 11232Coletta\nJeansonne40.7590297-73.97032191021 PARK AVE NY NY 10028Lorie Shriver40.650317\n-74.0081672127 RIVERSIDE DR NY NY 10024Antwan Fullilove40.7794132-73.95724755120\nBROADWAY NY NY 10034Normand Beerman40.7890613-73.98065697124 20 AVE NY NY 11204\nWes Nieman40.8714856-73.91303623506 BEDFORD AVE NY NY 11210Marlen Hutcherson\n40.6127972-73.9901551550 GRAND ST NY NY 10002Leonie Lablanc40.6168306-73.9501481\n1711 GROVE ST NY NY 11385Doris Herrman40.7143151-73.9800558785 W END AVE NY NY\n10025Cyndy Kossman40.7032053-73.91119426040 HUXLEY AVE NY NY 10471Donya Ponte\n40.796763-73.972483Head Over to Mapbox Studio\nWhile we can technically do everything programmatically, Mapbox's GUI is simply\ntoo easy to ignore. Using Mapbox Studio\n[https://www.mapbox.com/studio/datasets/], we can upload our data and turn it\ninto a tileset; the heart and soul of what makes our maps interesting. \n\nOnce you've uploaded your CSV (or JSON, or whatever) as a dataset, we can\nimmediately see what this information looks like on a map by previewing it as a\ntileset. Mapbox is surprisingly intelligent in that it can deduce lat/long\nvalues from poorly named or formatted columns (such as Lat/Long, \nLatitutde/Longitude, start_longitude_lol/start_latitude_lmao, etc). Mapbox gets\nit right most of the time.\n\nIf y'all went well you should see a cluster of points on a map - this is a\npreview of your Tileset. Think of this as a layer in Photoshop: we can stack\nthese layers of information atop one another continuously to make something\ngreater than the sum of its parts.\n\nIf all looks good, export your Tileset via the \"export\" button on the top right.\n\nUpload your dataset and click \"edit\"Switch Over to Your Map \"Style\"\nYou map 'style' is your blank canvas. Get in there and add a layer, and from\nthere select the Tileset you just created. Once your Tileset is loaded, you can\nstyle the points themselves and even label them with the data in your dataset as\nyou see fit:\n\nSo many colorful layers.Simply clicking around the preloaded Tilesets should\nstart giving you ideas of what's possible down the line. Just look at those\nhorrifically bright Miami Vice themed streets.\n\nFeel free to get creative with Mapbox's tools to clarify the visual story you're\ntrying to tell. I've distinguished points from others after adding a third data\nset: Every Starbucks in New York City.  Yes, those map pins have been replaced\nwith that terrifying Starbucks Logo Mermaid Sea-demon\n\nTake a look at that perfect grid of mocha frappa-whatevers and tell me these\nguys don't have a business strategy:\n\nGod that's an ugly map.For all it's worth, I'd like to sincerely apologize for\nblinding your eyes with classless use of gifs paired with the useless corporate\nmonstrosity of a map I've created. I have faith that you'll do better.\n\nNow that we've spent enough time covering the n00b stuff, it's time to take the\ngloves off. While Mapbox studio's GUI serves as an amazing crutch and way to\ncustomize the look of our data, we must not forget: we're programmers, God damn\nit! True magic lies in 1s and 0s, not WYSIWYG editors.\n\nUntil we start using Plot.ly Dash, that is.\n\n(Suddenly, thousands of fans erupt into a roaring cheer at the very mention of\nPlot.ly. It's about time.™)","html":"<p>This wouldn't be a proper data blog unless we spend a vast majority of our time talking about cleaning data. Chances are if you're pursuing analysis that's groundbreaking (or worthwhile), we're probably starting with some ugly, untapped information. It turns out Mapbox has an API specifically for this purpose: the <a href=\"https://www.mapbox.com/api-documentation/#geocoding\">Mapbox Geocoding API</a>.</p><p><strong>Geocoding</strong> is a blanket term for turning vague information into specific Lat/Long coordinates. How vague, you ask? The API covers:</p><ul><li>Pinpointing exact location via street address.</li><li>Locating regions or cities by recognizable name (ie: Rio de Janeiro).</li><li>Locating cities by highly unspecific name (Geocoding for \"Springfield\" will return results for 41 American cities)</li><li>Locating cities or venues by name within a given region (such as searching for Ray's Pizza in NYC).</li></ul><p>We can also use Geocoding to do the reverse of this, where passing in coordinates will return location names. If you find this useful, I'm assuming you're a spy.</p><h3 id=\"chipping-away-at-a-real-use-case\">Chipping Away at a Real Use Case</h3><p>In a real-life example, I have two sets of data: one represents general places of residence for a particular sample group. The goal is to see how they interact with the second dataset: a list of locations they will be traveling to. I'd love to go into more detail, but:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/tenor.gif\" class=\"kg-image\"><figcaption>I get one cliche meme per year.</figcaption></figure><!--kg-card-end: image--><p>So how can we use the Mapbox Geocoding API to systematically extract coordinates for thousands of addresses, from multiple datasets? With Pandas, of course!</p><h2 id=\"i-m-just-happy-to-be-writing-about-pandas-right-now\">I'm Just Happy to be Writing About Pandas Right Now</h2><p>Pardon my excitement; I've been far overdue for posting anything Pandas-related. It's been killing me on the inside.</p><p>We need to make sense of some vague data. As seen in <a href=\"https://hackersandslackers.com/map-data-visualization-with-mapbox/\">our Citibike example</a>, New York has plenty of public datasets with information like Taxi pickup/dropoffs, public transit, etc. These start and end points are typically too fluid to have Lat/Long coordinates associated with them, so we'll add them in ourselves. Given that we're about to pass hundreds or thousands of addresses and locations, we'll use Pandas <code>.apply()</code> to fill out the missing Lat/Long columns in our dataset. </p><p>Instead of using Mapbox's Python SDK, I'll actually be using <code>requests</code> to hit the Mapbox REST API. For some reason, the Python SDK was a bit unpredictable on my last run.<strong>*</strong></p><p><strong>*UPDATE: </strong>the Python SDK \"wasn't working\" because I apparently don't know the difference between longitude and latitude. Awesome, so I'm a moron.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport os\nimport pandas as pd\nimport requests\nimport json\n\n\nclass GeocodeAddresses:\n    &quot;&quot;&quot;Add missing lat/long information to exisiting dataset.&quot;&quot;&quot;\n\n    def __init__(self, address_data):\n        self.data = address_data\n        self.address_df = pd.read_csv(self.data)\n        self.complete_data = self.get_coords(self.address_df)\n\n\n    @classmethod\n    def get_coords(self, employee_address_df):\n        &quot;&quot;&quot;Fill Dataframe lat/long columns.&quot;&quot;&quot;\n\n        def fill_coords(row):\n            &quot;&quot;&quot;Create a route object by passing GeoJSON start/end objects.&quot;&quot;&quot;\n            base_url = 'https://api.mapbox.com/geocoding/v5/mapbox.places/'\n            address = str(row.home_address)\n            format = '.json'\n            endpoint = base_url + address + format\n            params = {\n                'access_token':  'pk.eyJ1IjNoYXJkd2VthisisreallolaXdyNHQ3OTUifQ.VTAUrmzD91Ppxr1AJww'\n            }\n            headers = {\n                'Content-Type': 'application/json'\n            }\n            r = requests.get(endpoint, params=params, headers=headers)\n            try:\n                Lat = r.json()['features'][0]['geometry']['coordinates'][0]\n                Long = r.json()['features'][0]['geometry']['coordinates'][1]\n                print(pd.Series([Lat, Long]))\n                return pd.Series([Lat, Long])\n            except IndexError:\n                pass\n\n        address_df[['Lat', 'Long']] = address_df.head(100).apply(fill_coords, axis=1)\n        address_df.to_csv('geocoded.csv')\n</code></pre>\n<!--kg-card-end: markdown--><p>In the above example, we're using <code>.apply()</code> against an empty series (our Lat/Long columns) as opposed to our entire Dataframe. When <code>get_coords()</code> returns two values, these values will fill the empty columns on a per-row basis.</p><p>For the scope of this tutorial, we'll simply focus on getting these points plotted. Don't worry, this is only part 2 of our Mapbox series! Yes, an entire series!</p><h2 id=\"turning-your-datasets-into-tilesets\">Turning Your Datasets into Tilesets</h2><p>In Mapbox terms, a <strong>Tileset</strong> is essentially a layer of data we can overlay on top of our blank map. The map <strong>style</strong> we created last time was really just the aesthetic unpinning of all the interesting data we can pile on time.</p><p>Tilesets can be stacked on one another, thus giving us infinite possibilities of the types of data we can communicate: especially when you consider that Mapbox supports Heatmaps and topology - far more than just plotted points.</p><p>First, we'll plot our origins. I've put together a dataset of completely falsified names (with presumably real addresses?) to demonstrate how we'd plot these points. Here's a sample of the garbage I'll be feeding into Mapbox:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n\t\t\t<table>\n\t\t\t\t<thead>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<th>address</th>\n\t\t\t\t\t\t<th>name</th>\n\t\t\t\t\t\t<th>long</th>\n\t\t\t\t\t\t<th>lat</th>\n\t\t\t\t\t</tr>\n\t\t\t\t</thead>\n\t\t\t\t<tbody>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>761 ST ANNS AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Royal Hiett</td>\n\t\t\t\t\t\t<td>40.754466</td>\n\t\t\t\t\t\t<td>-73.9794552</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>5 COLUMBUS CIR NY NY 10019</td>\n\t\t\t\t\t\t<td>Yolanda Antonio</td>\n\t\t\t\t\t\t<td>40.8201997</td>\n\t\t\t\t\t\t<td>-73.9110324</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1145 LENOX RD NY NY 11212</td>\n\t\t\t\t\t\t<td>Marguerita Autry</td>\n\t\t\t\t\t\t<td>40.7667595</td>\n\t\t\t\t\t\t<td>-73.9815704</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>2800 VICTORY BLVD NY NY 10314</td>\n\t\t\t\t\t\t<td>Alyse Peranio</td>\n\t\t\t\t\t\t<td>40.6597804</td>\n\t\t\t\t\t\t<td>-73.9183181</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>750 LEXINGTON AVE NY NY 10022</td>\n\t\t\t\t\t\t<td>Sina Walberg</td>\n\t\t\t\t\t\t<td>40.6080557</td>\n\t\t\t\t\t\t<td>-74.1532418</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>29 BAY RIDGE AVE NY NY 11220</td>\n\t\t\t\t\t\t<td>Ignacia Frasher</td>\n\t\t\t\t\t\t<td>40.7625148</td>\n\t\t\t\t\t\t<td>-73.9685564</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>550 RIVERSIDE DR NY NY 10027</td>\n\t\t\t\t\t\t<td>Marta Haymond</td>\n\t\t\t\t\t\t<td>40.6386587</td>\n\t\t\t\t\t\t<td>-74.034633</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>808 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Angie Tseng</td>\n\t\t\t\t\t\t<td>40.8159612</td>\n\t\t\t\t\t\t<td>-73.960318</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>41-03 69 ST NY NY 11377</td>\n\t\t\t\t\t\t<td>Marcella Weinstock</td>\n\t\t\t\t\t\t<td>40.797233</td>\n\t\t\t\t\t\t<td>-73.9713245</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>50 PARK AVE NY NY 10016</td>\n\t\t\t\t\t\t<td>Filiberto Everett</td>\n\t\t\t\t\t\t<td>40.7444514</td>\n\t\t\t\t\t\t<td>-73.8956728</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>739 BROOK AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Vernia Mcgregor</td>\n\t\t\t\t\t\t<td>40.7492656</td>\n\t\t\t\t\t\t<td>-73.9803386</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>777 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Michelina Althoff</td>\n\t\t\t\t\t\t<td>40.8199675</td>\n\t\t\t\t\t\t<td>-73.9122757</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>866 E 165 ST NY NY 10459</td>\n\t\t\t\t\t\t<td>Dave Tauber</td>\n\t\t\t\t\t\t<td>40.7965956</td>\n\t\t\t\t\t\t<td>-73.9726135</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>130 E 37 ST NY NY 10016</td>\n\t\t\t\t\t\t<td>Tandra Gowen</td>\n\t\t\t\t\t\t<td>40.8237011</td>\n\t\t\t\t\t\t<td>-73.8990202</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>797 ST ANNS AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Toby Philbrick</td>\n\t\t\t\t\t\t<td>40.7482336</td>\n\t\t\t\t\t\t<td>-73.978566</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>41 AARON LN NY NY 10309</td>\n\t\t\t\t\t\t<td>Aisha Grief</td>\n\t\t\t\t\t\t<td>40.82089</td>\n\t\t\t\t\t\t<td>-73.9109118</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>641 LEXINGTON AVE NY NY 10022</td>\n\t\t\t\t\t\t<td>Tarah Sinkler</td>\n\t\t\t\t\t\t<td>40.5541368</td>\n\t\t\t\t\t\t<td>-74.2126653</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>4201 4 AVE NY NY 11232</td>\n\t\t\t\t\t\t<td>Coletta Jeansonne</td>\n\t\t\t\t\t\t<td>40.7590297</td>\n\t\t\t\t\t\t<td>-73.9703219</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1021 PARK AVE NY NY 10028</td>\n\t\t\t\t\t\t<td>Lorie Shriver</td>\n\t\t\t\t\t\t<td>40.650317</td>\n\t\t\t\t\t\t<td>-74.0081672</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>127 RIVERSIDE DR NY NY 10024</td>\n\t\t\t\t\t\t<td>Antwan Fullilove</td>\n\t\t\t\t\t\t<td>40.7794132</td>\n\t\t\t\t\t\t<td>-73.9572475</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>5120 BROADWAY NY NY 10034</td>\n\t\t\t\t\t\t<td>Normand Beerman</td>\n\t\t\t\t\t\t<td>40.7890613</td>\n\t\t\t\t\t\t<td>-73.9806569</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>7124 20 AVE NY NY 11204</td>\n\t\t\t\t\t\t<td>Wes Nieman</td>\n\t\t\t\t\t\t<td>40.8714856</td>\n\t\t\t\t\t\t<td>-73.9130362</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>3506 BEDFORD AVE NY NY 11210</td>\n\t\t\t\t\t\t<td>Marlen Hutcherson</td>\n\t\t\t\t\t\t<td>40.6127972</td>\n\t\t\t\t\t\t<td>-73.9901551</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>550 GRAND ST NY NY 10002</td>\n\t\t\t\t\t\t<td>Leonie Lablanc</td>\n\t\t\t\t\t\t<td>40.6168306</td>\n\t\t\t\t\t\t<td>-73.9501481</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1711 GROVE ST NY NY 11385</td>\n\t\t\t\t\t\t<td>Doris Herrman</td>\n\t\t\t\t\t\t<td>40.7143151</td>\n\t\t\t\t\t\t<td>-73.9800558</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>785 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Cyndy Kossman</td>\n\t\t\t\t\t\t<td>40.7032053</td>\n\t\t\t\t\t\t<td>-73.9111942</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>6040 HUXLEY AVE NY NY 10471</td>\n\t\t\t\t\t\t<td>Donya Ponte</td>\n\t\t\t\t\t\t<td>40.796763</td>\n\t\t\t\t\t\t<td>-73.972483</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</tbody>\n\t\t\t</table>\n</div><!--kg-card-end: html--><h3 id=\"head-over-to-mapbox-studio\">Head Over to Mapbox Studio</h3><p>While we can technically do everything programmatically, Mapbox's GUI is simply too easy to ignore. Using <a href=\"https://www.mapbox.com/studio/datasets/\">Mapbox Studio</a>, we can upload our data and turn it into a <em><strong>tileset; </strong></em>the heart and soul of what makes our maps interesting. </p><p>Once you've uploaded your CSV (or JSON, or whatever) as a dataset, we can immediately see what this information looks like on a map by previewing it as a tileset. Mapbox is surprisingly intelligent in that it can deduce lat/long values from poorly named or formatted columns (such as <em>Lat/Long</em>, <em>Latitutde/Longitude</em>, <em>start_longitude_lol/start_latitude_lmao</em>, etc). Mapbox gets it right most of the time.</p><p>If y'all went well you should see a cluster of points on a map - this is a preview of your Tileset. Think of this as a layer in Photoshop: we can stack these layers of information atop one another continuously to make something greater than the sum of its parts.</p><p>If all looks good, export your Tileset via the \"export\" button on the top right.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/tileset.gif\" class=\"kg-image\"><figcaption>Upload your dataset and click \"edit\"</figcaption></figure><!--kg-card-end: image--><h3 id=\"switch-over-to-your-map-style\">Switch Over to Your Map \"Style\"</h3><p>You map 'style' is your blank canvas. Get in there and add a layer, and from there select the Tileset you just created. Once your Tileset is loaded, you can style the points themselves and even label them with the data in your dataset as you see fit:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/layers.gif\" class=\"kg-image\"><figcaption>So many colorful layers.</figcaption></figure><!--kg-card-end: image--><p>Simply clicking around the preloaded Tilesets should start giving you ideas of what's possible down the line. Just look at those horrifically bright Miami Vice themed streets.</p><p>Feel free to get creative with Mapbox's tools to clarify the visual story you're trying to tell. I've distinguished points from others after adding a third data set: <strong>Every Starbucks in New York City.</strong> Yes, those map pins have been replaced with that terrifying Starbucks Logo Mermaid Sea-demon</p><p>Take a look at that perfect grid of mocha frappa-whatevers and tell me these guys don't have a business strategy:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mapstarbucks2.gif\" class=\"kg-image\"><figcaption>God that's an ugly map.</figcaption></figure><!--kg-card-end: image--><p>For all it's worth, I'd like to sincerely apologize for blinding your eyes with classless use of gifs paired with the useless corporate monstrosity of a map I've created. I have faith that you'll do better.</p><p>Now that we've spent enough time covering the n00b stuff, it's time to take the gloves off. While Mapbox studio's GUI serves as an amazing crutch and way to customize the look of our data, we must not forget: we're programmers, God damn it! True magic lies in 1s and 0s, not WYSIWYG editors.</p><p>Until we start using <strong>Plot.ly Dash</strong>, that is.</p><!--kg-card-begin: html--><span class=\"subtext\">(Suddenly, thousands of fans erupt into a roaring cheer at the very mention of Plot.ly. It's about time.™)</span><!--kg-card-end: html-->","url":"https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/","uuid":"b2e24775-df44-464e-839a-be24e2a3eb42","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c0ee5bd8687896e154a9376"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ed","title":"Dynamic Tension! Creating and Using Dynamic Named Ranges in Excel","slug":"dynamic-named-ranges-in-excel","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/maxidk@2x.jpg","excerpt":"Dynamically load data in smart pivot tables.","custom_excerpt":"Dynamically load data in smart pivot tables.","created_at_pretty":"30 August, 2018","published_at_pretty":"01 September, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-08-30T17:00:23.000-04:00","published_at":"2018-08-31T21:13:00.000-04:00","updated_at":"2019-02-02T03:50:02.000-05:00","meta_title":"Dynamic Named Ranges in Excel | Hackers And Slackers","meta_description":"Use a named dynamic range and a form control to dynamically auto-populate and refresh Excel pivot tables.","og_description":"Use a named dynamic range and a form control to dynamically auto-populate and refresh Excel pivot tables.","og_image":"https://hackersandslackers.com/content/images/2018/08/maxidk@2x.jpg","og_title":"Dynamic Tension! Creating and Using Dynamic Named Ranges in Excel","twitter_description":"Use a named dynamic range and a form control to dynamically auto-populate and refresh Excel pivot tables.","twitter_image":"https://hackersandslackers.com/content/images/2018/08/maxidk@2x.jpg","twitter_title":"Dynamic Tension! Creating and Using Dynamic Named Ranges in Excel","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"First things first, 10 points to anyone who understands what the title is\nreferring to (that's either Charles Atlas' workout philosophy, or one of core\ntenants of Bokononism in Kurt Vonnegut's standout Cat's Cradle). \n\nNow, with my obscure reference quota filled for the day, let's assume that\nyou've been working as an analyst for some time now. As stated in a previous\npost, generally you're likely to get promoted into a management position, where\nyou no longer need your formidable skills of analysis, and they too, like all\ngood things, will atrophy to nothingness. However, for those of you who have\ndemonstrated such overwhelming nuance of technique that your company fears the\nloss of your truly awe-inspiring abilities more than they love adhering to the\nPeter Principle, there may indeed be another road.\n\nThis unlikely confluence of events generally begins after the fifth or sixth\ntime that you've been asked to write a step-by-step tutorial showing how you\nbuilt a specific report, only to find out that if written out in a truly \nstep-wise fashion would take up 16 pages, and thus would never EVER be looked\nat. This phenomenon serves a dual purpose: \n\n 1. It misleads your company into believing that what you do is complex.\n 2. It is taken as evidence that you are a literal wizard, and you can't train a\n    muggle to do magic. \n\nOnce your company has taken the bait, they may realize that you are a true elite\noperator, and that the loss of your abilities would result in a substantial\ndrain on your business' intelligence. Enter the Lead Analyst (also called a head\nanalyst, chief analyst, or executive analyst if where you work is incredibly far\nup it's own ass). This job title exists solely to reward those of you who have\nbuilt up enough number crunching credit to make your employer realize that if\nthey promote you to a manager, they lose, and if they don't promote you at all,\nyou can leave at any time. \n\nSo, hotshot, what does this job involve? Simply put, your company no longer\nquestions you about...basically anything (and if they ever do, say you're\ncompiling, always works) and you might even be entrusted to speak directly to\nthe notorious Dataratti that we spoke of several posts ago (they're all actually\nreally cool guys, and appreciate being able to talk to someone who understands\nwhat their job and capabilities actually are). In exchange for these two awesome\nperks, all you have to be able to do is build reports that can be used by anyone\n...\n\nNow all of you who work in the field and have to engage in this practice just\nlet out an enormous sigh, and the reason for this is because now you have to\nwrite reports that:\n\n1. Can't use macros because the end user doesn't know what a macro is.\n2. Need to have built in constraints because \"it just needs to be there to work\"\nis literally begging someone to mess up a perfectly good formula.\n3. Can't rely on the manual typing of actual formulae because explaining to\nsomeone that the \"dollar sign isn't a typo\" to someone never quite sticks.\n4. Can't utilize your own macros because the end user isn't going to have access\nto them, and finally... \n5. You can no longer say things like \"pivot out\", \"lookup\", or \"index\" and\ninstead need to break each of those things down into a series of button clicks\n(hence why the documentation becomes 16 pages).\n\nWith that being said, let's jump into one of the easiest ways that you can build\nreports that allow even the unenlightened the ability to touch the sun: using a\nnamed dynamic range and a form control to auto-populate and refresh a pivot\ntable.\n\nNow, if any of those words are unfamiliar to you, you need to go back to the\nbeginning of the series (with special attention to the post regarding naming\nranges and active-X controls) so that you know how to do the following:\n\n 1. Build a pivot table\n 2. Name a range\n\nIf you've got the tricks in the bag, let's get going. In order to get this trick\nto work the first step is building a named dynamic range. This is a glorious\ntrick that takes the convenience of naming a range (being able to grab that\ninformation from anywhere in a workbook) along with the UNLIMITED POWER of\nassociating anything that someone may plug into the range with the name. \n\nFirst, let's start with a sample data-set of super heroes and their secret\nidentities, like so (you'll need developer active for this to work): \n\nMiles Morales and Amadeus Cho notwithstandingSo, now's where the magic happens:\nclick formulas-->Name Manager\n\nNote the dialogue box.In the resulting dialog box, click \"new\":\n\nSeen here: how these become 16 page tutorialsIn the resulting dialog box, name\nthe range whatever you want (just remember...no spaces), but the magic really\nhappens at the bottom where it says \"refers to\", as shown here: \n\nPictured here: what the coolest thing looks like before it happensNow, in that\nbox, you'll notice that you can put in a formula, and this one is KEY to\nremember both for this, and anytime you want to make a dynamic range in the\nfuture, and it is: \n=OFFSET(Sheet1!$A$1,0,0,COUNTA(Sheet1!$A:$A),COUNTA(Sheet1!$1:$1))\n* substitute the name of your worksheet for sheet1. \nEssentially, OFFSET  is how you set your imaginary cursor (remember, you CAN\ntechnically do everything you see in Excel from a command line, and this is how\nit knows where you are). It requires 3-5 variables: \n\n * The first (the reference) is what you want your \"mouse\" to select before it\n   finds it's way elsewhere.\n * The second and third is the number of rows and columns you want to move in\n   order to select your next item. You can end here, but you'd just wind up\n   moving your \"cursor\" to another cell. Instead, we set this to zero for both\n   because you don't want to move anywhere. \n * The 4th is the hight (in cells) of what you're selecting (that way, you can\n   select any number of cells vertically from where your cursor wound up) and we\n   use \"COUNTA\" here to mean the number of cells that aren't blank in column A.\n * The 5th and final variable is the width which serves the same function as\n   height in the other direction. By using \"COUNTA\" here, we're selecting the\n   number of columns that aren't blank in row 1.\n\nIf you did everything right, you should end up like this: \n\nNow you can officially claim to be \"Dynamic\" on your resumeIf you're thinking\nwith portals, you may have already pieced together what we've done here: by\nnaming the range, we can substitute any variable that'll support an array with\nthe name of the range, AND by using that offset formula, we've just made it so\nthat no matter what you throw into this first sheet, it gets added to the range.\n \n\nThis by itself is very cool, and very useful, but let's take it a step further\nand I'll show you how you can use this dynamic named range to make an \"idiot\nproof\" updater for a pivot table. \n\nThe first step is to of course make a pivot table, however, we're going to do\nsomething daring: a pivot table requires an array...and we've just created an\narray that's DYNAMIC. So, click insert, pivot table, and then when it asks you\nwhat you what you want in the table, tell it that you want to put powers in\nthere (that's the name of my range) like so:\n\nNo one man should have all this power...and boom:\n\nNot for VillainsBUT IT DOESN'T END THERE FOLKS. As your range is dynamic, you\ncan add whatever rows or columns to the first page as you wish, and then if you\nrefresh your pivot table (right click, refresh) these changes will be\nreflected...watch\n\nSpecifically for VillainsIT'S DYNAMIC!So, now you've learned how to:\n\n * Create a named dynamic range\n * Use the offset function to set a virtual cursor (this becomes exceedingly\n   important if ever you decide you want to code for real).\n * Refresh a pivot table with information updated in the dynamic range. \n\nNext time, we'll take it up a notch, and remove any sort of thought needed to\nupdate these tables! \n\nIt's Munchin' Time, \n\n-Snacks","html":"<p>First things first, 10 points to anyone who understands what the title is referring to (that's either Charles Atlas' workout philosophy, or one of core tenants of Bokononism in Kurt Vonnegut's standout <em>Cat's Cradle). </em></p><p>Now, with my obscure reference quota filled for the day, let's assume that you've been working as an analyst for some time now. As stated in a previous post, generally you're likely to get promoted into a management position, where you no longer need your formidable skills of analysis, and they too, like all good things, will atrophy to nothingness. However, for those of you who have demonstrated such overwhelming nuance of technique that your company fears the loss of your truly awe-inspiring abilities more than they love adhering to the Peter Principle, there may indeed be another road.</p><p>This unlikely confluence of events generally begins after the fifth or sixth time that you've been asked to write a step-by-step tutorial showing how you built a specific report, only to find out that if written out in a <em>truly </em>step-wise fashion would take up 16 pages, and thus would never EVER be looked at. This phenomenon serves a dual purpose: </p><ol><li>It misleads your company into believing that what you do is <em>complex.</em></li><li>It is taken as evidence that you are a literal wizard, and you can't train a muggle to do magic. <em> </em></li></ol><p>Once your company has taken the bait, they may realize that you are a true elite operator, and that the loss of your abilities would result in a substantial drain on your business' intelligence. Enter the Lead Analyst (also called a head analyst, chief analyst, or executive analyst if where you work is incredibly far up it's own ass). This job title exists solely to reward those of you who have built up enough number crunching credit to make your employer realize that if they promote you to a manager, they lose, and if they don't promote you at all, you can leave at any time. </p><p>So, hotshot, what does this job involve? Simply put, your company no longer questions you about...basically anything (and if they ever do, say you're compiling, always works) and you might even be entrusted to speak directly to the notorious Dataratti that we spoke of several posts ago (they're all actually really cool guys, and appreciate being able to talk to someone who understands what their job and capabilities actually are). In exchange for these two awesome perks, all you have to be able to do is build reports that can be used by <em>anyone</em>...</p><p>Now all of you who work in the field and have to engage in this practice just let out an enormous sigh, and the reason for this is because now you have to write reports that:<br><br>1. Can't use macros because the end user doesn't know what a macro is.<br>2. Need to have built in constraints because \"it just needs to be there to work\" is literally begging someone to mess up a perfectly good formula.<br>3. Can't rely on the manual typing of actual formulae because explaining to someone that the \"dollar sign isn't a typo\" to someone never quite sticks.<br>4. Can't utilize your own macros because the end user isn't going to have access to them, and finally... <br>5. You can no longer say things like \"pivot out\", \"lookup\", or \"index\" and instead need to break each of those things down into a series of button clicks (hence why the documentation becomes 16 pages).</p><p>With that being said, let's jump into one of the easiest ways that you can build reports that allow even the unenlightened the ability to touch the sun: using a named dynamic range and a form control to auto-populate and refresh a pivot table.</p><p>Now, if any of those words are unfamiliar to you, you need to go back to the beginning of the series (with special attention to the post regarding naming ranges and active-X controls) so that you know how to do the following:</p><ol><li>Build a pivot table</li><li>Name a range</li></ol><p>If you've got the tricks in the bag, let's get going. In order to get this trick to work the first step is building a <strong>named dynamic range. </strong>This is a glorious trick that takes the convenience of naming a range (being able to grab that information from anywhere in a workbook) along with the <em>UNLIMITED POWER </em>of associating anything that someone may plug into the range with the name. </p><p>First, let's start with a sample data-set of super heroes and their secret identities, like so (you'll need developer active for this to work): </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-7.png\" class=\"kg-image\"><figcaption>Miles Morales and Amadeus Cho notwithstanding</figcaption></figure><p>So, now's where the magic happens: click formulas--&gt;Name Manager</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-8.png\" class=\"kg-image\"><figcaption>Note the dialogue box.</figcaption></figure><p>In the resulting dialog box, click \"new\":</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-9.png\" class=\"kg-image\"><figcaption>Seen here: how these become 16 page tutorials</figcaption></figure><p>In the resulting dialog box, name the range whatever you want (just remember...no spaces), but the magic really happens at the bottom where it says \"refers to\", as shown here: </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-10.png\" class=\"kg-image\"><figcaption>Pictured here: what the coolest thing looks like before it happens</figcaption></figure><p>Now, in that box, you'll notice that you can put in a formula, and this one is KEY to remember both for this, and anytime you want to make a dynamic range in the future, and it is: <code>=OFFSET(Sheet1!$A$1,0,0,COUNTA(Sheet1!$A:$A),COUNTA(Sheet1!$1:$1))</code><br>* substitute the name of your worksheet for sheet1. <br>Essentially, <strong>OFFSET</strong> is how you set your imaginary cursor (remember, you CAN technically do everything you <em>see </em>in Excel from a command line, and this is how it knows where you are). It requires 3-5 variables: </p><ul><li>The first (the reference) is what you want your \"mouse\" to select before it finds it's way elsewhere.</li><li>The second and third is the number of rows and columns you want to move in order to select your next item. You can end here, but you'd just wind up moving your \"cursor\" to another cell. Instead, we set this to zero for both because you don't want to <em>move </em>anywhere. </li><li>The 4th is the hight (in cells) of what you're selecting (that way, you can select any number of cells vertically from where your cursor wound up) and we use \"COUNTA\" here to mean the number of cells that aren't blank in column A.</li><li>The 5th and final variable is the width which serves the same function as height in the other direction. By using \"COUNTA\" here, we're selecting the number of columns that aren't blank in row 1.</li></ul><p>If you did everything right, you should end up like this: </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-11.png\" class=\"kg-image\"><figcaption>Now you can officially claim to be \"Dynamic\" on your resume</figcaption></figure><p>If you're thinking with portals, you may have already pieced together what we've done here: by naming the range, we can substitute any variable that'll support an array with the name of the range, AND by using that offset formula, we've just made it so that no matter what you throw into this first sheet, it gets added to the range. </p><p>This by itself is very cool, and very useful, but let's take it a step further and I'll show you how you can use this dynamic named range to make an \"idiot proof\" updater for a pivot table. </p><p>The first step is to of course make a pivot table, however, we're going to do something daring: a pivot table requires an array...and we've just created an array that's DYNAMIC. So, click insert, pivot table, and then when it asks you what you what you want in the table, tell it that you want to put powers in there (that's the name of my range) like so:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-12.png\" class=\"kg-image\"><figcaption>No one man should have all this power...</figcaption></figure><p>and boom:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-13.png\" class=\"kg-image\"><figcaption>Not for Villains</figcaption></figure><p>BUT IT DOESN'T END THERE FOLKS. As your range is dynamic, you can add whatever rows or columns to the first page as you wish, and then if you refresh your pivot table (right click, refresh) these changes will be reflected...watch</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-14.png\" class=\"kg-image\"><figcaption>Specifically for Villains</figcaption></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-15.png\" class=\"kg-image\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/image-16.png\" class=\"kg-image\"><figcaption>IT'S DYNAMIC!</figcaption></figure><p>So, now you've learned how to:</p><ul><li>Create a named dynamic range</li><li>Use the offset function to set a virtual cursor (this becomes exceedingly important if ever you decide you want to code for real).</li><li>Refresh a pivot table with information updated in the dynamic range. </li></ul><p>Next time, we'll take it up a notch, and remove any sort of thought needed to update these tables! </p><p>It's Munchin' Time, </p><p>-Snacks</p>","url":"https://hackersandslackers.com/dynamic-named-ranges-in-excel/","uuid":"b7ef3b91-ecfd-42ab-8288-d2de7d4ceb32","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b885ae7075b34075786e311"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673670","title":"Getting Iffy With it: Conditional Statements in Excel","slug":"getting-iffy-with-it-conditionals-in-excel","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/iffy@2x.jpg","excerpt":"Effectively utilize conditionals such as IF statements in your Excel workflow.","custom_excerpt":"Effectively utilize conditionals such as IF statements in your Excel workflow.","created_at_pretty":"10 June, 2018","published_at_pretty":"10 June, 2018","updated_at_pretty":"21 January, 2019","created_at":"2018-06-10T12:12:20.000-04:00","published_at":"2018-06-10T14:19:42.000-04:00","updated_at":"2019-01-21T13:44:06.000-05:00","meta_title":"Getting Iffy With it: Conditional Statements in Excel | Hackers and Slackers","meta_description":"Effectively utilize conditionals such as IF statements in your Excel workflow.","og_description":"Effectively utilize conditionals such as IF statements in your Excel workflow","og_image":"https://hackersandslackers.com/content/images/2018/06/iffy@2x.jpg","og_title":"Getting Iffy With it: Conditional Statements in Excel","twitter_description":"Effectively utilize conditionals such as IF statements in your Excel workflow","twitter_image":"https://hackersandslackers.com/content/images/2018/06/iffy@2x.jpg","twitter_title":"Getting Iffy With it: Conditional Statements in Excel","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"If you've been following along, we discussed in the last several posts of this\nseries how, if you're not working in a very \"tech forward\" organization (like my\ntwo compatriots on his site), but you have the same title, you're probably\nobtaining your data from another department (or it might be a sentient sponge,\nor a gang of squirrels with dreams of world domination, you'll actually have no\nidea) who you will have no contact with. As a side effect of this...rather\nstrange situation (ya know, like being a child and being abandoned briefly by\nyour mother in a laboratory into the care of a stranger in order to track your\nresponse...what, no other Bowlby heads out there?) you'll almost certainly get\nthis data in a form that is completely unusable until you, dear reader, get your\nhands on it.\n\nAs such, in this post, we'll begin to dive into the myriad ways in which you\nmight go about doing that, but first, I need to put you in the right frame of\nmind, I am a therapist, after all. Remember back to one of my first posts: when\nyou get down to it, all computer code boils down to a series of binary choices\n(generally represented as 1s and 0s) indicating that a specific gate on the\nactual hardware is \"on\" or \"off\", and by layering millions of these on top of\neach other is how every single piece of computer software you have comes into\nbeing. Think about other binary decisions you've made in your life: Yes, or no,\nOdd or Even, Coke or Pepsi (despite what RC cola would like you to think), and\nperhaps the most bemoaned of all for everyone who has ever been a student: True\nor False.\n\nInterestingly, this true/false dichotomy is the very essence of how all of the\nchopping and screwing you'll be doing on your messy data (and in fact, the\nmajority of all computer programming) begins; by simply asking Excel if  what\nyou typed in is true...or false.\n\nNow that you've been successfully induced, we can jump into the the technical\nstuff. As you may have garnered from my post about formulae, Excel actually has,\nDEEP within it's bowels, a fairly robust programming language underpinning it:\nVisual Basic for Applications (VBA). Now, there are two general ways to tap into\nit:\n\n 1. The way that the majority of wizards do it: through the function bar (that\n    is, typing an \"=\" and letting it rip), and...\n 2. The way that the particularly adept wizards engage in it: through the\n    command line, which I'm not going to bother touching here, because if you're\n    writing entire Excel spreadsheets in VBA, why are you still using Excel?\n    Move onto Python or R and continue feeling smugly superior!\n\nSo, with that being said, how does one tap into this unlimited wellspring of\npotential using nothing but the features on the Excel mainpage? Well, I already\ntold you, simply ask Excel =If()!\nAll flowery language and allegory aside, the majority of computer programming\nlanguages (the one underneath Excel included) hinge on this \"If\" statement, the\nonly difference is how the particular language wants you to write these\nstatements.\n\nIn the Excel parlance, when one types in =If(  you'll be asked for three things\nseparated by commas (that Excel could really make a bit clearer), they are:\n\n 1. The logical argument (AKA, what do you want Excel to check as being true or\n    false?) A good example would be say \"Does the value in Cell A1 equal the\n    value of the Cell in B1?\" which would be written as =if(A1=B1 (Protip!, \n    unlike other programming languages, the way you write \"does not equal\" in\n    Excel is <>)\n 2. The value you want to grab if the value is true (leaving this blank will\n    just write \"true\") but in this case, let's make it say \"Yes\". This will be\n    written as =if(A1=B1,\"Yes\".(Protip!,  within Excel, if ever you want to\n    return something other than a number  you always need to surround it in\n    quotation marks.)\n 3. The value that you want to return if the value is false (leaving this blank\n    will just write \"false\") in this case, let's make it say \"No\". This will be\n    written as =if(A1=B1,\"Yes\",\"No\").\n\nAfter completing this formula with the three elements followed by a closing\nparenthesis, you'll get a cell that either says \"Yes\" or \"No\" depending on if A1\nand B1 contain the same values. The powerful thing here is what comes next: by\ndouble clicking the lower right hand corner of the cell with the formula, it'll\nautomatically populate all of the rows which are adjacent to information with Ax\n= Bx (where x equals the row number) allowing you to check each row to see if\nthe two columns match!\n\nTo add an extra wrinkle, let's say you want to check to see if each cell in a\ncolumn is equal to the values in a specific cell, you'd do that as follows:\n=if(A1 = $B$1,\"Yes\",\"No\"). If you then double click the cell as you did before,\nthat will check every value in column A against ONLY the value in B1. By adding\nthe dollar sign to the location of the cell, You've identified the \"Absolute\nReference\" as opposed to the \"Relative Reference\". Feel free to disregard these\nnames immediately, and start referring to it as \"dollar sign\". You can even\nmanipulate it by only using the Absolute reference on the row (A$1) OR the\ncolumn ($A1) if you're populating , vertically, horizontally, or both (yes,\ntechnically putting B$1 in the above example would have gotten you the same\nanswers).\n\nIn summation, in this post we've learned:\n\n 1. Excel is actually built on top of a \"real\" programming language called VBA.\n 2. You can write tiny programs in Excel through the function bar (or big\n    programs through the command line...showoff).\n 3. All \"programming\" really is, is the manipulation of true/false statements\n    underpinning the binary code even further beneath all computational tasks.\n 4. How to write your own \"If\" statement in Excel and how to \"phrase\" the\n    returning of numbers vs. anything else using quotation marks.\n 5. How to utilize the absolute reference (AKA the \"Dollar Sign\") to change what\n    gets included in the statements in each line of your spreadsheet.\n\nNext time, we'll get more grammatically complex, leveraging Ands and Ors in our\nIfs. We might even get crazy and throw some Ifs in our Ifs...so you can drive\nwhile you drive.\n\nEver Upward,\n\n-Snacks","html":"<p>If you've been following along, we discussed in the last several posts of this series how, if you're not working in a very \"tech forward\" organization (like my two compatriots on his site), but you have the same title, you're probably obtaining your data from another department (or it might be a sentient sponge, or a gang of squirrels with dreams of world domination, you'll actually have no idea) who you will have no contact with. As a side effect of this...rather strange situation (ya know, like being a child and being abandoned briefly by your mother in a laboratory into the care of a stranger in order to track your response...what, no other Bowlby heads out there?) you'll almost certainly get this data in a form that is completely unusable until <strong>you</strong>, dear reader, get your hands on it.</p><p>As such, in this post, we'll begin to dive into the myriad ways in which you might go about doing that, but first, I need to put you in the right frame of mind, I am a therapist, after all. Remember back to one of my first posts: when you get down to it, all computer code boils down to a series of binary choices (generally represented as 1s and 0s) indicating that a specific gate on the actual hardware is \"on\" or \"off\", and by layering millions of these on top of each other is how every single piece of computer software you have comes into being. Think about other binary decisions you've made in your life: Yes, or no, Odd or Even, Coke or Pepsi (despite what RC cola would like you to think), and perhaps the most bemoaned of all for everyone who has ever been a student: True or False.</p><p>Interestingly, this true/false dichotomy is the very essence of how all of the chopping and screwing you'll be doing on your messy data (and in fact, the majority of all computer programming) begins; by simply asking Excel <em><strong>if</strong></em> what you typed in is true...or false.</p><p>Now that you've been successfully induced, we can jump into the the technical stuff. As you may have garnered from my post about formulae, Excel actually has, DEEP within it's bowels, a fairly robust programming language underpinning it: Visual Basic for Applications (VBA). Now, there are two general ways to tap into it:</p><ol><li>The way that the majority of wizards do it: through the function bar (that is, typing an \"=\" and letting it rip), and...</li><li>The way that the particularly adept wizards engage in it: through the command line, which I'm not going to bother touching here, because if you're writing entire Excel spreadsheets in VBA, why are you still using Excel? Move onto Python or R and continue feeling smugly superior!</li></ol><p>So, with that being said, how does one tap into this unlimited wellspring of potential using nothing but the features on the Excel mainpage? Well, I already told you, simply ask Excel <strong>=If()</strong>!<br>All flowery language and allegory aside, the majority of computer programming languages (the one underneath Excel included) hinge on this \"If\" statement, the only difference is how the particular language wants you to write these statements.</p><p>In the Excel parlance, when one types in <strong>=If(</strong> you'll be asked for three things separated by commas (that Excel could really make a bit clearer), they are:</p><ol><li>The logical argument (AKA, what do you want Excel to check as being true or false?) A good example would be say \"Does the value in Cell A1 equal the value of the Cell in B1?\" which would be written as =if(A1=B1 (<strong>Protip!,</strong> unlike other programming languages, the way you write \"does not equal\" in Excel is <strong>&lt;&gt;</strong>)</li><li>The value you want to grab if the value is true (leaving this blank will just write \"true\") but in this case, let's make it say \"Yes\". This will be written as =if(A1=B1,\"Yes\".(<strong>Protip!,</strong> within Excel, if ever you want to return something <em>other than a number</em> you always need to surround it in quotation marks.)</li><li>The value that you want to return if the value is false (leaving this blank will just write \"false\") in this case, let's make it say \"No\". This will be written as =if(A1=B1,\"Yes\",\"No\").</li></ol><p>After completing this formula with the three elements followed by a closing parenthesis, you'll get a cell that either says \"Yes\" or \"No\" depending on if A1 and B1 contain the same values. The powerful thing here is what comes next: by double clicking the lower right hand corner of the cell with the formula, it'll automatically populate all of the rows which are adjacent to information with Ax = Bx (where x equals the row number) allowing you to check each row to see if the two columns match!</p><p>To add an extra wrinkle, let's say you want to check to see if each cell in a column is equal to the values in a specific cell, you'd do that as follows: =if(A1 = $B$1,\"Yes\",\"No\"). If you then double click the cell as you did before, that will check every value in column A against ONLY the value in B1. By adding the dollar sign to the location of the cell, You've identified the \"Absolute Reference\" as opposed to the \"Relative Reference\". Feel free to disregard these names immediately, and start referring to it as \"dollar sign\". You can even manipulate it by only using the Absolute reference on the row (A$1) OR the column ($A1) if you're populating , vertically, horizontally, or both (yes, technically putting B$1 in the above example would have gotten you the same answers).</p><p>In summation, in this post we've learned:</p><ol><li>Excel is actually built on top of a \"real\" programming language called VBA.</li><li>You can write tiny programs in Excel through the function bar (or big programs through the command line...showoff).</li><li>All \"programming\" really is, is the manipulation of true/false statements underpinning the binary code even further beneath all computational tasks.</li><li>How to write your own \"If\" statement in Excel and how to \"phrase\" the returning of numbers vs. anything else using quotation marks.</li><li>How to utilize the absolute reference (AKA the \"Dollar Sign\") to change what gets included in the statements in each line of your spreadsheet.</li></ol><p>Next time, we'll get more grammatically complex, leveraging Ands and Ors in our Ifs. We might even get crazy and throw some Ifs in our Ifs...so you can drive while you drive.</p><p>Ever Upward,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/getting-iffy-with-it-conditionals-in-excel/","uuid":"df65eee0-8935-4e9f-86d5-0c763c8baa08","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b1d4de445d23b542808c735"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867366a","title":"Taking Out the Trash: Dirty Data in Excel","slug":"dirty-data-in-excel-part-2","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/maxdataagain3@2x.jpg","excerpt":"Dealing With Dirty Data in Excel (continued).","custom_excerpt":"Dealing With Dirty Data in Excel (continued).","created_at_pretty":"05 June, 2018","published_at_pretty":"05 June, 2018","updated_at_pretty":"21 January, 2019","created_at":"2018-06-05T16:11:19.000-04:00","published_at":"2018-06-05T19:43:36.000-04:00","updated_at":"2019-01-21T13:40:46.000-05:00","meta_title":"Taking Out the Trash: Dirty Data in Excel | Hackers and Slackers","meta_description":"Dealing With Dirty Data in Excel (continued)","og_description":"Dealing With Dirty Data in Excel (continued)","og_image":"https://hackersandslackers.com/content/images/2018/06/maxdataagain3@2x.jpg","og_title":"Taking Out the Trash: Dirty Data in Excel","twitter_description":"Dealing With Dirty Data in Excel (continued)","twitter_image":"https://hackersandslackers.com/content/images/2018/06/maxdataagain3@2x.jpg","twitter_title":"Taking Out the Trash: Dirty Data in Excel","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"In my last post, we explored the organizational structure of many large\ncompanies and how this pertains to one's duties as a fledgling data analyst. I\nhighly recommend you go back and read the first post on \"dirty\" data, but just\nin case you're one of those rebels who thinks that they're too cool to read part\n1, here's a quick refresher to put you back in the analytical mindset (which is\nthe perfect combination tactical laziness, ADHD, and a complete disregard for\ndownside risk).\n\nEssentially, the reason why most data analysts exist within a company (remember,\nNOT a tech company, hence the usage of Excel) is because for whatever reason,\nmost companies do not give analysts access to the data on demand, and instead it\nis tightly controlled by a mysterious cabal of DBAs or developers which I have\nchristened the dataratti.\n\nWhile this setup is workable in theory, the dataratti often do not have a strong\nunderstanding of how the data is used in practice, and there are generally not\nopen lines of communication between the dataratti and the analysts. Thus it\nfalls upon the analyst to do three crucial jobs:\n\n 1. Obtain raw  data from the dataratti (remember, raw means that the data is\n    just a bunch of numbers, words, and headings...meaningless on their own).\n 2. manipulate  the data so that it's in a readable format and does not have\n    unnecessary data.\n 3. Present this data in narrative form to the business people.\n\nNow, you may be asking why they can't just hire someone who knows both sides of\nthis equation, the answer is sadly simple: manipulating data in a tool like\nExcel is antithetical to the general idea of how databases are created (we'll\ntalk about that some more next time) and thus someone who has both skill-sets,\nas well as the ability to present to stakeholders (remember, Those Who Sit Above\nin Shadow?) is extremely rare, and trying to become  that person may not be the\nbest idea because it may actually be cheaper to hire two employees than hire\nsomeone with chops in both skills (and you can't just know  how to\nmanipulate/present and code, you have to know both very well).\n\nWith all that being said, last time we discussed how one might grab data from\nthe dataratti from a tool like Tableau (which is essentially a data\nvisualization toolkit that's really good at creating dashboards, but can only\nhandle limited variables...a.k.a. columns, and exports the actual data in a\nparticularly messy way). Now we'll talk about one of the other possibilities for\nthe enterprising analyst to grab raw data: Business Intelligence (BI) tools.\n\nDespite how cool it sounds and despite it immediately conjuring thoughts of\nengaging in corporate espionage as part of the Business Intelligence Agency, BI\nis at it's core really just an interface where someone who didn't design a\ndatabase can still query (that literally means \"ask\") the database for the data\nthat they want.\n\nThe way this works is pretty straightforward: there will be an interface with a\nseries of folders (just like you'd see on a desktop) and within each, there will\nbe a list of variables that you can drag-and-drop into your query. After putting\nin all of the variables that you want in your report, you just hit \"go\" and\nit'll show you all of the data associated with the variables...essentially\nletting you grab data from the database without touching it, and without knowing\nSQL. To sweeten the already sweet deal, you can even generally manipulate the\ndata after the query is done but before downloading (using a very similar format\nto Excel, although, why fix what ain't broken?), and you can set it up to re-run\nwhatever query you decided upon on a regular schedule.\n\nWell then, doesn't that just sound ideal, why didn't I lead with this clearly\nsuperior strategy? Obviously, there's a catch: not only are BI toolkits VERY\nexpensive, the implementation of a BI tool requires an entire OTHER level of\nemployee between the coders and the analysts. Remember way back when, in a\nprevious post, I stated that databases only build on top of themselves and grow\nincreasingly unwieldy the longer your company is in existence? Also, remember\nwhen I stated that the developers rarely know why their users need the data\nwithin the database? Well, that leads to the developers coding their databases\nin a way that only they  truly understand (if we were talking about anything\nother than a database, I'd even say that their code is...inelegant). Now imagine\nhaving having to take this mess of code on top of code that's starting to look\nlike Tetsuo at the end of Akira, and translating all of it to plain English that\nsomeone with no knowledge of database structures would be able to understand,\nand you're starting to get an idea of the struggle that a BI developer has ahead\nof them. To make matters worse, each BI tool requires this to be done in a\nspecific way, so the skills are only moderately transferable. Of note, in a\ncompany that relies on their data quite a bit, and when their database is\nconsiderably large, this quickly becomes a full-time job, if not the job of a\nsmall team.\n\nNow, despite the considerable front-end investment in a good BI team, the\nflexibility of being able to grab data on-demand ends up being significantly\nbetter in the long-term, especially when an analyst knows how to use tableau\nthemselves, and can make super slick graphic representations of the data...and\neveryone knows that the way to a business person's heart is with sick graphics.\n\nIn summary, today we explored the following:\n\n * A refresher of the counter-intuitive model of data siloing within companies.\n * An explanation of the role of an analyst within this company hierarchy.\n * How BI tools work and what some of the benefits and challenges of\n   implementation may be.\n\nNow that you know why the data you get comes out dirty, next time, we'll explore\nsome examples of how your data may arrive, and how to sculpt your data so that\nyou can make it work for you.\n\nPaz,\n\n-Snacks","html":"<p>In my last post, we explored the organizational structure of many large companies and how this pertains to one's duties as a fledgling data analyst. I highly recommend you go back and read the first post on \"dirty\" data, but just in case you're one of those rebels who thinks that they're too cool to read part 1, here's a quick refresher to put you back in the analytical mindset (which is the perfect combination tactical laziness, ADHD, and a complete disregard for downside risk).</p><p>Essentially, the reason why most data analysts exist within a company (remember, NOT a tech company, hence the usage of Excel) is because for whatever reason, most companies do not give analysts access to the data on demand, and instead it is tightly controlled by a mysterious cabal of DBAs or developers which I have christened the <em>dataratti</em>.</p><p>While this setup is workable in theory, the dataratti often do not have a strong understanding of how the data is used in practice, and there are generally not open lines of communication between the dataratti and the analysts. Thus it falls upon the analyst to do three crucial jobs:</p><ol><li>Obtain <em>raw</em> data from the dataratti (remember, raw means that the data is just a bunch of numbers, words, and headings...meaningless on their own).</li><li><strong>manipulate</strong> the data so that it's in a readable format and does not have unnecessary data.</li><li>Present this data in narrative form to the <em>business people</em>.</li></ol><p>Now, you may be asking why they can't just hire someone who knows both sides of this equation, the answer is sadly simple: manipulating data in a tool like Excel is antithetical to the general idea of how databases are created (we'll talk about that some more next time) and thus someone who has both skill-sets, as well as the ability to present to stakeholders (remember, Those Who Sit Above in Shadow?) is extremely rare, and trying to <em>become</em> that person may not be the best idea because it may actually be cheaper to hire two employees than hire someone with chops in both skills (and you can't just <em>know</em> how to manipulate/present and code, you have to know both very well).</p><p>With all that being said, last time we discussed how one might grab data from the dataratti from a tool like Tableau (which is essentially a data visualization toolkit that's really good at creating dashboards, but can only handle limited variables...a.k.a. columns, and exports the actual data in a particularly messy way). Now we'll talk about one of the other possibilities for the enterprising analyst to grab raw data: Business Intelligence (BI) tools.</p><p>Despite how cool it sounds and despite it immediately conjuring thoughts of engaging in corporate espionage as part of the Business Intelligence Agency, BI is at it's core really just an interface where someone who didn't design a database can still query (that literally means \"ask\") the database for the data that they want.</p><p>The way this works is pretty straightforward: there will be an interface with a series of folders (just like you'd see on a desktop) and within each, there will be a list of variables that you can drag-and-drop into your query. After putting in all of the variables that you want in your report, you just hit \"go\" and it'll show you all of the data associated with the variables...essentially letting you grab data from the database without touching it, and without knowing SQL. To sweeten the already sweet deal, you can even generally manipulate the data after the query is done but before downloading (using a very similar format to Excel, although, why fix what ain't broken?), and you can set it up to re-run whatever query you decided upon on a regular schedule.</p><p>Well then, doesn't that just sound ideal, why didn't I lead with this clearly superior strategy? Obviously, there's a catch: not only are BI toolkits VERY expensive, the implementation of a BI tool requires an entire OTHER level of employee between the coders and the analysts. Remember way back when, in a previous post, I stated that databases only build on top of themselves and grow increasingly unwieldy the longer your company is in existence? Also, remember when I stated that the developers rarely know why their users need the data within the database? Well, that leads to the developers coding their databases in a way that only <em>they</em> truly understand (if we were talking about anything other than a database, I'd even say that their code is...inelegant). Now imagine having having to take this mess of code on top of code that's starting to look like Tetsuo at the end of Akira, and translating all of it to plain English that someone with no knowledge of database structures would be able to understand, and you're starting to get an idea of the struggle that a BI developer has ahead of them. To make matters worse, each BI tool requires this to be done in a specific way, so the skills are only moderately transferable. Of note, in a company that relies on their data quite a bit, and when their database is considerably large, this quickly becomes a full-time job, if not the job of a small team.</p><p>Now, despite the considerable front-end investment in a good BI team, the flexibility of being able to grab data on-demand ends up being significantly better in the long-term, especially when an analyst knows how to use tableau themselves, and can make super slick graphic representations of the data...and everyone knows that the way to a business person's heart is with sick graphics.</p><p>In summary, today we explored the following:</p><ul><li>A refresher of the counter-intuitive model of data siloing within companies.</li><li>An explanation of the role of an analyst within this company hierarchy.</li><li>How BI tools work and what some of the benefits and challenges of implementation may be.</li></ul><p>Now that you know why the data you get comes out dirty, next time, we'll explore some examples of how your data may arrive, and how to sculpt your data so that you can make it work for you.</p><p>Paz,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/dirty-data-in-excel-part-2/","uuid":"ebf58744-d2fb-4210-8e99-06fa163ba193","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b16ee675bd3653f82f3a947"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673661","title":"Dealing with Dirty Data in Excel","slug":"dealing-with-dirty-data-in-excel","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","excerpt":"Cleaning data Excel in the absence of tools designed to do so.","custom_excerpt":"Cleaning data Excel in the absence of tools designed to do so.","created_at_pretty":"30 May, 2018","published_at_pretty":"31 May, 2018","updated_at_pretty":"21 January, 2019","created_at":"2018-05-30T03:45:53.000-04:00","published_at":"2018-05-31T09:11:14.000-04:00","updated_at":"2019-01-21T13:41:02.000-05:00","meta_title":"Dealing with Dirty Data in Excel | Hackers and Slackers","meta_description":"Cleaning data Excel in the absence of tools designed to do so.","og_description":"Cleaning data Excel in the absence of tools designed to do so.","og_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","og_title":"Dealing with Dirty Data in Excel","twitter_description":"Cleaning data Excel in the absence of tools designed to do so.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/max@2x.jpg","twitter_title":"Dealing with Dirty Data in Excel","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"In my last post, we discussed what separates a true  analyst (read: technical)\nfrom a project manager wearing the mask of an analyst like some Scott Snyder era\nJoker (I figure that there's a solid overlap between fans of comic books and\nfans of the real world application of data. Note that this is a study with an N\n= 1 so it bares no statistical significance, but I have a funny feeling...call\nit spidey sense).\n\nFull disclosure, this post comes mostly out of my inability to sleep in my hotel\nroom in Chicago following a grueling day of doing the very things I discuss in\nthis blog, and preceding a day where I'll have to literally explain my last post\nto the suits, but perhaps this is the best mindset to begin discussing the\nmyriad ways in which you may encounter dirty data in the wild, and how a savvy\nanalyst may pivot and match their way around it. However, if my prose isn't as\non point as you have grown accustomed...blame it on the 4AM haze.\n\nAlas, let's begin by discussing the organizational structure of the majority of\ncorporate entities that leverage data to some degree (note, this isn't all\ncorporations...and what does that say about the state of business?) and how, at\neach step of abstraction in this process that you are from the data, the data\ngets dirtier and dirtier.\n\nEssentially, there's always going to be a group of about 5-10\nfewer-than-necessary legitimately skilled data scientists and/or computer\nprogrammers/DBAs who are really solid at building and maintaining a database as\nwell as coding in some sort of compiling language (nowadays, that's probably\npython, but not exclusively, nor does this matter). However, depending on your\nindustry (unless of course your industry IS data), it's nearly impossible to\nrecruit people who have these skills to the level necessary AND have some\nfamiliarity with why this data is needed, and/or the ability to explain how the\ninternal products that they build can be used by an end user. As such, this team\nhas their own project manager(s) who's only job is to keep these guys from\ndeveloping a sentient AI that's sole goal is the annihilation of unfolded\nlaundry...when your industry is healthcare. This team should also have at least\none analyst who will take the raw code base and do the first step of translation \n to a more user friendly form. This generally takes shape as either dashboards\nin a system like Tableau, or  if your company has a group of particularly strong\ndata/business analysts (or particularly weak programmers) an interface written\nin plain(enough) English on a Business Intelligence platform such as Microsoft\nBI/SAP Business Objects or whatever other system your company utilizes. As a fun\nlittle note, this team ALMOST ALWAYS  is referred to by some sort of acronym\nsuch as QDAR! (Quality data and reporting!) or KMnR! (Knowledge management and\nreporting!) or Those Fucking Guys (who have something to do with reporting)\n(TFG(whstdwr)). On a less fun little note...neither you, nor seemingly ANYONE\nELSE will have contact with this team. In light of this information, how do the\nreports that they build get chosen and who decides how these databases are\nbuilt? The world may never know.\n\nSo let's assume the first type of reporting: the Dataratti  (which is how I will\nrefer to the acronym defined team described above moving forward) produces\ndashboards utilizing a tool such as Tableau or Crystal Reports. You may be\nthinking to yourself: \"hey, isn't my job taking the data and putting it in a\nform where people who are scared by more than two nested groups of parenthesis,\nand thus this renders my job unnecessary?\" The answer to the question is\ntwofold: Yes, and of course not! As mentioned previously, the decision to create\nthese dashboards, the data contained therein, and how you want them to look is\ndecided upon by a mythical creature who has full access and understanding of the\ndata warehouse, AND has full access to and understanding of the stakeholders\n(AKA, Those Who Sit Above in Shadow; that's a reference from a famous run of\nThor comics that refers to to a mysterious cabal of gods who perpetuate the\ncycle of Ragnarok in order to subsist upon the energies created by this\nstrife...which as I write this, is an almost disgustingly on-the-nose metaphor\nfor upper management). Now, if you believe that you may be this mythical\ncreature (as I do), I DARE  you to apply for a job with this job description,\nand once you clinch it with the advice from this blog,  rapidly realize that\nyour job will involve either one of these job duties or the other.\n\nWith that digression, even if somehow a useful dashboard for YOU is created, the\nlimitations inherent in these dashboarding tools make one CRUCIAL issue\nomnipresent: one can only effectively illustrate up to 16 different variables at\na time before the system breaks down (for example, Tableau's documentation\nspecifically warns against this). So even if you have the nicest, most\nillustrative dashboards on the planet from the Dataratti, there is a nearly 100%\nchance that the information that you actually need will be scattered across 2-3\ndifferent dashboards...rendering the nice looking dashboards essentially useless\nfor your purposes, and as previously stated, you have no contact with the\nDataratti, nor do you have access to the underlying data from which these\ndashboards are created. So pop quiz hot shot, what DO you do?\n\nWell, mercifully, all of these dashboard tools allow an end user to download a\n\"data dump\" (our parlance for \"a buncha numbers with headings\"). Using Tableau\nas an example, one can download either a \"crosstab\" or a text file of the data\nrepresented by the dashboard (in both \"summary\" and \"full data\" format). Now,\njust to get the truly gifted in Tableau off my back, yes, the functionality does\nexist to build in the ability to download the data in the exact format necessary\nfor your  needs through a specific combination of custom web server views and\nJavascript, but...\n\n 1. If the users of the dash are exclusively using this function, why do the\n    dashboard at all? And...\n 2. This forces the developers in the Dataratti to have decent web design skills\n    on top of really high level Tableau skills, and it requires someone to\n    anticipate exactly how the data will be used by the end user by the\n    Dataratti (which is incredibly hard as it's impossible to speak to this\n    department directly, and as previously stated, the lack of this  knowledge\n    on their end is the entire reason why my department exists).\n\nA few things to note before downloading data from Tableau:\n\n * You must highlight at least one element of the dashboard before downloading a\n   crosstab.\n * Depending on what kind of dashboard you're working with, you may need to\n   highlight the entirety of one column in order to capture the entirety of your\n   data (click the first element in any column  and then scroll down to the\n   bottom of the report...which may be enormously long, hit shift  and click the\n   last element in the report) before downloading either the data or the\n   crosstab.\n * If you are downloading a crosstab, be wary, Tableau web server caps how many\n   rows you can download in this method at a time, this can be avoided by\n   downloading the text version of the data (by clicking data as opposed to\n   crosstab). HOWEVER...\n * If you are going the data route, it defaults to summary view. Look over all\n   the headings, and ensure that this covers everything you need, otherwise\n   click \"full data\". Interestingly, this still isn't actually the entirety of\n   your data, and continue to check to make sure all of your headings are\n   covered, otherwise, click the display all columns  box, and then download all\n   the rows as a text file.\n\nNow, repeat these steps until all of the data that you need in your  report is\ncontained across these text files (.csv, AKA the Comma Separated Value file\ntype). With all that lunacy completed, you now have several sheets with some\ncommon columns, but all with different information; only some of which you need,\nso what do you think you do?\n\nSimple, you use the tools given to you in the previous posts: you lookup  on the\ncommon factors across the sheets and return the data that you want until you\nhave all the data you need, in the correct order, on one sheet, and then\ndepending on the ask, you may want to pivot that data out in order to summarize \nthe whole mess of data. THIS IS YOUR FINAL PRODUCT  well done. Another protip:\nif you want to reposition data that you've obtained via a lookup, highlight the\nwhole column, hit control+C  to copy the data and then hit control+V  pause a\nsecond (press NOTHING else) and then press control FOLLOWED by V. This takes the\nvalues generated by a formula and replaces them with the values obtained.\nFunctionally, this looks  exactly the same, but now you can move the data around\nwithout affecting or being affected by other data.\n\nAs explaining only one possible dirty data scenario took over 1500 words, next\ntime, we'll discuss the other most common form of taking the dirty data from the\nDataratti and making it useful to you: using business intelligence portals as\nopposed to dashboards in order to grab the data that you need. Also, if I don't\nget roasted on a spit for being half asleep for tomorrow's (today's?) meeting,\nI'll try and write up a companion post with an example of how this works out in\npractice.\n\nIn summary, in this post we've learned:\n\n 1. How data is generally siloed and sequestered within the corporate\n    environment, leading to a bevy of unnecessary steps on behalf of the analyst\n    in order to distill a functional report for the powers-that-be\n 2. Two major methods in which data comes from the data team (henceforth known\n    as the Dataratti) to your team: Dashboards and Business Intelligence\n    interfaces, and...\n 3. Assuming you get data in the form of dashboards, how to take these\n    dashboards, download the underlying data, recombine and manipulate the data,\n    and package it in a way acceptable for your needs.\n\nCongrats, you've just learned the crucial skill of the Slice n' Dice!\n\nQuite sleepily,\n\n-Snacks","html":"<p>In my last post, we discussed what separates a <em>true</em> analyst (read: technical) from a project manager wearing the mask of an analyst like some Scott Snyder era Joker (I figure that there's a solid overlap between fans of comic books and fans of the real world application of data. Note that this is a study with an N = 1 so it bares no statistical significance, but I have a funny feeling...call it spidey sense).</p><p>Full disclosure, this post comes mostly out of my inability to sleep in my hotel room in Chicago following a grueling day of doing the very things I discuss in this blog, and preceding a day where I'll have to literally explain my last post to the suits, but perhaps this is the best mindset to begin discussing the myriad ways in which you may encounter dirty data in the wild, and how a savvy analyst may pivot and match their way around it. However, if my prose isn't as on point as you have grown accustomed...blame it on the 4AM haze.</p><p>Alas, let's begin by discussing the organizational structure of the majority of corporate entities that leverage data to some degree (note, this isn't all corporations...and what does that say about the state of business?) and how, at each step of abstraction in this process that you are from the data, the data gets dirtier and dirtier.</p><p>Essentially, there's always going to be a group of about 5-10 fewer-than-necessary legitimately skilled data scientists and/or computer programmers/DBAs who are really solid at building and maintaining a database as well as coding in some sort of compiling language (nowadays, that's probably python, but not exclusively, nor does this matter). However, depending on your industry (unless of course your industry IS data), it's nearly impossible to recruit people who have these skills to the level necessary AND have some familiarity with why this data is needed, and/or the ability to explain how the internal products that they build can be used by an end user. As such, this team has their own project manager(s) who's only job is to keep these guys from developing a sentient AI that's sole goal is the annihilation of unfolded laundry...when your industry is healthcare. This team should also have at least one analyst who will take the raw code base and do the <strong>first step of translation</strong> to a more user friendly form. This generally takes shape as either dashboards in a system like Tableau, <strong>or</strong> if your company has a group of particularly strong data/business analysts (or particularly weak programmers) an interface written in plain(enough) English on a Business Intelligence platform such as Microsoft BI/SAP Business Objects or whatever other system your company utilizes. As a fun little note, this team <strong>ALMOST ALWAYS</strong> is referred to by some sort of acronym such as QDAR! (Quality data and reporting!) or KMnR! (Knowledge management and reporting!) or Those Fucking Guys (who have something to do with reporting) (TFG(whstdwr)). On a less fun little note...neither you, nor seemingly ANYONE ELSE will have contact with this team. In light of this information, how do the reports that they build get chosen and who decides how these databases are built? The world may never know.</p><p>So let's assume the first type of reporting: the Dataratti  (which is how I will refer to the acronym defined team described above moving forward) produces dashboards utilizing a tool such as Tableau or Crystal Reports. You may be thinking to yourself: \"hey, isn't my job taking the data and putting it in a form where people who are scared by more than two nested groups of parenthesis, and thus this renders my job unnecessary?\" The answer to the question is twofold: Yes, and of course not! As mentioned previously, the decision to create these dashboards, the data contained therein, and how you want them to look is decided upon by a mythical creature who has full access and understanding of the data warehouse, AND has full access to and understanding of the stakeholders (AKA, <strong>Those Who Sit Above in Shadow</strong>; that's a reference from a famous run of Thor comics that refers to to a mysterious cabal of gods who perpetuate the cycle of Ragnarok in order to subsist upon the energies created by this strife...which as I write this, is an almost disgustingly on-the-nose metaphor for upper management). Now, if you believe that you may be this mythical creature (as I do), I <em>DARE</em> you to apply for a job with this job description, and once you clinch it with the advice from this blog,  rapidly realize that your job will involve either one of these job duties or the other.</p><p>With that digression, even if somehow a useful dashboard for YOU is created, the limitations inherent in these dashboarding tools make one CRUCIAL issue omnipresent: one can only effectively illustrate up to 16 different variables at a time before the system breaks down (for example, Tableau's documentation specifically warns against this). So even if you have the nicest, most illustrative dashboards on the planet from the Dataratti, there is a nearly 100% chance that the information that you actually need will be scattered across 2-3 different dashboards...rendering the nice looking dashboards essentially useless for your purposes, and as previously stated, you have no contact with the Dataratti, nor do you have access to the underlying data from which these dashboards are created. So pop quiz hot shot, what DO you do?</p><p>Well, mercifully, all of these dashboard tools allow an end user to download a \"data dump\" (our parlance for \"a buncha numbers with headings\"). Using Tableau as an example, one can download either a \"crosstab\" or a text file of the data represented by the dashboard (in both \"summary\" and \"full data\" format). Now, just to get the truly gifted in Tableau off my back, yes, the functionality does exist to build in the ability to download the data in the exact format necessary for <em>your</em> needs through a specific combination of custom web server views and Javascript, but...</p><ol><li>If the users of the dash are exclusively using this function, why do the dashboard at all? And...</li><li>This forces the developers in the Dataratti to have decent web design skills on top of really high level Tableau skills, and it requires someone to anticipate exactly how the data will be used by the end user by the Dataratti (which is incredibly hard as it's impossible to speak to this department directly, and as previously stated, the lack of <em>this</em> knowledge on their end is the entire reason why my department exists).</li></ol><p>A few things to note before downloading data from Tableau:</p><ul><li>You must highlight at least one element of the dashboard before downloading a crosstab.</li><li>Depending on what kind of dashboard you're working with, you may need to highlight the entirety of one column in order to capture the entirety of your data (<strong>click the first element in any column</strong> and then scroll down to the bottom of the report...which may be enormously long, <strong>hit shift</strong> and click the last element in the report) before downloading either the data or the crosstab.</li><li>If you are downloading a crosstab, be wary, Tableau web server caps how many rows you can download in this method at a time, this can be avoided by downloading the text version of the data (by clicking data as opposed to crosstab). HOWEVER...</li><li>If you are going the data route, it defaults to summary view. Look over all the headings, and ensure that this covers everything you need, otherwise click <strong>\"full data\"</strong>. Interestingly, this still isn't actually the entirety of your data, and continue to check to make sure all of your headings are covered, otherwise, click the <strong>display all columns</strong> box, and then download all the rows as a text file.</li></ul><p>Now, repeat these steps until all of the data that you need in <em>your</em> report is contained across these text files (.csv, AKA the Comma Separated Value file type). With all that lunacy completed, you now have several sheets with some common columns, but all with different information; only some of which you need, so what do you think you do?</p><p>Simple, you use the tools given to you in the previous posts: you <em>lookup</em> on the common factors across the sheets and return the data that you want until you have all the data you need, in the correct order, on one sheet, and then depending on the ask, you may want to pivot that data out in order to <em>summarize</em> the whole mess of data. <strong>THIS IS YOUR FINAL PRODUCT</strong> well done. Another protip: if you want to reposition data that you've obtained via a lookup, highlight the whole column, hit <strong>control+C</strong> to copy the data and then hit <strong>control+V</strong> pause a second (press NOTHING else) and then press <strong>control FOLLOWED by V</strong>. This takes the values generated by a formula and replaces them with the values obtained. Functionally, this <em>looks</em> exactly the same, but now you can move the data around without affecting or being affected by other data.</p><p>As explaining only one possible dirty data scenario took over 1500 words, next time, we'll discuss the other most common form of taking the dirty data from the Dataratti and making it useful to you: using business intelligence portals as opposed to dashboards in order to grab the data that you need. Also, if I don't get roasted on a spit for being half asleep for tomorrow's (today's?) meeting, I'll try and write up a companion post with an example of how this works out in practice.</p><p>In summary, in this post we've learned:</p><ol><li>How data is generally siloed and sequestered within the corporate environment, leading to a bevy of unnecessary steps on behalf of the analyst in order to distill a functional report for the powers-that-be</li><li>Two major methods in which data comes from the data team (henceforth known as the Dataratti) to your team: Dashboards and Business Intelligence interfaces, and...</li><li>Assuming you get data in the form of dashboards, how to take these dashboards, download the underlying data, recombine and manipulate the data, and package it in a way acceptable for your needs.</li></ol><p>Congrats, you've just learned the crucial skill of the <em>Slice n' Dice</em>!</p><p>Quite sleepily,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/dealing-with-dirty-data-in-excel/","uuid":"4753f845-b70e-4159-9a45-90753b8620b4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b0e56b16cb7ee206e3e518a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867365f","title":"Doing the Excel Data Dance","slug":"doing-the-excel-data-dance","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/maxexcel4@2x.jpg","excerpt":"Taking a step back to reflect on the problems Excel intends to solve.","custom_excerpt":"Taking a step back to reflect on the problems Excel intends to solve.","created_at_pretty":"29 May, 2018","published_at_pretty":"29 May, 2018","updated_at_pretty":"21 January, 2019","created_at":"2018-05-28T22:07:04.000-04:00","published_at":"2018-05-29T08:00:00.000-04:00","updated_at":"2019-01-21T14:27:30.000-05:00","meta_title":"Doing the Excel Data Dance | Hackers and Slackers","meta_description":"Taking a step back to reflect on the problems Excel intends to solve","og_description":"Taking a step back to reflect on the problems Excel intends to solve","og_image":"https://hackersandslackers.com/content/images/2018/05/maxexcel4@2x.jpg","og_title":"Doing the Excel Data Dance","twitter_description":"Taking a step back to reflect on the problems Excel intends to solve","twitter_image":"https://hackersandslackers.com/content/images/2018/05/maxexcel4@2x.jpg","twitter_title":"Doing the Excel Data Dance","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"If you've been reading along, over the last several posts you've learned the two\nmajor skills that any self-respecting Excel jockey counts as their go-tos: the\nability to lookup  (remember, I'm partial to index-match, but if you learned VH\nlookup, ride that until you crash your system) and the ability to pivot.\n\nNow here's something really interesting: until we pierce the veil on doing some\nVB macros, by truly mastering just those two skills, you can handle basically\nany job that Excel can actually handle, anything more advanced is just a matter\nof doing it faster (and faster, and faster). However, I've neglected to discuss\nwhat types of things you'll actually encounter out in the wild that requires\nsome fancy Excel footwork. Well, that's what this post is all about.\n\nThe first question: what is Excel actually for. Truth be told, if you separate\nthe VB elements and the pivot tables from Excel, it was really just a way for\naccountants to make small changes to tax forms and have them reflected later in\nthe form (something that had to be done by hand prior to the invention of\nExcel's progenitor...which had to have been horrible). With that in mind, one\nneeds to understand that everything else that Excel can do is built on top  of\nthis relatively rudimentary foundation, and interestingly enough, the vast\nmajority of software functions in the same way: for example, the reason why the\nlast dozen or so Bethesda games all have similar glitches and gameplay is\nbecause they're all built on a nearly decade old engine, and anyone who works at\na company that managed a database for a lengthy period of time knows that it's\nmuch easier to build on top of the garbage than to clean anything up (that's\nalso why there's all that cut content that still exists in games when you used a\ngame genie as a kid...it's too difficult to take that stuff out without harming\nthe juicy center!).\n\nThe pros of this model is that because Excel was made to handle accounting work,\nit handles relatively small datasets (and \"small\" here is under a million\nrows...I've managed to push excel to about 250k rows without it exploding...but\nnot much more than that) very quickly, and in a very user friendly and visual\nformat. In other words, Your boss uses Excel because it's pretty easy to\nunderstand what's going on (and you can see  the data). As such, you can think\nof Excel as the \"Lingua Franca\" (the language that two dignitaries of separate\nnations speak to conduct diplomacy...or perhaps, the Basic to your Star Wars\nUniverse) of the data world. To put this as simply as possible, unless you're\nworking for a company where EVERYONE has a background in computer/data science,\nyou're going to need to figure out a way to get your data into an Excel friendly\nformat somehow. The reason for this may have to do with self selection, people\nwho like getting into code go into heavy data, people like me who like to fix\nproblems tend to be intermediaries, and the people who actually make money for\nour companies (or at least are able to convince others that money is made)...are\nbusiness people.\n\nNow brace yourself, because this sounds completely insane: the daily workflow of\na Business Analyst (or a BA) at many companies is taking data that already\nexists in one form on one platform (generally either tableau, a business\nintelligence program, or some sort of database), exporting it to an Excel\nfriendly format, manipulating the data so it makes sense, drawing some sort of\nsalient conclusion from it (usually using a lookup, a pivot, or both),\npresenting it to management, and then putting it back into a format that the\nheavy data people can use to start this engine all over again.\n\nThe funny part is, the actual mechanics of doing this work isn't hard, if\neveryone is on the same page, you could probably take care of this whole\nworkflow in maybe half a day. Alas, blogs like this wouldn't need to be written\nif mechanics always worked the way they should, and another golden rule is that\nmultiple departments are never  on the same page. In other words, the data you\nget is going to be \"dirty\" and next time, I'm going to show you some common\ndirty data dilemmas, and how to polish it up with nothing but a few = signs, a\ncouple of commands, a pivot table or two, and a shitload of ingenuity.\n\nBut as always, a summary. In this post, we learned:\n\n * What Excel is good at\n * What Excel perhaps isn't so good at\n * Why you still need to deal with Excel\n * The day to day of someone who deals in both the data space and the management\n   space...and\n * That troubleshooting this step is what ultimately earns you your paycheck.\n\nFinally, a quick aside to placate my fellow authors and friends on this fine\nblog, your data being dirty on my end doesn't mean you did anything wrong, it's\nthat bouncing from one platform to another and back was never meant to be easy.","html":"<p>If you've been reading along, over the last several posts you've learned the two major skills that any self-respecting Excel jockey counts as their go-tos: the ability to <em>lookup</em> (remember, I'm partial to index-match, but if you learned VH lookup, ride that until you crash your system) and the ability to <em>pivot</em>.</p><p>Now here's something really interesting: until we pierce the veil on doing some VB macros, by truly mastering just those two skills, you can handle basically any job that Excel can actually handle, anything more advanced is just a matter of doing it faster (and faster, and faster). However, I've neglected to discuss what types of things you'll actually encounter out in the wild that requires some fancy Excel footwork. Well, that's what this post is all about.</p><p>The first question: what is Excel actually <em>for</em>. Truth be told, if you separate the VB elements and the pivot tables from Excel, it was really just a way for accountants to make small changes to tax forms and have them reflected later in the form (something that had to be done by hand prior to the invention of Excel's progenitor...which had to have been horrible). With that in mind, one needs to understand that everything else that Excel can do is built <strong>on top</strong> of this relatively rudimentary foundation, and interestingly enough, the vast majority of software functions in the same way: for example, the reason why the last dozen or so Bethesda games all have similar glitches and gameplay is because they're all built on a nearly decade old engine, and anyone who works at a company that managed a database for a lengthy period of time knows that it's much easier to build on top of the garbage than to clean anything up (that's also why there's all that cut content that still exists in games when you used a game genie as a kid...it's too difficult to take that stuff out without harming the juicy center!).</p><p>The pros of this model is that because Excel was made to handle accounting work, it handles relatively small datasets (and \"small\" here is under a million rows...I've managed to push excel to about 250k rows without it exploding...but not much more than that) very quickly, and in a very user friendly and visual format. In other words, Your boss uses Excel because it's pretty easy to understand what's going on (and you can <strong>see</strong> the data). As such, you can think of Excel as the \"Lingua Franca\" (the language that two dignitaries of separate nations speak to conduct diplomacy...or perhaps, the Basic to your Star Wars Universe) of the data world. To put this as <em>simply as possible</em>, unless you're working for a company where EVERYONE has a background in computer/data science, you're going to need to figure out a way to get your data into an Excel friendly format somehow. The reason for this may have to do with self selection, people who like getting into code go into heavy data, people like me who like to fix problems tend to be intermediaries, and the people who actually make money for our companies (or at least are able to convince others that money is made)...are business people.</p><p>Now brace yourself, because this sounds completely insane: the daily workflow of a Business Analyst (or a BA) at many companies is taking data that already exists in one form on one platform (generally either tableau, a business intelligence program, or some sort of database), exporting it to an Excel friendly format, manipulating the data so it makes sense, drawing some sort of salient conclusion from it (usually using a lookup, a pivot, or both), presenting it to management, and then putting it back into a format that the heavy data people can use to start this engine all over again.</p><p>The funny part is, the actual mechanics of doing this work isn't hard, if everyone is on the same page, you could probably take care of this whole workflow in maybe half a day. Alas, blogs like this wouldn't need to be written if mechanics always worked the way they should, and another golden rule is that multiple departments are <strong>never</strong> on the same page. In other words, the data you get is going to be \"dirty\" and next time, I'm going to show you some common dirty data dilemmas, and how to polish it up with nothing but a few = signs, a couple of commands, a pivot table or two, and a shitload of ingenuity.</p><p>But as always, a summary. In this post, we learned:</p><ul><li>What Excel is good at</li><li>What Excel perhaps isn't so good at</li><li>Why you still need to deal with Excel</li><li>The day to day of someone who deals in both the data space and the management space...and</li><li>That troubleshooting this step is what ultimately earns you your paycheck.</li></ul><p>Finally, a quick aside to placate my fellow authors and friends on this fine blog, your data being dirty on my end doesn't mean you did anything wrong, it's that bouncing from one platform to another and back was never meant to be easy.</p>","url":"https://hackersandslackers.com/doing-the-excel-data-dance/","uuid":"53af345c-d12b-4629-a339-8c03b7677c80","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b0cb5c848a62e1c7dcb0c54"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867365c","title":"Adventures in Excel: Power to the Pivot","slug":"powering-up-your-excel-pivot-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/maxexcel3-3.jpg","excerpt":"Power up your Excel Pivot Tables with these pro moves.","custom_excerpt":"Power up your Excel Pivot Tables with these pro moves.","created_at_pretty":"23 May, 2018","published_at_pretty":"25 May, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-05-23T18:57:23.000-04:00","published_at":"2018-05-24T20:35:00.000-04:00","updated_at":"2019-02-28T02:37:16.000-05:00","meta_title":"Adventures in Excel: Power to the Pivot | Hackers and Slackers","meta_description":"Power up your Excel Pivot Tables with these pro moves.","og_description":"Power up your Excel Pivot Tables with these pro moves.","og_image":"https://hackersandslackers.com/content/images/2019/02/maxexcel3-3.jpg","og_title":"Adventures in Excel: Power to the Pivot","twitter_description":"Power up your Excel Pivot Tables with these pro moves.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/maxexcel3-3.jpg","twitter_title":"Adventures in Excel: Power to the Pivot","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"During the last discussion, you've (hopefully) learned how to generate a pivot\ntable, and learned about the four \"buckets\" that can house your columns:\n\n * Filter\n * Row\n * Column\n * Value\n\nI'm also going to make the wild assumption that you've played around with your\nnewly birthed pivot table, taking your column headings from your \"raw\" data (in\nthe lingua franca of the numerically inclined, this means the data just as\nyou've received it, before you add formulae or engage in any other\nmanipulation.) Hopefully this gave you a feel of how a pivot table works, and if\nyour synapses are wired in a very specific way  you may even begin to feel out\nwhy pivot tables are such a game-changer. With this base in mind, in this post,\nwe're going to learn what each of those buckets actually do, and explore some\nreally cool ways that you can mess with your raw data within the pivot\nenvironment.\n\nBefore we go further, we must first understand some foundation facts about\nworking with Pivots that will save you heartache and frustration out of the\nprocess:\n\n 1.   Remember, any data within the pivot table does not affect the raw data,\n    this makes pivot tables the ideal way to look at your data through different\n    lenses. \n 2.   You can utilize a heading in your pivot table in multiple locations (or\n    even multiple times in the same location!) to view your data in different\n    ways. The exact way this works is difficult to explain, so try it yourself\n    and find out. \n 3.   Any changes to your raw data (especially if you didn't highlight the\n    entirety of your data) will not  be reflected in the pivot table UNLESS you\n    right click the table, and press \"refresh\", this will then take into account\n    any data changed in the raw data (assuming that the change took place within\n    the area you highlighted, if you add a column or row, you need to redo the\n    table) \n 4.   You can copy/paste any part of the pivot table, and contents that you\n    highlighted to copy will appear. HOWEVER, if you highlight all  visible\n    parts of the pivot table, you'll actually copy the whole pivot (which you\n    can continue modifying). This is very useful if you want to make a minor\n    change to a pivot table, and compare them side by side (we'll dive deeper\n    into this topic in a later post). \n 5.   This isn't exclusive to pivot tables, but remember the absolute golden\n    rule  of Excel: much like an obnoxiously large Leatherman (or a Swiss army\n    knife, depending on how over-prepared you want to be) while there are nearly\n    infinite tools or settings available, a hypermajority  of Excel users will\n    never touch even half of the options available, and are still strong in the\n    ways of magic (#wizard), so just because you can press all the buttons,\n    doesn't mean you should. \n\nWith all three of these rules in mind, let's dive into each of the buckets, what\nthey do, and how you can use this to really start to get wild with your data.\nLet's start with the most straightforward:\n\nFilters\nThe filter bucket allows you to place one (or more!) of your headings (AKA your\ncolumns in the raw data) in order to filter in or out those values out of the\nentire pivot table. For instance, If you have a column of people's names, and\nyou put names into the filter bucket, you can then choose the names that you\nwant to include the associated data. One great use for the filter bucket is if\nyou have a dataset with dates, you can filter out a specific Month/Day/Year and\nthe rest of your data will only display information from that particular time\nperiod.\n\nRows\nRows are the bread and butter of a pivot table (and as we discussed last time,\nthe thing that clicking on a heading defaults to). This is the main area where\nyour data will live. For example, if you have a dataset of Names, IDs, and\nBirthdays, and you click Names, the pivot table will show all of the DISTINCT \nnames in a column within the pivot. For example, if you have the dataset of\nnames:\n\nCOLUMN A COLUMN B Bob Sally Bob Jim Tom Larry Bob Your pivot table will display:\n\nCOLUMN A COLUMN B Bob Sally Jim Tom Larry In other words, it treats all Bobs the\nsame, and this functionality is *Muy importanté\n\nHowever, what if you want to see how many Bobs exist within your data? Well...\n\nValues\nThe values tab is perhaps the most flexible part of the pivot table. By putting\na heading within the value tab, it will default to showing you the Count, or the\nnumber of appearances that a particular value shows up within a column of your\ndataset. So for example, if you drag names into both the row and values buckets,\nyou'll wind up with this (a pivot table will always show a grand total (unless\nyou specifically tell the pivot not to, which is a possibility, however, because\nyou can copy and paste any part of the pivot, it's rarely necessary):\n\n             COLUMN ACOLUMN B        Bob3        Sally1        Bob1        Jim1\n       Tom1        **Total****7**\n\nNow, notice that above I said that by dragging a heading into the values bucket\nit'll default to a count (unless the column is all numbers, then it'll default\nto a sum), however, this is not the only possibility. By left clicking the\nheading within the value bucket and choosing \"value field settings), you can\nchoose from a mind-boggling array of possible mathematical operations, the ones\nthat you will generally use most often are probably going to be count  (the\nnumber of appearances of a given value within a dataset) and sum  (the combined\ntotal of all the numbers associated with each value in your row bucket). One\nimportant piece of advice though, you can not  sum text, Excel won't let you\n(even if the text looks like a number), so if you're using numbers, be sure to\ntell Excel that you're looking at numbers (this issue will happen more often\nthan you could possibly imagine!)\n\nOther useful operators are count distinct  (which you can only utilize if you\nchecked add to data model  when you built the table) and min or max (returning\nthe lowest or highest value associated with a particular value in your rows, and\nyou can only use this if you did not  click add to data model  when you built\nthe table). A good example of using Max or Min is when you have say names and\nsalaries, where you can show the highest and lowest salary for a particular\nname. Keep in mind that you can have more than one value operator (like min and\nmax) per heading, thus putting both the minimum and maximum value next to each\nother on the table. Which leads us to the generally least used bucket...\n\nColumns\nOne thing I intentionally left out was what a pivot table actually does  with\nyour data. A gross over-simplification of the functionality is that it\nconsolidates (picking out those distinct values), summarizes (shows totals, and\ngrand totals), and most importantly, it flips your columns and rows (hence the\nreason it's called a pivot  table.) Thus, the column bucket is generally just\nused as a place where your values go automatically when you populate the values\nbucket. The reason columns get kind of cumbersome is because it literally widens\nyour pivot by how many ever values exist in your dataset, thus, anything more\nthan say 20 values becomes almost impossible to read, let alone use in a\nmeaningful way.\n\nThere are some rare cases in which using a heading in the column bucket may make\nsense though, for instance, if you have birth months as one of your headings,\nand names for rows, you can put the months in the column bucket and then get a\ncount showing each how many people with a particular name land at a particular\nmonth in a grid or \"heat map\" (a fancy way to say a chart showing concentrations\nof something) fashion. Another use case (and one I use on the day to day) is\nfinding the numbers of providers who utilize a particular procedure for which\nthere is a known value (there are 12 that I look at). You have the procedure\nnames as the columns, and you have the providers who administer the procedures\nas the rows. You then put measure in the value bucket. This gives me a nice,\nconcise grid for which I can see which providers give how many of which of the\n12 procedures...which on it's own is sorta useful, but when combined with a\ntrick we already learned, some epic level spellcasting is afoot. Unfortunately,\nthat'll have to wait until next time.\n\nIn summary, we learned:\n\n * The purpose of the filter bucket within a pivot table\n * The purpose of the rows bucket within a pivot table and the fact that it\n   shows distinct values\n * The purpose of the values bucket within a pivot table, and how you can have\n   the column use different mathematical operators (such as count, sum, min, and\n   max) to find different information within a dataset... and\n * The purpose (or lack thereof) of the columns bucket within a pivot table, and\n   potential use case scenarios for this function.\n\nNext time, we start combining knowledge in order to get freaky with your data in\nways that the lord of Excel themselves did not consider.\n\nPivot till you puke,\n\n-Snacks","html":"<p>During the last discussion, you've (hopefully) learned how to generate a pivot table, and learned about the four \"buckets\" that can house your columns:</p><ul><li>Filter</li><li>Row</li><li>Column</li><li>Value</li></ul><p>I'm also going to make the wild assumption that you've played around with your newly birthed pivot table, taking your column headings from your \"raw\" data (in the lingua franca of the numerically inclined, this means the data just as you've received it, before you add formulae or engage in any other manipulation.) Hopefully this gave you a feel of how a pivot table works, and if your synapses are wired in a very specific way  you may even begin to feel out why pivot tables are such a game-changer. With this base in mind, in this post, we're going to learn what each of those buckets actually do, and explore some really cool ways that you can mess with your raw data within the pivot environment.</p><p>Before we go further, we must first understand some foundation facts about working with Pivots that will save you heartache and frustration out of the process:</p><ol><li> Remember, any data within the pivot table does not affect the raw data, this makes pivot tables the ideal way to look at your data through different lenses. </li><li> You can utilize a heading in your pivot table in multiple locations (or even multiple times in the same location!) to view your data in different ways. The exact way this works is difficult to explain, so try it yourself and find out. </li><li> Any changes to your raw data (especially if you didn't highlight the entirety of your data) <strong>will not</strong> be reflected in the pivot table UNLESS you right click the table, and press \"refresh\", this will then take into account any data changed in the raw data (assuming that the change took place within the area you highlighted, if you add a column or row, you need to redo the table) </li><li> You can copy/paste any part of the pivot table, and contents that you highlighted to copy will appear. HOWEVER, if you highlight <strong>all</strong> visible parts of the pivot table, you'll actually copy the whole pivot (which you can continue modifying). This is very useful if you want to make a minor change to a pivot table, and compare them side by side (we'll dive deeper into this topic in a later post). </li><li> This isn't exclusive to pivot tables, but remember the <strong>absolute golden rule</strong> of Excel: much like an obnoxiously large Leatherman (or a Swiss army knife, depending on how over-prepared you want to be) while there are nearly infinite tools or settings available, a <em>hypermajority</em> of Excel users will never touch even half of the options available, and are still strong in the ways of magic (#wizard), so just because you can press all the buttons, doesn't mean you should. </li></ol><p>With all three of these rules in mind, let's dive into each of the buckets, what they do, and how you can use this to really start to get wild with your data. Let's start with the most straightforward:</p><h3 id=\"filters\">Filters</h3><p>The filter bucket allows you to place one (or more!) of your headings (AKA your columns in the raw data) in order to filter in or out those values out of the entire pivot table. For instance, If you have a column of people's names, and you put names into the filter bucket, you can then choose the names that you want to include the associated data. One great use for the filter bucket is if you have a dataset with dates, you can filter out a specific Month/Day/Year and the rest of your data will only display information from that particular time period.</p><h3 id=\"rows\">Rows</h3><p>Rows are the bread and butter of a pivot table (and as we discussed last time, the thing that clicking on a heading defaults to). This is the main area where your data will live. For example, if you have a dataset of Names, IDs, and Birthdays, and you click Names, the pivot table will show all of the <em><strong>DISTINCT</strong></em> names in a column within the pivot. For example, if you have the dataset of names:</p><div class=\"tableContainer\">\n<table>\n  <thead>\n    <tr>\n      <th> COLUMN A </th>\n      <th> COLUMN B </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td> Bob </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Sally </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Bob </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Jim </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Tom </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Larry </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Bob </td>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>Your pivot table will display:</p><div class=\"tableContainer\">\n<table>\n  <thead>\n    <tr>\n      <th> COLUMN A </th>\n      <th> COLUMN B </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td> Bob </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Sally </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Jim </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Tom </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td> Larry </td>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n</div><p>In other words, it treats all Bobs the same, and this functionality is *<strong>Muy importanté</strong></p><p>However, what if you want to see how many Bobs exist within your data? Well...</p><h3 id=\"values-function values() { [native code] }1\">Values</h3><p>The values tab is perhaps the most flexible part of the pivot table. By putting a heading within the value tab, it will default to showing you the <em>Count</em>, or the number of appearances that a particular value shows up within a column of your dataset. So for example, if you drag names into both the row and values buckets, you'll wind up with this (a pivot table will always show a grand total (unless you specifically tell the pivot not to, which is a possibility, however, because you can copy and paste any part of the pivot, it's rarely necessary):</p><p>            COLUMN ACOLUMN B        Bob3        Sally1        Bob1        Jim1        Tom1        **Total****7**    </p><p>Now, notice that above I said that by dragging a heading into the values bucket it'll default to a count (unless the column is all numbers, then it'll default to a sum), however, this is not the only possibility. By left clicking the heading within the value bucket and choosing \"value field settings), you can choose from a mind-boggling array of possible mathematical operations, the ones that you will generally use most often are probably going to be <em>count</em> (the number of appearances of a given value within a dataset) and <em>sum</em> (the combined total of all the numbers associated with each value in your row bucket). One important piece of advice though, you <strong>can not</strong> sum text, Excel won't let you (even if the text looks like a number), so if you're using numbers, be sure to tell Excel that you're looking at numbers (this issue will happen more often than you could possibly imagine!)</p><p>Other useful operators are <em>count distinct</em> (which you can only utilize if you checked <strong>add to data model</strong> when you built the table) and min or max (returning the lowest or highest value associated with a particular value in your rows, and you can only use this if you <strong><em>did not</em> click add to data model</strong> when you built the table). A good example of using Max or Min is when you have say names and salaries, where you can show the highest and lowest salary for a particular name. Keep in mind that you can have more than one value operator (like min and max) per heading, thus putting both the minimum and maximum value next to each other on the table. Which leads us to the generally least used bucket...</p><h3 id=\"columns\">Columns</h3><p>One thing I intentionally left out was what a pivot table actually <em>does</em> with your data. A gross over-simplification of the functionality is that it consolidates (picking out those distinct values), summarizes (shows totals, and grand totals), and most importantly, it flips your columns and rows (hence the reason it's called a <em>pivot</em> table.) Thus, the column bucket is generally just used as a place where your values go automatically when you populate the values bucket. The reason columns get kind of cumbersome is because it literally widens your pivot by how many ever values exist in your dataset, thus, anything more than say 20 values becomes almost impossible to read, let alone use in a meaningful way.</p><p>There are some rare cases in which using a heading in the column bucket may make sense though, for instance, if you have birth months as one of your headings, and names for rows, you can put the months in the column bucket and then get a count showing each how many people with a particular name land at a particular month in a grid or \"heat map\" (a fancy way to say a chart showing concentrations of something) fashion. Another use case (and one I use on the day to day) is finding the numbers of providers who utilize a particular procedure for which there is a known value (there are 12 that I look at). You have the procedure names as the columns, and you have the providers who administer the procedures as the rows. You then put measure in the value bucket. This gives me a nice, concise grid for which I can see which providers give how many of which of the 12 procedures...which on it's own is sorta useful, but when combined with a trick we already learned, some epic level spellcasting is afoot. Unfortunately, that'll have to wait until next time.</p><p>In summary, we learned:</p><ul><li>The purpose of the filter bucket within a pivot table</li><li>The purpose of the rows bucket within a pivot table and the fact that it shows distinct values</li><li>The purpose of the values bucket within a pivot table, and how you can have the column use different mathematical operators (such as count, sum, min, and max) to find different information within a dataset... and</li><li>The purpose (or lack thereof) of the columns bucket within a pivot table, and potential use case scenarios for this function.</li></ul><p>Next time, we start combining knowledge in order to get freaky with your data in ways that the lord of Excel themselves did not consider.</p><p>Pivot till you puke,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/powering-up-your-excel-pivot-tables/","uuid":"d28897a1-583d-4a92-9e42-66f3176c58eb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b05f1d389a1112d7e8bc96a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673655","title":"The One Excel Formula to Rule Them All","slug":"the-one-excel-formula-to-rule-them-all","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/excel3@2x.jpg","excerpt":"Unlock the secrets of Excel's Index Match.","custom_excerpt":"Unlock the secrets of Excel's Index Match.","created_at_pretty":"18 May, 2018","published_at_pretty":"18 May, 2018","updated_at_pretty":"20 January, 2019","created_at":"2018-05-18T15:54:14.000-04:00","published_at":"2018-05-18T17:57:37.000-04:00","updated_at":"2019-01-20T18:50:22.000-05:00","meta_title":"The One Excel Formula to Rule Them All | Hackers and Slackers","meta_description":"Unlock the secrets of Excel's Index Match.","og_description":"Unlock the secrets of Excel's Index Match.","og_image":"https://hackersandslackers.com/content/images/2018/05/excel3@2x.jpg","og_title":"The One Excel Formula to Rule Them All","twitter_description":"Unlock the secrets of Excel's Index Match.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/excel3@2x.jpg","twitter_title":"The One Excel Formula to Rule Them All","authors":[{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Max Mileaf","slug":"snacks","bio":"One of three therapists in the known universe who knows how to use a computer. Finds meaning in highly protected data, in a cave, utilizing nothing but a box of scraps. ","profile_image":"https://hackersandslackers.com/content/images/2019/03/max2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},"tags":[{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Adventures in Excel","slug":"adventures-in-excel","description":"Excel secrets and magic. The kind of industry knowledge that could put financial analysts out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"internal"}],"plaintext":"In my last entry, we discussed how to write a formula, and you've been armed\nwith what each piece of the formula represents (the command, the variables, and\nthe definition of an array). With this knowledge, you've actually been armed\nwith the keys to the kingdom, and you're finally going to learn how to do\nsomething fancy.\n\nThere comes a time in every person's life where just by learning one piece of\ninformation, suddenly everything else you've learned clicks into place. For\nthose who utilize Excel, learning this formula WILL  BE THAT FOR YOU. In our\nlast entry, I stated very plainly that there are only a handful of formulae that\nare used on a regular basis, and this is easily the most useful and  the most\nflexible, to the point in which knowing how to use this formula (along with a\ntool that we'll discuss next time) are the SOLE job skills required to get an\nentry level job in finance note...put this somewhere on your resume.\n\nWhat is this mythical savior of white collar workers worldwide? In the past, you\nmay have heard muffled whispers of \"v-lookup\" or if your data was particularly\npoorly formatted perhaps even \"h-lookup\", however, after 2007, the minds behind\nthe spreadsheets threw both of those formulae into a blender, mixed it with\nRedbull and amphetamines, and spit out the king of functions: the =Index(Match))\n.\n\nSo, after all that incredibly well deserved hype, what is index-match, and why\nis it so special? To put things incredibly simply, index-match checks (or as the\ncool kids say: \"bump-up against\" one piece of information (say, a list of ID\nnumbers) against another piece of information (say a second list of IDs) and\nspits out the data associated with the matching values (Say, the name associated\nwith that ID number). The reasons for the importance of these commands can not\nbe overstated, as anyone who works with data in any capacity knows that\nidentifying changes (and where the changes are) over different iterations of a\nparticular dataset is an essential  job function.\n\nNow, hopefully you're starting to see the light, and understanding the power\nthat this formula holds, so now we're going to learn how to use it.\nFirst things first, if you're a particularly perceptive reader, you may have\nguessed from how I wrote the formula that =Index(Match)) is actually the\ncombination of two separate formulae that alone are of limited use, but\ntogether, they allow you to accomplish great things. These two formulae are:\n\n 1. INDEX  which asks to look at an array (essentially a list of cells), and\n    pull out the data at a specific row and column number within that array (\n    interesting fact: despite using letters for columns and numbers for rows,\n    Excel actually translates the column letters into a number with 1=A and so\n    on). Index by itself isn't super useful, as, much like when you're viewing a\n    particle, or cooking Meth, you rarely know both dimensions of a cell that\n    you're looking for (that was a Heisenberg uncertainty principle joke...or a\n    Breaking Bad reference, depending on how you roll.)\n 2. MATCH  is the formula that does most of the heavy lifting. Match checks to\n    see where a value exists within an array that you specify, and returns the\n    position in the array (for example, if you match \"C\" on a list of letters of\n    the Roman alphabet, match will tell you \"3\").\n\nNow here's where the magic happens. Remember when I said that everything in\nExcel is basically just a collection of numbers? Well, in that respect when a\nformula asks you for a number, there's no reason why you can't throw in ANOTHER \nformula that spits out a number. As such, if you tell Index where you want to\ngrab something from, and then instead of giving it a position, you give it the\nnumber that pops up from Match (which is a formula which gives you the location\nof where something matches) you've given Index all the information it needs to\ndo it's work, all without needing to know where (or if at all) the value exists.\n\nThe formula in practice is written like this:\n\n**=INDEX(*column or row where you want to grab the information from*,MATCH(*the cell containing the value that you want to look for*,*column or row where you want to check for the value*,0)) (don't forget the zero at the end, that just means you want to match the value exactly)\n\nAs such, if you have the following data\n\nSheet 1\nCOLUMN ACOLUMN B\n 1bob\n 2sam\n 3steve\n 4larry\n Sheet 2\nCOLUMN ACOLUMN B\n 2\n 3\n 12\n 18\n If you want to return the name associated with any of the numbers in sheet 2,\nyou can plug the column containing the names in the Index, the cell containing\nthe value that you want to find the associated name with as the first part of\nthe match, and the column containing the IDs associated with the names as the\nsecond part of the match, the formula will spit out the name (or an error\nstating that the value doesn't match a name). The best part is, this formula can\nget dragged down the entire list of numbers to return the names associated with \nall  of the IDs. Now, this is with a very small set of data, but it works just\nas well, and just as quickly if you're looking at 10,000 IDs.\n\nWith that being said, I hope you've learned the answer to a question you didn't\neven know you had, and I hope index-match starts saving you time (or snagging\nyou a promotion!) immediately.\n\nIn summary we've learned the following:\n\n 1. The function of INDEX\n 2. The function of MATCH (protip: if all you want to do is see if something\n    matches and don't need to see what it's associated with, you can use match\n    by itself, is =ISNUMBER(Match)) which will give you a nice true false\n 3. How you can utilize a formula as a variable in another formula (you don't\n    need to put in a second equals sign if you're doing this, but you do need\n    another set of parenthesis)\n 4. How to write an Index(Match)) formula\n 5. The main use for Index-Match (by thinking outside the box, there are an\n    infinite number of uses)\n 6. The realization that by knowing this formula, you never need to compare\n    things by hand again and eye again!\n\nNext time, we'll discuss the other frequently used Excel tool, that together\nwith the powers of Index-Match, literally make up the entry level analyst's\ntoolkit. Also, as a bonus entry in the interim, you'll get a quick rundown of\nhow you can leverage your knowledge of Index-match in a conversation with\nsomeone who knows what they're talking about.\n\nIn Hoc Signo,\n\n-Snacks","html":"<p>In my last entry, we discussed how to write a formula, and you've been armed with what each piece of the formula represents (the command, the variables, and the definition of an array). With this knowledge, you've actually been armed with the keys to the kingdom, and you're finally going to learn how to do something <em><strong>fancy</strong></em>.</p><p>There comes a time in every person's life where just by learning one piece of information, suddenly everything else you've learned clicks into place. For those who utilize Excel, learning this formula <strong>WILL</strong> BE THAT FOR YOU. In our last entry, I stated very plainly that there are only a handful of formulae that are used on a regular basis, and this is easily the most useful <strong>and</strong> the most flexible, to the point in which knowing how to use this formula (along with a tool that we'll discuss next time) are the SOLE job skills required to get an entry level job in finance <strong>note...put this somewhere on your resume.</strong></p><p>What is this mythical savior of white collar workers worldwide? In the past, you may have heard muffled whispers of \"v-lookup\" or if your data was particularly poorly formatted perhaps even \"h-lookup\", however, after 2007, the minds behind the spreadsheets threw both of those formulae into a blender, mixed it with Redbull and amphetamines, and spit out the king of functions: the <code>=Index(Match))</code>.</p><p>So, after all that incredibly well deserved hype, what is index-match, and why is it so special? To put things incredibly simply, index-match checks (or as the cool kids say: \"bump-up against\" one piece of information (say, a list of ID numbers) against another piece of information (say a second list of IDs) and spits out the data associated with the matching values (Say, the name associated with that ID number). The reasons for the importance of these commands can not be overstated, as anyone who works with data in any capacity knows that identifying changes (and where the changes are) over different iterations of a particular dataset is an <strong>essential</strong> job function.</p><p>Now, hopefully you're starting to see the light, and understanding the power that this formula holds, so now we're going to learn how to use it.<br>First things first, if you're a particularly perceptive reader, you may have guessed from how I wrote the formula that =Index(Match)) is actually the combination of two separate formulae that alone are of limited use, but together, they allow you to accomplish great things. These two formulae are:</p><ol><li><strong>INDEX</strong> which asks to look at an array (essentially a list of cells), and pull out the data at a specific row and column number within that array (<em>interesting fact: despite using letters for columns and numbers for rows, Excel actually translates the column letters into a number with 1=A and so on)</em>. Index by itself isn't super useful, as, much like when you're viewing a particle, or cooking Meth, you rarely know both dimensions of a cell that you're looking for (that was a Heisenberg uncertainty principle joke...or a Breaking Bad reference, depending on how you roll.)</li><li><strong>MATCH</strong> is the formula that does most of the heavy lifting. Match checks to see where a value exists within an array that you specify, and returns the position in the array (for example, if you match \"C\" on a list of letters of the Roman alphabet, match will tell you \"3\").</li></ol><p>Now here's where the magic happens. Remember when I said that everything in Excel is basically just a collection of numbers? Well, in that respect when a formula asks you for a number, there's no reason why you can't throw in <strong>ANOTHER</strong> formula that spits out a number. As such, if you tell Index where you want to grab something from, and then instead of giving it a position, you give it the number that pops up from Match (which is a formula which gives you the location of where something matches) you've given Index all the information it needs to do it's work, all without needing to know where (or if at all) the value exists.</p><p>The formula in practice is written like this:</p><pre><code>**=INDEX(*column or row where you want to grab the information from*,MATCH(*the cell containing the value that you want to look for*,*column or row where you want to check for the value*,0)) (don't forget the zero at the end, that just means you want to match the value exactly)</code></pre><p>As such, if you have the following data</p><h3 id=\"sheet-1\">Sheet 1</h3><table>\n    <tbody>\n        <tr><th>COLUMN A</th><th>COLUMN B</th></tr>\n        <tr><td>1</td><td>bob</td></tr>\n        <tr><td>2</td><td>sam</td></tr>\n        <tr><td>3</td><td>steve</td></tr>\n        <tr><td>4</td><td>larry</td></tr>\n    </tbody>\n</table><h3 id=\"sheet-2\">Sheet 2</h3><table>\n    <tbody>\n        <tr><th>COLUMN A</th><th>COLUMN B</th></tr>\n        <tr><td>2</td><td></td></tr>\n        <tr><td>3</td><td></td></tr>\n        <tr><td>12</td><td></td></tr>\n        <tr><td>18</td><td></td></tr>\n    </tbody>\n</table><p>If you want to return the name associated with any of the numbers in sheet 2, you can plug the column containing the names in the Index, the cell containing the value that you want to find the associated name with as the first part of the match, and the column containing the IDs associated with the names as the second part of the match, the formula will spit out the name (or an error stating that the value doesn't match a name). The best part is, this formula can get dragged down the entire list of numbers to return the names associated with <strong>all</strong> of the IDs. Now, this is with a very small set of data, but it works just as well, and just as quickly if you're looking at 10,000 IDs.</p><p>With that being said, I hope you've learned the answer to a question you didn't even know you had, and I hope index-match starts saving you time (or snagging you a promotion!) immediately.</p><p>In summary we've learned the following:</p><ol><li>The function of INDEX</li><li>The function of MATCH (<em>protip: if all you want to do is see if something matches and don't need to see what it's associated with, you can use match by itself, is </em><code>=ISNUMBER(Match)</code><em>) which will give you a nice true false</em></li><li>How you can utilize a formula as a variable in another formula (you don't need to put in a second equals sign if you're doing this, but you do need another set of parenthesis)</li><li>How to write an Index(Match)) formula</li><li>The main use for Index-Match (by thinking outside the box, there are an infinite number of uses)</li><li>The realization that by knowing this formula, you never need to compare things by hand again and eye again!</li></ol><p>Next time, we'll discuss the other frequently used Excel tool, that together with the powers of Index-Match, literally make up the entry level analyst's toolkit. Also, as a bonus entry in the interim, you'll get a quick rundown of how you can leverage your knowledge of Index-match in a conversation with someone who knows what they're talking about.</p><p>In Hoc Signo,</p><p>-Snacks</p>","url":"https://hackersandslackers.com/the-one-excel-formula-to-rule-them-all/","uuid":"59f81d21-fa84-4849-8506-a175860704bc","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5aff2f668ba5ce278d71d6c5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673636","title":"Dropping Rows of Data Using Pandas","slug":"pandas-dataframe-drop","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","custom_excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","created_at_pretty":"22 November, 2017","published_at_pretty":"18 April, 2018","updated_at_pretty":"08 March, 2019","created_at":"2017-11-22T01:21:36.000-05:00","published_at":"2018-04-18T15:00:00.000-04:00","updated_at":"2019-03-08T14:23:37.000-05:00","meta_title":"Dropping Rows Using Pandas | Hackers and Slackers","meta_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","og_title":"Dropping Rows Using Pandas","twitter_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","twitter_title":"Dropping Rows Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"You've heard the cliché before: it is often cited that roughly %80~ of a data\nscientist's role is dedicated to cleaning data sets. I Personally haven't looked\nin to the papers or clinical trials which prove this number (that was a joke),\nbut the idea holds true: in the data profession, we find ourselves doing away\nwith blatantly corrupt or useless data. The simplistic approach is to discard\nsuch data entirely, thus here we are.\n\nWhat constitutes 'filthy' data is project-specific, and at times borderline\nsubjective. Occasionally, the offenders are more obvious: these might include\nchunks of data which are empty, poorly formatted, or simply irrelevant. While\n'bad' data can occasionally be fixed or salvaged via transforms, in many cases\nit's best to do away with rows entirely to ensure that only the fittest survive.\n\nDrop Empty Rows or Columns\nIf you're looking to drop rows (or columns) containing empty data, you're in\nluck: Pandas' dropna()  method is specifically for this. \n\nUsing dropna()  is a simple one-liner which accepts a number of useful\narguments:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\nTechnically you could run MyDataFrame.dropna()  without any parameters, and this\n would default to dropping all rows where are completely empty. If thats all you\nneeded, well, I guess you're done already. Otherwise, here are the parameters\nyou can include:\n\n * Axis: Specifies to drop by row  or column. 0  means row, 1  means column.\n * How: Accepts one of two possible values: any  or all. This will either drop\n   an axis which is completely empty (all), or an axis with even just a single\n   empty cell (any).\n * Thresh: Here's an interesting one: thresh  accepts an integer, and will drop\n   an axis only if that number threshold of empty cells is breached.\n * Subset: Accepts an array of which axis' to consider, as opposed to\n   considering all by default.\n * Inplace: If you haven't come across inplace  yet, learn this now: changes\n   will NOT be made to the DataFrame you're touching unless this is set to True.\n   It's False  by default.\n\nPandas' .drop() Method\nThe pandas .drop()  method is used to remove entire rows or columns based on\ntheir name. If we can see that our DataFrame contains extraneous information\n(perhaps for example, the HR team is storing a preferred_icecream_flavor  in\ntheir master records), we can destroy the column (or row) outright.\n\nUsing drop()  looks something like this:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n\nWe'll attempt to cover the usage of these parameters in plain English before\ninevitably falling into useless lingo which you have not yet learned.\n\n *   Axis: Similar to the above, setting the axis specifies if you're trying to\n   drop rows or columns. \n *   Labels: May refer to either the name (string) of the target axis, or its\n   index (int). Of course, whether this is referring to columns or rows in the\n   DataFrame is dependent on the value of the axis parameter. Labels are always\n   defined in the 0th axis of the target DataFrame, and may accept multiple\n   values in the form of an array when dropping multiple rows/columns at once. \n\nDrop by Index:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n\n\nDrop by Label:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n\n\n *   Index, Columns: An alternative method for specifying the same as the above.\n   Accepts single or multiple values. Setting columns=labels  is equivalent to \n   labels, axis=1.  index=0* is equivalent to *labels=0.  \n *   Levels: Used in sets of data which contain multiple hierarchical levels,\n   similar to that of nested arrays. A high-level few of Hierarchical indexing\n   can be found here\n   [https://pandas.pydata.org/pandas-docs/stable/advanced.html]. \n *   Inplace: Again, drop methods are not carried out on the target Dataframe\n   unless explicitly stated. The purpose of this is to presumably preserve the\n   original set of data during ad hoc manipulation.This adheres to the Python\n   style-guide which states that actions should not be performed on live sets of\n   data unless explicitly stated. Here\n   [https://www.youtube.com/watch?v=XaCSdr7pPmY]  is a video of some guy\n   describing this for some reason. \n *   Errors: Accepts either ignore  or raise, with 'raise' set as default. When \n   errors='ignore'  is set, no errors will be thrown and existing labels are\n   dropped. \n\nDrop by Criteria\nWe can also remove rows or columns based on whichever criteria your little heart\ndesires. For example, if you really hate people named Chad, you can drop all\nrows in your Customer database who have the name Chad. Screw Chad.\n\nUnlike previous methods, the popular way of handling this is simply by saving\nyour Dataframe over itself give a passed value. Here's how we'd get rid of Chad:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n\n\nThe syntax may seem a bit off-putting to newcomers (note the repetition of \nmy_dataframe  3 times). The format of my_dataframe[CONDITION]  simply returns a\nmodified version of my_dataframe, where only the data matching the given\ncondition is affected. \n\nSince we're purging this data altogether, statingmy_dataframe =\nmy_dataframe[CONDITION]  is an easy (albeit destructive) method for shedding\ndata and moving on with our lives.","html":"<p>You've heard the cliché before: it is often cited that roughly %80~ of a data scientist's role is dedicated to cleaning data sets. I Personally haven't looked in to the papers or clinical trials which prove this number (that was a joke), but the idea holds true: in the data profession, we find ourselves doing away with blatantly corrupt or useless data. The simplistic approach is to discard such data entirely, thus here we are.</p><p>What constitutes 'filthy' data is project-specific, and at times borderline subjective. Occasionally, the offenders are more obvious: these might include chunks of data which are empty, poorly formatted, or simply irrelevant. While 'bad' data can occasionally be fixed or salvaged via transforms, in many cases it's best to do away with rows entirely to ensure that only the fittest survive.</p><h2 id=\"drop-empty-rows-or-columns\">Drop Empty Rows or Columns</h2><p>If you're looking to drop rows (or columns) containing empty data, you're in luck: Pandas' <code>dropna()</code> method is specifically for this. </p><p>Using <code>dropna()</code> is a simple one-liner which accepts a number of useful arguments:</p><!--kg-card-begin: code--><pre><code>import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)</code></pre><!--kg-card-end: code--><p>Technically you could run <code>MyDataFrame.dropna()</code> without any parameters, and this  would default to dropping all rows where are completely empty. If thats all you needed, well, I guess you're done already. Otherwise, here are the parameters you can include:</p><ul><li><strong>Axis</strong>: Specifies to drop by <em>row</em> or <em>column</em>. <code>0</code> means <em>row</em>, <code>1</code> means <em>column</em>.</li><li><strong>How</strong>: Accepts one of two possible values: <em>any</em> or <em>all</em>. This will either drop an axis which is completely empty (all), or an axis with even just a single empty cell (any).</li><li><strong>Thresh</strong>: Here's an interesting one: <em>thresh</em> accepts an integer, and will drop an axis only if that number threshold of empty cells is breached.</li><li><strong>Subset</strong>: Accepts an array of which axis' to consider, as opposed to considering all by default.</li><li><strong>Inplace</strong>: If you haven't come across <code>inplace</code> yet, learn this now: changes will NOT be made to the DataFrame you're touching unless this is set to <code>True</code>. It's <code>False</code> by default.</li></ul><h2 id=\"pandas-drop-method\">Pandas' .drop() Method</h2><p>The pandas <code>.drop()</code> method is used to remove entire rows or columns based on their name. If we can see that our DataFrame contains extraneous information (perhaps for example, the HR team is storing a <strong>preferred_icecream_flavor</strong> in their master records), we can destroy the column (or row) outright.</p><p>Using <code>drop()</code> looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n</code></pre>\n<!--kg-card-end: markdown--><p>We'll attempt to cover the usage of these parameters in plain English before inevitably falling into useless lingo which you have not yet learned.</p><ul><li> <strong>Axis</strong>: Similar to the above, setting the axis specifies if you're trying to drop rows or columns. </li><li> <strong>Labels</strong>: May refer to either the name (string) of the target axis, or its index (int). Of course, whether this is referring to columns or rows in the DataFrame is dependent on the value of the axis parameter. Labels are always defined in the 0th axis of the target DataFrame, and may accept multiple values in the form of an array when dropping multiple rows/columns at once. </li></ul><h3 id=\"drop-by-index-\">Drop by Index:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"drop-by-label-\">Drop by Label:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n</code></pre>\n<!--kg-card-end: markdown--><ul><li> <strong>Index, Columns</strong>: An alternative method for specifying the same as the above. Accepts single or multiple values. Setting <em>columns=labels</em> is equivalent to <em>labels, axis=1.</em> <em>index=0</em>* is equivalent to *<em>labels=0.</em> </li><li> <strong>Levels</strong>: Used in sets of data which contain multiple hierarchical levels, similar to that of nested arrays. A high-level few of Hierarchical indexing can be found <a href=\"https://pandas.pydata.org/pandas-docs/stable/advanced.html\">here</a>. </li><li> <strong>Inplace</strong>: Again, drop methods are not carried out on the target Dataframe unless explicitly stated. The purpose of this is to presumably preserve the original set of data during ad hoc manipulation.This adheres to the Python style-guide which states that actions should not be performed on live sets of data unless explicitly stated. <a href=\"https://www.youtube.com/watch?v=XaCSdr7pPmY\">Here</a> is a video of some guy describing this for some reason. </li><li> <strong>Errors</strong>: Accepts either <em>ignore</em> or <em>raise</em>, with 'raise' set as default. When <em>errors='ignore'</em> is set, no errors will be thrown and existing labels are dropped. </li></ul><h2 id=\"drop-by-criteria\">Drop by Criteria</h2><p>We can also remove rows or columns based on whichever criteria your little heart desires. For example, if you really hate people named Chad, you can drop all rows in your Customer database who have the name Chad. Screw Chad.</p><p>Unlike previous methods, the popular way of handling this is simply by saving your Dataframe over itself give a passed value. Here's how we'd get rid of Chad:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n</code></pre>\n<!--kg-card-end: markdown--><p>The syntax may seem a bit off-putting to newcomers (note the repetition of <code>my_dataframe</code> 3 times). The format of <code>my_dataframe[CONDITION]</code> simply returns a modified version of <code>my_dataframe</code>, where only the data matching the given condition is affected. </p><p>Since we're purging this data altogether, stating  <code>my_dataframe = my_dataframe[CONDITION]</code> is an easy (albeit destructive) method for shedding data and moving on with our lives.</p>","url":"https://hackersandslackers.com/pandas-dataframe-drop/","uuid":"6f57d667-6bab-4d97-a62a-adfb2e887d6c","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a151770ade7aa41676efce7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673632","title":"Merge Sets of Data in Python Using Pandas","slug":"merge-dataframes-with-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","excerpt":"Perform SQL-like merges of data using Python's Pandas.","custom_excerpt":"Perform SQL-like merges of data using Python's Pandas.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"26 December, 2018","created_at":"2017-11-17T19:09:32.000-05:00","published_at":"2017-11-17T19:22:25.000-05:00","updated_at":"2018-12-26T04:29:22.000-05:00","meta_title":"Merging Dataframes with Pandas | Hackers and Slackers","meta_description":"Perform merges of data similar to SQL JOINs using Python's Pandas library: the essential library for data analysis in Oython. ","og_description":"Perform SQL-like merges of data using Python's Pandas","og_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","og_title":"Merging Dataframes with Pandas","twitter_description":"Perform SQL-like merges of data using Python's Pandas","twitter_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","twitter_title":"Merging Dataframes with Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you have two obscenely large sets of data. \n\nThese sets of data contain information on a similar topic, such as customers. \nDataset #1 might contain a high-level view of all customers of a business, while\n Datatset #2  contains a lifetime history of orders for a company.\nUnsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any\nbusiness' orders are made by customers.\n\nWelcome to Relational Databases\nWhat we just described is the core foundation for relational databases  which\nhave been running at the core of businesses since the 1970s. Starting with\nfamiliar names like MySQL,  Oracle, and Postgres,  the concept of maintaining\nmultiple -related- tables of data are the bare minimum technology stack for any\ncompany, regardless of what said company does.\n\nWhile our example of Datasets #1 and #2  can be thought of as isolated tables,\nthe process of 'joining' them (in SQL terms) or 'merging' them (in Pandas terms) \n is  trivial. What's more, we can do far more than with JOINS (or merges) than\nsimply combining our data into a single set.\n\nEnter The Panda\nPython happens to have an obscenely popular library for performing SQL-like\nlogic, dubbed Pandas. If it remains unclear as to what Pandas is, just remember:\n Databases are basically Excel spreadsheets are basically an interface for\nPandas. The technicality of that explanation may be horrendous to those who\nunderstand the differences, but the fundamental truth remains: we're dealing\nwith information, inside of cells, on a two-dimensional grid. When you hear the\nnext idiot spew a catch phrase like \"data is the new oil\", the \"data\" they're\nreferring to is akin to that sick Excel sheet you made at work.\n\nScenario: Finding Mismatches in Data\nThis scenario actually stems from a real-life example which, sure enough, was my\nfirst encounter with Pandas. One could argue I owe much 0f my data career to a\n3am Google Hangout with Snkia.\n\nIn our scenario, our company has signed up for a very expensive software product\nwhich charges by individual license. To our surprise, the number of licenses for\nthis software totaled over 1000  seats!  After giving this data a quick glance,\nhowever, it's clear that many of these employees have actually been terminated,\nthus resulting in unspeakable loss in revenue. \n\nThe good news is we have another dataset called active employees (aka: employees\nwhich have not been terminated... yet). So, how do we use these two sets of data\nto determine which software licenses are valid? First, let's look at the types \nof ways we can merge data in Pandas.\n\nTerminology\nMERGE\nSets of data can be merged in a number of ways. Merges can either be used to\nfind similarities in two Dataframes and merge associated information, or may be\nentirely non-destructive in the way that two sets of data are merged.\n\nKEY\nIn many cases (such as the one in this tutorial) you'd likely want to merge two\nDataframes based on the value of a key. A key is the authoritative column by\nwhich the Dataframes will be merged. When merging Dataframes in this way, keys\nwill stay in tact as an identifier while the values of columns in the same row\nassociated to that key.\n\nThis type of merge can be used when two Dataframes hold differing fields for\nsimilar rows. If Dataframe 1 contains the phone numbers of customers by name,\nand Dataframe 2 contains emails of a similar grouping of people, these two may\nbe merged to create a single collection of data with all of this information.\n\nAXIS\nA parameter of pandas functions which determines whether the function should be\nrun against a Dataframe's columns or rows. An axis of 0 determines that the\naction will be taken on a per-row basis, where an axis of 1 denotes column.\n\nFor example, performing a drop with axis 0 on key X will drop the row where\nvalue of a cell is equal to X.\n\nLEFT/RIGHT MERGE\nAn example of a left/right merge can be seen below:\n\nJoin on keys found in left Dataframe.The two data frames above hold similar keys\nwith different associated information per axis, thus the result is a combination\nof these two Dataframes where the keys remain intact.\n\n\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from\nboth Dataframes do not match 1-to-1, specifying a left/right merge determines\nwhich Dataframe's keys will be considered the authority to be preserved in the\nmerge.\n\nJoin on keys found in right Dataframe.INNER MERGE\nAn inner merge will merge two Dataframes based on overlap of values between keys\nin both Dataframes:\n\nJoin on keys found in right Dataframe.OUTER MERGE\nAn outer merge will preserve the most data by not dropping keys which are\nuncommon to both Dataframes. Keys which exist in a single Dataframe will be\nadded to the resulting Dataframe, with empty values populated for any columns\nbrought in by the other Dataframe:\n\nBack to our Scenario: Merging Two Dataframes via Left Merge\nLet's get it going. Enter the iPython shell.\n\nImport Pandas and read both of your CSV files.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"csv1.csv\")  \ndf2 = pd.read_csv(\"csv2.csv\")\n\n\nThe above opens the CSVs as Dataframes recognizable by pandas.\nNext, we'll merge the two CSV files.\n\nHow  specifies the type of merge, and on  specifies the column to merge by\n(key). The key must be present in both Dataframes.\n\nFor the purpose of this exercise we'll be merging left, as that is the CSV which\ncontains the keys we'd like to maintain.\n\nmergedDF = df2.merge(df, how=“left”, on=\"email\")\n\nprint(mergedDF)\n\n\nThis should return a dataset of all common rows, with columns from both CSVs\nincluded in the merge.\n\nScenario 2: Missing Data\nBefore we go, let's toss in another scenario for good measure.\n\nThis time around we have two datasets which should actually probably be a single\ndataset. Dataset #1  contains all customers once again, but for some reason, \nDataset #1  contains email address where set Dataset #2  does not. Similarly, \nDataset #2  contains addresses which are  missing in Dataset #1. We assume there\nis no reason to keep these sets of data isolated other than human error.\n\nIn the case where we are confident that employees exist in both datasets but\ncontain different information, performing an inner  merge will join these two\nsets by a key such as customer ID or email. If all goes well, the final dataset\nshould equal the same number of rows found in both Datasets #1 and #2.\n\nDocumentation\nFor more on merging, check out the official Pandas documentation here\n[https://pandas.pydata.org/pandas-docs/stable/merging.html].","html":"<style>\n    img {\n        border: 0 !important;\n    }\n </style>   <p>Let's say you have two obscenely large sets of data. </p><p>These sets of data contain information on a similar topic, such as customers. <strong>Dataset #1 </strong>might contain a high-level view of all customers of a business, while <strong>Datatset #2</strong> contains a lifetime history of orders for a company. Unsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any business' orders are made by customers.</p><h2 id=\"welcome-to-relational-databases\">Welcome to Relational Databases</h2><p>What we just described is the core foundation for <em>relational databases</em> which have been running at the core of businesses since the 1970s. Starting with familiar names like <strong>MySQL</strong>,<strong> Oracle</strong>, and <strong>Postgres,</strong> the concept of maintaining multiple -<em>related- </em>tables of data are the bare minimum technology stack for any company, regardless of what said company does.</p><p>While our example of <strong>Datasets #1 and #2</strong> can be thought of as isolated tables, the process of 'joining' them <em>(in SQL terms) </em>or 'merging' them <em>(in Pandas terms)</em> is  trivial. What's more, we can do far more than with JOINS (or merges) than simply combining our data into a single set.</p><h3 id=\"enter-the-panda\">Enter The Panda</h3><p>Python happens to have an obscenely popular library for performing SQL-like logic, dubbed <strong>Pandas. </strong>If it remains unclear as to what Pandas is, just remember: <em>Databases are basically Excel spreadsheets are basically an interface for Pandas</em>. The technicality of that explanation may be horrendous to those who understand the differences, but the fundamental truth remains: we're dealing with information, inside of cells, on a two-dimensional grid. When you hear the next idiot spew a catch phrase like <em>\"data is the new oil\"</em>, the \"data\" they're referring to is akin to that sick Excel sheet you made at work.</p><h3 id=\"scenario-finding-mismatches-in-data\">Scenario: Finding Mismatches in Data</h3><p>This scenario actually stems from a real-life example which, sure enough, was my first encounter with Pandas. One could argue I owe much 0f my data career to a 3am Google Hangout with Snkia.</p><p>In our scenario, our company has signed up for a very expensive software product which charges by individual license. To our surprise, the number of licenses for this software totaled <strong>over 1000</strong> seats!<strong> </strong>After giving this data a quick glance, however, it's clear that many of these employees have actually been terminated, thus resulting in unspeakable loss in revenue. </p><p>The good news is we have another dataset called <em>active employees </em>(aka: employees which have not been terminated... yet). So, how do we use these two sets of data to determine which software licenses are valid? First, let's look at the <em>types</em> of ways we can merge data in Pandas.</p><h2 id=\"terminology\">Terminology</h2><h3 id=\"merge\">MERGE</h3><p>Sets of data can be merged in a number of ways. Merges can either be used to find similarities in two Dataframes and merge associated information, or may be entirely non-destructive in the way that two sets of data are merged.</p><h3 id=\"key\">KEY</h3><p>In many cases (such as the one in this tutorial) you'd likely want to merge two Dataframes based on the value of a key. A key is the authoritative column by which the Dataframes will be merged. When merging Dataframes in this way, keys will stay in tact as an identifier while the values of columns in the same row associated to that key.</p><p>This type of merge can be used when two Dataframes hold differing fields for similar rows. If Dataframe 1 contains the phone numbers of customers by name, and Dataframe 2 contains emails of a similar grouping of people, these two may be merged to create a single collection of data with all of this information.</p><h3 id=\"axis\">AXIS</h3><p>A parameter of pandas functions which determines whether the function should be run against a Dataframe's columns or rows. An axis of 0 determines that the action will be taken on a per-row basis, where an axis of 1 denotes column.</p><p>For example, performing a drop with axis 0 on key X will drop the row where value of a cell is equal to X.</p><h3 id=\"left-right-merge\">LEFT/RIGHT MERGE</h3><p>An example of a left/right merge can be seen below:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasleftjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in left Dataframe.</figcaption></figure><p>The two data frames above hold similar keys with different associated information per axis, thus the result is a combination of these two Dataframes where the keys remain intact.</p><p>\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from both Dataframes do not match 1-to-1, specifying a left/right merge determines which Dataframe's keys will be considered the authority to be preserved in the merge.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasrightjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"inner-merge\">INNER MERGE</h3><p>An inner merge will merge two Dataframes based on overlap of values between keys in both Dataframes:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasinnerjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"outer-merge\">OUTER MERGE</h3><p>An outer merge will preserve the most data by not dropping keys which are uncommon to both Dataframes. Keys which exist in a single Dataframe will be added to the resulting Dataframe, with empty values populated for any columns brought in by the other Dataframe:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasouterjoin.png\" class=\"kg-image\"></figure><h2 id=\"back-to-our-scenario-merging-two-dataframes-via-left-merge\">Back to our Scenario: Merging Two Dataframes via Left Merge</h2><p>Let's get it going. Enter the iPython shell.</p><p>Import Pandas and read both of your CSV files.</p><pre><code class=\"language-python\">import pandas as pd\n\ndf = pd.read_csv(&quot;csv1.csv&quot;)  \ndf2 = pd.read_csv(&quot;csv2.csv&quot;)\n</code></pre>\n<p>The above opens the CSVs as Dataframes recognizable by pandas.<br>Next, we'll merge the two CSV files.</p><p><strong>How</strong> specifies the type of merge, and <strong>on</strong> specifies the column to merge by (key). The key must be present in both Dataframes.</p><p>For the purpose of this exercise we'll be merging left, as that is the CSV which contains the keys we'd like to maintain.</p><pre><code class=\"language-python\">mergedDF = df2.merge(df, how=“left”, on=&quot;email&quot;)\n\nprint(mergedDF)\n</code></pre>\n<p>This should return a dataset of all common rows, with columns from both CSVs included in the merge.</p><h2 id=\"scenario-2-missing-data\">Scenario 2: Missing Data</h2><p>Before we go, let's toss in another scenario for good measure.</p><p>This time around we have two datasets which should actually probably be a single dataset. <strong>Dataset #1</strong> contains all customers once again, but for some reason, <strong>Dataset #1</strong> contains email address where set <strong>Dataset #2</strong> does not. Similarly, <strong>Dataset #2</strong> contains addresses which are  missing in <strong>Dataset #1</strong>. We assume there is no reason to keep these sets of data isolated other than human error.</p><p>In the case where we are confident that employees exist in both datasets but contain different information, performing an <em>inner</em> merge will join these two sets by a key such as customer ID or email. If all goes well, the final dataset should equal the same number of rows found in both <strong>Datasets #1 and #2</strong>.</p><h2 id=\"documentation\">Documentation</h2><p>For more on merging, check out the official Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/merging.html\">here</a>.</p>","url":"https://hackersandslackers.com/merge-dataframes-with-pandas/","uuid":"d2c59476-d879-484c-b719-55f53b3d4980","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f7a3ce38d612cc8261316"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867363f","title":"Another \"Intro to Data Analysis in Python Using Pandas\" Post","slug":"intro-to-data-analysis-in-python-using-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","custom_excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","created_at_pretty":"19 April, 2018","published_at_pretty":"16 November, 2017","updated_at_pretty":"10 April, 2019","created_at":"2018-04-18T21:18:27.000-04:00","published_at":"2017-11-16T10:52:00.000-05:00","updated_at":"2019-04-10T10:33:17.000-04:00","meta_title":"Intro to Data Analysis in Python Using Pandas | Hackers and Slackers","meta_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","og_title":"Intro to Data Analysis in Python Using Pandas","twitter_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","twitter_title":"Intro to Data Analysis in Python Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"}],"plaintext":"Let’s face it: the last thing the world needs is another “Intro to Pandas” post.\nAnybody strange enough to read this blog surely had the same reaction to\ndiscovering Pandas as I did: a manic euphoria that can only be described as love\nat first sight. We wanted to tell the world, and that we did. A lot. Yet here I\nam, about to helplessly sing cliche praises one more time. \n\nI’m a prisoner of circumstance here. As it turns out, the vast (and I mean vast)\nmajority of our fans have a raging Pandas addiction. They come to our humble\nmom-and-pop shop here at Hackers and Slackers foaming at the mouth, going on\nraging benders for all Pandas-related content. If I had half a brain, I’d rename\nthis site Pandas and Pandas  and delete all non-Pandas-related content. Talk\nabout cash money. \n\nAs a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro\nto Pandas” post was an unflattering belligerent mess jotted into a Confluence\ninstance long ago during a Friday night  pregame. That mess snuck its way on to\nthis blog, and has gone unnoticed for a year now. I've decided that this\nprobably wasn't the best way to open up a series about the most influential\nPython library of all time. We're going to try this again... For the Pandas.\n\nIntro to Pandas Rereleased: in IMAX 8k 4D \nPandas is used to analyze and modify tabular data in Python. When we say\n“tabular data,” we mean any instance in life where data is represented in a\ntable format. Excel, SQL databases, shitty HTML tables.... they’ve all been the\nsame thing with different syntax this whole time. Pandas can achieve anything\nthat any other table can. \n\nIf you’re reasonably green to data analysis and are experiencing the \n“oh-my-God-all-data-professions-are-kinda-just-Excel”  realization as we speak,\nfeel free to take a moment. Great, that’s behind us now.\n\nThe Anatomy of a DataFrame\nTabular data in Pandas is referred to as a “DataFrame.” We can’t call everything \n “tables-” otherwise, our choice of vague terminology would grow horribly\nconfusing when we refer to data in different systems. Between you and me though,\nDataFrames are basically tables.\n\nSo how do we represent two-dimensional data via command line: a concept which\ninherently interprets and displays information one-dimensionally? \n\n\"Oh nothing, we think it's cute.\"DataFrames consist of individual parts which\nare easy-to-understand at face value. It’s the complexity of these things\ntogether, creating a sum greater than the whole of its parts, which fuels the\nseemingly endless power of DataFrames. If we want any  hope of contributing to\nthe field of Data Science, we need to not only understand the terminology but at\nleast be aware of core concepts of what a DataFrame is beneath the hood. This\nunderstanding is what separates engineers from Excel monkeys. \n\nEngineers\n1\nWinExcel Nerds\n0\nLoseParts of a DataFrame\nWere you expecting this post just to be a bunch of one-liners in Pandas? Good, I\nhope you're disappointed. Strap yourself in, we might actually learn something\ntoday. Class is now in session, baby. Let's break apart what makes a DataFrame,\npiece-by-piece:\n\nThe most basic description of any table would be a collection of columns and \nrows. Despite looking at a two-dimensional grid, columns are in fact very\ndifferent from rows. Unlike rows, all data in a column  typically abides by the\nsame data type. In the above example, any value saved in the startTime  column\nwill always be a time. Rows, on the other hand, are simply an entry- an instance\nof a \"thing\", where each thing is described by attributes stored horizontally\nacross columns.\n\nThis seems like really elementary stuff, but I mention it for a reason. By our\ndefinition, columns  are more pivotal to structuring a table than rows, because\neven empty columns have meaning, whereas an empty row with no columns will\nalways equal infinite nothingness. Rows are made up of values in columns, not\nthe other way around. Thus, columns in Pandas are actually their own type of\nobject called a Series. \n\n * Series' are objects native to Pandas (and Numpy) which refer to\n   one-dimensional sequences of data. Another example of a one-dimensional\n   sequence of data could be an array, but series' are much more than arrays:\n   they're a class of their own for many powerful reasons, which we'll see in a\n   moment.\n * Axis  refers to the 'direction' of a series, or in other words \"column\" or\n   \"row\". A series with an axis of 0  is a row, whereas a series with an axis of\n    1  is a column. \n * A series contains labels, which are given visual names for a row/column\n   specifying labels allows us to call upon any labeled series in the same way\n   we would access a value in a Python dictionary. For instance, accessing \n   dataframe['awayTeamName']  returns the entire column matching the header \n   \"awayTeamName\".\n * Every row and column has a numerical index. Most of the time, a row's label \n   will be equivalent to the row's index. While it's common practice to define\n   headers for columns, columns have indexes as well, which simply aren't shown.\n   In this regard, Series share an attribute with lists/arrays, in that they are\n   a collection of indexed values\n\nConsider the last two points: we just described a series to work the same way as\na Python dictionary, but also the same way as a Python list. That's right:\nseries' objects are like the biracial offspring of lists and dicts. We can\naccess any column by either its name or its index, and the same goes for rows.\nEven if we rip a column out from a DataFrame, each cell in that series will\nstill retain the row labels for every cell. This means we can say things like \nget me column #3, and then find me the value for whatever was in the row labeled\n\"Y\".  Of course, this works in the reverse as well. It's crazy how things get\nexponentially more powerful and complicated when we add entire dimensions, isn't\nit?\n\nLoading Data Into Pandas\nIf you've made it this far, you've earned the right to start getting hands-on.\nLuckily, Pandas has plenty of methods to load tabular data into DataFrames,\nregardless if you're using static files, SQL, or quirkier methods, Pandas has\nyou covered. Here are some of my favorite examples:\n\nimport pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n\n\nAll of these achieve the same result of creating a DataFrame. No matter what\nhorrible data sources you may have been forced to inherit, Pandas is here to\nhelp. Pandas knows our pain. Pandas is love. Pandas is life.\n\nWith data loaded, let's see how we can apply our new knowledge of series'  to\ninteract with out data.\n\nFinding Data in Our Dataframe\nPandas has a method for finding a series by label, as well as a separate method\nfor finding a series by index. These methods are .iloc  and .loc, respectively.\nLet's say our DataFrame from the example above is stored as a variable named \nbaseball_df. To get the values of a column by name, we would do the following:\n\nbaseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n\n\nThis would return the following:\n\n0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n\n\nThat's our column! We can see the row labels being listed alongside each row's\nvalue. Told ya so. Getting a column will also return the column's dtype, or data\ntype. Data types can be set on columns explicitly. If they aren't, Pandas will\ngenerally either default to detecting that the data in the column is a float \n(returned for any column which only holds numerical values, despite number of\ndecimal points) or an 'object', which is a fancy catch-all meaning \"fuck if I\nknow, there's letters and shit in there, it could be anything probably.\" Pandas\ndoesn't try hard on its own to discern the types of data in each field.\n\nIf you're thinking ahead, you might see a looming conflict of interest with iloc\n. Since we've established that columns and rows are the same, and we're\naccessing series' based on criteria that is met by both  columns and rows (every\ntable has a first row and a first column), how does Pandas know what we want\nwith .loc()? Short answer: It doesn't, so it just returns both! \n\nbaseball_df = baseball_df.loc[3]\nprint(baseball_df)\n\n\n    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n\n\nAhhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a\nclever, intentional way. \"Clever and intentional\"  is actually a great way to\ndescribe Pandas as a library. This combination of ease and power is what makes\nPandas so magnetic to curious newcomers. \n\nWant another example? How about leveraging the unique attributes of series'  to\nsplice DataFrames as though they were arrays?\n\nsliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n\n\n    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n\n\n...Did we just do that? We totally did. We were able to slice a two-dimensional\nset of data by using the same syntax that we'd used to slice arrays, thanks to\nthe power of the series object.\n\nWelcome to the Club\nThere are a lot more entertaining, mind-blowing ways to introduce people to\nPandas. If our goal had been sheer amusement, we would have leveraged the\ncookie-cutter route to Pandas tutorials: overloading readers with Pandas\n\"tricks\" displaying immense power in minimal effort. Unfortunately, we took the\napplicable approach to actually retaining information. Surely this model of\n\"informational and time consuming\" will beat out \"useless but instantly\ngratifying,\" right? RIGHT? \n\nWhatever. I’ll schedule the Pandas and Pandas rebrand for next week. From now on\nwhen people want that quick fix, you can call me Pablo Escobar. Join us next\ntime when we use Pandas data analysis to determine which private Caribbean\nisland offers the best return on investment with all the filthy money we’ll\nmake.\n\nHint: it’s definitely not the Fyre festival one.","html":"<p>Let’s face it: the last thing the world needs is another “<strong>Intro to Pandas</strong>” post. Anybody strange enough to read this blog surely had the same reaction to discovering Pandas as I did: a manic euphoria that can only be described as love at first sight. We wanted to tell the world, and that we did. A lot. Yet here I am, about to helplessly sing cliche praises one more time. </p><p>I’m a prisoner of circumstance here. As it turns out, the vast (and I mean <em><strong>vast</strong></em>) majority of our fans have a raging Pandas addiction. They come to our humble mom-and-pop shop here at <strong>Hackers and Slackers </strong>foaming at the mouth, going on raging benders for all Pandas-related content. If I had half a brain, I’d rename this site <strong>Pandas and Pandas</strong> and delete all non-Pandas-related content. Talk about cash money. </p><p>As a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro to Pandas” post was an unflattering belligerent mess jotted into a Confluence instance long ago during a <a>Friday night</a> pregame. That mess snuck its way on to this blog, and has gone unnoticed for a year now. I've decided that this probably wasn't the best way to open up a series about the most influential Python library of all time. We're going to try this again... For the Pandas.</p><h2 id=\"intro-to-pandas-rereleased-in-imax-8k-4d\">Intro to Pandas Rereleased: in IMAX 8k 4D </h2><p>Pandas is used to analyze and modify tabular data in Python. When we say “tabular data,” we mean any instance in life where data is represented in a table format. Excel, SQL databases, shitty HTML tables.... they’ve all been the same thing with different syntax this whole time. Pandas can achieve anything that any other table can. </p><p>If you’re reasonably green to data analysis and are experiencing the <em>“oh-my-God-all-data-professions-are-kinda-just-Excel”</em> realization as we speak, feel free to take a moment. Great, that’s behind us now.</p><h2 id=\"the-anatomy-of-a-dataframe\">The Anatomy of a DataFrame</h2><p>Tabular data in Pandas is referred to as a “DataFrame.” We can’t call <em>everything</em> “tables-” otherwise, our choice of vague terminology would grow horribly confusing when we refer to data in different systems. Between you and me though, DataFrames are basically tables.</p><p>So how do we represent two-dimensional data via command line: a concept which inherently interprets and displays information one-dimensionally? </p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/moon.jpg\" class=\"kg-image\"><figcaption>\"Oh nothing, we think it's cute.\"</figcaption></figure><!--kg-card-end: image--><p>DataFrames consist of individual parts which are easy-to-understand at face value. It’s the complexity of these things together, creating a sum greater than the whole of its parts, which fuels the seemingly endless power of DataFrames. If we want <em>any</em> hope of contributing to the field of Data Science, we need to not only understand the terminology but <em>at least </em>be aware of core concepts of what a DataFrame is beneath the hood. This understanding is what separates engineers from Excel monkeys. </p><!--kg-card-begin: html--><div class=\"scoreboard\">\n\t<!-- Score Header -->\n\t<div class=\"score-header\">\n\t\t<!-- Background -->\n\t\t<div class=\"score-header-background\">\n\t\t\t<div class=\"score-header-background__left\"></div>\n\t\t\t<div class=\"score-header-background__right\"></div>\n\t\t\t<div class=\"score-header-background__logo\"></div>\n\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"score-header-foreground\">\n\t\t\t<div class=\"score-header-foreground__left\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Engineers</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">1</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Win</span>\n\t\t\t</div>\n\t\t\t<div class=\"score-header-foreground__right\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Excel Nerds</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">0</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Lose</span>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</div><!--kg-card-end: html--><h2 id=\"parts-of-a-dataframe\">Parts of a DataFrame</h2><p>Were you expecting this post just to be a bunch of one-liners in Pandas? Good, I hope you're disappointed. Strap yourself in, we might actually learn something today. Class is now in session, baby. Let's break apart what makes a DataFrame, piece-by-piece:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/03/dataframe2.jpg\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>The most basic description of any table would be a collection of <strong>columns </strong>and <strong>rows. </strong>Despite looking at a two-dimensional grid, columns are in fact very different from rows. Unlike rows, all data in a <em>column</em> typically abides by the same data type. In the above example, any value saved in the <strong>startTime</strong> column will always be a <strong>time</strong>. Rows, on the other hand, are simply an entry- an instance of a \"thing\", where each thing is described by attributes stored horizontally across columns.</p><p>This seems like really elementary stuff, but I mention it for a reason. By our definition, <em>columns</em> are more pivotal to structuring a table than rows, because even empty columns have meaning, whereas an empty row with no columns will always equal infinite nothingness. <strong>Rows are made up of values in columns, not the other way around. </strong> Thus, columns in Pandas are actually their own type of object called a <strong>Series</strong>. </p><ul><li><strong>Series</strong>' are objects native to Pandas (and Numpy) which refer to one-dimensional sequences of data. Another example of a one-dimensional sequence of data could be an <strong><em>array</em></strong><em>, </em>but series' are much more than arrays: they're a class of their own for many powerful reasons, which we'll see in a moment.</li><li><strong>Axis</strong> refers to the 'direction' of a series, or in other words \"column\" or \"row\". A series with an axis of <code>0</code> is a <em>row</em>, whereas a series with an axis of <code>1</code> is a <em>column</em>. </li><li>A series contains <strong>labels</strong>, which are given visual names for a row/column specifying labels allows us to call upon any labeled series in the same way we would access a value in a Python dictionary. For instance, accessing <code>dataframe['awayTeamName']</code> returns the entire column matching the header <em>\"awayTeamName\"</em>.</li><li>Every row and column has a numerical <strong>index. </strong>Most of the time, a row's <strong>label</strong> will be equivalent to the row's <strong>index. </strong>While it's common practice to define headers for columns, columns have indexes as well, which simply aren't shown. In this regard, Series share an attribute with lists/arrays, in that they are a collection of indexed values</li></ul><p>Consider the last two points: we just described a series to work the same way as a Python dictionary, but also the same way as a Python list. That's right: series' objects are like the biracial offspring of lists and dicts. We can access any column by either its name or its index, and the same goes for rows. Even if we rip a column out from a DataFrame, each cell in that series will still retain the row labels for every cell. This means we can say things like <strong>get me column #3, and then find me the value for whatever was in the row labeled \"Y\".</strong> Of course, this works in the reverse as well. It's crazy how things get exponentially more powerful and complicated when we add entire dimensions, isn't it?</p><h2 id=\"loading-data-into-pandas\">Loading Data Into Pandas</h2><p>If you've made it this far, you've earned the right to start getting hands-on. Luckily, Pandas has plenty of methods to load tabular data into DataFrames, regardless if you're using static files, SQL, or quirkier methods, Pandas has you covered. Here are some of my favorite examples:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n</code></pre>\n<!--kg-card-end: markdown--><p>All of these achieve the same result of creating a DataFrame. No matter what horrible data sources you may have been forced to inherit, Pandas is here to help. Pandas knows our pain. Pandas is love. Pandas is life.</p><p>With data loaded, let's see how we can apply our new knowledge of <strong>series'</strong> to interact with out data.</p><h2 id=\"finding-data-in-our-dataframe\">Finding Data in Our Dataframe</h2><p>Pandas has a method for finding a series by label, as well as a separate method for finding a series by index. These methods are <code>.iloc</code> and <code>.loc</code>, respectively. Let's say our DataFrame from the example above is stored as a variable named <code>baseball_df</code>. To get the values of a column by name, we would do the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><p>This would return the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n</code></pre>\n<!--kg-card-end: markdown--><p>That's our column! We can see the row labels being listed alongside each row's value. Told ya so. Getting a column will also return the column's <strong>dtype</strong>, or <em>data type</em>. Data types can be set on columns explicitly. If they aren't, Pandas will generally either default to detecting that the data in the column is a <strong>float</strong> (returned for any column which only holds numerical values, despite number of decimal points) or an '<strong>object'</strong>, which is a fancy catch-all meaning \"fuck if I know, there's letters and shit in there, it could be anything probably.\" Pandas doesn't try hard on its own to discern the types of data in each field.</p><p>If you're thinking ahead, you might see a looming conflict of interest with <code>iloc</code>. Since we've established that columns and rows are the same, and we're accessing series' based on criteria that is met by <em>both</em> columns and rows (every table has a first row and a first column), how does Pandas know what we want with <code>.loc()</code>? Short answer: It doesn't, so it just returns both! </p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.loc[3]\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n</code></pre>\n<!--kg-card-end: markdown--><p>Ahhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a clever, intentional way. \"<strong>Clever and intentional\"</strong> is actually a great way to describe Pandas as a library. This combination of ease and power is what makes Pandas so magnetic to curious newcomers. </p><p>Want another example? How about leveraging the unique attributes of <strong>series'</strong> to splice DataFrames as though they were arrays?</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">sliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n</code></pre>\n<!--kg-card-end: markdown--><p>...Did we just do that? We totally did. We were able to slice a two-dimensional set of data by using the same syntax that we'd used to slice arrays, thanks to the power of the series object.</p><h2 id=\"welcome-to-the-club\">Welcome to the Club</h2><p>There are a lot more entertaining, mind-blowing ways to introduce people to Pandas. If our goal had been sheer amusement, we would have leveraged the cookie-cutter route to Pandas tutorials: overloading readers with Pandas \"tricks\" displaying immense power in minimal effort. Unfortunately, we took the applicable approach to actually retaining information. Surely this model of \"informational and time consuming\" will beat out \"useless but instantly gratifying,\" right? <em>RIGHT? </em></p><p>Whatever. I’ll schedule the <strong>Pandas and Pandas </strong>rebrand for next week. From now on when people want that quick fix, you can call me Pablo Escobar. Join us next time when we use Pandas data analysis to determine which private Caribbean island offers the best return on investment with all the filthy money we’ll make.</p><p>Hint: it’s definitely not the Fyre festival one.</p>","url":"https://hackersandslackers.com/intro-to-data-analysis-in-python-using-pandas/","uuid":"828b4a6f-e6f2-446f-b51e-64fe19e05ba0","page":false,"codeinjection_foot":null,"codeinjection_head":"<style>\n  *,\n  *::before,\n  *::after {\n    box-sizing: inherit;\n  }\n\n  .scoreboard {\n    margin: auto;\n    max-width: 980px;\n    box-shadow: 0 0 10px #b0bddd;\n  }\n\n  .score-header {\n    position: relative;\n    height: 74px;\n    overflow: hidden;\n  }\n\n  .score-header-background {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: calc(100% + 63px + 4px);\n    left: -33.5px;\n  }\n\n  .score-header-background .score-header-background__left,\n  .score-header-background .score-header-background__right {\n    position: relative;\n    flex: 1 1 100%;\n    overflow: hidden;\n    border-bottom: 4px solid #fff;\n    transform: skewX(-40deg);\n  }\n\n  .score-header-background .score-header-background__left::before,\n  .score-header-background .score-header-background__right::before {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    transform: skewX(40deg);\n  }\n\n  .score-header-background .score-header-background__left::after,\n  .score-header-background .score-header-background__right::after {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    opacity: 0.35;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeAgMAAABGXkYxAAAACVBMVEUAAAD///8AAABzxoNxAAAAAnRSTlMAAHaTzTgAAAAlSURBVHhe3cmhEQAACMPAjIhhv+pOiahAsAGvchc6asMhjvdrAFlGOgM9VYUmAAAAAElFTkSuQmCC');\n  }\n\n  .score-header-background .score-header-background__left {\n    margin-right: 5px;\n    border-color: #19d9ff;\n  }\n\n  .score-header-background .score-header-background__left::before {\n    right: -31.5px;\n    background: linear-gradient(to left, #19d9ff 31.5px, #a9f6ff 60%);\n  }\n\n  .score-header-background .score-header-background__right {\n    margin-left: 5px;\n    border-color: #ff1979;\n  }\n\n  .score-header-background .score-header-background__right::before {\n    left: -31.5px;\n    background: linear-gradient(to right, #ff1979 31.5px, #ffb5ee 60%);\n  }\n\n  .score-header-background .score-header-background__logo {\n    position: absolute;\n    top: 0;\n    right: 0;\n    bottom: 0;\n    left: 0;\n    margin: auto;\n    width: 60px;\n    height: 60px;\n    border: 5px solid #ffffff;\n    border-radius: 100%;\n    background-color: rgba(111, 77, 238, 0.51);\n    background-image: url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiP…A3NDcsNTQ2LjMgDQoJCQkJCQkJIi8+DQoJCTwvZz4NCgk8L2c+DQo8L2c+DQo8L3N2Zz4NCg==);\n  }\n\n  .score-header-foreground {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: 100%;\n  }\n\n  .score-header-foreground .score-header-foreground__left,\n  .score-header-foreground .score-header-foreground__right {\n    display: flex;\n    margin: 0 20px;\n    flex: 1 1 100%;\n    align-items: baseline;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__score {\n    margin-right: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__right {\n    flex-direction: row-reverse;\n    text-align: right;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n    margin-left: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__title,\n  .score-header-foreground .score-header-foreground__score,\n  .score-header-foreground .score-header-foreground__win {\n    margin: 0 10px;\n    line-height: 69px;\n    text-transform: uppercase;\n  }\n\n  .score-header-foreground .score-header-foreground__title {\n    margin: 0;\n    flex: 1 1 auto;\n    font-size: 30px;\n  }\n\n  .score-header-foreground .score-header-foreground__score {\n    order: 1;\n    text-shadow: 0 0 4px rgba(255, 255, 255, 0.75), 0 0 8px rgba(255, 255, 255, 0.45);\n    font-size: 30px;\n    color: white;\n  }\n\n  .score-header-foreground .score-header-foreground__win {\n    font-size: 18px;\n    font-weight: 600;\n    font-family: 'TTNorms-Medium', sans-serif;\n    color: white;\n    color: #4a47a2;\n    mix-blend-mode: color-burn;\n  }\n\n  .player-list {\n    display: flex;\n    padding: 0;\n    flex-flow: column;\n    list-style: none;\n  }\n\n  .player-row {\n    display: flex;\n    margin: 20px 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player {\n    display: flex;\n    padding: 20px;\n    min-width: 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player.player--left {\n    padding-left: 20px;\n  }\n\n  .player.player--left .player__avatar {\n    margin-right: 20px;\n    border-color: #19d9ff;\n  }\n\n  .player.player--right {\n    padding-right: 20px;\n    text-align: right;\n    flex-flow: row-reverse;\n  }\n\n  .player.player--right .player__avatar {\n    margin-left: 20px;\n    border-color: #ff1979;\n  }\n\n  .player .player__avatar {\n    position: relative;\n    min-height: 60px;\n    min-width: 60px;\n    overflow: hidden;\n    border: 3px solid #fff;\n    border-radius: 100%;\n    background-color: #222;\n  }\n\n  .player .player__avatar::before {\n    content: \"\";\n    position: absolute;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    opacity: 0.1;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA8CAMAAAANIilAAAABTVBMVEU7PUP////4+Pj7+/v29vZAQkj6+vpCRErx8fL+/v5BQ0lNT1T9/f3t7u5HSU/8/Pz09PRGSE5aW2BOUFY8PkRJSlB/gITk5eWqq64+QEWPkJSVlppoam5KTFHZ2tvDw8XBwsPa29zP0NGdnqH6+/vExMVVV1z+/v/LzMxRUlidn6HKy8zFxce3uLuVlpmBgYZSVFnz8/SvsLOUlZhkZmv5+flRU1jg4OFDRUrExMepqqyWl5uAgYXMzc7r6+xZWl/AwMKenqFjZWnn6OjBwcJ5en7U1dZ+gIPNzs8/QUbi4+Smp6pSU1lUVVuLjI/j5OWwsLNpam9BQ0hLTFK7vL7p6ep0dXmIio3LzM339/fi4+OrrK5cXmI9P0Wlpqny8/PLy8xzdHh6e3/ExMZVVlzs7e2hoqU+QEbR0tO4ubyHiIzs7O1FR01bXWLz8/PjR6/UAAABIklEQVR4Xu3UxW7FMBCGUU+Sy8xFZmZmZmZmhvdfVuqi9rTprf8uIlXKtz+yNGON+Ke5uQ30NQb/JoMlozGimtopD259DWH6yCxfRG1pGX1WFQVxNSkVYbbOUHF8G8JNUuJPN3NcAeEcx5UQtjgeQ6yHeBaCM8TzOoZP6zmOC6Rzjqch3M/xHIRbOG6FcBvH7RDuMFXb6RNQXSruFlg9MWl7X0EcfZM4Al+xQYmHBNqwxCMwLpY46ygen5B4ElpzOjFDapHZQm06v+AlXmBpeUWHrq6tk13GxubWLzS0Y9JPBXb38sjE/sEh5e3o+MSeJs/8pNHFpc1nvTJIr/D1t9Hf3JJ2d4J3/0D6WSGOkwYBPXL8REjPLwwXQNifcgK72MXv0xEfs26TMDAAAAAASUVORK5CYII=');\n    background-size: cover;\n  }\n\n  .player .player__username {\n    font-size: 24px;\n    text-overflow: ellipsis;\n    overflow: hidden;\n    white-space: nowrap;\n  }\n\n  .language {\n    font-size: 24px;\n  }\n\n  .language.language--html {\n    color: #f69c24;\n  }\n\n  .language.language--css {\n    color: #299bf7;\n  }\n\n  .language.language--js {\n    color: #ffce22;\n  }\n\n  @media (max-width:600px) {\n    .score-header-foreground__title {\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n    }\n\n    .score-header-foreground__left,\n    .score-header-foreground__right {\n      margin: 10px 11px 0;\n      display: block;\n      position: relative;\n    }\n\n    .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n          right: 10px;\n    }\n\n    .score-header-foreground .score-header-foreground__left, .score-header-foreground .score-header-foreground__right {\n          margin: 0 10px;\n          display: block;\n    }\n\n    .score-header-foreground__left .score-header-foreground__title {\n      left:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground__right .score-header-foreground__title {\n      right:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground .score-header-foreground__win {\n      font-size: 12px;\n      font-weight: 600;\n      font-family: 'TTNorms-Medium', sans-serif;\n      color: white;\n      color: #4a47a2;\n      mix-blend-mode: multiply;\n      position: absolute;\n      width: fit-content;\n      bottom: 32px;\n    }\n\n    .score-header-foreground__score {\n      padding: 0;\n      display: block;\n      position: absolute;\n      top: 2px;\n      padding: 0 !important;\n      margin: 0 !important;\n      line-height: 1 !important;\n      height: fit-content;\n          top: 14px;\n    }\n\n    .score-header-foreground__win {\n      position: absolute;\n      bottom: 35px;\n      width: fit-content;\n    }\n\n    .score-header-foreground__right .score-header-foreground__win {\n          right: 36px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground__left .score-header-foreground__win {\n      left: 19px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground .score-header-foreground__title {\n      right: 10px;\n    }\n  }\n</style>","comment_id":"5ad7ee6365cd784d6288cb03"}}]}},"pageContext":{"slug":"data-analysis","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}