{"data":{"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373b","title":"Lynx Roundup, November 18th","slug":"lynx-roundup-november-18th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/lynx57-2@2x.jpg","excerpt":"Testing in Python!  Machine Learning & Signal Processing!  Optimizing Hyperparameters with a PID Controller!","custom_excerpt":"Testing in Python!  Machine Learning & Signal Processing!  Optimizing Hyperparameters with a PID Controller!","created_at_pretty":"12 November, 2018","published_at_pretty":"18 November, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-11-11T23:59:01.000-05:00","published_at":"2018-11-18T07:00:00.000-05:00","updated_at":"2019-02-14T05:01:07.000-05:00","meta_title":"Lynx Roundup, November 18th | Hackers And Slackers","meta_description":"Testing in Python!  Machine Learning & Signal Processing!  Optimizing Hyperparameters with a PID Controller! | Hackers And Slackers","og_description":"Testing in Python!  Machine Learning & Signal Processing!  Optimizing Hyperparameters with a PID Controller!","og_image":"https://hackersandslackers.com/content/images/2019/02/lynx57-2@2x.jpg","og_title":"Lynx Roundup, November 18th","twitter_description":"Testing in Python!  Machine Learning & Signal Processing!  Optimizing Hyperparameters with a PID Controller!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/lynx57-2@2x.jpg","twitter_title":"Lynx Roundup, November 18th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://institute.coop/\n\nhttps://realpython.com/python-testing/\n\nhttps://www.quora.com/What-are-the-connections-between-machine-learning-and-signal-processing\n\nhttp://www.fast.ai/2018/04/29/categorical-embeddings/\n\nhttps://twitter.com/Rainmaker1973/status/1054462683045654529\n\nhttps://twitter.com/_inside/status/1054551854972129280\n\nhttps://github.com/tensorboy/PIDOptimizer","html":"<p></p><p><a href=\"https://institute.coop/\">https://institute.coop/</a></p><p><a href=\"https://realpython.com/python-testing/\">https://realpython.com/python-testing/</a></p><p><a href=\"https://www.quora.com/What-are-the-connections-between-machine-learning-and-signal-processing\">https://www.quora.com/What-are-the-connections-between-machine-learning-and-signal-processing</a></p><p><a href=\"http://www.fast.ai/2018/04/29/categorical-embeddings/\">http://www.fast.ai/2018/04/29/categorical-embeddings/</a></p><p><a href=\"https://twitter.com/Rainmaker1973/status/1054462683045654529\">https://twitter.com/Rainmaker1973/status/1054462683045654529</a></p><p><a href=\"https://twitter.com/_inside/status/1054551854972129280\">https://twitter.com/_inside/status/1054551854972129280</a></p><p><a href=\"https://github.com/tensorboy/PIDOptimizer\">https://github.com/tensorboy/PIDOptimizer</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-18th/","uuid":"14d0e00c-3c4a-4b27-8dd6-1da3b3a384b4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5be9089574f90031d0a6170b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673738","title":"Lynx Roundup, November 17th","slug":"lynx-roundup-november-17th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/72-1-3@2x.jpg","excerpt":"Unix types!  Rust!  NLP with Gensim!","custom_excerpt":"Unix types!  Rust!  NLP with Gensim!","created_at_pretty":"12 November, 2018","published_at_pretty":"17 November, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-11-11T23:34:06.000-05:00","published_at":"2018-11-17T07:00:00.000-05:00","updated_at":"2019-02-14T05:01:53.000-05:00","meta_title":"Unix types!  Rust!  NLP with Gensim! | Hackers And Slackers","meta_description":"Unix types!  Rust!  NLP with Gensim! | Hackers And Slackers","og_description":"Unix types!  Rust!  NLP with Gensim!","og_image":"https://hackersandslackers.com/content/images/2019/02/72-1-3@2x.jpg","og_title":"Lynx Roundup, November 17th","twitter_description":"Unix types!  Rust!  NLP with Gensim!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/72-1-3@2x.jpg","twitter_title":"Lynx Roundup, November 17th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.theverge.com/2018/10/17/17990534/microsoft-amazon-letters-medium-rekognition-jedi-military\n\nhttps://medium.com/netflix-techblog/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c\n\nhttps://hackernoon.com/10-basic-tips-on-working-fast-in-unix-or-linux-terminal-5746ae42d277\n\nhttps://hackernoon.com/a-millennials-guide-to-blockchain-with-gifs-and-memes-cc565547f283\n\nhttp://thiagomarzagao.com/2015/12/07/model-persistence-without-pickles/\n\nhttps://www.fpcomplete.com/blog/2018/10/is-rust-functional\n\nhttps://www.machinelearningplus.com/nlp/gensim-tutorial/","html":"<p></p><p><a href=\"https://www.theverge.com/2018/10/17/17990534/microsoft-amazon-letters-medium-rekognition-jedi-military\">https://www.theverge.com/2018/10/17/17990534/microsoft-amazon-letters-medium-rekognition-jedi-military</a></p><p><a href=\"https://medium.com/netflix-techblog/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c\">https://medium.com/netflix-techblog/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c</a></p><p><a href=\"https://hackernoon.com/10-basic-tips-on-working-fast-in-unix-or-linux-terminal-5746ae42d277\">https://hackernoon.com/10-basic-tips-on-working-fast-in-unix-or-linux-terminal-5746ae42d277</a></p><p><a href=\"https://hackernoon.com/a-millennials-guide-to-blockchain-with-gifs-and-memes-cc565547f283\">https://hackernoon.com/a-millennials-guide-to-blockchain-with-gifs-and-memes-cc565547f283</a></p><p><a href=\"http://thiagomarzagao.com/2015/12/07/model-persistence-without-pickles/\">http://thiagomarzagao.com/2015/12/07/model-persistence-without-pickles/</a></p><p><a href=\"https://www.fpcomplete.com/blog/2018/10/is-rust-functional\">https://www.fpcomplete.com/blog/2018/10/is-rust-functional</a></p><p><a href=\"https://www.machinelearningplus.com/nlp/gensim-tutorial/\">https://www.machinelearningplus.com/nlp/gensim-tutorial/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-17th/","uuid":"af3bc2c3-c349-410c-9e2a-2f998a7304b6","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5be902be74f90031d0a616fb"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373a","title":"Lynx Roundup, November 16th","slug":"lynx-roundup-november-16th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/74-1@2x.jpg","excerpt":"Onion's Guide to Blockchain!  Python's range object!  Worker co-ops and how great they are!","custom_excerpt":"Onion's Guide to Blockchain!  Python's range object!  Worker co-ops and how great they are!","created_at_pretty":"12 November, 2018","published_at_pretty":"16 November, 2018","updated_at_pretty":"07 December, 2018","created_at":"2018-11-11T23:44:06.000-05:00","published_at":"2018-11-16T07:00:00.000-05:00","updated_at":"2018-12-07T06:21:41.000-05:00","meta_title":"Lynx Roundup, November 16th | Hackers and Slackers","meta_description":"Onion's Guide to Blockchain!  Python's range object!  Worker co-ops and how great they are!","og_description":"Onion's Guide to Blockchain!  Python's range object!  Worker co-ops and how great they are!","og_image":"https://hackersandslackers.com/content/images/lynx/74-1@2x.jpg","og_title":"Lynx Roundup, November 16th","twitter_description":"Onion's Guide to Blockchain!  Python's range object!  Worker co-ops and how great they are!","twitter_image":"https://hackersandslackers.com/content/images/lynx/74-1@2x.jpg","twitter_title":"Lynx Roundup, November 16th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://venturebeat.com/2018/10/16/github-launches-actions-to-execute-code-in-containers-and-security-alerts-for-java-and-net-projects/\n\nhttps://www.theonion.com/the-onion-s-guide-to-blockchain-technology-1829819640\n\nhttps://www.knowablemagazine.org/article/society/2018/when-courtroom-science-goes-wrong-and-how-stats-can-fix-it\n\nhttps://www.shareable.net/blog/six-operational-advantages-of-worker-cooperatives\n\nhttps://realpython.com/python-boto3-aws-s3/\n\nhttps://realpython.com/python-range/\n\nhttps://www.fastcompany.com/40572926/more-u-s-businesses-are-becoming-worker-co-ops-heres-why","html":"<p></p><p><a href=\"https://venturebeat.com/2018/10/16/github-launches-actions-to-execute-code-in-containers-and-security-alerts-for-java-and-net-projects/\">https://venturebeat.com/2018/10/16/github-launches-actions-to-execute-code-in-containers-and-security-alerts-for-java-and-net-projects/</a></p><p><a href=\"https://www.theonion.com/the-onion-s-guide-to-blockchain-technology-1829819640\">https://www.theonion.com/the-onion-s-guide-to-blockchain-technology-1829819640</a></p><p><a href=\"https://www.knowablemagazine.org/article/society/2018/when-courtroom-science-goes-wrong-and-how-stats-can-fix-it\">https://www.knowablemagazine.org/article/society/2018/when-courtroom-science-goes-wrong-and-how-stats-can-fix-it</a></p><p><a href=\"https://www.shareable.net/blog/six-operational-advantages-of-worker-cooperatives\">https://www.shareable.net/blog/six-operational-advantages-of-worker-cooperatives</a></p><p><a href=\"https://realpython.com/python-boto3-aws-s3/\">https://realpython.com/python-boto3-aws-s3/</a></p><p><a href=\"https://realpython.com/python-range/\">https://realpython.com/python-range/</a></p><p><a href=\"https://www.fastcompany.com/40572926/more-u-s-businesses-are-becoming-worker-co-ops-heres-why\">https://www.fastcompany.com/40572926/more-u-s-businesses-are-becoming-worker-co-ops-heres-why</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-16th/","uuid":"046389f6-db51-4004-b0af-b21eb1b43dd9","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be9051674f90031d0a61705"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373d","title":"MongoDB Cloud: \"Backend as a Service\" with Atlas & Stitch","slug":"mongodb-cloud-backend-as-a-service-with-atlas-and-stitch","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","custom_excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","created_at_pretty":"13 November, 2018","published_at_pretty":"15 November, 2018","updated_at_pretty":"15 February, 2019","created_at":"2018-11-13T16:05:20.000-05:00","published_at":"2018-11-15T08:00:00.000-05:00","updated_at":"2019-02-15T12:49:05.000-05:00","meta_title":"MongoDB Cloud: \"Backend as a Service\" | Hackers and Slackers","meta_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","og_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","twitter_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","twitter_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Unless you've been living under a rock (or only visit this site via work-related\nGoogle Searches, like most people) you've probably heard me drone on here and\nthere about MongoDB Atlas  and MongoDB Stitch. I even went so far as to hack\ntogether an awful workflow that somehow utilized Tableau as an ETL tool to feed\nJIRA information into Mongo. I'd like to formally apologize for that entire\nseries: I can't imagine there's a single soul on this planet interested in\nlearning about all of those things simultaneously. Such hobbies reserved for\nmasochists with blogging addictions. I apologize. Let's start over.\n\nFirst off, this is not a tutorial on how to use MongoDB: the database. I have\nzero interest cluttering the internet by reiterating what a MEAN stack is for\nthe ten thousandth time, nor will I bore you with core NoSQL concepts you\nalready understand. I'm here to talk about the giant on the horizon we didn't\nsee coming, where MongoDB the database decided to become MongoDB Inc\n[https://en.wikipedia.org/wiki/MongoDB_Inc.]:  the enterprise cloud provider.\nThe same MongoDB that recently purchased mLab\n[https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab]\n, the other  cloud-hosted solution for Mongo databases. MongoDB the company is\nbold enough to place its bets on building a cloud far  simpler and restricted\nthan either AWS or GCloud. The core of that bet implies that most of us aren't\nexactly building unicorn products as much as we're reinventing the wheel: and\nthey're probably right.\n\nWelcome to our series on MongoDB cloud, where we break down every service\nMongoDB has to offer; one by one.\n\nWhat is MongoDB Cloud, and Does it Exist?\nWhat I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual\nname of the suite MongoDB offers) is actually two products:\n\n * MongoDB Atlas: A cloud-hosted MongoDB cluster with a beefy set of features.\n   Real-time dashboards, high-availability, security features,  an awesome\n   desktop client, and a CLI to top it all off.\n * MongoDB Stitch: A group of services designed to interact with Atlas in every\n   conceivable way, including creating endpoints, triggers, user authentication\n   flows, serverless functions, and a UI to handle all of this.\n\nI'm spying on you and every query you make.Atlas as a Standalone Database\nThere are plenty of people who simply want an instance of MongoDB hosted in the\ncloud as-is: just ask the guys at mLab. This was in fact how I got pulled into\nMongo's cloud myself.\n\nMongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo,\nwhich Mongo itself is confident in by offering a free tier\n[https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/]  of Atlas to\nprospective buyers. If you're a company or enterprise, the phrases High\nAvailability, Horizontal Scalability, relatively Higher Performance  will\nprobably be enough for you. But for us hobbyists, why pay for a Mongo cloud\ninstance?\n\nMongo themselves gives this comparison:\n\nOverview\n MongoDB Atlas\n Compose\n ObjectRocket\n Free Tier\n Yes\nStorage: 512 MB\nRAM: Variable\n No\n30-day free trial\n No\n30-day free trial\n Live migration\n Yes\nNo\nNo\nChoice of cloud providers\n AWS, Azure & GCP\n AWS, Softlayer & GCP\nAvailable in 2 regions for each provider\n Rackspace\n Choice of instance configuration\n Yes\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n Availability of latest MongoDB version\n Yes\nNew versions of the database are available on MongoDB Atlas as soon as they are\nreleased\n No\nNew versions typically available 1-2 quarters following database release\nNo\nNew versions typically available 1-2 quarters following database release\nReplica Set Configuration\n Up to 7 replicas\nAll replicas configured as data-bearing nodes\n 3 data-bearing nodes\nOne of the data-bearing nodes is hidden and used for backups only\n 3 data-bearing nodes\nAutomatic Sharding Support\n Yes\nNo\nYes\nData explorer\n Yes\nYes\nNo\nSQL-based BI Connectivity\n Yes\nNo\nNo\nPause and resume clusters\n Yes\nNo\nNo\nDatabase supported in on-premise deployments\n Yes\nMongoDB Enterprise Advanced [/products/mongodb-enterprise-advanced]\n No\nNo\nGlobal writes Low-latency writes from anywhere in the world Yes\n No\n No\n Cross-region replication Distribute data around the world for multi-region\nfault tolerance and local reads Yes\n No\n No\n Monitoring of database health with automated alerting\n Yes\nMongoDB Atlas UI & support for APM platforms (New Relic)\n Yes\nNew Relic\n Yes\nNew Relic\n Continuous backup\n Yes\nBackups maintained\nseconds behind production cluster\n No\nBackups taken with mongodump against hidden replica set member\n No\nBackups taken with mongodump\n Queryable backups\n Yes\nNo\nNo\nAutomated & consistent snapshots of sharded clusters\n Yes\nNot Applicable\nNo support for auto-sharding\n No\nRequires manually coordinating the recovery of mongodumps across shards\n Access control & IP whitelisting\n Yes\nYes\nYes\nAWS VPC Peering\n Yes\nBeta Release\nYes\nAdditional Charge\n Encryption of data in-flight\n Yes\nTLS/SSL as standard\n Yes\nYes\nEncryption of data at-rest\n Yes\nAvailable for AWS deployments; always on with Azure and GCP\n No\nYes\nAvailable only with specific pricing plans and data centers\n LDAP Integration\n Yes\n No\nNo\n Database-level auditing\nTrack DDL, DML, DCL operations\n Yes\n No\nNo\n Bring your own KMS\n Yes\n No\nNo\n Realistically there are probably only a number of items that stand out on the\ncomparison list when we go strictly database-to-database. Freedom over instance\nconfiguration sounds great, but in practice is more similar to putting a cap on\nhow much MongoDB decides to charge you that month (by the way, it's usually a\nlot; keep this mind). Having the Latest Version  seems great, but this can just\nas easily mean breaking production unannounced as much as it means new features.\n\nMongoDB clearly wins over the enterprise space with Continuous & queryable\nbackups, integration with LDAP, and automatic sharding support. Truthfully if\nthis were merely a database-level feature and cost comparison, the decision to\ngo with  MongoDB Atlas  would come down to how much you like their pretty\ndesktop interface:\n\nA perfectly legitimate reason to pay up, imho.So let's say MongoDB Atlas is\nmarginally better than a competitor in the confined realm of \"being a database.\"\nAre Stitch microservices enough to justify keeping your instance with the\nMongoDB team?\n\nService-by-Service Breakdown of Stitch\nStitch is kind of like if AWS exited in an alternative universe, where JSON and\nJavaScript were earth's only technologies. Thinking back to how we create APIs\nin AWS, the status quo almost always involves spinning up a Dynamo  (NoSQL)\ndatabase to put behind Lambda functions, accessible by API Gateway endpoints.\nStitch's core use case revolves around this use-case of end-user-accessing-data,\nwith a number of services dedicated specifically to supporting or improving this\nflow. The closest comparison to Stitch would be GCloud's Firebase. \n\nSo what makes Stitch so special?\n\nService 1: Querying Atlas Securely via Frontend Code\nSomething that cannot be understated is the ability to query Atlas via frontend\nJavascript. We're not passing API keys, Secrets, or any sort of nonsense;\nbecause you're configured things correctly, whitelisted domains can run queries\nof any complexity without ever interacting with an app's backend.  This is not a\ncrazy use case: consider this blog for example, or more so lately, mobile\napplications:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n<script>\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('<DATABASE>');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => \n    db.collection('<COLLECTION>').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() => \n    db.collection('<COLLECTION>').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs => {\n      console.log(\"Found docs\", docs)\n      console.log(\"[MongoDB Stitch] Connected to Stitch\")\n  }).catch(err => {\n    console.error(err)\n  });\n</script>\n\n\nThis isn't to say we're allowing any user to query any data all willy-nilly just\nbecause they're on our whitelisted IP: all data stored in Atlas is restricted to\nspecified Users  by defining User Roles. Joe Schmoe can't just inject a query\ninto any presumed database and wreak havoc, because Joe Schmoe can only access\ndata we've permitted his user account to view or write to. What is this \"user\naccount\" you ask? This brings us to the next big feature...\n\nService 2: End-User Account Creation & Management\nStitch will handle user account creation for you without the boilerplate.\nCreating an app with user accounts is a huge pain in the ass. Cheeky phrases\nlike 'Do the OAuth Dance'  can't ever hope to minimize the agonizing repetitive\npain of creating user accounts or managing relationships between users and data\n(can user X  see a comment from user Y?). Stitch allows most of the intolerably\nbenign logic behind these features to be handled via a UI.\n\nIt would be a far cry to say these processes have been \"trivialized\", but the\ntime saved is perhaps just enough to keep a coding hobbyist interested in their\nside projects as opposed to giving up and playing Rocket League.\n\nAs far as the permissions to read comments go... well, here's a self-explanatory\nscreenshot of how Stitch handles read/write document permission in its simplest\nform:\n\nOwners of comments can write their comments. Everybody else reads. Seems simple.\nService 3: Serverless Functions\nStitch functions are akin to AWS Lambda functions, but much easier to configure\nfor cross-service integration (and also limited to JavaScript ECMA 2015 or\nsomething). Functions benefit from the previous two features, in that they too\ncan be triggered from a whitelisted app's frontend, and are governed by a simple\n\"rules\" system, eliminating the need for security group configurations etc.\n\nThis is what calling a function from an app's frontend looks like:\n\n<script>\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n     client.callFunction(\"numCards\", [\"In Progress\"]).then(results => {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n</script>\n\n\nFunctions can run any query against Atlas, retrieve values  (such as environment\nvariables), and even call other functions. Functions can also be fired by\ndatabase triggers,  where a change to a collection will prompt an action such as\nan alert.\n\nService 4: HTTP Webhooks\nWebhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to\none another in that they are one-off URLs to perform single tasks. We could\nnever build a well-designed API using Stitch Webhooks, as we could with API\nGateway; this simply isn't the niche MongoDB is trying to hit (the opposite, in\nfact). \n\nConfiguration for a single Webhook.This form with a mere 6 fields clearly\nillustrates what Stitch intends to do: trivializing the creation of\ntraditionally non-trivial features.\n\nService 5: Storing 'Values' in Stitch\nA \"value\" is equivalent to an environment variable. These can be used to store\nAPI keys, secrets, or whatever. Of course, values are retrieved via functions.\n\nShhh, it's a secret ;)Service 6+: A Bunch of Mostly Bloated Extras\nFinally, Stitch has thrown in a few third-party integrations for good measure.\nSome integrations like S3 Integration could definitely come in handy, but it's\nworth asking why Mongo constantly over advertises their integrations with Github \n and Twilio. We've already established that we can create endpoints which accept\ninformation, and we can make functions which GET  information... so isn't\nanything with an API pretty easy to 'integrate' with?\n\nThis isn't to say the extra services aren't useful, they just seem a bit... odd.\nIt feels a lot like bloating the catalog, but the catalog isn't nearly bloated\nenough where it feels normal (like Heroku add-ons, for example). The choice to\nlaunch Stitch with a handful of barely-useful integrations only comes off as\nmore and more aimless as time passes; as months turn to years and no additions\nor updates are made to service offerings, it's worth questioning what the vision\nhad been for the product in the first place. In my experience, feature sets like\nthese happen when Product Managers are more powerful than they are useful.\n\nThe Breathtaking Climax: Is Stitch Worth It?\nI've been utilizing Stitch to fill in the blanks in development for months now,\nperhaps nearly a year. Each time I find myself working with Stitch or looking at\nthe bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive\ntoy with an infuriating lack of accurate documentation.\n\n  Stitch is very much a copy-and-paste-cookie-cutter-code  type of product,\nwhich begs the question of why their tutorials are recklessly outdated;\nsometimes to the point where MongoDB's own tutorial source code doesn't work. \nThere are so many use cases and potential benefits to Stitch, so why is the \nGithub repo [https://github.com/mongodb/stitch-examples]  containing example\ncode snippets so unmaintained, and painfully irrelevant? Lastly, why am I\nselling this product harder than their own internal team?\n\nStitch is a good product with a lot of unfortunate oversight. That said, Google\nFirebase still doesn't even have an \"import data\" feature, so I suppose it's\ntime to dig deep into this vendor lock and write a 5-post series about it before\nSilicon Valley's best and brightest get their shit together enough to actually\ncreate something useful and intuitive for other human beings to use. In the\nmeantime, feel free to steal source from tutorials I'll be posting, because\nthey'll be sure to, you know, actually work.","html":"<p>Unless you've been living under a rock (or only visit this site via work-related Google Searches, like most people) you've probably heard me drone on here and there about <strong>MongoDB Atlas</strong> and <strong>MongoDB Stitch</strong>. I even went so far as to hack together an awful workflow that somehow utilized Tableau as an ETL tool to feed JIRA information into Mongo. I'd like to formally apologize for that entire series: I can't imagine there's a single soul on this planet interested in learning about all of those things simultaneously. Such hobbies reserved for masochists with blogging addictions. I apologize. Let's start over.</p><p>First off, this is not a tutorial on how to use <em>MongoDB: the database</em>. I have zero interest cluttering the internet by reiterating what a MEAN stack is for the ten thousandth time, nor will I bore you with core NoSQL concepts you already understand. I'm here to talk about the giant on the horizon we didn't see coming, where MongoDB the database decided to become <a href=\"https://en.wikipedia.org/wiki/MongoDB_Inc.\"><strong>MongoDB Inc</strong></a><strong>:</strong> the enterprise cloud provider. The same MongoDB that recently purchased <a href=\"https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab\">mLab</a>, the <em>other</em> cloud-hosted solution for Mongo databases. MongoDB the company is bold enough to place its bets on building a cloud <em>far</em> simpler and restricted than either AWS or GCloud. The core of that bet implies that most of us aren't exactly building unicorn products as much as we're reinventing the wheel: and they're probably right.</p><p>Welcome to our series on MongoDB cloud, where we break down every service MongoDB has to offer; one by one.</p><h2 id=\"what-is-mongodb-cloud-and-does-it-exist\">What is MongoDB Cloud, and Does it Exist?</h2><p>What I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual name of the suite MongoDB offers) is actually two products:</p><ul><li><strong>MongoDB Atlas</strong>: A cloud-hosted MongoDB cluster with a beefy set of features. Real-time dashboards, high-availability, security features,  an awesome desktop client, and a CLI to top it all off.</li><li><strong>MongoDB Stitch: </strong>A group of services designed to interact with Atlas in every conceivable way, including creating endpoints, triggers, user authentication flows, serverless functions, and a UI to handle all of this.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/metrics.gif\" class=\"kg-image\"><figcaption>I'm spying on you and every query you make.</figcaption></figure><h3 id=\"atlas-as-a-standalone-database\">Atlas as a Standalone Database</h3><p>There are plenty of people who simply want an instance of MongoDB hosted in the cloud as-is: just ask the guys at mLab. This was in fact how I got pulled into Mongo's cloud myself.</p><p>MongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo, which Mongo itself is confident in by offering a <a href=\"https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/\">free tier</a> of Atlas to prospective buyers. If you're a company or enterprise, the phrases <strong>High Availability</strong>, <strong>Horizontal Scalability, </strong>relatively <strong>Higher Performance</strong> will probably be enough for you. But for us hobbyists, why pay for a Mongo cloud instance?</p><p>Mongo themselves gives this comparison:</p><div class=\"tableContainer\">\n<table class=\"table left\">\n  <thead>\n    <tr>\n      <th>\n        <strong>Overview</strong>\n      </th>\n      <th>\n        <strong>MongoDB Atlas</strong>\n      </th>\n      <th>\n        <strong>Compose</strong>\n      </th>\n      <th>\n        <strong>ObjectRocket</strong>\n      </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\n        Free Tier\n      </td>\n      <td>\n        Yes<br><small>Storage: 512 MB<br>RAM: Variable</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Live migration\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of cloud providers\n      </td>\n      <td>\n        AWS, Azure &amp; GCP\n      </td>\n      <td>\n        AWS, Softlayer &amp; GCP<br><small>Available in 2 regions for each provider</small>\n      </td>\n      <td>\n        Rackspace\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of instance configuration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Availability of latest MongoDB version\n      </td>\n      <td>\n        Yes<br><small>New versions of the database are available on MongoDB Atlas as soon as they are released</small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Replica Set Configuration\n      </td>\n      <td>\n        Up to 7 replicas<br><small>All replicas configured as data-bearing nodes</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small>One of the data-bearing nodes is hidden and used for backups only</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automatic Sharding Support\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Data explorer\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        SQL-based BI Connectivity\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Pause and resume clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database supported in on-premise deployments\n      </td>\n      <td>\n        Yes<br><small><a href=\"/products/mongodb-enterprise-advanced\">MongoDB Enterprise Advanced</a></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Global writes <small>Low-latency writes from anywhere in the world </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Cross-region replication <small>Distribute data around the world for multi-region fault tolerance and local reads </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr><tr>\n      <td>\n        Monitoring of database health with automated alerting\n      </td>\n      <td>\n        Yes<br><small>MongoDB Atlas UI &amp; support for APM platforms (New Relic)</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Continuous backup\n      </td>\n      <td>\n        Yes<br><small>Backups maintained<br>seconds behind production cluster</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump against hidden replica set member</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Queryable backups\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automated &amp; consistent snapshots of sharded clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Not Applicable<br><small>No support for auto-sharding</small>\n      </td>\n      <td>\n        No<br><small>Requires manually coordinating the recovery of mongodumps across shards</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Access control &amp; IP whitelisting\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        AWS VPC Peering\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Beta Release<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Additional Charge</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data in-flight\n      </td>\n      <td>\n        Yes<br><small>TLS/SSL as standard</small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data at-rest\n      </td>\n      <td>\n        Yes<br><small>Available for AWS deployments; always on with Azure and GCP</small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Available only with specific pricing plans and data centers</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        LDAP Integration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database-level auditing<br><small>Track DDL, DML, DCL operations</small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Bring your own KMS\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Realistically there are probably only a number of items that stand out on the comparison list when we go strictly database-to-database. Freedom over <strong>instance configuration </strong>sounds great, but in practice is more similar to putting a cap on how much MongoDB decides to charge you that month (by the way, it's usually a lot; keep this mind). Having the <strong>Latest Version</strong> seems great, but this can just as easily mean breaking production unannounced as much as it means new features.</p><p>MongoDB clearly wins over the enterprise space with <strong>Continuous &amp; queryable backups</strong>, integration with <strong>LDAP, </strong>and <strong>automatic sharding support. </strong>Truthfully if this were merely a database-level feature and cost comparison, the decision to go with<strong> MongoDB Atlas</strong> would come down to how much you like their pretty desktop interface:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/compass.gif\" class=\"kg-image\"><figcaption>A perfectly legitimate reason to pay up, imho.</figcaption></figure><p>So let's say MongoDB Atlas is marginally better than a competitor in the confined realm of \"being a database.\" Are Stitch microservices enough to justify keeping your instance with the MongoDB team?</p><h2 id=\"service-by-service-breakdown-of-stitch\">Service-by-Service Breakdown of Stitch</h2><p>Stitch is kind of like if AWS exited in an alternative universe, where JSON and JavaScript were earth's only technologies. Thinking back to how we create APIs in AWS, the status quo almost always involves spinning up a <strong>Dynamo</strong> (NoSQL) database to put behind <strong>Lambda </strong>functions, accessible by <strong>API Gateway </strong>endpoints. Stitch's core use case revolves around this use-case of <em>end-user-accessing-data</em>, with a number of services dedicated specifically to supporting or improving this flow. The closest comparison to Stitch would be GCloud's <strong>Firebase</strong>. </p><p>So what makes Stitch so special?</p><h3 id=\"service-1-querying-atlas-securely-via-frontend-code\">Service 1: Querying Atlas Securely via Frontend Code</h3><p>Something that cannot be understated is the ability to query Atlas via frontend Javascript. We're not passing API keys, Secrets, or any sort of nonsense; because you're configured things correctly, whitelisted domains can run queries of any complexity <em>without ever interacting with an app's backend.</em> This is not a crazy use case: consider this blog for example, or more so lately, mobile applications:</p><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('&lt;DATABASE&gt;');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; \n    db.collection('&lt;COLLECTION&gt;').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() =&gt; \n    db.collection('&lt;COLLECTION&gt;').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs =&gt; {\n      console.log(&quot;Found docs&quot;, docs)\n      console.log(&quot;[MongoDB Stitch] Connected to Stitch&quot;)\n  }).catch(err =&gt; {\n    console.error(err)\n  });\n&lt;/script&gt;\n</code></pre>\n<p>This isn't to say we're allowing any user to query any data all willy-nilly just because they're on our whitelisted IP: all data stored in Atlas is restricted to specified <strong>Users</strong> by defining <strong>User Roles. </strong>Joe Schmoe can't just inject a query into any presumed database and wreak havoc, because Joe Schmoe can only access data we've permitted his user account to view or write to. What is this \"user account\" you ask? This brings us to the next big feature...</p><h3 id=\"service-2-end-user-account-creation-management\">Service 2: End-User Account Creation &amp; Management</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-5.59.03-PM.png\" class=\"kg-image\"><figcaption>Stitch will handle user account creation for you without the boilerplate.</figcaption></figure><p>Creating an app with user accounts is a huge pain in the ass. Cheeky phrases like '<strong>Do the OAuth Dance'</strong> can't ever hope to minimize the agonizing repetitive pain of creating user accounts or managing relationships between users and data (can <em>user X</em> see a comment from <em>user Y</em>?). Stitch allows most of the intolerably benign logic behind these features to be handled via a UI.</p><p>It would be a far cry to say these processes have been \"trivialized\", but the time saved is perhaps just enough to keep a coding hobbyist interested in their side projects as opposed to giving up and playing Rocket League.</p><p>As far as the permissions to read comments go... well, here's a self-explanatory screenshot of how Stitch handles read/write document permission in its simplest form:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-6.09.25-PM.png\" class=\"kg-image\"><figcaption>Owners of comments can write their comments. Everybody else reads. Seems simple.</figcaption></figure><h3 id=\"service-3-serverless-functions\">Service 3: Serverless Functions</h3><p>Stitch functions are akin to AWS Lambda functions, but much easier to configure for cross-service integration (and also limited to JavaScript ECMA 2015 or something). Functions benefit from the previous two features, in that they too can be triggered from a whitelisted app's frontend, and are governed by a simple \"rules\" system, eliminating the need for security group configurations etc.</p><p>This is what calling a function from an app's frontend looks like:</p><pre><code class=\"language-javascript\">&lt;script&gt;\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n     client.callFunction(&quot;numCards&quot;, [&quot;In Progress&quot;]).then(results =&gt; {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n&lt;/script&gt;\n</code></pre>\n<p>Functions can run any query against Atlas, retrieve <em>values</em> (such as environment variables), and even call other functions. Functions can also be fired by database <strong>triggers,</strong> where a change to a collection will prompt an action such as an alert.</p><h3 id=\"service-4-http-webhooks\">Service 4: HTTP Webhooks</h3><p>Webhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to one another in that they are one-off URLs to perform single tasks. We could never build a well-designed API using Stitch Webhooks, as we could with <strong>API Gateway</strong>; this simply isn't the niche MongoDB is trying to hit (the opposite, in fact). </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-4.33.27-PM.png\" class=\"kg-image\"><figcaption>Configuration for a single Webhook.</figcaption></figure><p>This form with a mere 6 fields clearly illustrates what Stitch intends to do: trivializing the creation of traditionally non-trivial features.</p><h3 id=\"service-5-storing-values-in-stitch\">Service 5: Storing 'Values' in Stitch</h3><p>A \"value\" is equivalent to an environment variable. These can be used to store API keys, secrets, or whatever. Of course, values are retrieved via functions.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-7.45.28-PM.png\" class=\"kg-image\"><figcaption>Shhh, it's a secret ;)</figcaption></figure><h3 id=\"service-6-a-bunch-of-mostly-bloated-extras\">Service 6+: A Bunch of Mostly Bloated Extras</h3><p>Finally, Stitch has thrown in a few third-party integrations for good measure. Some integrations like <strong>S3 Integration </strong>could definitely come in handy, but it's worth asking why Mongo constantly over advertises their integrations with <strong>Github</strong> and <strong>Twilio</strong>. We've already established that we can create endpoints which accept information, and we can make functions which <em>GET</em> information... so isn't anything with an API pretty easy to 'integrate' with?</p><p>This isn't to say the extra services aren't useful, they just seem a bit... odd. It feels a lot like bloating the catalog, but the catalog isn't nearly bloated enough where it feels normal (like Heroku add-ons, for example). The choice to launch Stitch with a handful of barely-useful integrations only comes off as more and more aimless as time passes; as months turn to years and no additions or updates are made to service offerings, it's worth questioning what the vision had been for the product in the first place. In my experience, feature sets like these happen when Product Managers are more powerful than they are useful.</p><h2 id=\"the-breathtaking-climax-is-stitch-worth-it\">The Breathtaking Climax: Is Stitch Worth It?</h2><p>I've been utilizing Stitch to fill in the blanks in development for months now, perhaps nearly a year. Each time I find myself working with Stitch or looking at the bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive toy with an infuriating lack of accurate documentation.</p><p> Stitch is very much a <em>copy-and-paste-cookie-cutter-code</em> type of product, which begs the question of why their tutorials are recklessly outdated; sometimes to the point where MongoDB's own tutorial source code <em>doesn't work. </em>There are so many use cases and potential benefits to Stitch, so why is the <a href=\"https://github.com/mongodb/stitch-examples\">Github repo</a> containing example code snippets so unmaintained, and painfully irrelevant? Lastly, why am I selling this product harder than their own internal team?</p><p>Stitch is a good product with a lot of unfortunate oversight. That said, Google Firebase still doesn't even have an \"import data\" feature, so I suppose it's time to dig deep into this vendor lock and write a 5-post series about it before Silicon Valley's best and brightest get their shit together enough to actually create something useful and intuitive for other human beings to use. In the meantime, feel free to steal source from tutorials I'll be posting, because they'll be sure to, you know, actually work.</p>","url":"https://hackersandslackers.com/mongodb-cloud-backend-as-a-service-with-atlas-and-stitch/","uuid":"5555fa6e-07f0-4f9a-8069-e1e68868e608","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5beb3c900dbec217f3ce801b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673739","title":"Lynx Roundup, November 15th","slug":"lynx-roundup-november-15th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/126@2x.jpg","excerpt":"Awk one-liners!  Deep learning for medicine!  Natural language processing!","custom_excerpt":"Awk one-liners!  Deep learning for medicine!  Natural language processing!","created_at_pretty":"12 November, 2018","published_at_pretty":"15 November, 2018","updated_at_pretty":"15 November, 2018","created_at":"2018-11-11T23:41:32.000-05:00","published_at":"2018-11-15T07:00:00.000-05:00","updated_at":"2018-11-15T07:00:00.000-05:00","meta_title":"Lynx Roundup, November 15th | Hackers and Slackers","meta_description":"Awk one-liners!  Deep learning for medicine!  Natural language processing!","og_description":"Awk one-liners!  Deep learning for medicine!  Natural language processing!","og_image":"https://hackersandslackers.com/content/images/lynx/126@2x.jpg","og_title":"Lynx Roundup, November 15th","twitter_description":"Awk one-liners!  Deep learning for medicine!  Natural language processing!","twitter_image":"https://hackersandslackers.com/content/images/lynx/126@2x.jpg","twitter_title":"Lynx Roundup, November 15th | Hackers and Slackers","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://medium.com/swlh/a-guide-to-aws-lambdas-using-python-triggered-by-an-api-call-1e768edd13c5\n\nhttps://blog.goodaudience.com/https-medium-com-ethankoch-how-to-become-an-nlp-expert-81cb94989612\n\nhttps://github.com/perone/medicaltorch\n\nhttps://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602\n\nhttps://arxiv.org/abs/1810.04793\n\nhttp://queirozf.com/entries/gnu-awk-one-line-examples\n\nhttps://www.gsb.stanford.edu/insights/are-influencers-overrated","html":"<p></p><p><a href=\"https://medium.com/swlh/a-guide-to-aws-lambdas-using-python-triggered-by-an-api-call-1e768edd13c5\">https://medium.com/swlh/a-guide-to-aws-lambdas-using-python-triggered-by-an-api-call-1e768edd13c5</a></p><p><a href=\"https://blog.goodaudience.com/https-medium-com-ethankoch-how-to-become-an-nlp-expert-81cb94989612\">https://blog.goodaudience.com/https-medium-com-ethankoch-how-to-become-an-nlp-expert-81cb94989612</a></p><p><a href=\"https://github.com/perone/medicaltorch\">https://github.com/perone/medicaltorch</a></p><p><a href=\"https://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602\">https://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602</a></p><p><a href=\"https://arxiv.org/abs/1810.04793\">https://arxiv.org/abs/1810.04793</a></p><p><a href=\"http://queirozf.com/entries/gnu-awk-one-line-examples\">http://queirozf.com/entries/gnu-awk-one-line-examples</a></p><p><a href=\"https://www.gsb.stanford.edu/insights/are-influencers-overrated\">https://www.gsb.stanford.edu/insights/are-influencers-overrated</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-15th/","uuid":"7fb85145-1785-4886-9e92-9b32375c6f1b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be9047c74f90031d0a61700"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673737","title":"Lynx Roundup, November 13th","slug":"lynx-roundup-november-13th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/71-1@2x.jpg","excerpt":"Building dashboards!  Mosquito-killing robots!  Pathogen internet!","custom_excerpt":"Building dashboards!  Mosquito-killing robots!  Pathogen internet!","created_at_pretty":"12 November, 2018","published_at_pretty":"13 November, 2018","updated_at_pretty":"14 November, 2018","created_at":"2018-11-11T23:29:24.000-05:00","published_at":"2018-11-13T07:00:00.000-05:00","updated_at":"2018-11-13T20:19:53.000-05:00","meta_title":"Building dashboards!  Mosquito-killing robots!  Pathogen internet! | Hackers And Slackers","meta_description":"Building dashboards!  Mosquito-killing robots!  Pathogen internet! | Hackers And Slackers","og_description":"Building dashboards!  Mosquito-killing robots!  Pathogen internet!","og_image":"https://hackersandslackers.com/content/images/lynx/71-1@2x.jpg","og_title":"Lynx Roundup, November 13th","twitter_description":"Building dashboards!  Mosquito-killing robots!  Pathogen internet!","twitter_image":"https://hackersandslackers.com/content/images/lynx/71-1@2x.jpg","twitter_title":"Lynx Roundup, November 13th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://dataschool.com/courses/building-a-dashboard-best-practices/\n\nhttps://twitter.com/TwoBitHistory/status/1052381675844489216\n\nhttps://www.quantamagazine.org/broadband-networks-of-viruses-may-help-bacteria-evolve-faster-20181016/\n\nhttps://www.springboard.com/blog/highest-data-scientist-salary-possible/\n\nhttps://futurism.com/the-byte/robot-mosquitoes-autonomous-laser-cannon\n\nhttps://dev.to/elabftw/stop-using-sudo-pip-install-52mn\n\nhttps://flowingdata.com/2018/10/17/ask-the-question-visualize-the-answer/","html":"<p></p><p><a href=\"https://dataschool.com/courses/building-a-dashboard-best-practices/\">https://dataschool.com/courses/building-a-dashboard-best-practices/</a></p><p><a href=\"https://twitter.com/TwoBitHistory/status/1052381675844489216\">https://twitter.com/TwoBitHistory/status/1052381675844489216</a></p><p><a href=\"https://www.quantamagazine.org/broadband-networks-of-viruses-may-help-bacteria-evolve-faster-20181016/\">https://www.quantamagazine.org/broadband-networks-of-viruses-may-help-bacteria-evolve-faster-20181016/</a></p><p><a href=\"https://www.springboard.com/blog/highest-data-scientist-salary-possible/\">https://www.springboard.com/blog/highest-data-scientist-salary-possible/</a></p><p><a href=\"https://futurism.com/the-byte/robot-mosquitoes-autonomous-laser-cannon\">https://futurism.com/the-byte/robot-mosquitoes-autonomous-laser-cannon</a></p><p><a href=\"https://dev.to/elabftw/stop-using-sudo-pip-install-52mn\">https://dev.to/elabftw/stop-using-sudo-pip-install-52mn</a></p><p><a href=\"https://flowingdata.com/2018/10/17/ask-the-question-visualize-the-answer/\">https://flowingdata.com/2018/10/17/ask-the-question-visualize-the-answer/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-13th/","uuid":"09f6f5e6-c52b-4f52-83e1-5de284123135","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be901a474f90031d0a616f6"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673736","title":"Lynx Roundup, November 12th","slug":"lynx-roundup-november-12th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx70-1@2x.jpg","excerpt":"Contributing to Open Source!  Cool tech podcast!  Ways your boss is spying on you!","custom_excerpt":"Contributing to Open Source!  Cool tech podcast!  Ways your boss is spying on you!","created_at_pretty":"12 November, 2018","published_at_pretty":"12 November, 2018","updated_at_pretty":"13 November, 2018","created_at":"2018-11-11T22:08:49.000-05:00","published_at":"2018-11-12T07:00:00.000-05:00","updated_at":"2018-11-12T22:12:30.000-05:00","meta_title":"Lynx Roundup, November 11th | Hackers and Slackers","meta_description":"Contributing to Open Source!  Cool tech podcast!  Ways your boss is spying on you! | Hackers And Slackers","og_description":"Contributing to Open Source!  Cool tech podcast!  Ways your boss is spying on you!","og_image":"https://hackersandslackers.com/content/images/lynx/lynx70-1@2x.jpg","og_title":"Lynx Roundup, November 11th | Hackers and Slackers","twitter_description":"Contributing to Open Source!  Cool tech podcast!  Ways your boss is spying on you!","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx70-1@2x.jpg","twitter_title":"Lynx Roundup, November 11th | Hackers and Slackers","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.digitaltrends.com/cool-tech/fly-reconstructed-genes-140-million-years-ago/\n\nhttps://soundcloud.com/blockchainboys\n\nhttps://primer.commonstransition.org/1-short-articles\n\nhttps://insights.dice.com/2018/10/15/tech-employers-monitoring-you-excessively/\n\nhttps://shore-dedication.glitch.me/\n\nhttps://beta.observablehq.com/@kristw/boba-science\n\nhttp://matthewrocklin.com/blog/work/2018/10/12/so-you-want-to-contribute-to-open-source","html":"<p></p><p><a href=\"https://www.digitaltrends.com/cool-tech/fly-reconstructed-genes-140-million-years-ago/\">https://www.digitaltrends.com/cool-tech/fly-reconstructed-genes-140-million-years-ago/</a></p><p><a href=\"https://soundcloud.com/blockchainboys\">https://soundcloud.com/blockchainboys</a></p><p><a href=\"https://primer.commonstransition.org/1-short-articles\">https://primer.commonstransition.org/1-short-articles</a></p><p><a href=\"https://insights.dice.com/2018/10/15/tech-employers-monitoring-you-excessively/\">https://insights.dice.com/2018/10/15/tech-employers-monitoring-you-excessively/</a></p><p><a href=\"https://shore-dedication.glitch.me/\">https://shore-dedication.glitch.me/</a></p><p><a href=\"https://beta.observablehq.com/@kristw/boba-science\">https://beta.observablehq.com/@kristw/boba-science</a></p><p><a href=\"http://matthewrocklin.com/blog/work/2018/10/12/so-you-want-to-contribute-to-open-source\">http://matthewrocklin.com/blog/work/2018/10/12/so-you-want-to-contribute-to-open-source</a></p>","url":"https://hackersandslackers.com/lynx-roundup-november-12th/","uuid":"94cb0bbe-2a87-401b-bfce-b4cb943c6314","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be8eec174f90031d0a616e8"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673735","title":"Reselling AWS Load Balancing","slug":"reselling-aws-load-balancer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","custom_excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","created_at_pretty":"12 November, 2018","published_at_pretty":"12 November, 2018","updated_at_pretty":"13 November, 2018","created_at":"2018-11-11T19:49:57.000-05:00","published_at":"2018-11-11T20:16:49.000-05:00","updated_at":"2018-11-12T23:05:57.000-05:00","meta_title":"Reselling AWS Load Balancing | Hackers and Slackers","meta_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","og_title":"Reselling AWS Load Balancing | Hackers and Slackers","twitter_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","twitter_title":"Reselling AWS Load Balancing | Hackers and Slackers","authors":[{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"}],"primary_author":{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Let's say we have a hosting service for users who bring their own domain name.\nIn this scenario we'd like to be able to service customers no matter who manages\ntheir DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore\nplace, etc.\n\nAt the same time, we'd also like to provide Load balancing so no one-user can\noverload any of our systems. This means, instead of having a customer's domain\nname point directly to the system where their webserver or app resides, it will\npoint the HTTP connection to a Load Balancer which is prepared to handle serious\nconnection load before divvying it out to whichever cluster of systems is ready\nto deliver the user's content. \n\nIn an ideal world, we would have the user point their domain name to the Load\nBalancer's IP address. Very simple DNS A-Record adjustment. \n\nIn the real world, these type of cloud load balancers run over several ip\naddresses that rotate over time. So, if we were to place one of these IP\naddresses in a domain name's A-Record, it would soon be useless as it rotates\nout. Instead, the cloud load balancer offers us an end point (also an A-Record)\nsuch as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static\nwhile they dynamically rotate the IP addresses the entrypoint leads to. \n\nThe catch? You can't place an A-Record in another DNS A-Record, you can only\nplace an IP address in an A-Record. the DNS A-Record is simply a key-value pair\nwhere the key  is the Domain name (yoursiteEndpoint.com) and the value  is an IP\naddress (and nothing else). \n\nThen how do we leverage a cloud load balancer for our customers?\n\n{workaround}  Each customer with their own domain name must make the following\nchanges in their DNS provider records. \n\n * Make a CNAME Record called \"www\" which leads to the AWS Load Balancer\n   A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)\n * Setup DNS forwarding so customersite.com forwards to www.customersite.com\n\nThe Problem:  The customer will literally be entering evident AWS data into\ntheir config, and it's much more information to update than just an IP address\nin an A-Record. \n\nMore Options: \n\n{Route 53 Nameservers}  You have to automate Route 53, adding a new Hosted Zone\nbased on the customer's domain name, retrieve and deliver the Route 53 Hosted\nZone nameservers to the customer so the customer can update their DNS records at\ntheir service of choice.\n\nThe Problem:  Lots more automation and costs, AWS 500 Hosted Zone limit, more\ncustomer sync interaction\n\nMy Ultimatum:\nMake my own Load Balancer out of a network-enhanced AWS EC2 instance. I will\ngive two options for the customers - the simple A-record update to EC2-instance\nstatic IP. If they want DDoS protection and load balancing, they can do the \n{workaround}  step above additionally. If they decide not to do {workaround} \nstep above, the customer understands that we are leaving leaving the uptime\ncompletely up to the EC2 instance IP address.  Also, forget that Route 53\nnameservers update BS, as that is way too much additional business logic\nautomation and costs for reselling standpoint.","html":"<p>Let's say we have a hosting service for users who bring their own domain name. In this scenario we'd like to be able to service customers no matter who manages their DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore place, etc.</p><p>At the same time, we'd also like to provide Load balancing so no one-user can overload any of our systems. This means, instead of having a customer's domain name point directly to the system where their webserver or app resides, it will point the HTTP connection to a Load Balancer which is prepared to handle serious connection load before divvying it out to whichever cluster of systems is ready to deliver the user's content. </p><p>In an ideal world, we would have the user point their domain name to the Load Balancer's IP address. Very simple DNS A-Record adjustment. </p><p>In the real world, these type of cloud load balancers run over several ip addresses that rotate over time. So, if we were to place one of these IP addresses in a domain name's A-Record, it would soon be useless as it rotates out. Instead, the cloud load balancer offers us an end point (also an A-Record) such as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static while they dynamically rotate the IP addresses the entrypoint leads to. </p><p>The catch? You can't place an A-Record in another DNS A-Record, you can only place an IP address in an A-Record. the DNS A-Record is simply a key-value pair where the <em>key</em> is the Domain name (yoursiteEndpoint.com) and the <em>value</em> is an IP address (and nothing else). </p><p>Then how do we leverage a cloud load balancer for our customers?</p><p><em>{workaround}</em> Each customer with their own domain name must make the following changes in their DNS provider records. </p><ul><li>Make a CNAME Record called \"www\" which leads to the AWS Load Balancer A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)</li><li>Setup DNS forwarding so customersite.com forwards to www.customersite.com</li></ul><p><em>The Problem:</em><strong> </strong>The customer will literally be entering evident AWS data into their config, and it's much more information to update than just an IP address in an A-Record. </p><p>More Options: </p><p><em>{Route 53 Nameservers}</em> You have to automate Route 53, adding a new Hosted Zone based on the customer's domain name, retrieve and deliver the Route 53 Hosted Zone nameservers to the customer so the customer can update their DNS records at their service of choice.</p><p><em>The Problem:</em> Lots more automation and costs, AWS 500 Hosted Zone limit, more customer sync interaction</p><h2 id=\"my-ultimatum-\">My Ultimatum:</h2><p>Make my own Load Balancer out of a network-enhanced AWS EC2 instance. I will give two options for the customers - the simple A-record update to EC2-instance static IP. If they want DDoS protection and load balancing, they can do the <em>{workaround}</em> step above additionally. If they decide not to do <em>{workaround}</em> step above, the customer understands that we are leaving leaving the uptime completely up to the EC2 instance IP address.  Also, forget that Route 53 nameservers update BS, as that is way too much additional business logic automation and costs for reselling standpoint. </p>","url":"https://hackersandslackers.com/reselling-aws-load-balancer/","uuid":"5dd15e2a-c368-4717-900c-e85095331c4b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be8ce3574f90031d0a61650"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673734","title":"Scraping Data on the Web with BeautifulSoup","slug":"scraping-urls-with-beautifulsoup","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","excerpt":"The honest act of systematically stealing data without permission.","custom_excerpt":"The honest act of systematically stealing data without permission.","created_at_pretty":"11 November, 2018","published_at_pretty":"11 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-11T04:53:44.000-05:00","published_at":"2018-11-11T08:35:09.000-05:00","updated_at":"2019-01-05T13:21:06.000-05:00","meta_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","meta_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","og_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","twitter_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","twitter_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"There are plenty of reliable and open sources of data on the web. Datasets are\nfreely released to the public domain by the likes of Kaggle, Google Cloud, and\nof course local & federal government. Like most things free and open, however,\nfollowing the rules to obtain public data can be a bit... boring. I'm not\nsuggesting we go and blatantly break some grey-area laws by stealing data, but\nthis blog isn't exactly called People Who Play It Safe And Slackers, either. \n\nMy personal Python roots can actually be traced back to an ambitious\nside-project: to aggregate all new music from across the web and deliver it the\nmasses. While that project may have been abandoned (after realizing it already\nexisted), BeautifulSoup  was more-or-less my first ever experience with Python. \n\nThe Tool(s) for the Job(s)\nBefore going any further, we'd be ill-advised to not at least mention Python's\nother web-scraping behemoth, Scrapy [https://scrapy.org/]. BeautifulSoup  and \nScrapy  have two very different agendas. BeautifulSoup is intended to parse or\nextract data one page at a time, with each page being served up via the requests \n library or equivalent. Scrapy,  on the other hand, is for creating crawlers: or\nrather absolute monstrosities unleashed upon the web like a swarm, loosely\nfollowing links and haste-fully grabbing data where data exists to be grabbed.\nTo put this in perspective, Google Cloud functions will not even let you import\nScrapy as a usable library.\n\nThis isn't to say that BeautifulSoup  can't be made into a similar monstrosity\nof its own. For now, we'll focus on a modest task: generating link previews for\nURLs by grabbing their metadata.\n\nStep 1: Stalk Your Prey\nBefore we steal any data, we should take a look at the data we're hoping to\nsteal.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape URLs to generate previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n\n\nThe above is the minimum needed to retrieve the DOM structure of an HTML page. \nBeautifulSoup  accepts the .content  output from a request, from which we can\ninvestigate the contents.\n\nUsing BeauitfulSoup will often result in different results for your scaper than\nyou might see as a human, such as 403 errors or blocked content. An easy way\naround this faking your headers into looking like normal browser agents, as we\ndo here: \nheaders.update({\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101\nFirefox/52.0',\n})`The result of print(soup.prettify())  will predictably output a \"pretty\" printed\nversion of your target DOM structure:\n\n<html class=\"gr__example_com\"><head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta property=\"og:site_name\" content=\"Example dot com\">\n    <meta property=\"og:type\" content=\"website\">\n    <meta property=\"og:title\" content=\"Example\">\n    <meta property=\"og:description\" content=\"An Example website.\">\n    <meta property=\"og:image\" content=\"http://example.com/img/image.jpg\">\n    <meta name=\"twitter:title\" content=\"Hackers and Slackers\">\n    <meta name=\"twitter:description\" content=\"An Example website.\">\n    <meta name=\"twitter:url\" content=\"http://example.com/\">\n    <meta name=\"twitter:image\" content=\"http://example.com/img/image.jpg\">\n</head>\n\n<body data-gr-c-s-loaded=\"true\">\n  <div>\n    <h1>Example Domain</h1>\n      <p>This domain is established to be used for illustrative examples in documents.</p>\n      <p>You may use this domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n  </div>\n</body>\n    \n</html>\n\n\nStep 2: The Extraction\nAfter turning our request content into a BeautifulSoup object, we access items\nin the DOM via dot notation as such:\n\ntitle = soup.title.string\n\n\n.string  gives us the actual content of the tag which is Example Domain, whereas\n soup.title  would return the entirety of the tag as <title>Example\nDomain</title>. \n\nDot notation is fine when pages have predictable hierarchies or structures, but\nbecomes much less useful for extracting patterns we see in the document. soup.a \nwill only return the first instance of a link, and probably isn't what we want.\n\nIf we wanted to extract all  <a>  tags of a page's content while avoiding the\nnoise of nav links etc, we can use CSS selectors to return a list of all\nelements matching the selection. soup.select('body p > a')  retrieves all links\nembedded in paragraph text, limited to the body of the page. \n\nSome other methods of grabbing elements:\n\n * soup.find(id=\"example\"): Useful for when a single element is expected.\n * soup.find_all('a'):  Returns a list of all elements matching the selection\n   after searching the document recursively.\n * .parent and .child: Relative selectors to a currently engaged element.\n\nGet Some Attributes\nChances are we'll almost always want the contents or the attributes of a tag, as\nopposed to the entire <a>  tag's HTML. A common example of going after a tag's\nattributes would be in the cases of img  and a  tags. Chances are we're most\ninterested in the src  and href  attributes of such tags, respectively. \n\nThe .get  method refers specifically to getting the value of attributes on a\ntag. For example, soup.find('.logo').get('href')  would find an element with the\nclass \"logo\", and return the url to that image.\n\nPesky Tags to Deal With\nIn our example of creating link previews, a good first source of information\nwould obviously be the page's meta tags: specifically the og  tags they've\nspecified to openly provide the bite-sized information we're looking for.\nGrabbing these tags are a bit more difficult to deal with:\n\nsoup.find(\"meta\", property=\"og:description\").get('content')\n\n\nOh yeah, now that's some ugly shit right there. Meta tags are especially\ninteresting because they're all uselessly dubbed 'meta', thus we need a second\ndifferentiator in addition to the tag name to specify which meta tag we care\nabout. Only then can we bother to get  the actual content of said tag.\n\nStep 3: Realizing Something Will Always Break\nIf we were to try the above selector on an HTML page which did not contain an \nog:description, our script would break unforgivingly. Not only do we miss this\ndata, but we miss out on everything entirely - this means we always need to\nbuild in a plan B, and at the very least deal with a lack of tag altogether.\n\nIt's best to break out this logic one tag at a time. First, let's look at an\nexample for a base scraper with all the knowledge we have so far:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p > a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n\n\nGreat - there's a base function for snatching all links out of the body of a\npage. Ultimately we'll create a JSON object for each of these links containing\npreview data, link_preview_dict.\n\nTo handle each value of our dict, we have individual functions:\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get a preview image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\nIn case you're wondering:\n\n * getTitle tries to get the <title>  tag, and falls back to the page's first \n   <h1>  tag (surprisingly enough some pages are in fact missing a title).\n * getDescription  looks for the OG description, and falls back to the content\n   of the page's first paragraph.\n * getImage looks for the OG image, and falls back to the page's first image.\n * getSiteName similarly tries to grab the OG attribute, otherwise it does it's\n   best to extract the domain name from the URL string under the assumption that\n   this is the origin's name (look, it ain't perfect).\n\nWhat Did We Just Build?\nBelieve it or not, the above is considered to be enough logic to be a paid\nservice with a monthly fee. Go ahead and Google it; or better yet, just steal my\nsource code entirely:\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p > a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)","html":"<p>There are plenty of reliable and open sources of data on the web. Datasets are freely released to the public domain by the likes of Kaggle, Google Cloud, and of course local &amp; federal government. Like most things free and open, however, following the rules to obtain public data can be a bit... boring. I'm not suggesting we go and blatantly break some grey-area laws by stealing data, but this blog isn't exactly called <strong>People Who Play It Safe And Slackers</strong>, either. </p><p>My personal Python roots can actually be traced back to an ambitious side-project: to aggregate all new music from across the web and deliver it the masses. While that project may have been abandoned (after realizing it already existed), <strong>BeautifulSoup</strong> was more-or-less my first ever experience with Python. </p><h2 id=\"the-tool-s-for-the-job-s-\">The Tool(s) for the Job(s)</h2><p>Before going any further, we'd be ill-advised to not at least mention Python's other web-scraping behemoth, <strong><a href=\"https://scrapy.org/\">Scrapy</a></strong>. <strong>BeautifulSoup</strong> and <strong>Scrapy</strong> have two very different agendas. BeautifulSoup is intended to parse or extract data one page at a time, with each page being served up via the <strong>requests</strong> library or equivalent. <strong>Scrapy,</strong> on the other hand, is for creating crawlers: or rather absolute monstrosities unleashed upon the web like a swarm, loosely following links and haste-fully grabbing data where data exists to be grabbed. To put this in perspective, Google Cloud functions will not even let you import Scrapy as a usable library.</p><p>This isn't to say that <strong>BeautifulSoup</strong> can't be made into a similar monstrosity of its own. For now, we'll focus on a modest task: generating link previews for URLs by grabbing their metadata.</p><h2 id=\"step-1-stalk-your-prey\">Step 1: Stalk Your Prey</h2><p>Before we steal any data, we should take a look at the data we're hoping to steal.</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape URLs to generate previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n</code></pre>\n<p>The above is the minimum needed to retrieve the DOM structure of an HTML page. <strong>BeautifulSoup</strong> accepts the <code>.content</code> output from a request, from which we can investigate the contents.</p><div class=\"protip\">\n    Using BeauitfulSoup will often result in different results for your scaper than you might see as a human, such as 403 errors or blocked content. An easy way around this faking your headers into looking like normal browser agents, as we do here: <br><code>headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })`</code>\n</div><p>The result of <code>print(soup.prettify())</code> will predictably output a \"pretty\" printed version of your target DOM structure:</p><pre><code class=\"language-html\">&lt;html class=&quot;gr__example_com&quot;&gt;&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n    &lt;meta charset=&quot;utf-8&quot;&gt;\n    &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;\n    &lt;meta property=&quot;og:site_name&quot; content=&quot;Example dot com&quot;&gt;\n    &lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n    &lt;meta property=&quot;og:title&quot; content=&quot;Example&quot;&gt;\n    &lt;meta property=&quot;og:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n    &lt;meta name=&quot;twitter:title&quot; content=&quot;Hackers and Slackers&quot;&gt;\n    &lt;meta name=&quot;twitter:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta name=&quot;twitter:url&quot; content=&quot;http://example.com/&quot;&gt;\n    &lt;meta name=&quot;twitter:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n&lt;/head&gt;\n\n&lt;body data-gr-c-s-loaded=&quot;true&quot;&gt;\n  &lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n      &lt;p&gt;This domain is established to be used for illustrative examples in documents.&lt;/p&gt;\n      &lt;p&gt;You may use this domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&quot;http://www.iana.org/domains/example&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n    \n&lt;/html&gt;\n</code></pre>\n<h2 id=\"step-2-the-extraction\">Step 2: The Extraction</h2><p>After turning our request content into a BeautifulSoup object, we access items in the DOM via dot notation as such:</p><pre><code class=\"language-python\">title = soup.title.string\n</code></pre>\n<p><code>.string</code> gives us the actual content of the tag which is <code>Example Domain</code>, whereas <code>soup.title</code> would return the entirety of the tag as <code>&lt;title&gt;Example Domain&lt;/title&gt;</code>. </p><p>Dot notation is fine when pages have predictable hierarchies or structures, but becomes much less useful for extracting patterns we see in the document. <code>soup.a</code> will only return the first instance of a link, and probably isn't what we want.</p><p>If we wanted to extract <em>all</em> <code>&lt;a&gt;</code> tags of a page's content while avoiding the noise of nav links etc, we can use CSS selectors to return a list of all elements matching the selection. <code>soup.select('body p &gt; a')</code> retrieves all links embedded in paragraph text, limited to the body of the page. </p><p>Some other methods of grabbing elements:</p><ul><li><strong>soup.find(id=\"example\")</strong>: Useful for when a single element is expected.</li><li><strong>soup.find_all('a')</strong>:<strong> </strong>Returns a list of all elements matching the selection after searching the document recursively.</li><li><strong>.parent </strong>and <strong>.child</strong>: Relative selectors to a currently engaged element.</li></ul><h3 id=\"get-some-attributes\">Get Some Attributes</h3><p>Chances are we'll almost always want the contents or the attributes of a tag, as opposed to the entire <code>&lt;a&gt;</code> tag's HTML. A common example of going after a tag's attributes would be in the cases of <code>img</code> and <code>a</code> tags. Chances are we're most interested in the <code>src</code> and <code>href</code> attributes of such tags, respectively. </p><p>The <code>.get</code> method refers specifically to getting the value of attributes on a tag. For example, <code>soup.find('.logo').get('href')</code> would find an element with the class \"logo\", and return the url to that image.</p><h3 id=\"pesky-tags-to-deal-with\">Pesky Tags to Deal With</h3><p>In our example of creating link previews, a good first source of information would obviously be the page's meta tags: specifically the <code>og</code> tags they've specified to openly provide the bite-sized information we're looking for. Grabbing these tags are a bit more difficult to deal with:</p><pre><code class=\"language-python\">soup.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n</code></pre>\n<p>Oh yeah, now that's some ugly shit right there. Meta tags are especially interesting because they're all uselessly dubbed 'meta', thus we need a second differentiator in addition to the tag name to specify <em>which </em>meta tag we care about. Only then can we bother to <em>get</em> the actual content of said tag.</p><h2 id=\"step-3-realizing-something-will-always-break\">Step 3: Realizing Something Will Always Break</h2><p>If we were to try the above selector on an HTML page which did not contain an <code>og:description</code>, our script would break unforgivingly. Not only do we miss this data, but we miss out on everything entirely - this means we always need to build in a plan B, and at the very least deal with a lack of tag altogether.</p><p>It's best to break out this logic one tag at a time. First, let's look at an example for a base scraper with all the knowledge we have so far:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p &gt; a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n</code></pre>\n<p>Great - there's a base function for snatching all links out of the body of a page. Ultimately we'll create a JSON object for each of these links containing preview data, <code>link_preview_dict</code>.</p><p>To handle each value of our dict, we have individual functions:</p><pre><code class=\"language-python\">def getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get a preview image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n</code></pre>\n<p>In case you're wondering:</p><ul><li><strong>getTitle </strong>tries to get the <code>&lt;title&gt;</code> tag, and falls back to the page's first <code>&lt;h1&gt;</code> tag (surprisingly enough some pages are in fact missing a title).</li><li><strong>getDescription</strong> looks for the OG description, and falls back to the content of the page's first paragraph.</li><li><strong>getImage </strong>looks for the OG image, and falls back to the page's first image.</li><li><strong>getSiteName </strong>similarly tries to grab the OG attribute, otherwise it does it's best to extract the domain name from the URL string under the assumption that this is the origin's name (look, it ain't perfect).</li></ul><h2 id=\"what-did-we-just-build\">What Did We Just Build?</h2><p>Believe it or not, the above is considered to be enough logic to be a paid service with a monthly fee. Go ahead and Google it; or better yet, just steal my source code entirely:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p &gt; a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)\n</code></pre>\n","url":"https://hackersandslackers.com/scraping-urls-with-beautifulsoup/","uuid":"c933218e-6bbf-44b7-8f01-bfd188c71d89","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be7fc282ec6e0035b4b16bc"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673733","title":"Python-Lambda: The Essential Library for AWS Cloud Functions","slug":"improve-your-aws-lambda-workflow-with-python-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","custom_excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","created_at_pretty":"07 November, 2018","published_at_pretty":"08 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-07T16:01:48.000-05:00","published_at":"2018-11-07T19:13:20.000-05:00","updated_at":"2019-01-05T13:22:03.000-05:00","meta_title":"Simplify Lambda Deployment with python-lambda | Hackers and Slackers","meta_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","og_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","twitter_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","twitter_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our series about building AWS APIs\n[https://hackersandslackers.com/tag/aws-api/], we've covered a lot of ground\naround learning the AWS ecosystem. Now that we're all feeling a bit more\ncomfortable, it may be time to let everybody in on the world's worst-kept\nsecret: Almost nobody builds architecture by interacting with the AWS UI\ndirectly. There are plenty examples of how this is done, with the main example\nbeing HashiCorp:  an entire business model based around the premise that AWS has\na shitty UI, to the point where it's easier to write code to make things which\nwill host your code. What a world.\n\nIn the case of creating Python Lambda functions, the \"official\" (aka: manual)\nworkflow of deploying your code to AWS is something horrible like this:\n\n * You start a project locally and begin development.\n * You opt to use virtualenv, because you're well aware that you're going to\n   need the source for any packages you use available.\n * When you're ready to 'deploy' to AWS, you copy all your dependencies from \n   /site-packages  and move them into your root directory, temporarily creating\n   an abomination of a project structure.\n * With your project fully bloated and confused, you cherry pick the files\n   needed to zip into an archive.\n * Finally, you upload your code via zip either to Lambda directory or to S3,\n   only to run your code, realize its broken, and need to start all over.\n\nThere Must be a Better Way\nIndeed there is, and surprisingly enough the solution is 100% Python (sorry\nHashiCorp, we'll talk another time). This \"better way\" is my personal method of\nleveraging the following:\n\n * The official AWS CLI\n   [https://docs.aws.amazon.com/cli/latest/userguide/installing.html].\n * Pipenv [https://pipenv.readthedocs.io/en/latest/]  as an environment manager.\n * Python's python-lambda [https://github.com/nficano/python-lambda]  package:\n   the magic behind it all.\n\nObligatory \"Installing the CLI\" Recap\nFirst off, make sure you're using a compatible version of Python on your system,\nas AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by\nthe way, Python 2.7 doesn't count as compatible - let it die before your career\ndoes).\n\n$ pip3 install awscli --upgrade --user\n\n\nIf you're working off an EC2 instance, it has come to my attention pip3 does not\ncome preinstalled. Remember to run: * $ apt update\n * $ apt upgrade\n * $ apt install python3-pip\n\nYou may be prompted to run apt install awscli  as well.Awesome, now that we have\nthe CLI installed on the real  version of Python, we need to store your\ncredentials. Your Access Key ID and Secret Access Key can be found in your IAM\npolicy manager.\n\n$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:\n\nOn both Linux and OSX, this should generate files found under cd ~/.aws  which\nwill be referenced by default whenever you use an AWS service moving forward.\n\nSet Up Your Environment\nAs mentioned, we'll use pipenv  for easy environment management. We'll create an\nenvironment using Lambda's preferred Python version:\n\n$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n\n\nSomething you should be aware of at the time of writing: Pip's latest version,\n18.1, is actually a breaking change  for Pipenv. Thus, the first thing we should\ndo is force usage of pip 18.0 (is there even a fix for this yet?). This is\nsolved by typing pip3 install pip==18.0  with the Pipenv shell activated. Now\nlet's get to the easy part.\n\npython-lambda: The Savior of AWS\nSo far we've made our lives easier in two ways: we're keeping our AWS\ncredentials safe and far away from ourselves, and we have what is by far the\nsuperior Python package management solution. But this is all foreplay leading up\nto python-lambda:\n\n$ pip3 install python-lambda\n\n\nThis library alone is about to do you the following favors:\n\n * Initiate your Lambda project structure for you.\n * Isolate Lambda configuration to a  config.yaml  file, covering everything\n   from the name of your entry point, handler function, and even\n   program-specific variables.\n * Allow you to run tests locally, where a test.json  file simulates a request\n   being made to your function locally.\n * Build a production-ready zip file with all dependencies completely separated \n   from your beautiful file structure.\n * The ability to deploy directly  to S3 or Lambda with said zip file from\n   command-line.\n\nCheck out the commands for yourself:\n\nCommands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n\n\nInitiate your project\nRunning lambda init  will generate the following file structure:\n\n.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n\n\nChecking out the entry point: service.py\npython-lambda starts you off with a basic handler as an example of a working\nproject. Feel free to rename service.py  and its handler function to whatever\nyou please, as we can configure that in a bit.\n\n# -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n\n\nEasy configuration via configure.yaml\nThe base config generated by lambda init  looks like this:\n\nregion: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n\n\nLook familiar? These are all the properties you would normally have to set up\nvia the UI. As an added bonus, you can store values (such as S3 bucket names for\nboto3) in this file as well. That's dope.\n\nSetting up event.json\nThe default event.json  is about as simplistic as you can get, and naturally not\nvery helpful at first (it isn't meant to be). These are the contents:\n\n{\n  \"pi\": 3.14,\n  \"e\": 2.718\n}\n\n\nWe can replace this a real test JSON which we can grab from Lambda itself.\nHere's an example of a Cloudwatch event we can use instead:\n\n{\n  \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n  \"detail-type\": \"Scheduled Event\",\n  \"source\": \"aws.events\",\n  \"account\": \"{{account-id}}\",\n  \"time\": \"1970-01-01T00:00:00Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n  ],\n  \"pi\": 3.14,\n  \"e\": 2.718\n  \"detail\": {}\n}\n\n\nRemember that event.json  is what is being passed to our handler as the event \nparameter. Thus, now we can run our Lambda function locally  to see if it works:\n\n$ lambda invoke\n5.8580000000000005\n\n\nPretty cool if you ask me.\n\nDeploy it, Ship it, Roll Credits\nAfter you express your coding genius, remember to output pip freeze >\nrequirements.txt. python-lambda  will use this as a reference for which packages\nneed to be included. This is neat because we can use Pipenv and the benefits of\nthe workflow it provides while still easily outputting what we need to deploy. \n\nBecause we already specified which Lambda we're going to deploy to in \nconfig.yaml, we can deploy to that Lambda immediately. lambda deploy  will use\nthe zip upload method, whereas lambda deploy-s3  will store your source on S3.\n\nIf you'd like to deploy the function yourself, run with lambda build  which will\nzip your source code plus dependencies  neatly into a /dist  directory. Suddenly\nwe never have to compromise our project structure, and now we can easily source\ncontrol our Lambdas by .gitignoring our build folders while hanging on to our\nPipfiles.\n\nHere's to hoping you never need to deploy Lambdas using any other method ever\nagain. Cheers.","html":"<p>In our series about building <a href=\"https://hackersandslackers.com/tag/aws-api/\">AWS APIs</a>, we've covered a lot of ground around learning the AWS ecosystem. Now that we're all feeling a bit more comfortable, it may be time to let everybody in on the world's worst-kept secret: Almost nobody builds architecture by interacting with the AWS UI directly. There are plenty examples of how this is done, with the main example being <strong>HashiCorp:</strong> an entire business model based around the premise that AWS has a shitty UI, to the point where it's easier to write code to make things which will host your code. What a world.</p><p>In the case of creating Python Lambda functions, the \"official\" (aka: manual) workflow of deploying your code to AWS is something horrible like this:</p><ul><li>You start a project locally and begin development.</li><li>You opt to use <strong>virtualenv, </strong>because you're well aware that you're going to need the source for any packages you use available.</li><li>When you're ready to 'deploy' to AWS, you <em>copy all your dependencies from </em><code>/site-packages</code> <em>and move them into your root directory</em>, temporarily creating an abomination of a project structure.</li><li>With your project fully bloated and confused, you cherry pick the files needed to zip into an archive.</li><li>Finally, you upload your code via zip either to Lambda directory or to S3, only to run your code, realize its broken, and need to start all over.</li></ul><h2 id=\"there-must-be-a-better-way\">There Must be a Better Way</h2><p>Indeed there is, and surprisingly enough the solution is 100% Python (sorry HashiCorp, we'll talk another time). This \"better way\" is my personal method of leveraging the following:</p><ul><li>The official <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\">AWS CLI</a>.</li><li><a href=\"https://pipenv.readthedocs.io/en/latest/\">Pipenv</a> as an environment manager.</li><li>Python's <strong><a href=\"https://github.com/nficano/python-lambda\">python-lambda</a></strong> package: the magic behind it all.</li></ul><h3 id=\"obligatory-installing-the-cli-recap\">Obligatory \"Installing the CLI\" Recap</h3><p>First off, make sure you're using a compatible version of Python on your system, as AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by the way, <em>Python 2.7 </em>doesn't count as compatible - let it die before your career does).</p><pre><code class=\"language-python\">$ pip3 install awscli --upgrade --user\n</code></pre>\n<div class=\"protip\">\n    If you're working off an EC2 instance, it has come to my attention pip3 does not come preinstalled. Remember to run:\n<ul>\n    <li><code>$ apt update</code></li>\n    <li><code>$ apt upgrade</code></li>\n    <li><code>$ apt install python3-pip</code></li>\n</ul>\n    \n    You may be prompted to run <code>apt install awscli</code> as well.\n</div><p>Awesome, now that we have the CLI installed on the <em>real</em> version of Python, we need to store your credentials. Your Access Key ID and Secret Access Key can be found in your IAM policy manager.</p><pre><code>$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:</code></pre><p>On both Linux and OSX, this should generate files found under <code>cd ~/.aws</code> which will be referenced by default whenever you use an AWS service moving forward.</p><h2 id=\"set-up-your-environment\">Set Up Your Environment</h2><p>As mentioned, we'll use <code>pipenv</code> for easy environment management. We'll create an environment using Lambda's preferred Python version:</p><pre><code class=\"language-python\">$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n</code></pre>\n<p>Something you should be aware of at the time of writing: Pip's latest version, 18.1, is actually a <em>breaking change</em> for Pipenv. Thus, the first thing we should do is force usage of pip 18.0 (is there even a fix for this yet?). This is solved by typing <code>pip3 install pip==18.0</code> with the Pipenv shell activated. Now let's get to the easy part.</p><h2 id=\"python-lambda-the-savior-of-aws\">python-lambda: The Savior of AWS</h2><p>So far we've made our lives easier in two ways: we're keeping our AWS credentials safe and far away from ourselves, and we have what is by far the superior Python package management solution. But this is all foreplay leading up to <code>python-lambda</code>:</p><pre><code class=\"language-bash\">$ pip3 install python-lambda\n</code></pre>\n<p>This library alone is about to do you the following favors:</p><ul><li>Initiate your Lambda project structure for you.</li><li>Isolate Lambda configuration to a<em> config.yaml</em> file, covering everything from the name of your entry point, handler function, and even program-specific variables.</li><li>Allow you to run tests locally, where a <em>test.json</em> file simulates a request being made to your function locally.</li><li>Build a production-ready zip file with all dependencies <em>completely separated </em>from your beautiful file structure.</li><li>The ability to deploy <em>directly</em> to S3 or Lambda with said zip file from command-line.</li></ul><p>Check out the commands for yourself:</p><pre><code class=\"language-bash\">Commands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n</code></pre>\n<h3 id=\"initiate-your-project\">Initiate your project</h3><p>Running <code>lambda init</code> will generate the following file structure:</p><pre><code class=\"language-bash\">.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n</code></pre>\n<h3 id=\"checking-out-the-entry-point-service-py\">Checking out the entry point: service.py</h3><p>python-lambda starts you off with a basic handler as an example of a working project. Feel free to rename <code>service.py</code> and its handler function to whatever you please, as we can configure that in a bit.</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n</code></pre>\n<h3 id=\"easy-configuration-via-configure-yaml\">Easy configuration via configure.yaml</h3><p>The base config generated by <code>lambda init</code> looks like this:</p><pre><code class=\"language-yaml\">region: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n</code></pre>\n<p>Look familiar? These are all the properties you would normally have to set up via the UI. As an added bonus, you can store values (such as S3 bucket names for boto3) in this file as well. That's dope.</p><h3 id=\"setting-up-event-json\">Setting up event.json</h3><p>The default <code>event.json</code> is about as simplistic as you can get, and naturally not very helpful at first (it isn't meant to be). These are the contents:</p><pre><code class=\"language-json\">{\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n}\n</code></pre>\n<p>We can replace this a real test JSON which we can grab from Lambda itself. Here's an example of a Cloudwatch event we can use instead:</p><pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;cdc73f9d-aea9-11e3-9d5a-835b769c0d9c&quot;,\n  &quot;detail-type&quot;: &quot;Scheduled Event&quot;,\n  &quot;source&quot;: &quot;aws.events&quot;,\n  &quot;account&quot;: &quot;{{account-id}}&quot;,\n  &quot;time&quot;: &quot;1970-01-01T00:00:00Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [\n    &quot;arn:aws:events:us-east-1:123456789012:rule/ExampleRule&quot;\n  ],\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n  &quot;detail&quot;: {}\n}\n</code></pre>\n<p>Remember that <code>event.json</code> is what is being passed to our handler as the <code>event</code> parameter. Thus, now we can run our Lambda function <em>locally</em> to see if it works:</p><pre><code class=\"language-bash\">$ lambda invoke\n5.8580000000000005\n</code></pre>\n<p>Pretty cool if you ask me.</p><h2 id=\"deploy-it-ship-it-roll-credits\">Deploy it, Ship it, Roll Credits</h2><p>After you express your coding genius, remember to output <code>pip freeze &gt; requirements.txt</code>. <strong>python-lambda</strong> will use this as a reference for which packages need to be included. This is neat because we can use Pipenv and the benefits of the workflow it provides while still easily outputting what we need to deploy. </p><p>Because we already specified which Lambda we're going to deploy to in <code>config.yaml</code>, we can deploy to that Lambda immediately. <code>lambda deploy</code> will use the zip upload method, whereas <code>lambda deploy-s3</code> will store your source on S3.</p><p>If you'd like to deploy the function yourself, run with <code>lambda build</code> which will zip your source code <em>plus dependencies</em> neatly into a /<em>dist</em> directory. Suddenly we never have to compromise our project structure, and now we can easily source control our Lambdas by <em>.gitignoring </em>our build folders while hanging on to our Pipfiles.</p><p>Here's to hoping you never need to deploy Lambdas using any other method ever again. Cheers.</p>","url":"https://hackersandslackers.com/improve-your-aws-lambda-workflow-with-python-lambda/","uuid":"08ad7706-8dd7-4475-875e-880c017de8d5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be352bc2aa81b1606ab77a7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673731","title":"Liberating Data from PDFs with Tabula and Pandas","slug":"liberating-data-from-pdfs-with-tabula-and-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","excerpt":"Making 'open' data more open.","custom_excerpt":"Making 'open' data more open.","created_at_pretty":"03 November, 2018","published_at_pretty":"04 November, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-11-03T13:04:50.000-04:00","published_at":"2018-11-04T14:23:41.000-05:00","updated_at":"2019-02-02T08:19:55.000-05:00","meta_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","meta_description":"Making 'open' data more open: use Python's Pandas library and Tabula to extract data from PDFs.","og_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","og_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","og_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","twitter_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","twitter_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Check out the accompanying GitHub repo for this article here\n[https://github.com/grahamalama/school_budget_aggregator].\n\nTechnically, the School District of Philadelphia's budget data for the 2019\nfiscal year is \"open\". It is, after all, made available through the district's \nOpen Data portal  and is freely available to download.\n\nBut just because data is freely available, doesn't mean it's easy to work with.\nThat's what found out when I downloaded the zipped folder, opened it up, and\nfound a heap of PDFs. Joy.\n\nAs a member of Code for Philly [https://codeforphilly.org/], I thought of my\ncompatriots who might want to use school district data in their projects. I knew\nwith a bit of data munging, I could provide a data set that would be more easily\nusable.\n\nData Liberation\nThe first hurdle was to find a way to get the data from the PDFs. After a bit\nGoogling, I came across tabula-py [https://github.com/chezou/tabula-py], a\nPython wrapper for Tabula [https://tabula.technology/].\n\nEach budget is composed of 5 tables:\n\n * General information about the school\n * Enrollment information\n * Operating Funded budget allotments\n * Grant Funded budget allotments\n * A summary table of allotment totals\n\nExtracting these tables from a budget with Tabula was as simple as:\n\ntabula.read_pdf(path_to_budget, multiple_tables=True)\n\n\nWhich returned a list of DataFrames, one for each table mentioned above.\nPerfect! \nSo, I iterated over all of the files in folder and appended them to a list:\n\nimport os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f\"{directory}/{filename}\", \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(\"SY1819_School_Budgets\")\n\n\nInitial Cleaning\nWhile this gave me a good start, I knew it wouldn't be that easy to liberate the\ndata from the PDFs. I took a look at each of the DataFrames to see what I'd be\nworking with. \n\n# an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n\n\nAfter I saw the output, I wrote a function to perform the same cleaning\noperation for each table in each budget.\n\nFor each table below, first I'll introduce the \"raw\" output that Tabula\nreturned, then I'll show the function that I wrote to fix that output.\n\nBasic Information\nRaw Output\nbasic_information = sample_budget[0] #basic information\nbasic_information\n\n\n0\n 1\n 0\n Basic Information\n NaN\n 1\n Council District\n 2nd\n 2\n Organization Code\n 1380\n 3\n School Level\n Elementary School\n 4\n Economically\n NaN\n 5\n Disadvantaged Rate*\n NaN\n 6\n NaN\n 83.44%\n Cleanup Function\ndef generate_basic_information_table(df):\n    '''Series representing the \"basic information\" table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [\"Economically Disadvantaged Rate\", df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n\n\nCleaned\nbasic_information = generate_basic_information_table(basic_information)\nbasic_information\n\n\n# Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n\n\nEnrollment\nRaw Output\n# Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n\n\n0\n 1\n 2\n 3\n 0\n NaN\n FY14\n FY18\n FY19 Projected\n 1\n Enrollment**\n 842\n 640\n 602\n Cleanup Function\ndef generate_enrollment_table(df):\n    '''returns a series representing the \"enrollment\" table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=\"enrollment\")\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n\n\nCleaned\n# Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n\n\nAllotments\nLuckily, both allotment tables were identical, so I could apply to the same\ncleanup steps to both.\n\nRaw Output\noperating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n\n\n0\n 1\n 2\n 3\n 0\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n 2\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n 3\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n 4\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n 5\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n 6\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n 7\n Secretaries\n 1.0\n 1.0\n 1.0\n 8\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n 9\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n 10\n Other\n 0.0\n 1.2\n 1.0\n 11\n Total Positions\n 60.0\n 43.8\n 50.1\n 12\n Supplies/Equipment/Non Full‐Time Salaries/Other\n $32,272\n $100,159\n $97,553\n grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n\n\nCleanup Function\nI decided to merge the two allotment tables into one DataFrame while building a\nMultiIndex to keep things in order. This would allow me to ask some more\ninteresting questions further on down the road.\n\ndef generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + \"ed_allotments\"\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(\"org_code\", \"fund\", \"allotment\")\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n\n\nCleaned\npd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, \"1410\", \"operating_fund\"\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, \"1410\", \"grant_fund\"\n    )\n])\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment\n 1410\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n Totals\nSince the final \"totals\" table could be calculated from the data already in the\nnew allotment table, I didn't bother transforming it in any way.\n\n# same data can be derived from the allotments table directly\nsample_budget[4]\n\n\n0\n 1\n 2\n 3\n 4\n 0\n NaN\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Total Positions\n NaN\n 76.1\n 64.8\n 67.8\n 2\n Total Supplies/Equipment/Non Full‐Time Salarie...\n NaN\n $230,726\n $120,136\n $112,719\n Once I figured out what transformations I needed for each table, I combined\nthem into a function so that, given a list of DataFames from Tabula, I'd get\nthose same tables back neatly formatted.\n\ndef generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n\n\nAggregation Time\nNow that I had cleaned the tables that Tabula produced, it was time to combine\nthem into some aggregated tables.\n\nFirst I wrote a function that would output a Series (representing one row) of\ninformation from all tables for a given school in a given fiscal year. \n\ndef generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +\": \" + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == \"FY19\" else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == \"FY19\" else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(\"FY18\", basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n\n\n# Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n\n\nThen, I applied this function to each list of budgets in the collection and\ncompiled them into a DataFrame.\n\ndef generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = \"FY14\"\nfy14.to_csv(\"output/combined_fy14.csv\")\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = \"FY18\"\nfy18.to_csv(\"output/combined_fy18.csv\")\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = \"FY19\"\nfy19.to_csv(\"output/combined_fy19.csv\")\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(\"output/all_budgets_tabular.csv\")\n\n\nFinally, I wanted to output a CSV that would preserve some of the multi-indexed\nnature of the allotment tables. Here's what I wrote for that.\n\ndef generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(\"output/all_budgets_hierarchical.csv\")\n\nhierarchical_budget\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment  \n 1380\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n grant_fund\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n 5160 rows × 3 columnsThis makes it easier to aggregate in interesting ways:\n\nhierarchical_budget.groupby('allotment').sum()\n\n\nFY14\n FY18\n FY19\n allotment\n Classroom Assistants/Teacher Assistants\n 2365.0\n 1720.0\n 1935.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 258.0\n 215.0\n 258.0\n Nurses/Health Services\n 129.0\n 215.0\n 215.0\n Other\n 215.0\n 258.0\n 215.0\n Principals/Assistant Principals\n 430.0\n 215.0\n 215.0\n Secretaries\n 215.0\n 215.0\n 215.0\n Student Climate Staff\n 1720.0\n 1720.0\n 1505.0\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 49606090.0\n 25829240.0\n 24234585.0\n Support Services Assistants\n 1505.0\n 1505.0\n 1720.0\n Teachers ‐ Regular Education\n 8234.5\n 7224.0\n 7224.0\n Teachers ‐ Special Education\n 1290.0\n 645.0\n 1075.0\n Total Positions\n 16361.5\n 13932.0\n 14577.0\n More Cleaning to be Done\nMy work here is done. I saved the data from their not-so-accessible PDF prisons.\nBut now it's time for someone with some domain-specific knowledge to make it\nactionable.\n\nThe biggest weakness with the data in its current form is that there is some\namount of ambiguity as to what the different allotments numbers represent in\nreal-dollar amounts. Only the Supplies/Equipment/Non Full‐Time Salaries/Other \nallotment category came in currency notation – the rest of the allotments were\nrepresented as simple decimal amounts with no context to help interpret what\nthey mean. Do they represent FTE\n[https://en.wikipedia.org/wiki/Full-time_equivalent]? Dollar amounts in\nscientific notation? I'm not sure, but I hope by handing this work off to the\nright people, these questions and more can be answered more easily thanks to a\ncleaner, more accessible data set.","html":"<p><em>Check out the accompanying GitHub repo for this article <a href=\"https://github.com/grahamalama/school_budget_aggregator\">here</a>.</em></p><p>Technically, the School District of Philadelphia's budget data for the 2019 fiscal year is \"open\". It is, after all, made available through the district's <a href=\"https://www.philasd.org/performance/programsservices/open-data/district-information/#budget\">Open Data portal</a> and is freely available to download.</p><p>But just because data is freely available, doesn't mean it's easy to work with. That's what found out when I downloaded the zipped folder, opened it up, and found a heap of PDFs. Joy.</p><p>As a member of <a href=\"https://codeforphilly.org/\">Code for Philly</a>, I thought of my compatriots who might want to use school district data in their projects. I knew with a bit of data munging, I could provide a data set that would be more easily usable.</p><h2 id=\"data-liberation\">Data Liberation</h2><p>The first hurdle was to find a way to get the data from the PDFs. After a bit Googling, I came across <a href=\"https://github.com/chezou/tabula-py\"><strong>tabula-py</strong></a>, a Python wrapper for <a href=\"https://tabula.technology/\">Tabula</a>.</p><p>Each budget is composed of 5 tables:</p><ul><li>General information about the school</li><li>Enrollment information</li><li>Operating Funded budget allotments</li><li>Grant Funded budget allotments</li><li>A summary table of allotment totals</li></ul><p>Extracting these tables from a budget with Tabula was as simple as:</p><pre><code class=\"language-python\">tabula.read_pdf(path_to_budget, multiple_tables=True)\n</code></pre>\n<p>Which returned a list of DataFrames, one for each table mentioned above. Perfect! <br>So, I iterated over all of the files in folder and appended them to a list:</p><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f&quot;{directory}/{filename}&quot;, \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(&quot;SY1819_School_Budgets&quot;)\n</code></pre>\n<h2 id=\"initial-cleaning\">Initial Cleaning</h2><p>While this gave me a good start, I knew it wouldn't be that easy to liberate the data from the PDFs. I took a look at each of the DataFrames to see what I'd be working with. </p><pre><code class=\"language-python\"># an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n</code></pre>\n<p>After I saw the output, I wrote a function to perform the same cleaning operation for each table in each budget.</p><p>For each table below, first I'll introduce the \"raw\" output that Tabula returned, then I'll show the function that I wrote to fix that output.</p><h2 id=\"basic-information\">Basic Information</h2><h3 id=\"raw-output\">Raw Output</h3><pre><code class=\"language-python\">basic_information = sample_budget[0] #basic information\nbasic_information\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Basic Information</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Council District</td>\n      <td>2nd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Organization Code</td>\n      <td>1380</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>School Level</td>\n      <td>Elementary School</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Economically</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Disadvantaged Rate*</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>83.44%</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_basic_information_table(df):\n    '''Series representing the &quot;basic information&quot; table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [&quot;Economically Disadvantaged Rate&quot;, df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n</code></pre>\n<h3 id=\"cleaned\">Cleaned</h3><pre><code class=\"language-python\">basic_information = generate_basic_information_table(basic_information)\nbasic_information\n</code></pre>\n<pre><code class=\"language-python\"># Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n</code></pre>\n<h2 id=\"enrollment\">Enrollment</h2><h4 id=\"raw-output-1\">Raw Output</h4><pre><code class=\"language-python\"># Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>FY14</td>\n      <td>FY18</td>\n      <td>FY19 Projected</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Enrollment**</td>\n      <td>842</td>\n      <td>640</td>\n      <td>602</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function-1\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_enrollment_table(df):\n    '''returns a series representing the &quot;enrollment&quot; table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=&quot;enrollment&quot;)\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n</code></pre>\n<h4 id=\"cleaned-1\">Cleaned</h4><pre><code class=\"language-python\"># Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n</code></pre>\n<h2 id=\"allotments\">Allotments</h2><p>Luckily, both allotment tables were identical, so I could apply to the same cleanup steps to both.</p><h4 id=\"raw-output-2\">Raw Output</h4><pre><code class=\"language-python\">operating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Principals/Assistant Principals</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Teachers ‐ Regular Education</td>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Teachers ‐ Special Education</td>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Counselors/Student Adv./ Soc. Serv. Liaisons</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nurses/Health Services</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Classroom Assistants/Teacher Assistants</td>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Secretaries</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Support Services Assistants</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Student Climate Staff</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Other</td>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Total Positions</td>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Supplies/Equipment/Non Full‐Time Salaries/Other</td>\n      <td>$32,272</td>\n      <td>$100,159</td>\n      <td>$97,553</td>\n    </tr>\n  </tbody>\n</table>\n</div><pre><code class=\"language-python\">grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n</code></pre>\n<h3 id=\"cleanup-function-2\">Cleanup Function</h3><p>I decided to merge the two allotment tables into one DataFrame while building a MultiIndex to keep things in order. This would allow me to ask some more interesting questions further on down the road.</p><pre><code class=\"language-python\">def generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + &quot;ed_allotments&quot;\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(&quot;org_code&quot;, &quot;fund&quot;, &quot;allotment&quot;)\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n</code></pre>\n<h4 id=\"cleaned-2\">Cleaned</h4><pre><code class=\"language-python\">pd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, &quot;1410&quot;, &quot;operating_fund&quot;\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, &quot;1410&quot;, &quot;grant_fund&quot;\n    )\n])\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"24\" valign=\"top\">1410</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"totals\">Totals</h2><p>Since the final \"totals\" table could be calculated from the data already in the new allotment table, I didn't bother transforming it in any way.</p><pre><code class=\"language-python\"># same data can be derived from the allotments table directly\nsample_budget[4]\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Total Positions</td>\n      <td>NaN</td>\n      <td>76.1</td>\n      <td>64.8</td>\n      <td>67.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total Supplies/Equipment/Non Full‐Time Salarie...</td>\n      <td>NaN</td>\n      <td>$230,726</td>\n      <td>$120,136</td>\n      <td>$112,719</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Once I figured out what transformations I needed for each table, I combined them into a function so that, given a list of DataFames from Tabula, I'd get those same tables back neatly formatted.</p><pre><code class=\"language-python\">def generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n</code></pre>\n<h2 id=\"aggregation-time\">Aggregation Time</h2><p>Now that I had cleaned the tables that Tabula produced, it was time to combine them into some aggregated tables.</p><p>First I wrote a function that would output a Series (representing one row) of information from all tables for a given school in a given fiscal year. </p><pre><code class=\"language-python\">def generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +&quot;: &quot; + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == &quot;FY19&quot; else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == &quot;FY19&quot; else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(&quot;FY18&quot;, basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n</code></pre>\n<pre><code class=\"language-python\"># Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n</code></pre>\n<p>Then, I applied this function to each list of budgets in the collection and compiled them into a DataFrame.</p><pre><code class=\"language-python\">def generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = &quot;FY14&quot;\nfy14.to_csv(&quot;output/combined_fy14.csv&quot;)\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = &quot;FY18&quot;\nfy18.to_csv(&quot;output/combined_fy18.csv&quot;)\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = &quot;FY19&quot;\nfy19.to_csv(&quot;output/combined_fy19.csv&quot;)\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(&quot;output/all_budgets_tabular.csv&quot;)\n</code></pre>\n<p>Finally, I wanted to output a CSV that would preserve some of the multi-indexed nature of the allotment tables. Here's what I wrote for that.</p><pre><code class=\"language-python\">def generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(&quot;output/all_budgets_hierarchical.csv&quot;)\n\nhierarchical_budget\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>       \n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"61\" valign=\"top\">1380</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">grant_fund</th>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n<div style=\"text-align: right;\n    width: 100%;\n    font-family: Gordita-Medium,sans-serif;\n    font-size: .9em;\n    margin-top: -20px;\">5160 rows × 3 columns</div>\n</div><p>This makes it easier to aggregate in interesting ways:</p><pre><code class=\"language-python\">hierarchical_budget.groupby('allotment').sum()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>2365.0</td>\n      <td>1720.0</td>\n      <td>1935.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>258.0</td>\n      <td>215.0</td>\n      <td>258.0</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>129.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>215.0</td>\n      <td>258.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Principals/Assistant Principals</th>\n      <td>430.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>215.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>1720.0</td>\n      <td>1720.0</td>\n      <td>1505.0</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>49606090.0</td>\n      <td>25829240.0</td>\n      <td>24234585.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>1505.0</td>\n      <td>1505.0</td>\n      <td>1720.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8234.5</td>\n      <td>7224.0</td>\n      <td>7224.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>1290.0</td>\n      <td>645.0</td>\n      <td>1075.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16361.5</td>\n      <td>13932.0</td>\n      <td>14577.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"more-cleaning-to-be-done\">More Cleaning to be Done</h2><p>My work here is done. I saved the data from their not-so-accessible PDF prisons. But now it's time for someone with some domain-specific knowledge to make it actionable.</p><p>The biggest weakness with the data in its current form is that there is some amount of ambiguity as to what the different allotments numbers represent in real-dollar amounts. Only the <strong>Supplies/Equipment/Non Full‐Time Salaries/Other</strong> allotment category came in currency notation – the rest of the allotments were represented as simple decimal amounts with no context to help interpret what they mean. Do they represent <a href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\">FTE</a>? Dollar amounts in scientific notation? I'm not sure, but I hope by handing this work off to the right people, these questions and more can be answered more easily thanks to a cleaner, more accessible data set.</p>","url":"https://hackersandslackers.com/liberating-data-from-pdfs-with-tabula-and-pandas/","uuid":"ab1a4ee3-9cc3-43a6-9ebe-5a885ae264a2","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bddd5323ea1e4769817c4c9"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372b","title":"Lynx Roundup, October 31st","slug":"lynx-roundup-october-31st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/124@2x.jpg","excerpt":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics!","custom_excerpt":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics!","created_at_pretty":"16 October, 2018","published_at_pretty":"31 October, 2018","updated_at_pretty":"03 November, 2018","created_at":"2018-10-16T02:00:55.000-04:00","published_at":"2018-10-31T07:00:00.000-04:00","updated_at":"2018-11-03T11:02:20.000-04:00","meta_title":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics! | Hackers And Slackers","meta_description":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics! | Hackers And Slackers","og_description":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics!","og_image":"https://hackersandslackers.com/content/images/lynx/124@2x.jpg","og_title":"Lynx Roundup, October 31st","twitter_description":"Write better Python functions!  Functional programming & Blockchain!  Prescriptive Analytics!","twitter_image":"https://hackersandslackers.com/content/images/lynx/124@2x.jpg","twitter_title":"Lynx Roundup, October 31st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://jeffknupp.com/blog/2018/10/11/write-better-python-functions//\n\nhttps://www.economist.com/finance-and-economics/2018/07/21/as-inequality-grows-so-does-the-political-influence-of-the-rich\n\nhttps://hackernoon.com/advantages-of-functional-programming-for-blockchain-protocols-1ca2d4ac1033\n\nhttps://towardsdatascience.com/going-prescriptive-the-real-super-power-of-data-science-66ebb79fb89c\n\nhttps://cointelegraph.com/news/bitcoin-mining-can-power-neuroscience-says-matrix-chief-ai-scientist\n\nhttps://neurosciencenews.com/ai-blockchain-healthcare-8035/\n\nhttp://explained.ai/rf-importance/index.html","html":"<p></p><p><a href=\"https://jeffknupp.com/blog/2018/10/11/write-better-python-functions//\">https://jeffknupp.com/blog/2018/10/11/write-better-python-functions//</a></p><p><a href=\"https://www.economist.com/finance-and-economics/2018/07/21/as-inequality-grows-so-does-the-political-influence-of-the-rich\">https://www.economist.com/finance-and-economics/2018/07/21/as-inequality-grows-so-does-the-political-influence-of-the-rich</a></p><p><a href=\"https://hackernoon.com/advantages-of-functional-programming-for-blockchain-protocols-1ca2d4ac1033\">https://hackernoon.com/advantages-of-functional-programming-for-blockchain-protocols-1ca2d4ac1033</a></p><p><a href=\"https://towardsdatascience.com/going-prescriptive-the-real-super-power-of-data-science-66ebb79fb89c\">https://towardsdatascience.com/going-prescriptive-the-real-super-power-of-data-science-66ebb79fb89c</a></p><p><a href=\"https://cointelegraph.com/news/bitcoin-mining-can-power-neuroscience-says-matrix-chief-ai-scientist\">https://cointelegraph.com/news/bitcoin-mining-can-power-neuroscience-says-matrix-chief-ai-scientist</a></p><p><a href=\"https://neurosciencenews.com/ai-blockchain-healthcare-8035/\">https://neurosciencenews.com/ai-blockchain-healthcare-8035/</a></p><p><a href=\"http://explained.ai/rf-importance/index.html\">http://explained.ai/rf-importance/index.html</a></p>","url":"https://hackersandslackers.com/lynx-roundup-october-31st/","uuid":"f340b421-3218-44e1-b713-ffe6b9786a23","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bc57e97c0b1ac3b7ed7ac70"}}]}},"pageContext":{"pageNumber":11,"humanPageNumber":12,"skip":132,"limit":12,"numberOfPages":33,"previousPagePath":"/page/11","nextPagePath":"/page/13"}}