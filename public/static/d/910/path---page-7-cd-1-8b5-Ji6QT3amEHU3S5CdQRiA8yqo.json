{"data":{"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c4befd7fd307f72b350ae0b","title":"Lynx Roundup, February 1st","slug":"lynx-roundup-february-1st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/98@2x.jpg","excerpt":"Custom loss functions in Keras!  Visualizing Gaussian Processes!  Game Developers' Union gaining steam!","custom_excerpt":"Custom loss functions in Keras!  Visualizing Gaussian Processes!  Game Developers' Union gaining steam!","created_at_pretty":"26 January, 2019","published_at_pretty":"01 February, 2019","updated_at_pretty":"01 February, 2019","created_at":"2019-01-26T00:27:51.000-05:00","published_at":"2019-02-01T07:00:00.000-05:00","updated_at":"2019-02-01T07:00:00.000-05:00","meta_title":"Lynx Roundup, February 1st | Hackers and Slackers","meta_description":"Custom loss functions in Keras!  Visualizing Gaussian Processes!  Game Developers' Union gaining steam!","og_description":"Custom loss functions in Keras!  Visualizing Gaussian Processes!  Game Developers' Union gaining steam!","og_image":"https://hackersandslackers.com/content/images/lynx/98@2x.jpg","og_title":"Lynx Roundup, February 1st","twitter_description":"Custom loss functions in Keras!  Visualizing Gaussian Processes!  Game Developers' Union gaining steam!","twitter_image":"https://hackersandslackers.com/content/images/lynx/98@2x.jpg","twitter_title":"Lynx Roundup, February 1st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n\nhttps://boingboing.net/2019/01/11/start-with-a-hypothesis.html\n\nhttps://towardsdatascience.com/the-cold-start-problem-how-to-build-your-machine-learning-portfolio-6718b4ae83e9\n\nhttps://www.jgoertler.com/visual-exploration-gaussian-processes/\n\nhttps://read.acloud.guru/save-time-and-money-with-aws-lambda-using-asynchronous-programming-3548ea65f751\n\nhttps://arxiv.org/abs/1811.12808\n\nhttps://gdconf.com/news/nearly-50-devs-support-unionization-new-gdc-state-industry-report","html":"<p></p><p><a href=\"https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\">https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618</a></p><p><a href=\"https://boingboing.net/2019/01/11/start-with-a-hypothesis.html\">https://boingboing.net/2019/01/11/start-with-a-hypothesis.html</a></p><p><a href=\"https://towardsdatascience.com/the-cold-start-problem-how-to-build-your-machine-learning-portfolio-6718b4ae83e9\">https://towardsdatascience.com/the-cold-start-problem-how-to-build-your-machine-learning-portfolio-6718b4ae83e9</a></p><p><a href=\"https://www.jgoertler.com/visual-exploration-gaussian-processes/\">https://www.jgoertler.com/visual-exploration-gaussian-processes/</a></p><p><a href=\"https://read.acloud.guru/save-time-and-money-with-aws-lambda-using-asynchronous-programming-3548ea65f751\">https://read.acloud.guru/save-time-and-money-with-aws-lambda-using-asynchronous-programming-3548ea65f751</a></p><p><a href=\"https://arxiv.org/abs/1811.12808\">https://arxiv.org/abs/1811.12808</a></p><p><a href=\"https://gdconf.com/news/nearly-50-devs-support-unionization-new-gdc-state-industry-report\">https://gdconf.com/news/nearly-50-devs-support-unionization-new-gdc-state-industry-report</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-1st/","uuid":"2e969a52-609b-409c-9750-e2fbdc5b2307","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4befd7fd307f72b350ae0b"}},{"node":{"id":"Ghost__Post__5c4beeccfd307f72b350ae06","title":"Lynx Roundup, January 31st","slug":"lynx-roundup-january-31st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/96@2x.jpg","excerpt":"Training med students with a virtual cadaver!  Analyzing steel microstructure with Computer Vision!  A tool for annotating text for NLP datasets!","custom_excerpt":"Training med students with a virtual cadaver!  Analyzing steel microstructure with Computer Vision!  A tool for annotating text for NLP datasets!","created_at_pretty":"26 January, 2019","published_at_pretty":"31 January, 2019","updated_at_pretty":"31 January, 2019","created_at":"2019-01-26T00:23:24.000-05:00","published_at":"2019-01-31T07:00:00.000-05:00","updated_at":"2019-01-31T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 31st | Hackers and Slackers","meta_description":"Training med students with a virtual cadaver!  Analyzing steel microstructure with Computer Vision!  A tool for annotating text for NLP datasets!","og_description":"Training med students with a virtual cadaver!  Analyzing steel microstructure with Computer Vision!  A tool for annotating text for NLP datasets!","og_image":"https://hackersandslackers.com/content/images/lynx/96@2x.jpg","og_title":"Lynx Roundup, January 31st","twitter_description":"Training med students with a virtual cadaver!  Analyzing steel microstructure with Computer Vision!  A tool for annotating text for NLP datasets!","twitter_image":"https://hackersandslackers.com/content/images/lynx/96@2x.jpg","twitter_title":"Lynx Roundup, January 31st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"http://www.technoblogy.com/show\n\nhttps://www.cuanschutztoday.org/virtual-human-a-living-cadaver-pushes-boundaries-of-anatomical-science/\n\nhttps://www.researchgate.net/publication/317711703_Advanced_Steel_Microstructural_Classification_by_Deep_Learning_Methods\n\nhttp://nlp.seas.harvard.edu/NamedTensor\n\nhttps://github.com/chakki-works/doccano\n\nhttps://github.com/quantumlib/Cirq\n\nhttps://santafe.edu/news-center/news/these-9-measures-reveal-how-forests-are-controlled-climate","html":"<p></p><p><a href=\"http://www.technoblogy.com/show\">http://www.technoblogy.com/show</a></p><p><a href=\"https://www.cuanschutztoday.org/virtual-human-a-living-cadaver-pushes-boundaries-of-anatomical-science/\">https://www.cuanschutztoday.org/virtual-human-a-living-cadaver-pushes-boundaries-of-anatomical-science/</a></p><p><a href=\"https://www.researchgate.net/publication/317711703_Advanced_Steel_Microstructural_Classification_by_Deep_Learning_Methods\">https://www.researchgate.net/publication/317711703_Advanced_Steel_Microstructural_Classification_by_Deep_Learning_Methods</a></p><p><a href=\"http://nlp.seas.harvard.edu/NamedTensor\">http://nlp.seas.harvard.edu/NamedTensor</a></p><p><a href=\"https://github.com/chakki-works/doccano\">https://github.com/chakki-works/doccano</a></p><p><a href=\"https://github.com/quantumlib/Cirq\">https://github.com/quantumlib/Cirq</a></p><p><a href=\"https://santafe.edu/news-center/news/these-9-measures-reveal-how-forests-are-controlled-climate\">https://santafe.edu/news-center/news/these-9-measures-reveal-how-forests-are-controlled-climate</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-31st/","uuid":"e4d71628-4003-4d4d-a5a8-a877531e3b43","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4beeccfd307f72b350ae06"}},{"node":{"id":"Ghost__Post__5c4bee67fd307f72b350ae01","title":"Lynx Roundup, January 30th","slug":"lynx-roundup-january-30th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/95-1@2x.jpg","excerpt":"Data cleaning!  Realtime SQL Queries with Kafka!  Good PySpark code snippets!","custom_excerpt":"Data cleaning!  Realtime SQL Queries with Kafka!  Good PySpark code snippets!","created_at_pretty":"26 January, 2019","published_at_pretty":"30 January, 2019","updated_at_pretty":"04 February, 2019","created_at":"2019-01-26T00:21:43.000-05:00","published_at":"2019-01-30T07:00:00.000-05:00","updated_at":"2019-02-04T10:11:55.000-05:00","meta_title":"Lynx Roundup, January 30th | Hackers and Slackers","meta_description":"Data cleaning!  Realtime SQL Queries with Kafka!  Good PySpark code snippets!","og_description":"Data cleaning!  Realtime SQL Queries with Kafka!  Good PySpark code snippets!","og_image":"https://hackersandslackers.com/content/images/lynx/95-1@2x.jpg","og_title":"Lynx Roundup, January 30th","twitter_description":"Data cleaning!  Realtime SQL Queries with Kafka!  Good PySpark code snippets!","twitter_image":"https://hackersandslackers.com/content/images/lynx/95-1@2x.jpg","twitter_title":"Lynx Roundup, January 30th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/the-simple-yet-practical-data-cleaning-codes-ad27c4ce0a38\n\nhttps://rockset.com/blog/real-time-analytics-using-sql-on-streaming-data-kafka-rockset/\n\nhttps://github.com/justusschock/delira\n\nhttp://www.alpha-epsilon.de/cca175/2017/09/13/filter-aggregate-join-rank-and-sort-datasets-spark-python/\n\nhttps://medium.com/@dimitritheoharatos/a-primer-on-a-b-testing-part-1-the-central-limit-theorem-fdd01154e86c\n\nhttps://www.cbronline.com/news/homomorphic-encryption-microsoft\n\nhttps://www.bloomberg.com/news/articles/2019-01-24/google-urged-the-u-s-to-limit-protection-for-activist-workers","html":"<p></p><p><a href=\"https://towardsdatascience.com/the-simple-yet-practical-data-cleaning-codes-ad27c4ce0a38\">https://towardsdatascience.com/the-simple-yet-practical-data-cleaning-codes-ad27c4ce0a38</a></p><p><a href=\"https://rockset.com/blog/real-time-analytics-using-sql-on-streaming-data-kafka-rockset/\">https://rockset.com/blog/real-time-analytics-using-sql-on-streaming-data-kafka-rockset/</a></p><p><a href=\"https://github.com/justusschock/delira\">https://github.com/justusschock/delira</a></p><p><a href=\"http://www.alpha-epsilon.de/cca175/2017/09/13/filter-aggregate-join-rank-and-sort-datasets-spark-python/\">http://www.alpha-epsilon.de/cca175/2017/09/13/filter-aggregate-join-rank-and-sort-datasets-spark-python/</a></p><p><a href=\"https://medium.com/@dimitritheoharatos/a-primer-on-a-b-testing-part-1-the-central-limit-theorem-fdd01154e86c\">https://medium.com/@dimitritheoharatos/a-primer-on-a-b-testing-part-1-the-central-limit-theorem-fdd01154e86c</a></p><p><a href=\"https://www.cbronline.com/news/homomorphic-encryption-microsoft\">https://www.cbronline.com/news/homomorphic-encryption-microsoft</a></p><p><a href=\"https://www.bloomberg.com/news/articles/2019-01-24/google-urged-the-u-s-to-limit-protection-for-activist-workers\">https://www.bloomberg.com/news/articles/2019-01-24/google-urged-the-u-s-to-limit-protection-for-activist-workers</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-30th/","uuid":"3e05dbcc-b76c-44a7-b0ba-9d3192719564","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4bee67fd307f72b350ae01"}},{"node":{"id":"Ghost__Post__5c4e57144b23df2da7332b80","title":"Downcast Numerical Data Types with Pandas","slug":"downcast-numerical-columns-python-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","excerpt":"Using an Example Where We Downcast Numerical Columns.","custom_excerpt":"Using an Example Where We Downcast Numerical Columns.","created_at_pretty":"28 January, 2019","published_at_pretty":"28 January, 2019","updated_at_pretty":"14 February, 2019","created_at":"2019-01-27T20:12:52.000-05:00","published_at":"2019-01-28T07:30:00.000-05:00","updated_at":"2019-02-13T22:50:18.000-05:00","meta_title":"Using Pandas' Assign Function on Multiple Columns | Hackers and Slackers","meta_description":"Using Pandas' Assign function on multiple columns via an example: downcasting numerical columns.","og_description":"Using Pandas' Assign by example: downcasting numerical columns.","og_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","og_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","twitter_description":"Using Pandas' Assign by example: downcasting numerical columns.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","twitter_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Recently, I had to find a way to reduce the memory footprint of a Pandas\nDataFrame in order to actually do operations on it.  Here's a trick that came in\nhandy!\n\nBy default, if you read a DataFrame from a file, it'll cast all the numerical\ncolumns as the float64  type.  This is in keeping with the philosophy behind\nPandas and NumPy - by using strict types (instead of normal Python \"duck\ntyping\"), you can do things a lot faster.  The float64  is the most flexible\nnumerical type - it can handle fractions, as well as turning missing values into\na NaN.  This will let us read it into memory, and then start messing with it.\n The downside is that it consumes a lot of memory.\n\nNow, let's say we want to save memory by manually downcasting our columns into\nthe smallest type that can handle its values?  And let's ALSO say that we want\nto be really, really lazy and don't want to look at a bunch of numbers by hand.\n And let's say we wanna do this via Method Chaining, because of all the\nadvantages outlined here: https://tomaugspurger.github.io/method-chaining\n\nLet's introduce our example DataFrame.  We'll convert all the values to floats\nmanually because that's what the default is when we read from a file.\n\ndf = pd.DataFrame({\n    \"stay_float\": [0.5, 3.7, 7.5],\n    \"to_int\": [-5, 7, 5],\n    \"to_uint\": [1, 100, 200]}).astype(float)\n\n\nFirst, let's introduce the workhorse of this exercise - Pandas's to_numeric \nfunction, and its handy optional argument, downcast.  This will take a numerical\ntype - float, integer  (not int), or unsigned  - and then downcast it to the\nsmallest version available.\n\nNext, let's make a function that checks to see if a column can be downcast from\na float to an integer.\n\ndef float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser\n\nWe're using the try/except pattern here because if we try to make a column with \nNaN  values into an integer column, it'll throw an error.  If it'd otherwise be\na good candidate for turning into an integer, we should figure a value to impute\nfor those missing values - but that'll be different for every column.  Sometimes\nit'd make sense to make it 0, other times the mean or median of the column, or\nsomething else entirely.\n\nI'd also like to direct your attention to Line 4, which has a very useful Pandas\npattern - if (ser == int_ser).all().  When you do operations on Pandas columns\nlike Equals or Greater Than, you get a new column where the operation was\napplied element-by-element.  If you're trying to set up a conditional, the\ninterpreter doesn't know what to do with an array containing [True, False, True] \n - you have to boil it down to a single value.  So, if you wan to check if two\ncolumns are completely equal, you have to call the .all()  method (which has a\nuseful sibling, any()) to make a conditional that can actually be used to\ncontrol execution.\n\nNext, let's make a function that lets us apply a transformation to multiple\ncolumns based on a condition.  The assign  method is pretty awesome, and it'd be\nfun to not have to leave it (or, if we do, to at least replace it with a\nfunction we can pipe as part of a chain of transformations to the DataFrame as a\nwhole).\n\ndef multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n\n\nassign  lets us do multiple assignments, so long as we make a dictionary of\ncolumn names and target values and then unpack it.  Really, it'd actually be\neasier to skip the function and go directly to using this syntax, except that\nI'm not aware of a method of accessing a filterable list of the DF's columns\nwhile still \"in\" the chain.  I think future versions of Pandas' syntax will\ninclude this, as I've read they want to support more Method Chaining.\n Personally, I find the reduction in Cognitive Load is worth it, with having a\nlot of little modular lego-piece transformations chained together.\n\nIt also works as a nice foundation for other little helper functions.  So,\nhere's one to turn as many float columns to integers as we can.\n\ndef all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[\"float\"])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSee the pattern in action!  We decide on a transformation function, we decide on\nwhat conditions we want to apply all these transformations (we could have a\nhundred columns, and who wants to make a note of all that?), and then we pass it\nto the multi-assign  function.\n\n(df\n     .pipe(all_float_to_int)).dtypes\n\n\nstay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n\n\nCool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of\ninteger takes up as many bytes as 64 bytes of float, just like how a hundred\npounds of feathers weighs as much as a hundred pounds of bricks.  What we did do\nis make it easier to downcast those columns later.\n\nNext, let's make a function that takes a subset of the columns, and tries to\ndowncast it to the smallest version that it can.  We've got fairly small values\nhere, so it should get some work done.\n\ndef downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSame basic pattern as before!  But now we have two arguments - one is the \ntarget_type, which tells us what types to try to downcast to.  By default, this\nwill be the same as the initial_type, with one exception that we'll grab in a\nsecond!\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n\n\nAlright, now we're getting somewhere!  Wonder if we can do even better, though?\n That last column has a conspicuous name!  And it has no values lower than 0 -\nmaybe we could save space if we store it as an unsigned integer!  Let's add a\npipe to our chain that'll try to downcast certain integers into unsigneds...\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n\n\nWhat do ya know, we can!\n\nLet's see how much memory we save by doing this.\n\ndf.info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n\n\nvs\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n\n\n152 down to 98 - we reduced it by more than 1/3rd!","html":"<p>Recently, I had to find a way to reduce the memory footprint of a Pandas DataFrame in order to actually do operations on it.  Here's a trick that came in handy!</p><p>By default, if you read a DataFrame from a file, it'll cast all the numerical columns as the <code>float64</code> type.  This is in keeping with the philosophy behind Pandas and NumPy - by using strict types (instead of normal Python \"duck typing\"), you can do things a lot faster.  The <code>float64</code> is the most flexible numerical type - it can handle fractions, as well as turning missing values into a <code>NaN</code>.  This will let us read it into memory, and then start messing with it.  The downside is that it consumes a lot of memory.</p><p>Now, let's say we want to save memory by manually downcasting our columns into the smallest type that can handle its values?  And let's ALSO say that we want to be really, really lazy and don't want to look at a bunch of numbers by hand.  And let's say we wanna do this via Method Chaining, because of all the advantages outlined here: <a href=\"https://tomaugspurger.github.io/method-chaining\">https://tomaugspurger.github.io/method-chaining</a></p><p>Let's introduce our example DataFrame.  We'll convert all the values to floats manually because that's what the default is when we read from a file.</p><pre><code class=\"language-python\">df = pd.DataFrame({\n    &quot;stay_float&quot;: [0.5, 3.7, 7.5],\n    &quot;to_int&quot;: [-5, 7, 5],\n    &quot;to_uint&quot;: [1, 100, 200]}).astype(float)\n</code></pre>\n<p>First, let's introduce the workhorse of this exercise - Pandas's <code>to_numeric</code> function, and its handy optional argument, <code>downcast</code>.  This will take a numerical type - <code>float</code>, <code>integer</code> (not <code>int</code>), or <code>unsigned</code> - and then downcast it to the smallest version available.</p><p>Next, let's make a function that checks to see if a column can be downcast from a float to an integer.</p><pre><code>def float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser</code></pre><p>We're using the try/except pattern here because if we try to make a column with <code>NaN</code> values into an integer column, it'll throw an error.  If it'd otherwise be a good candidate for turning into an integer, we should figure a value to impute for those missing values - but that'll be different for every column.  Sometimes it'd make sense to make it 0, other times the mean or median of the column, or something else entirely.</p><p>I'd also like to direct your attention to Line 4, which has a very useful Pandas pattern - <code>if (ser == int_ser).all()</code>.  When you do operations on Pandas columns like Equals or Greater Than, you get a new column where the operation was applied element-by-element.  If you're trying to set up a conditional, the interpreter doesn't know what to do with an array containing <code>[True, False, True]</code> - you have to boil it down to a single value.  So, if you wan to check if two columns are completely equal, you have to call the <code>.all()</code> method (which has a useful sibling, <code>any()</code>) to make a conditional that can actually be used to control execution.</p><p>Next, let's make a function that lets us apply a transformation to multiple columns based on a condition.  The <code>assign</code> method is pretty awesome, and it'd be fun to not have to leave it (or, if we do, to at least replace it with a function we can pipe as part of a chain of transformations to the DataFrame as a whole).</p><pre><code class=\"language-python\">def multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n</code></pre>\n<p><code>assign</code> lets us do multiple assignments, so long as we make a dictionary of column names and target values and then unpack it.  Really, it'd actually be easier to skip the function and go directly to using this syntax, except that I'm not aware of a method of accessing a filterable list of the DF's columns while still \"in\" the chain.  I think future versions of Pandas' syntax will include this, as I've read they want to support more Method Chaining.  Personally, I find the reduction in Cognitive Load is worth it, with having a lot of little modular lego-piece transformations chained together.  </p><p>It also works as a nice foundation for other little helper functions.  So, here's one to turn as many float columns to integers as we can.</p><pre><code class=\"language-python\">def all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[&quot;float&quot;])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>See the pattern in action!  We decide on a transformation function, we decide on what conditions we want to apply all these transformations (we could have a hundred columns, and who wants to make a note of all that?), and then we pass it to the <code>multi-assign</code> function.  </p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n</code></pre>\n<p>Cool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of integer takes up as many bytes as 64 bytes of float, just like how a hundred pounds of feathers weighs as much as a hundred pounds of bricks.  What we did do is make it easier to downcast those columns later.</p><p>Next, let's make a function that takes a subset of the columns, and tries to downcast it to the smallest version that it can.  We've got fairly small values here, so it should get some work done.</p><pre><code class=\"language-python\">def downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>Same basic pattern as before!  But now we have two arguments - one is the <code>target_type</code>, which tells us what types to try to downcast to.  By default, this will be the same as the <code>initial_type</code>, with one exception that we'll grab in a second!</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n</code></pre>\n<p>Alright, now we're getting somewhere!  Wonder if we can do even better, though?  That last column has a conspicuous name!  And it has no values lower than 0 - maybe we could save space if we store it as an unsigned integer!  Let's add a pipe to our chain that'll try to downcast certain integers into unsigneds...</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n</code></pre>\n<p>What do ya know, we can!</p><p>Let's see how much memory we save by doing this.</p><pre><code class=\"language-python\">df.info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n</code></pre>\n<p>vs</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n</code></pre>\n<p>152 down to 98 - we reduced it by more than 1/3rd!</p>","url":"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/","uuid":"58bbb902-99bb-404d-8a3c-232d56b6e776","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c4e57144b23df2da7332b80"}},{"node":{"id":"Ghost__Post__5c4bec7cfd307f72b350adf7","title":"Lynx Roundup, January 29th","slug":"lynx-roundup-january-29th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/93@2x.jpg","excerpt":"Generating textbooks from wikipedia!  SQL Queries on JSON files!  Data Science at Fortnite!","custom_excerpt":"Generating textbooks from wikipedia!  SQL Queries on JSON files!  Data Science at Fortnite!","created_at_pretty":"26 January, 2019","published_at_pretty":"28 January, 2019","updated_at_pretty":"28 January, 2019","created_at":"2019-01-26T00:13:32.000-05:00","published_at":"2019-01-28T07:00:00.000-05:00","updated_at":"2019-01-28T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 29th | Hackers and Slackers","meta_description":"Generating textbooks from wikipedia!  SQL Queries on JSON files!  Data Science at Fortnite!","og_description":"Generating textbooks from wikipedia!  SQL Queries on JSON files!  Data Science at Fortnite!","og_image":"https://hackersandslackers.com/content/images/lynx/93@2x.jpg","og_title":"Lynx Roundup, January 29th","twitter_description":"Generating textbooks from wikipedia!  SQL Queries on JSON files!  Data Science at Fortnite!","twitter_image":"https://hackersandslackers.com/content/images/lynx/93@2x.jpg","twitter_title":"Lynx Roundup, January 29th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://twitter.com/uxresearch/status/1079148047257395200\n\nhttps://twitter.com/kellabyte/status/1082338931503378432\n\nhttps://www.technologyreview.com/s/612726/this-algorithm-browses-wikipedia-to-auto-generate-textbooks/\n\nhttps://www.nextplatform.com/2019/01/15/spanning-the-database-world-with-google/\n\nhttps://rockset.com/blog/how-to-do-data-science-using-sql-on-raw-json/\n\nhttps://www.quora.com/What-textbooks-had-the-biggest-impact-on-you-as-a-machine-learning-researcher\n\nhttps://www.zdnet.com/article/how-fortnite-approaches-analytics-cloud-to-analyze-petabytes-of-game-data/","html":"<p></p><p><a href=\"https://twitter.com/uxresearch/status/1079148047257395200\">https://twitter.com/uxresearch/status/1079148047257395200</a></p><p><a href=\"https://twitter.com/kellabyte/status/1082338931503378432\">https://twitter.com/kellabyte/status/1082338931503378432</a></p><p><a href=\"https://www.technologyreview.com/s/612726/this-algorithm-browses-wikipedia-to-auto-generate-textbooks/\">https://www.technologyreview.com/s/612726/this-algorithm-browses-wikipedia-to-auto-generate-textbooks/</a></p><p><a href=\"https://www.nextplatform.com/2019/01/15/spanning-the-database-world-with-google/\">https://www.nextplatform.com/2019/01/15/spanning-the-database-world-with-google/</a></p><p><a href=\"https://rockset.com/blog/how-to-do-data-science-using-sql-on-raw-json/\">https://rockset.com/blog/how-to-do-data-science-using-sql-on-raw-json/</a></p><p><a href=\"https://www.quora.com/What-textbooks-had-the-biggest-impact-on-you-as-a-machine-learning-researcher\">https://www.quora.com/What-textbooks-had-the-biggest-impact-on-you-as-a-machine-learning-researcher</a></p><p><a href=\"https://www.zdnet.com/article/how-fortnite-approaches-analytics-cloud-to-analyze-petabytes-of-game-data/\">https://www.zdnet.com/article/how-fortnite-approaches-analytics-cloud-to-analyze-petabytes-of-game-data/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-29th/","uuid":"30726bfd-300a-4f44-8b50-a1c3c2c47351","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4bec7cfd307f72b350adf7"}},{"node":{"id":"Ghost__Post__5c4bea9afd307f72b350adeb","title":"Lynx Roundup, January 27th","slug":"lynx-roundup-january-27th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/94@2x.jpg","excerpt":"Useful sh scripts for Data Science!  Scaling Tensorflow with Kubeflow!  Machine Learning for kids!","custom_excerpt":"Useful sh scripts for Data Science!  Scaling Tensorflow with Kubeflow!  Machine Learning for kids!","created_at_pretty":"26 January, 2019","published_at_pretty":"27 January, 2019","updated_at_pretty":"26 February, 2019","created_at":"2019-01-26T00:05:30.000-05:00","published_at":"2019-01-27T07:00:00.000-05:00","updated_at":"2019-02-25T21:10:49.000-05:00","meta_title":"Lynx Roundup, January 27th | Hackers and Slackers","meta_description":"Useful sh scripts for Data Science!  Scaling Tensorflow with Kubeflow!  Machine Learning for kids!","og_description":"Useful sh scripts for Data Science!  Scaling Tensorflow with Kubeflow!  Machine Learning for kids!","og_image":"https://hackersandslackers.com/content/images/2019/02/94@2x.jpg","og_title":"Lynx Roundup, January 27th","twitter_description":"Useful sh scripts for Data Science!  Scaling Tensorflow with Kubeflow!  Machine Learning for kids!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/94@2x.jpg","twitter_title":"Lynx Roundup, January 27th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://github.com/nullbyte91/Simple-Sh-DataScience\n\nhttps://semiengineering.com/chip-industry-in-rapid-transition/\n\nhttps://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n\nhttps://blog.cloudflare.com/cloudflare-registrar/\n\nhttps://machinelearningmastery.com/books-for-deep-learning-practitioners/\n\nhttps://learnk8s.io/blog/scaling-machine-learning-with-kubeflow-tensorflow\n\nhttps://machinelearningforkids.co.uk/","html":"<p></p><p><a href=\"https://github.com/nullbyte91/Simple-Sh-DataScience\">https://github.com/nullbyte91/Simple-Sh-DataScience</a></p><p><a href=\"https://semiengineering.com/chip-industry-in-rapid-transition/\">https://semiengineering.com/chip-industry-in-rapid-transition/</a></p><p><a href=\"https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\">https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/</a></p><p><a href=\"https://blog.cloudflare.com/cloudflare-registrar/\">https://blog.cloudflare.com/cloudflare-registrar/</a></p><p><a href=\"https://machinelearningmastery.com/books-for-deep-learning-practitioners/\">https://machinelearningmastery.com/books-for-deep-learning-practitioners/</a></p><p><a href=\"https://learnk8s.io/blog/scaling-machine-learning-with-kubeflow-tensorflow\">https://learnk8s.io/blog/scaling-machine-learning-with-kubeflow-tensorflow</a></p><p><a href=\"https://machinelearningforkids.co.uk/\">https://machinelearningforkids.co.uk/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-27th/","uuid":"30713969-8248-4786-8429-8d30dab2bff9","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c4bea9afd307f72b350adeb"}},{"node":{"id":"Ghost__Post__5c4beb23fd307f72b350adf2","title":"Lynx Roundup, January 28th","slug":"lynx-roundup-january-28th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/92@2x.jpg","excerpt":"The future of spreadsheets!  A depot for pre-trained ML models!  Programming paradigms!","custom_excerpt":"The future of spreadsheets!  A depot for pre-trained ML models!  Programming paradigms!","created_at_pretty":"26 January, 2019","published_at_pretty":"27 January, 2019","updated_at_pretty":"27 January, 2019","created_at":"2019-01-26T00:07:47.000-05:00","published_at":"2019-01-27T07:00:00.000-05:00","updated_at":"2019-01-27T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 28th | Hackers and Slackers","meta_description":"The future of spreadsheets!  A depot for pre-trained ML models!  Programming paradigms!","og_description":"The future of spreadsheets!  A depot for pre-trained ML models!  Programming paradigms!","og_image":"https://hackersandslackers.com/content/images/lynx/92@2x.jpg","og_title":"Lynx Roundup, January 28th","twitter_description":"The future of spreadsheets!  A depot for pre-trained ML models!  Programming paradigms!","twitter_image":"https://hackersandslackers.com/content/images/lynx/92@2x.jpg","twitter_title":"Lynx Roundup, January 28th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.microsoft.com/en-us/research/blog/influencing-mainstream-software-applying-programming-language-research-ideas-to-transform-spreadsheets/\n\nhttps://modeldepot.io/\n\nhttps://medium.com/airbnb-engineering/empowering-data-science-with-data-engineering-education-ef2acabd3042\n\nhttps://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638\n\nhttps://www.psypost.org/2019/01/narcissism-and-sadism-predict-likelihood-of-using-brainteaser-questions-in-a-job-interview-52985\n\nhttps://www.healthcareitnews.com/news/pew-heres-what-apis-need-succeed-healthcare\n\nhttps://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/","html":"<p></p><p><a href=\"https://www.microsoft.com/en-us/research/blog/influencing-mainstream-software-applying-programming-language-research-ideas-to-transform-spreadsheets/\">https://www.microsoft.com/en-us/research/blog/influencing-mainstream-software-applying-programming-language-research-ideas-to-transform-spreadsheets/</a></p><p><a href=\"https://modeldepot.io/\">https://modeldepot.io/</a></p><p><a href=\"https://medium.com/airbnb-engineering/empowering-data-science-with-data-engineering-education-ef2acabd3042\">https://medium.com/airbnb-engineering/empowering-data-science-with-data-engineering-education-ef2acabd3042</a></p><p><a href=\"https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638\">https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638</a></p><p><a href=\"https://www.psypost.org/2019/01/narcissism-and-sadism-predict-likelihood-of-using-brainteaser-questions-in-a-job-interview-52985\">https://www.psypost.org/2019/01/narcissism-and-sadism-predict-likelihood-of-using-brainteaser-questions-in-a-job-interview-52985</a></p><p><a href=\"https://www.healthcareitnews.com/news/pew-heres-what-apis-need-succeed-healthcare\">https://www.healthcareitnews.com/news/pew-heres-what-apis-need-succeed-healthcare</a></p><p><a href=\"https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/\">https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-28th/","uuid":"ad4bb359-a77b-4fa5-8654-a84839a6ad8f","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4beb23fd307f72b350adf2"}},{"node":{"id":"Ghost__Post__5c4be6e70344f0628a9e40c4","title":"Lynx Roundup, January 26th","slug":"lynx-roundup-january-26th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/90@2x.jpg","excerpt":"Chemistry-inspired packet management!  Concurrency in Python!  Cool book on Python Regex!","custom_excerpt":"Chemistry-inspired packet management!  Concurrency in Python!  Cool book on Python Regex!","created_at_pretty":"26 January, 2019","published_at_pretty":"26 January, 2019","updated_at_pretty":"26 January, 2019","created_at":"2019-01-25T23:49:43.000-05:00","published_at":"2019-01-26T07:00:00.000-05:00","updated_at":"2019-01-26T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 26th | Hackers and Slackers","meta_description":"Chemistry-inspired packet management!  Concurrency in Python!  Cool book on Python Regex!","og_description":"Chemistry-inspired packet management!  Concurrency in Python!  Cool book on Python Regex!","og_image":"https://hackersandslackers.com/content/images/lynx/90@2x.jpg","og_title":"Lynx Roundup, January 26th","twitter_description":"Chemistry-inspired packet management!  Concurrency in Python!  Cool book on Python Regex!","twitter_image":"https://hackersandslackers.com/content/images/lynx/90@2x.jpg","twitter_title":"Lynx Roundup, January 26th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://functional.works-hub.com/learn/advent-of-schemes-f2821\n\nhttps://tmthyjames.github.io/2018/december/Think-Twice-Before-Accepting-That-Fancy-Data-Science-Role/\n\nhttps://leanpub.com/py_regex/c/P7erPYAm1386\n\nhttp://www.themathcitadel.com/2019/01/10/exploiting-chemistry-for-better-packet-flow-management-1-introduction/\n\nhttps://techcrunch.com/2015/09/11/legendary-productivity-and-the-fear-of-modern-programming/\n\nhttps://arxiv.org/abs/1901.01973\n\nhttps://realpython.com/python-concurrency/","html":"<p></p><p><a href=\"https://functional.works-hub.com/learn/advent-of-schemes-f2821\">https://functional.works-hub.com/learn/advent-of-schemes-f2821</a></p><p><a href=\"https://tmthyjames.github.io/2018/december/Think-Twice-Before-Accepting-That-Fancy-Data-Science-Role/\">https://tmthyjames.github.io/2018/december/Think-Twice-Before-Accepting-That-Fancy-Data-Science-Role/</a></p><p><a href=\"https://leanpub.com/py_regex/c/P7erPYAm1386\">https://leanpub.com/py_regex/c/P7erPYAm1386</a></p><p><a href=\"http://www.themathcitadel.com/2019/01/10/exploiting-chemistry-for-better-packet-flow-management-1-introduction/\">http://www.themathcitadel.com/2019/01/10/exploiting-chemistry-for-better-packet-flow-management-1-introduction/</a></p><p><a href=\"https://techcrunch.com/2015/09/11/legendary-productivity-and-the-fear-of-modern-programming/\">https://techcrunch.com/2015/09/11/legendary-productivity-and-the-fear-of-modern-programming/</a></p><p><a href=\"https://arxiv.org/abs/1901.01973\">https://arxiv.org/abs/1901.01973</a></p><p><a href=\"https://realpython.com/python-concurrency/\">https://realpython.com/python-concurrency/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-26th/","uuid":"8e1fcfde-f524-4895-839e-6f6f3f7b133e","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c4be6e70344f0628a9e40c4"}},{"node":{"id":"Ghost__Post__5c36e3ca797a4f6f1db68dfb","title":"Lynx Roundup, January 25th","slug":"lynx-roundup-january-25th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/89@2x.jpg","excerpt":"Finding free hotel wifi with Python!  Sets as datatypes for Pandas columns!  New Python kernel for Jupyter!","custom_excerpt":"Finding free hotel wifi with Python!  Sets as datatypes for Pandas columns!  New Python kernel for Jupyter!","created_at_pretty":"10 January, 2019","published_at_pretty":"25 January, 2019","updated_at_pretty":"25 January, 2019","created_at":"2019-01-10T01:18:50.000-05:00","published_at":"2019-01-25T07:00:00.000-05:00","updated_at":"2019-01-25T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 25th | Hackers and Slackers","meta_description":"Finding free hotel wifi with Python!  Sets as datatypes for Pandas columns!  New Python kernel for Jupyter!","og_description":"Finding free hotel wifi with Python!  Sets as datatypes for Pandas columns!  New Python kernel for Jupyter!","og_image":"https://hackersandslackers.com/content/images/lynx/89@2x.jpg","og_title":"Lynx Roundup, January 25th","twitter_description":"Finding free hotel wifi with Python!  Sets as datatypes for Pandas columns!  New Python kernel for Jupyter!","twitter_image":"https://hackersandslackers.com/content/images/lynx/89@2x.jpg","twitter_title":"Lynx Roundup, January 25th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"http://techascent.com/blog/high-performance-compilers.html\n\nhttps://gkbrk.com/2018/12/free-hotel-wifi-with-python-and-selenium/\n\nhttps://www.newscientist.com/article/2187224-your-whole-office-could-be-a-computer-thanks-to-sculpted-wi-fi-waves/\n\nhttps://tselai.com/pandas-sets.html\n\nhttps://github.com/tech-ascent/tvm-clj/blob/master/README.md\n\nhttp://wesmckinney.com/blog/thoughts-on-taxes/\n\nhttps://blog.jupyter.org/a-new-python-kernel-for-jupyter-fcdf211e30a8","html":"<p></p><p><a href=\"http://techascent.com/blog/high-performance-compilers.html\">http://techascent.com/blog/high-performance-compilers.html</a></p><p><a href=\"https://gkbrk.com/2018/12/free-hotel-wifi-with-python-and-selenium/\">https://gkbrk.com/2018/12/free-hotel-wifi-with-python-and-selenium/</a></p><p><a href=\"https://www.newscientist.com/article/2187224-your-whole-office-could-be-a-computer-thanks-to-sculpted-wi-fi-waves/\">https://www.newscientist.com/article/2187224-your-whole-office-could-be-a-computer-thanks-to-sculpted-wi-fi-waves/</a></p><p><a href=\"https://tselai.com/pandas-sets.html\">https://tselai.com/pandas-sets.html</a></p><p><a href=\"https://github.com/tech-ascent/tvm-clj/blob/master/README.md\">https://github.com/tech-ascent/tvm-clj/blob/master/README.md</a></p><p><a href=\"http://wesmckinney.com/blog/thoughts-on-taxes/\">http://wesmckinney.com/blog/thoughts-on-taxes/</a></p><p><a href=\"https://blog.jupyter.org/a-new-python-kernel-for-jupyter-fcdf211e30a8\">https://blog.jupyter.org/a-new-python-kernel-for-jupyter-fcdf211e30a8</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-25th/","uuid":"6ba6c721-85bd-46da-af32-b0f18c6f523d","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c36e3ca797a4f6f1db68dfb"}},{"node":{"id":"Ghost__Post__5c36e37f797a4f6f1db68df6","title":"Lynx Roundup, January 24th","slug":"lynx-roundup-january-24th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/88@2x.jpg","excerpt":"Color-coding Pandas cells!  Prolog!  User-Defined Functions in PySpark!","custom_excerpt":"Color-coding Pandas cells!  Prolog!  User-Defined Functions in PySpark!","created_at_pretty":"10 January, 2019","published_at_pretty":"24 January, 2019","updated_at_pretty":"24 January, 2019","created_at":"2019-01-10T01:17:35.000-05:00","published_at":"2019-01-24T07:00:00.000-05:00","updated_at":"2019-01-24T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 24th | Hackers and Slackers","meta_description":"Color-coding Pandas cells!  Prolog!  User-Defined Functions in PySpark!","og_description":"Color-coding Pandas cells!  Prolog!  User-Defined Functions in PySpark!","og_image":"https://hackersandslackers.com/content/images/lynx/88@2x.jpg","og_title":"Lynx Roundup, January 24th","twitter_description":"Color-coding Pandas cells!  Prolog!  User-Defined Functions in PySpark!","twitter_image":"https://hackersandslackers.com/content/images/lynx/88@2x.jpg","twitter_title":"Lynx Roundup, January 24th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://kanoki.org/2019/01/02/pandas-trick-for-the-day-color-code-columns-rows-cells-of-dataframe/\n\nhttps://www.thestrikewave.com/original-content/2019/1/2/green-union-jobs-organizing-at-buffalos-tesla-factory\n\nhttps://dev.to/donaldkellett/less-than-12-days-of-prolog---a-guide--brief-review-of-the-prolog-programming-language-1dgl\n\nhttps://changhsinlee.com/pyspark-udf/\n\nhttps://www.genengnews.com/news/google-helps-unravel-the-complex-structure-of-enzymes/\n\nhttp://jeffe.cs.illinois.edu/teaching/algorithms/\n\nhttps://www.businessinsider.com/boldstart-ventures-founder-hottest-enterprise-startup-trends-2018-12","html":"<p></p><p><a href=\"https://kanoki.org/2019/01/02/pandas-trick-for-the-day-color-code-columns-rows-cells-of-dataframe/\">https://kanoki.org/2019/01/02/pandas-trick-for-the-day-color-code-columns-rows-cells-of-dataframe/</a></p><p><a href=\"https://www.thestrikewave.com/original-content/2019/1/2/green-union-jobs-organizing-at-buffalos-tesla-factory\">https://www.thestrikewave.com/original-content/2019/1/2/green-union-jobs-organizing-at-buffalos-tesla-factory</a></p><p><a href=\"https://dev.to/donaldkellett/less-than-12-days-of-prolog---a-guide--brief-review-of-the-prolog-programming-language-1dgl\">https://dev.to/donaldkellett/less-than-12-days-of-prolog---a-guide--brief-review-of-the-prolog-programming-language-1dgl</a></p><p><a href=\"https://changhsinlee.com/pyspark-udf/\">https://changhsinlee.com/pyspark-udf/</a></p><p><a href=\"https://www.genengnews.com/news/google-helps-unravel-the-complex-structure-of-enzymes/\">https://www.genengnews.com/news/google-helps-unravel-the-complex-structure-of-enzymes/</a></p><p><a href=\"http://jeffe.cs.illinois.edu/teaching/algorithms/\">http://jeffe.cs.illinois.edu/teaching/algorithms/</a></p><p><a href=\"https://www.businessinsider.com/boldstart-ventures-founder-hottest-enterprise-startup-trends-2018-12\">https://www.businessinsider.com/boldstart-ventures-founder-hottest-enterprise-startup-trends-2018-12</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-24th/","uuid":"babd83be-f4eb-40bc-bd68-15162e940468","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c36e37f797a4f6f1db68df6"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apache’s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c36e2cc797a4f6f1db68df1","title":"Lynx Roundup, January 23rd","slug":"lynx-roundup-january-23rd","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/140@2x.jpg","excerpt":"Python Anti-Patterns!  Tidyverse in Pandas!  Data pipelines in Flupy!","custom_excerpt":"Python Anti-Patterns!  Tidyverse in Pandas!  Data pipelines in Flupy!","created_at_pretty":"10 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"23 January, 2019","created_at":"2019-01-10T01:14:36.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-01-23T07:00:00.000-05:00","meta_title":"Lynx Roundup, January 23rd | Hackers and Slackers","meta_description":"Python Anti-Patterns!  Tidyverse in Pandas!  Data pipelines in Flupy!","og_description":"Python Anti-Patterns!  Tidyverse in Pandas!  Data pipelines in Flupy!","og_image":"https://hackersandslackers.com/content/images/lynx/140@2x.jpg","og_title":"Lynx Roundup, January 23rd","twitter_description":"Python Anti-Patterns!  Tidyverse in Pandas!  Data pipelines in Flupy!","twitter_image":"https://hackersandslackers.com/content/images/lynx/140@2x.jpg","twitter_title":"Lynx Roundup, January 23rd","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://docs.quantifiedcode.com/python-anti-patterns/index.html\n\nhttps://www.smh.com.au/national/how-do-you-feel-what-you-can-t-touch-scientists-crack-the-nerve-code-20181211-p50lmu.html\n\nhttps://arxiv.org/abs/1811.09989\n\nhttps://towardsdatascience.com/tidying-up-pandas-4572bfa38776\n\nhttps://github.com/cosmicBboy/pandera\n\nhttps://github.com/olirice/flupy\n\nhttps://github.com/facebookresearch/nevergrad","html":"<p></p><p><a href=\"https://docs.quantifiedcode.com/python-anti-patterns/index.html\">https://docs.quantifiedcode.com/python-anti-patterns/index.html</a></p><p><a href=\"https://www.smh.com.au/national/how-do-you-feel-what-you-can-t-touch-scientists-crack-the-nerve-code-20181211-p50lmu.html\">https://www.smh.com.au/national/how-do-you-feel-what-you-can-t-touch-scientists-crack-the-nerve-code-20181211-p50lmu.html</a></p><p><a href=\"https://arxiv.org/abs/1811.09989\">https://arxiv.org/abs/1811.09989</a></p><p><a href=\"https://towardsdatascience.com/tidying-up-pandas-4572bfa38776\">https://towardsdatascience.com/tidying-up-pandas-4572bfa38776</a></p><p><a href=\"https://github.com/cosmicBboy/pandera\">https://github.com/cosmicBboy/pandera</a></p><p><a href=\"https://github.com/olirice/flupy\">https://github.com/olirice/flupy</a></p><p><a href=\"https://github.com/facebookresearch/nevergrad\">https://github.com/facebookresearch/nevergrad</a></p>","url":"https://hackersandslackers.com/lynx-roundup-january-23rd/","uuid":"b3bfecde-fafe-4cf4-99d4-1e114318647a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c36e2cc797a4f6f1db68df1"}}]}},"pageContext":{"pageNumber":6,"humanPageNumber":7,"skip":72,"limit":12,"numberOfPages":33,"previousPagePath":"/page/6","nextPagePath":"/page/8"}}