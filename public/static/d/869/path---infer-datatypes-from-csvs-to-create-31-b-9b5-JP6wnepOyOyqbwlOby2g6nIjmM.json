{"data":{"ghostPost":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apacheâ€™s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},"pageContext":{"slug":"infer-datatypes-from-csvs-to-create"}}