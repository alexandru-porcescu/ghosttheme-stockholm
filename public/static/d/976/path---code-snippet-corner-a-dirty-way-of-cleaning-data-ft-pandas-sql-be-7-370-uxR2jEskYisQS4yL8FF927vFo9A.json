{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a1","title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","excerpt":"Cleaning data in Pandas the dirty way.","custom_excerpt":"Cleaning data in Pandas the dirty way.","created_at_pretty":"12 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"14 April, 2019","created_at":"2018-07-11T21:54:04.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2019-04-14T14:40:57.000-04:00","meta_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL) | Hackers and Slackers","meta_description":"Clean datasets using Python's Panda's library. Learn how to load data from an SQL query and wash out unwanted information.","og_description":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","og_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","og_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","twitter_description":"Code Snippet Corner ft. Pandas & SQL","twitter_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","twitter_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Warning:  The following is FANTASTICALLY not-secure.  Do not put this in a\nscript that's going to be running unsupervised.  This is for interactive\nsessions where you're prototyping the data cleaning methods that you're going to\nuse, and/or just manually entering stuff.  Especially if there's any chance\nthere could be something malicious hiding in the data to be uploaded.  We're\ngoing to be executing formatted strings of SQL unsanitized code.  Also, this\nwill lead to LOTS of silent failures, which are arguably The Worst Thing - if\nguaranteed correctness is a requirement, leave this for the tinkering table.\n Alternatively, if it's a project where \"getting something in there is better\nthan nothing\", this can provide a lot of bang for your buck.  Actually, it's\npurely for entertainment purposes and not for human consumption.\n\nLet's say you were helping someone take a bunch of scattered Excel files and\nCSVs and input them all into a MySQL database.  This is a very iterative, trial\n& error process.  We certainly don't want to be re-entering a bunch of\nboilerplate.  Pandas to the rescue!  We can painlessly load those files into a\nDataFrame, then just export them to the db!\n\nWell, not so fast  First off, loading stuff into a DB is a task all its own -\nPandas and your RDBMS have different kinds of tolerance for mistakes, and differ\nin often-unpredictable ways.  For example, one time I was performing a task\nsimilar to the one described here (taking scattered files and loading them into\na DB) - I was speeding along nicely, but then ran into a speedbump: turns out\nPandas generally doesn't infer that a column is a date unless you tell it\nspecifically, and will generally parse dates as strings.  Now, this was fine\nwhen the dates were present - MySQL is pretty smart about accepting different\nforms of dates & times.  But one thing it doesn't like is accepting an empty\nstring ''  into a date or time column.  Not a huge deal, just had to cast the\ncolumn as a date:\n\ndf['date'] = pd.to_datetime(df['date'])\n\n\nNow the blank strings are NaT, which MySQL knows how to handle!\n\nThis was simple enough, but there's all kinds of little hiccups that can happen.\n And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing\naffair - if there's one error, that means none of the rows will write.  Which\ncan get pretty annoying if you were trying to write a decent-sized DataFrame,\nespecially if the first error doesn't show up until one of the later rows.\n Waiting sucks.  And it's not just about being impatient - long waiting times\ncan disrupt your flow.\n\nRapid prototyping & highly-interactive development are some of Python's greatest\nstrengths, and they are great strengths indeed!  Paul Graham (one of the guys\nbehind Y Combinator) once made the comparison between REPL-heavy development and\nthe popularizing of oil paints (he was talking about LISP, but it's also quite\ntrue of Python, as Python took a lot of its cues from LISP):\n\n> Before oil paint became popular, painters used a medium, called tempera , that\ncannot be blended or over-painted. The cost of mistakes was high, and this\ntended to make painters conservative. Then came oil paint, and with it a great\nchange in style. Oil \"allows for second thoughts\". This proved a decisive\nadvantage in dealing with difficult subjects like the human figure.The new\nmedium did not just make painters' lives easier. It made possible a new and more\nambitious kind of painting. Janson writes:Without oil, the Flemish Masters'\nconquest of visible reality would have been much more limited. Thus, from a\ntechnical point of view, too, they deserve to be called the \"fathers of modern\npainting\" , for oil has been the painter's basic medium ever since. As a\nmaterial, tempera is no less beautiful than oil. But the flexibility of oil\npaint gives greater scope to the imagination--that was the deciding factor.\nProgramming is now undergoing a similar change...Meanwhile, ideas borrowed from\nLisp increasingly turn up in the mainstream: interactive programming\nenvironments, garbage collection, and run-time typing  to name a few.More\npowerful tools are taking the risk out of exploration. That's good news for\nprogrammers, because it means that we will be able to undertake more ambitious\nprojects. The use of oil paint certainly had this effect. The period immediately\nfollowing its adoption was a golden age for painting. There are signs already\nthat something similar is happening in programming.\n(Emphasis mine)\nFrom here: http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\nA little scenario to demonstrate:\n\nLet's pretend we have a MySQL instance running, and have already created a\ndatabase named items:\n\nimport pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items')\n\npd.io.sql.execute(\"\"\"CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)\"\"\", cnx)\n\ndf = pd.DataFrame({\n    \"author\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"copies\": [2, \"\", 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n\n\nSoo, let's tighten this feedback loop, shall we?\n\nWe'll iterate through the DataFrame with the useful iterrows()  method.  This\ngives us essentially an enum  made from our DataFrame - we'll get a bunch of\ntuples giving us the index as the first element and the row as its own Pandas\nSeries as the second.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n\n\nLet's unpack that a bit.\n\nRemember that we're getting a two-element tuple, with the good stuff in the\nsecond element, so\n\nx[1]\n\nNext, we convert the Series to a one-entry DataFrame, because the Series doesn't\nhave the DataFrame's to_sql()  method.\n\npd.DataFrame(x[1])\n\nThe default behavior will assume this is a single column with, each variable\nbeing the address of a different row.  MySQL isn't going to be having it.  Sooo,\nwe transpose!\n\npd.DataFrame(x[1]).transpose()\n\nAnd finally, we use our beloved to_sql  method on that.\n\nLet's check our table now!\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\n\n  \tauthor\tcopies\nid\n1\tAlice\t2\n\n\nIt wrote the first row!  Not much of a difference with this toy example, but\nonce you were writing a few thousand rows and the error didn't pop up until the\n3000th, this would make a pretty noticeable difference in your ability to\nquickly experiment with different cleaning schemes.\n\nNote that this will still short-circuit as soon as we hit the error.  If we\nwanted to make sure we got all the valid input before working on our tough\ncases, we could make a little try/except  block.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n\n\nThis will try  to write each line, and if it encounters an Exception  it'll \ncontinue  the loop.\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n\n\nAlright, now the bulk of our data's in the db!  Whatever else happens, you've\ndone that much!  Now you can relax a bit, which is useful for stimulating the\ncreativity you'll need for the more complicated edge cases.\n\nSo, we're ready to start testing new cleaning schemes?  Well, not quite yet...\n\nLet's say we went and tried to think up a fix.  We go to test it out and...\n\n#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '1' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '3' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n\n\nThe error we're interested is in there, but what's all this other nonsense\ncrowding it?\n\nWell, one of the handy things about a database is that it'll enforce uniqueness\nbased on the constraints you give it.  It's already got an entry with an id \nvalue of 1, so it's going to complain if you try to put another one.  In\naddition to providing a lot of distraction, this'll also slow us down\nconsiderably - after all, part of the point was to make our experiments with\ndata-cleaning go faster!\n\nLuckily, Pandas' wonderful logical indexing will make it a snap to ensure that\nwe only bother with entries that aren't in the database yet.\n\n#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n\n\nThis will also be shockingly quick - Pandas' logical indexing takes advantage of\nall that magic going on under the hood.  Using it, instead of manually\niteration, can literally bring you from waiting minutes to waiting seconds.\n\nBuuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so\nhow about we just turn it into a function?\n\n#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n    return df[~df.index.isin(usedIDs)]\n\n\nSo, next time we think we've made some progress on an edge case, we just call...\n\n#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nActually, let's clean that up even more - the more keys we hit, the more\nopportunities to make a mistake!  The most bug-free code is the code you don't\nwrite.\n\ndef writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nSo, finally, we can work on our new cleaning scheme, and whenever we think we're\ndone...\n\nwriteNewRows(cleanedDF)\n\nAnd boom!  Instant feedback!","html":"<p><strong>Warning:</strong> The following is FANTASTICALLY not-secure.  Do not put this in a script that's going to be running unsupervised.  This is for interactive sessions where you're prototyping the data cleaning methods that you're going to use, and/or just manually entering stuff.  Especially if there's any chance there could be something malicious hiding in the data to be uploaded.  We're going to be executing formatted strings of SQL unsanitized code.  Also, this will lead to LOTS of silent failures, which are arguably The Worst Thing - if guaranteed correctness is a requirement, leave this for the tinkering table.  Alternatively, if it's a project where \"getting something in there is better than nothing\", this can provide a lot of bang for your buck.  Actually, it's purely for entertainment purposes and not for human consumption.</p><p>Let's say you were helping someone take a bunch of scattered Excel files and CSVs and input them all into a MySQL database.  This is a very iterative, trial &amp; error process.  We certainly don't want to be re-entering a bunch of boilerplate.  Pandas to the rescue!  We can painlessly load those files into a DataFrame, then just export them to the db!</p><p>Well, not so fast  First off, loading stuff into a DB is a task all its own - Pandas and your RDBMS have different kinds of tolerance for mistakes, and differ in often-unpredictable ways.  For example, one time I was performing a task similar to the one described here (taking scattered files and loading them into a DB) - I was speeding along nicely, but then ran into a speedbump: turns out Pandas generally doesn't infer that a column is a date unless you tell it specifically, and will generally parse dates as strings.  Now, this was fine when the dates were present - MySQL is pretty smart about accepting different forms of dates &amp; times.  But one thing it doesn't like is accepting an empty string <code>''</code> into a date or time column.  Not a huge deal, just had to cast the column as a date:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df['date'] = pd.to_datetime(df['date'])\n</code></pre>\n<!--kg-card-end: markdown--><p>Now the blank strings are <code>NaT</code>, which MySQL knows how to handle!</p><p>This was simple enough, but there's all kinds of little hiccups that can happen.  And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing affair - if there's one error, that means none of the rows will write.  Which can get pretty annoying if you were trying to write a decent-sized DataFrame, especially if the first error doesn't show up until one of the later rows.  Waiting sucks.  And it's not just about being impatient - long waiting times can disrupt your flow.</p><p>Rapid prototyping &amp; highly-interactive development are some of Python's greatest strengths, and they are great strengths indeed!  Paul Graham (one of the guys behind Y Combinator) once made the comparison between REPL-heavy development and the popularizing of oil paints (he was talking about LISP, but it's also quite true of Python, as Python took a lot of its cues from LISP):</p><blockquote>Before oil paint became popular, painters used a medium, called tempera , that cannot be blended or over-painted. The cost of mistakes was high, and this tended to make painters conservative. Then came oil paint, and with it a great change in style. Oil \"allows for second thoughts\". This proved a decisive advantage in dealing with difficult subjects like the human figure.The new medium did not just make painters' lives easier. It made possible a new and more ambitious kind of painting. Janson writes:Without oil, the Flemish Masters' conquest of visible reality would have been much more limited. Thus, from a technical point of view, too, they deserve to be called the \"fathers of modern painting\" , for oil has been the painter's basic medium ever since. As a material, tempera is no less beautiful than oil. But the flexibility of oil paint gives greater scope to the imagination--that was the deciding factor.<br>Programming is now undergoing a similar change...Meanwhile, ideas borrowed from Lisp increasingly turn up in the mainstream: <strong>interactive programming environments, garbage collection, and run-time typing</strong> to name a few.More powerful tools are taking the risk out of exploration. That's good news for programmers, because it means that we will be able to undertake more ambitious projects. The use of oil paint certainly had this effect. The period immediately following its adoption was a golden age for painting. There are signs already that something similar is happening in programming.<br>(Emphasis mine)<br>From here: <a href=\"http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\">http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html</a></blockquote><p>A little scenario to demonstrate:</p><p>Let's pretend we have a MySQL instance running, and have already created a database named <code>items</code>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items')\n\npd.io.sql.execute(&quot;&quot;&quot;CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)&quot;&quot;&quot;, cnx)\n\ndf = pd.DataFrame({\n    &quot;author&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],\n    &quot;copies&quot;: [2, &quot;&quot;, 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n</code></pre>\n<!--kg-card-end: markdown--><p>Soo, let's tighten this feedback loop, shall we?</p><p>We'll iterate through the DataFrame with the useful <code>iterrows()</code> method.  This gives us essentially an <code>enum</code> made from our DataFrame - we'll get a bunch of tuples giving us the index as the first element and the row as its own Pandas Series as the second.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's unpack that a bit.</p><p>Remember that we're getting a two-element tuple, with the good stuff in the second element, so</p><p><code>x[1]</code></p><p>Next, we convert the Series to a one-entry DataFrame, because the Series doesn't have the DataFrame's <code>to_sql()</code> method.</p><p><code>pd.DataFrame(x[1])</code></p><p>The default behavior will assume this is a single column with, each variable being the address of a different row.  MySQL isn't going to be having it.  Sooo, we transpose!</p><p><code>pd.DataFrame(x[1]).transpose()</code></p><p>And finally, we use our beloved <code>to_sql</code> method on that.</p><p>Let's check our table now!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">  \tauthor\tcopies\nid\n1\tAlice\t2\n</code></pre>\n<!--kg-card-end: markdown--><p>It wrote the first row!  Not much of a difference with this toy example, but once you were writing a few thousand rows and the error didn't pop up until the 3000th, this would make a pretty noticeable difference in your ability to quickly experiment with different cleaning schemes.</p><p>Note that this will still short-circuit as soon as we hit the error.  If we wanted to make sure we got all the valid input before working on our tough cases, we could make a little <code>try/except</code> block.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n</code></pre>\n<!--kg-card-end: markdown--><p>This will <code>try</code> to write each line, and if it encounters an <code>Exception</code> it'll <code>continue</code> the loop.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n</code></pre>\n<!--kg-card-end: markdown--><p>Alright, now the bulk of our data's in the db!  Whatever else happens, you've done that much!  Now you can relax a bit, which is useful for stimulating the creativity you'll need for the more complicated edge cases.</p><p>So, we're ready to start testing new cleaning schemes?  Well, not quite yet...</p><p>Let's say we went and tried to think up a fix.  We go to test it out and...</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '1' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '3' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n</code></pre>\n<!--kg-card-end: markdown--><p>The error we're interested is in there, but what's all this other nonsense crowding it?</p><p>Well, one of the handy things about a database is that it'll enforce uniqueness based on the constraints you give it.  It's already got an entry with an <code>id</code> value of 1, so it's going to complain if you try to put another one.  In addition to providing a lot of distraction, this'll also slow us down considerably - after all, part of the point was to make our experiments with data-cleaning go faster!</p><p>Luckily, Pandas' wonderful logical indexing will make it a snap to ensure that we only bother with entries that aren't in the database yet.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n</code></pre>\n<!--kg-card-end: markdown--><p>This will also be shockingly quick - Pandas' logical indexing takes advantage of all that magic going on under the hood.  Using it, instead of manually iteration, can literally bring you from waiting minutes to waiting seconds.</p><p>Buuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so how about we just turn it into a function?</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n    return df[~df.index.isin(usedIDs)]\n</code></pre>\n<!--kg-card-end: markdown--><p>So, next time we think we've made some progress on an edge case, we just call...</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<!--kg-card-end: markdown--><p>Actually, let's clean that up even more - the more keys we hit, the more opportunities to make a mistake!  The most bug-free code is the code you don't write.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<!--kg-card-end: markdown--><p>So, finally, we can work on our new cleaning scheme, and whenever we think we're done...</p><p><code>writeNewRows(cleanedDF)</code></p><p>And boom!  Instant feedback!</p>","url":"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/","uuid":"9788b54d-ef44-4a35-9ec6-6a8678038480","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b46b4bcc6a9e951f8a6cc32"}},"pageContext":{"slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql"}}