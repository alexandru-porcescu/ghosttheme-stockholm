{"data":{"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369b","title":"Data Could Save Humanity if it Weren't for Humanity","slug":"data-could-save-humanity","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","excerpt":"A compelling case for robot overlords.","custom_excerpt":"A compelling case for robot overlords.","created_at_pretty":"03 July, 2018","published_at_pretty":"20 July, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-07-03T03:41:14.000-04:00","published_at":"2018-07-20T00:14:00.000-04:00","updated_at":"2019-02-19T03:42:08.000-05:00","meta_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity | Hackers and Slackers","meta_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","og_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","twitter_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","twitter_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"}],"plaintext":"A decade has passed since I stumbled into technical product development. Looking\nback, I've spent that time almost exclusively in the niche of data-driven\nproducts and engineering. While it seems obvious now, I realized in the 2000s\nthat you could generally create two types of product: you could either build a\n(likely uninspired) UI for existing data, or you could build products which\nproduced new data or interpreted existing data in a new useful way. Betting on\nthe latter seemed like an obvious choice. The late 2000’s felt like building\napps for the sake of apps most of the time. Even today Product Hint is littered\nwith weather apps and rehashed tools, solving problems so insignificant that\nthey almost seem satirical.\n\nYears passed, thus our data-centric tools evolved to fit cultural mind shifts in\nbusinesses which speculated on how these tools could be used. This began to\nbuild a clear yet slowly-growing narrative about how enterprises consider data\nanalysis in their org structures. Unfortunately, I can't say that much about\nthat shift has been positive. There are a number of major problems I believe we\nneed to address:\n\n *   SaaS is created with the goal of selling the product to enterprises. While\n   humanity's understanding of data science reach unprecedented territory, we\n   choose to perfect the  sales pitch  while neglecting education on these\n   tools.\n * As an atrocity to science, individual actors commonly cherry pick information\n   to confirm conclusions for personal benefit, without checks and balances. \n * Data which contradicts knee jerk assumptions made by executives are sometimes\n   taken as personal threats or attacks.\n * Most importantly, data professionals are horribly siloed. Analysts,\n   scientists, and engineers waste far too much time drawing lines between\n   roles: I find it absolutely absurd  to unanimously agree that tool X is for\n   BI  while tool Y is for data cleanup. Considering we all know  these tools\n   are running stacks on Python, R, SQL, etc, there is no reason to succumb to\n   the limitations of proprietary software (such as Tableau). We've turned a\n   blind eye to the possibility of 'data as a service': a chance to overlap\n   responsibilities by building a better  tool to reduce friction, as opposed to\n   increase it in the interest of selling more software.\n\nWhile we might all agree that collective 'data addiction' is reaching a peak,\nmost of us barely know what we mean by that. We conceptually understand that\ndata is important, but our imaginations on how to utilize this power effectively\nleaves a lot to be desired. IBM Watson probably had profound capabilities, but\nits failure lies with the humans tasked to make this technology relevant and\nuseful for humankind.\n\nThe Analytics Honeymoon\nAs I imagine most in the Product Management professionals do, I originally\nconsidered  data analysis to come in the form of web and app analytics. This was\na one-dimensional era; consumer-facing data served the sole purpose of\noptimizing sales and ad revenue, and there were much fewer choices of\nEnterprise-level tools to fall in love with. While the cheaper tools were just\nfine, corporate America had already fallen in love with a fickle mistress known\nas Omniture. \n\nOmniture was in fact in many ways the superior product on the market. As I'm\nsure Sales reps explained in those years, Omniture allowed for a vast level of\nevent tracking customization which was otherwise rare at the time: with the\nproper logic, effort, and willingness, executives could theoretically identify\ngranular issues in their product's conversation flow: issues which came attached\nwith cold hard facts in the form of numbers.\n\nThus, a game of numbers it was: in order to receive the level of granular detail\nexecutives wanted, there came a nominal fee. Well, many fees in fact: the\nproduct out-priced competitors tenfold upfront for the license itself. Since you\njust agreed to spend that much money on proprietary software, it only makes\nsense at that point that you should then hire a certified affiliated consultant\nto implement the custom reports, and then of course pay the lifelong upkeep that\ncomes with tracking events in ever-changing software. Despite all of these\ncosts, companies consistently moved forward with the choice under the\nrealization that the money saved from this data could far outweigh the cost.\n\nSo what happened when we actually collected all that data?\n\nIf You Could Get That Analytics Report, That Would be Great.\nEnterprises and data analysis were made for each other... but in a way that most\nclosely resembles a cliché romcom starring Julia Roberts. This romance follows\nthe beat of a metronome: while executives begin to grasp the impact of\ndata-based decisions, the commitment to actually acknowledge the abundance of\nthis information has its own lifespan. A/B testing, conversation funnels,\nsegmentation, etc: while the foreplay of implementing these buzzwords rolls on\nfor a number of weeks, it gives powerful figures time to reflect on one thing in\nparticular: they've owed their success to a lack of quantifiable accountability,\nand numbers are right around the corner. While this might not be a conscious\nact, it is an entirely real phenomenon.\n\nI've traditionally been a product manager, yet during that period of my career,\nI've found myself nominated to be Gatekeeper of Digital and Financial Data...\nfor whatever reason (it's worth reiterating that I am not nor ever have been an\nanalyst). The phenomenon followed a pattern. Given their expectations,\nexecutives reach their nerves end when the budget they allocated for\nenterprise-level sotftware is still under configuration, and has produced no\nresults. There's a reason why patience is a virtue isn't a phrase you see in\nmany sales pitches: we want what we're being sold, and we want it now. \n\nThats where I'd typically come in. As a product manager, analytics is a valuable\nweapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed\nlike ammunition for change. When a company's problems are become as large as\nthey are obvious, some numerical correlations are nearly common sense..\n\nCue the dashboards, custom reports, event tracking, you name it. Often times\nexecutives would set aside a weekly cadence to review the expensive conclusions\nour software could finally produce. The weekly email newsletters I would produce\nwould be met with a euphoric chain of satisfied stakeholders, time and time\nagain. Finally it seemed, the conclusions were clear and our problems were\nquantifiable. And yet, nothing seemed to change.\n\nHuman Insecurities Versus World Problems\nAs a data enthusiast, I did what we all would've done: I placed analytics\ntracking on our analytics reports themselves. \"Great stuff, groundbreaking work\nhere\" said one CEO, who I'd seen had not bothered to click the link provided. At\na certain point, I began attaching empty Excel spreadsheets and posting dead\nlinks as the content of our beloved reports. Those too were 'groundbreaking',\napparently.\n\nThis is far from an isolated phenomenon in technology. Meeting after meeting,\nclient after client, I took front-row seats to blatant dismissal of numerical\nevidence in favor of  ego-driven decisions. Test A would prove to yield 30%\nhigher conversion rates than Test B, but Test B would prevail thanks to the\nsubjective emotional opinions of talking heads. In retrospect, I can see now how\na grown adult with a household would find the sudden introduction of facts\nthreatening. We have have imposter syndrome, and the twenty-something year old\nanalyst attempting to improve a company will almost always lose to an adult\nprotecting a family.\n\nConsider a recent example uncovered by mistake. While auditing usage for a\nwidely-know project management tool, something seemed off about our volume of\nusage with a product costing us unspeakable dollars. Our department had mostly\nbeen tasked  to upkeep this 'critical' internal system at all costs. As it turns\nout, over 80% of all activity had been out own internal upkeep. That's millions\nof dollars invested in something never used over the course of several years,\nall for the purpose of upholding a guise of value-add.\n\nBI, Data Science, and the Choice to Make That Distinction\nI've spent the last several months working deep under the hood attempting to\ndismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about\nto rip in to a Tableau tangent here, but I promise there's a point.\n\nI was introduced to Tableau as a tool to fill a niche: quick analysis and\none-off extracts on tight timelines. The type of timelines where digging into\nPandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t\nan option. I was pleased with its ability to serve this purpose- such that it\nsparked a spontaneous 1 thousand purchase for a personal license. Tableau\nDesktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the\nrest of my life.\n\nFrom my naïve perspective it seemed logical that Tableau could help assist in\nthe data cleanup and automation I had been handling in scripts previously. This\ncould not be more incorrect. Even with full access to my own Tableau instance,\nit is clear that Tableau has one motive only: to show you your data, and ensure\nyou don't take it elsewhere. Consider this:\n\nCheck out the worksheets and and dashboards you've published to Server.\nConsidering these are equivalent to simple database views, you'd expect the API\ncalls to be exposed in your dev tools... why not? They aren't.\n\nTableau runs on a Postgres database on your personal server. However, no mention\nof \"postgres\" or anything of the sort is searchable to a useful degree. There is\na highly protected Tableau superadmin account which has controls to all tables\nand views in this server, but most research will point users to unlock the\n\"readonly\" user which is essentially a red herring account, or perhaps useful if\nyou're spying on your employee's actions.\n\nAnd then we have the Tableau server API. Ah, what a gift it would be to query\nthose views we created, running on scheduled extracts, so that we might build\nsomething from this information. As it turns out, Tableau's REST API does little\nmore than reveal meta data about files you already knew about. Just in case you\nwere wondering the date it was created, for some weird useless reason.\n\nI'm not just picking on Tableau here (although I'll continue my series about\nhacking them soon enough). This has exposed a massive dichotomy in the way we\nsee and treat data as a profession, or rather, a series of professions. between\nthose who look at data, and those who manipulate, iterate one, and create things\nwith Data. Nobody has ever expressed this realization to me, and many of you\nlikely still don't see what the big deal is. That, to me, is the big deal.\n\nData should be a passion to those looking to improve humanity, without a doubt.\nIf we know personalities are wining the battles against numbers, and feel numb\nto the fact that our proprietary tools prevent us from using data effectively,\nthere's something to be said about the complacency of humanity as we commit to\nconsumption over production. \n\nCompany attitudes towards data are one thing, but individuals are an entirely\ndifferent story. That's a long-winded post for another time.","html":"<p>A decade has passed since I stumbled into technical product development. Looking back, I've spent that time almost exclusively in the niche of data-driven products and engineering. While it seems obvious now, I realized in the 2000s that you could generally create two types of product: you could either build a (likely uninspired) UI for existing data, or you could build products which produced new data or interpreted existing data in a new useful way. Betting on the latter seemed like an obvious choice. The late 2000’s felt like building apps for the sake of apps most of the time. Even today Product Hint is littered with weather apps and rehashed tools, solving problems so insignificant that they almost seem satirical.</p><p>Years passed, thus our data-centric tools evolved to fit cultural mind shifts in businesses which speculated on how these tools could be used. This began to build a clear yet slowly-growing narrative about how enterprises consider data analysis in their org structures. Unfortunately, I can't say that much about that shift has been positive. There are a number of major problems I believe we need to address:</p><ul><li><em> </em>SaaS is created with the goal of <em>selling </em>the product to enterprises. While humanity's understanding of data science reach unprecedented territory, we choose to perfect the<em> sales pitch</em> while neglecting education on these tools.</li><li>As an atrocity to science, individual actors commonly cherry pick information to confirm conclusions for personal benefit, without checks and balances. </li><li>Data which contradicts knee jerk assumptions made by executives are sometimes taken as personal threats or attacks.</li><li>Most importantly, data professionals are <em><strong>horribly siloed. </strong></em>Analysts, scientists, and engineers waste far too much time drawing lines between roles: I find it <em><strong>absolutely absurd</strong></em> to unanimously agree that <strong>tool X is for BI</strong> while <strong>tool Y is for data cleanup</strong>. Considering we <em>all know</em> these tools are running stacks on Python, R, SQL, etc, there is no reason to succumb to the limitations of proprietary software (such as Tableau). We've turned a blind eye to the possibility of 'data as a service': a chance to overlap responsibilities by building a <em>better</em> tool to reduce friction, as opposed to increase it in the interest of selling more software.</li></ul><p>While we might all agree that collective 'data addiction' is reaching a peak, most of us barely know what we mean by that. We conceptually understand that data is important, but our imaginations on how to utilize this power effectively leaves a lot to be desired. IBM Watson probably had profound capabilities, but its failure lies with the humans tasked to make this technology relevant and useful for humankind.</p><h2 id=\"the-analytics-honeymoon\">The Analytics Honeymoon</h2><p>As I imagine most in the Product Management professionals do, I originally considered  data analysis to come in the form of web and app analytics. This was a one-dimensional era; consumer-facing data served the sole purpose of optimizing sales and ad revenue, and there were much fewer choices of Enterprise-level tools to fall in love with. While the cheaper tools were just fine, corporate America had already fallen in love with a fickle mistress known as Omniture. </p><p>Omniture was in fact in many ways the superior product on the market. As I'm sure Sales reps explained in those years, Omniture allowed for a vast level of event tracking customization which was otherwise rare at the time: with the proper logic, effort, and willingness, executives could theoretically identify granular issues in their product's conversation flow: issues which came attached with cold hard facts in the form of numbers.</p><p>Thus, a game of numbers it was: in order to receive the level of granular detail executives wanted, there came a nominal fee. Well, many fees in fact: the product out-priced competitors tenfold upfront for the license itself. Since you just agreed to spend that much money on proprietary software, it only makes sense at that point that you should then hire a certified affiliated consultant to implement the custom reports, and then of course pay the lifelong upkeep that comes with tracking events in ever-changing software. Despite all of these costs, companies consistently moved forward with the choice under the realization that the money saved from this data could far outweigh the cost.</p><p>So what happened when we actually collected all that data?</p><h2 id=\"if-you-could-get-that-analytics-report-that-would-be-great-\">If You Could Get That Analytics Report, That Would be Great.</h2><p>Enterprises and data analysis were made for each other... but in a way that most closely resembles a cliché romcom starring Julia Roberts. This romance follows the beat of a metronome: while executives begin to grasp the impact of data-based decisions, the commitment to actually acknowledge the abundance of this information has its own lifespan. A/B testing, conversation funnels, segmentation, etc: while the foreplay of implementing these buzzwords rolls on for a number of weeks, it gives powerful figures time to reflect on one thing in particular: they've owed their success to a lack of quantifiable accountability, and numbers are right around the corner. While this might not be a conscious act, it is an entirely real phenomenon.</p><p>I've traditionally been a product manager, yet during that period of my career, I've found myself nominated to be Gatekeeper of Digital and Financial Data... for whatever reason (it's worth reiterating that I am not nor ever have been an analyst). The phenomenon followed a pattern. Given their expectations, executives reach their nerves end when the budget they allocated for enterprise-level sotftware is still under configuration, and has produced no results. There's a reason why patience is a virtue isn't a phrase you see in many sales pitches: we want what we're being sold, and we want it now. </p><p>Thats where I'd typically come in. As a product manager, analytics is a valuable weapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed like ammunition for change. When a company's problems are become as large as they are obvious, some numerical correlations are nearly common sense..</p><p>Cue the dashboards, custom reports, event tracking, you name it. Often times executives would set aside a weekly cadence to review the expensive conclusions our software could finally produce. The weekly email newsletters I would produce would be met with a euphoric chain of satisfied stakeholders, time and time again. Finally it seemed, the conclusions were clear and our problems were quantifiable. And yet, nothing seemed to change.</p><h2 id=\"human-insecurities-versus-world-problems\">Human Insecurities Versus World Problems</h2><p>As a data enthusiast, I did what we all would've done: I placed analytics tracking on our analytics reports themselves. \"Great stuff, groundbreaking work here\" said one CEO, who I'd seen had not bothered to click the link provided. At a certain point, I began attaching empty Excel spreadsheets and posting dead links as the content of our beloved reports. Those too were 'groundbreaking', apparently.</p><p>This is far from an isolated phenomenon in technology. Meeting after meeting, client after client, I took front-row seats to blatant dismissal of numerical evidence in favor of  ego-driven decisions. Test A would prove to yield 30% higher conversion rates than Test B, but Test B would prevail thanks to the subjective emotional opinions of talking heads. In retrospect, I can see now how a grown adult with a household would find the sudden introduction of facts threatening. We have have imposter syndrome, and the twenty-something year old analyst attempting to improve a company will almost always lose to an adult protecting a family.</p><p>Consider a recent example uncovered by mistake. While auditing usage for a widely-know project management tool, something seemed off about our volume of usage with a product costing us unspeakable dollars. Our department had mostly been tasked  to upkeep this 'critical' internal system at all costs. As it turns out, over 80% of all activity had been out own internal upkeep. That's millions of dollars invested in something never used over the course of several years, all for the purpose of upholding a guise of value-add.</p><h2 id=\"bi-data-science-and-the-choice-to-make-that-distinction\">BI, Data Science, and the Choice to Make That Distinction</h2><p>I've spent the last several months working deep under the hood attempting to dismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about to rip in to a Tableau tangent here, but I promise there's a point.</p><p>I was introduced to Tableau as a tool to fill a niche: quick analysis and one-off extracts on tight timelines. The type of timelines where digging into Pandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t an option. I was pleased with its ability to serve this purpose- such that it sparked a spontaneous 1 thousand purchase for a personal license. Tableau Desktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the rest of my life.</p><p>From my naïve perspective it seemed logical that Tableau could help assist in the data cleanup and automation I had been handling in scripts previously. This could not be more incorrect. Even with full access to my own Tableau instance, it is clear that Tableau has one motive only: to show you your data, and ensure you don't take it elsewhere. Consider this:</p><p>Check out the worksheets and and dashboards you've published to Server. Considering these are equivalent to simple database views, you'd expect the API calls to be exposed in your dev tools... why not? They aren't.</p><p>Tableau runs on a Postgres database on your personal server. However, no mention of \"postgres\" or anything of the sort is searchable to a useful degree. There is a highly protected Tableau superadmin account which has controls to all tables and views in this server, but most research will point users to unlock the \"readonly\" user which is essentially a red herring account, or perhaps useful if you're spying on your employee's actions.</p><p>And then we have the Tableau server API. Ah, what a gift it would be to query those views we created, running on scheduled extracts, so that we might build something from this information. As it turns out, Tableau's REST API does little more than reveal meta data about files you already knew about. Just in case you were wondering the date it was created, for some weird useless reason.</p><p>I'm not just picking on Tableau here (although I'll continue my series about hacking them soon enough). This has exposed a massive dichotomy in the way we see and treat data as a profession, or rather, a series of professions. between those who look at data, and those who manipulate, iterate one, and create things with Data. Nobody has ever expressed this realization to me, and many of you likely still don't see what the big deal is. That, to me, is the big deal.</p><p>Data should be a passion to those looking to improve humanity, without a doubt. If we know personalities are wining the battles against numbers, and feel numb to the fact that our proprietary tools prevent us from using data effectively, there's something to be said about the complacency of humanity as we commit to consumption over production. </p><p>Company attitudes towards data are one thing, but individuals are an entirely different story. That's a long-winded post for another time.</p>","url":"https://hackersandslackers.com/data-could-save-humanity/","uuid":"01f56f59-a9ad-494a-993d-216716f68a7d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3b289ad0ac8a143588f360"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a7","title":"Lynx Roundup, July 19th","slug":"lynx-roundup-july-19th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","custom_excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","created_at_pretty":"13 July, 2018","published_at_pretty":"19 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:32:02.000-04:00","published_at":"2018-07-19T07:00:00.000-04:00","updated_at":"2018-07-24T22:43:44.000-04:00","meta_title":"Lynx Roundup, July 19th | Hackers and Slackers","meta_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","og_title":"Lynx Roundup, July 19th","twitter_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","twitter_title":"Lynx Roundup, July 19th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\n\n\n\nhttps://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\n\n\n\nhttps://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\n\n\n\nhttps://github.com/Microsoft/dowhy\n\n\n\nhttps://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/","html":"<p></p><p><a href=\"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\">https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428</a></p><p></p><p><a href=\"https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\">https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e</a></p><p></p><p><a href=\"https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\">https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113</a></p><p></p><p><a href=\"https://github.com/Microsoft/dowhy\">https://github.com/Microsoft/dowhy</a></p><p></p><p><a href=\"https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/\">https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-19th/","uuid":"a9739b8c-a429-4948-8b54-9ba44dea7cee","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486382c6a9e951f8a6cc63"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a6","title":"Lynx Roundup, July 18th","slug":"lynx-roundup-july-18th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","excerpt":"Agile Data!  Medical Data!  Python string tricks! ","custom_excerpt":"Agile Data!  Medical Data!  Python string tricks! ","created_at_pretty":"13 July, 2018","published_at_pretty":"18 July, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-07-13T04:30:13.000-04:00","published_at":"2018-07-18T07:00:00.000-04:00","updated_at":"2019-02-28T02:27:37.000-05:00","meta_title":"Lynx Roundup, July 18th | Hackers and Slackers","meta_description":"Agile Data!  Medical Data!  Python string tricks! ","og_description":"Agile Data!  Medical Data!  Python string tricks! ","og_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","og_title":"Lynx Roundup, July 18th","twitter_description":"Agile Data!  Medical Data!  Python string tricks! ","twitter_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","twitter_title":"Lynx Roundup, July 18th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.locallyoptimistic.com/post/agile-analytics-p1/\n\n\n\nhttps://realpython.com/python-string-formatting/\n\n\n\nhttps://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/\n\n\n\nhttps://css-tricks.com/prototyping-in-the-browser/\n\n\n\nhttps://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13","html":"<p></p><p><a href=\"https://www.locallyoptimistic.com/post/agile-analytics-p1/\">https://www.locallyoptimistic.com/post/agile-analytics-p1/</a></p><p></p><p><a href=\"https://realpython.com/python-string-formatting/\">https://realpython.com/python-string-formatting/</a></p><p></p><p><a href=\"https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/\">https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/</a></p><p></p><p><a href=\"https://css-tricks.com/prototyping-in-the-browser/\">https://css-tricks.com/prototyping-in-the-browser/</a></p><p></p><p><a href=\"https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13\">https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-18th/","uuid":"864fbc9d-d98d-483f-8636-66ffe8ec9987","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b486315c6a9e951f8a6cc5f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a5","title":"Lynx Roundup, July 17th","slug":"lynx-roundup-july-17th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx8@2x.jpg","excerpt":"Scaling a Graph db, presenting survey data, badly presenting data.","custom_excerpt":"Scaling a Graph db, presenting survey data, badly presenting data.","created_at_pretty":"13 July, 2018","published_at_pretty":"17 July, 2018","updated_at_pretty":"28 July, 2018","created_at":"2018-07-13T04:24:26.000-04:00","published_at":"2018-07-17T07:00:00.000-04:00","updated_at":"2018-07-28T16:26:36.000-04:00","meta_title":"Lynx Roundup, July 17th | Hackers and Slackers","meta_description":"Scaling a Graph db, presenting survey data, badly presenting data","og_description":"Scaling a Graph db, presenting survey data, badly presenting data","og_image":"https://hackersandslackers.com/content/images/lynx/lynx8@2x.jpg","og_title":"Lynx Roundup, July 17th","twitter_description":"Scaling a Graph db, presenting survey data, badly presenting data","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx8@2x.jpg","twitter_title":"Lynx Roundup, July 17th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Graph Databases","slug":"graph-databases","description":null,"feature_image":null,"meta_description":null,"meta_title":"Graph Databases | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://neo4j.com/blog/scale-out-neo4j-using-apache-mesos-and-dc-os/\n\n\n\nhttps://www.r-bloggers.com/presenting-survey-data/\n\n\n\nQuantifying stuff has a reputation for creating absurdities.  Some would say\nthat other methods create an equal number of absurdities, except they're just\nway harder to see.\nhttp://andrewgelman.com/2018/07/03/flaws-stupid-horrible-algorithm-revealed-made-numerical-predictions/\n\n\n\nhttps://flowingdata.com/2018/06/28/why-people-make-bad-charts-and-what-to-do-when-it-happens/\n\n\n\nhttps://github.com/solid/solid","html":"<p></p><p><a href=\"https://neo4j.com/blog/scale-out-neo4j-using-apache-mesos-and-dc-os/\">https://neo4j.com/blog/scale-out-neo4j-using-apache-mesos-and-dc-os/</a></p><p></p><p><a href=\"https://www.r-bloggers.com/presenting-survey-data/\">https://www.r-bloggers.com/presenting-survey-data/</a></p><p></p><p>Quantifying stuff has a reputation for creating absurdities.  Some would say that other methods create an equal number of absurdities, except they're just way harder to see.  <a href=\"http://andrewgelman.com/2018/07/03/flaws-stupid-horrible-algorithm-revealed-made-numerical-predictions/\">http://andrewgelman.com/2018/07/03/flaws-stupid-horrible-algorithm-revealed-made-numerical-predictions/</a></p><p></p><p><a href=\"https://flowingdata.com/2018/06/28/why-people-make-bad-charts-and-what-to-do-when-it-happens/\">https://flowingdata.com/2018/06/28/why-people-make-bad-charts-and-what-to-do-when-it-happens/</a></p><p></p><p><a href=\"https://github.com/solid/solid\">https://github.com/solid/solid</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-17th/","uuid":"18f1733b-316c-4d1d-addf-1519fcafbdaa","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b4861bac6a9e951f8a6cc59"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a1","title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","excerpt":"Code Snippet Corner ft. Pandas & SQL","custom_excerpt":"Code Snippet Corner ft. Pandas & SQL","created_at_pretty":"12 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"25 November, 2018","created_at":"2018-07-11T21:54:04.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-11-25T12:50:00.000-05:00","meta_title":"Code Snippet Corner: A Dirty Way of Cleaning Data (ft. Pandas & SQL) | Hackers and Slackers","meta_description":"Code Snippet Corner ft. Pandas & SQL","og_description":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","og_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","og_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","twitter_description":"Code Snippet Corner ft. Pandas & SQL","twitter_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","twitter_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Warning  The following is FANTASTICALLY not-secure.  Do not put this in a script\nthat's going to be running unsupervised.  This is for interactive sessions where\nyou're prototyping the data cleaning methods that you're going to use, and/or\njust manually entering stuff.  Especially if there's any chance there could be\nsomething malicious hiding in the data to be uploaded.  We're going to be\nexecuting formatted strings of SQL unsanitized code.  Also, this will lead to\nLOTS of silent failures, which are arguably The Worst Thing - if guaranteed\ncorrectness is a requirement, leave this for the tinkering table.\n Alternatively, if it's a project where \"getting something in there is better\nthan nothing\", this can provide a lot of bang for your buck.  Actually, it's\npurely for entertainment purposes and not for human consumption.\n\nLet's say you were helping someone take a bunch of scattered Excel files and\nCSVs and input them all into a MySQL database.  This is a very iterative, trial\n& error process.  We certainly don't want to be re-entering a bunch of\nboilerplate.  Pandas to the rescue!  We can painlessly load those files into a\nDataFrame, then just export them to the db!\n\nWell, not so fast  First off, loading stuff into a DB is a task all its own -\nPandas and your RDBMS have different kinds of tolerance for mistakes, and differ\nin often-unpredictable ways.  For example, one time I was performing a task\nsimilar to the one described here (taking scattered files and loading them into\na DB) - I was speeding along nicely, but then ran into a speedbump: turns out\nPandas generally doesn't infer that a column is a date unless you tell it\nspecifically, and will generally parse dates as strings.  Now, this was fine\nwhen the dates were present - MySQL is pretty smart about accepting different\nforms of dates & times.  But one thing it doesn't like is accepting an empty\nstring ''  into a date or time column.  Not a huge deal, just had to cast the\ncolumn as a date:\n\ndf['date'] = pd.to_datetime(df['date'])\n\nNow the blank strings are NaT, which MySQL knows how to handle!\n\nThis was simple enough, but there's all kinds of little hiccups that can happen.\n And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing\naffair - if there's one error, that means none of the rows will write.  Which\ncan get pretty annoying if you were trying to write a decent-sized DataFrame,\nespecially if the first error doesn't show up until one of the later rows.\n Waiting sucks.  And it's not just about being impatient - long waiting times\ncan disrupt your flow.\n\nRapid prototyping & highly-interactive development are some of Python's greatest\nstrengths, and they are great strengths indeed!  Paul Graham (one of the guys\nbehind Y Combinator) once made the comparison between REPL-heavy development and\nthe popularizing of oil paints (he was talking about LISP, but it's also quite\ntrue of Python, as Python took a lot of its cues from LISP):\n\nBefore oil paint became popular, painters used a medium, called tempera , that\ncannot be blended or over-painted. The cost of mistakes was high, and this\ntended to make painters conservative. Then came oil paint, and with it a great\nchange in style. Oil \"allows for second thoughts\". This proved a decisive\nadvantage in dealing with difficult subjects like the human figure.The new\nmedium did not just make painters' lives easier. It made possible a new and more\nambitious kind of painting. Janson writes:Without oil, the Flemish\nMasters'conquest of visible reality would have been much more limited. Thus,\nfrom a technical point of view, too, they deserve to be called the \"fathers of\nmodern painting\" , for oil has been the painter's basic medium ever since. As a\nmaterial, tempera is no lesss beautiful than oil. But the flexibility of oil\npaint gives greater scope to the imagination--that was the deciding factor.\nProgramming is now undergoing a similar change...Meanwhile, ideas borrowed from\nLisp increasingly turn up in the mainstream: interactive programming\nenvironments, garbage collection, and run-time typing  to name a few.More\npowerful tools are taking the risk out of exploration. That's good news for\nprogrammers, because it means that we will be able to undertake more ambitious\nprojects. The use of oil paint certainly had this effect. The period immediately\nfollowing its adoption was a golden age for painting. There are signs already\nthat something similar is happening in programming.\n(Emphasis mine)\nFrom here: http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.htmlA\nlittle scenario to demonstrate:\n\nLet's pretend we have a MySQL instance running, and have already created a\ndatabase named items\n\nimport pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(\"\"\"CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)\"\"\", cnx)\n\ndf = pd.DataFrame({\n    \"author\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"copies\": [2, \"\", 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n\n\nSoo, let's tighten this feedback loop, shall we?\n\nWe'll iterate through the DataFrame with the useful iterrows()  method.  This\ngives us essentially an enum  made from our DataFrame - we'll get a bunch of\ntuples giving us the index as the first element and the row as its own Pandas\nSeries as the second.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n\n\nLet's unpack that a bit.\n\nRemember that we're getting a two-element tuple, with the good stuff in the\nsecond element, so\n\nx[1]\n\nNext, we convert the Series to a one-entry DataFrame, because the Series doesn't\nhave the DataFrame's to_sql()  method.\n\npd.DataFrame(x[1])\n\nThe default behavior will assume this is a single column with, each variable\nbeing the address of a different row.  MySQL isn't going to be having it.  Sooo,\nwe transpose!\n\npd.DataFrame(x[1]).transpose()\n\nAnd finally, we use our beloved to_sql  method on that.\n\nLet's check our table now!\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n\n\nIt wrote the first row!  Not much of a difference with this toy example, but\nonce you were writing a few thousand rows and the error didn't pop up until the\n3000th, this would make a pretty noticeable difference in your ability to\nquickly experiment with different cleaning schemes.\n\nNote that this will still short-circuit as soon as we hit the error.  If we\nwanted to make sure we got all the valid input before working on our tough\ncases, we could make a little try/except  block.\n\n\n\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n\n\nThis will try  to write each line, and if it encounters an Exception  it'll \ncontinue  the loop.\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n\n\nAlright, now the bulk of our data's in the db!  Whatever else happens, you've\ndone that much!  Now you can relax a bit, which is useful for stimulating the\ncreativity you'll need for the more complicated edge cases.\n\nSo, we're ready to start testing new cleaning schemes?  Well, not quite yet...\n\nLet's say we went and tried to think up a fix.  We go to test it out and...\n\n#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '1' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '3' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n\n\nThe error we're interested is in there, but what's all this other nonsense\ncrowding it?\n\nWell, one of the handy things about a database is that it'll enforce uniqueness\nbased on the constraints you give it.  It's already got an entry with an id \nvalue of 1, so it's going to complain if you try to put another one.  In\naddition to providing a lot of distraction, this'll also slow us down\nconsiderably - after all, part of the point was to make our experiments with\ndata-cleaning go faster!\n\nLuckily, Pandas' wonderful logical indexing will make it a snap to ensure that\nwe only bother with entries that aren't in the database yet.\n\n#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n\n\nThis will also be shockingly quick - Pandas' logical indexing takes advantage of\nall that magic going on under the hood.  Using it, instead of manually\niteration, can literally bring you from waiting minutes to waiting seconds.\n\nBuuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so\nhow about we just turn it into a function?\n\n#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n    return df[~df.index.isin(usedIDs)]\n\n\nSo, next time we think we've made some progress on an edge case, we just call...\n\n#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nActually, let's clean that up even more - the more keys we hit, the more\nopportunities to make a mistake!  The most bug-free code is the code you don't\nwrite.\n\ndef writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nSo, finally, we can work on our new cleaning scheme, and whenever we think we're\ndone...\n\nwriteNewRows(cleanedDF)\n\nAnd boom!  Instant feedback!","html":"<p><strong>Warning</strong> The following is FANTASTICALLY not-secure.  Do not put this in a script that's going to be running unsupervised.  This is for interactive sessions where you're prototyping the data cleaning methods that you're going to use, and/or just manually entering stuff.  Especially if there's any chance there could be something malicious hiding in the data to be uploaded.  We're going to be executing formatted strings of SQL unsanitized code.  Also, this will lead to LOTS of silent failures, which are arguably The Worst Thing - if guaranteed correctness is a requirement, leave this for the tinkering table.  Alternatively, if it's a project where \"getting something in there is better than nothing\", this can provide a lot of bang for your buck.  Actually, it's purely for entertainment purposes and not for human consumption.</p><p>Let's say you were helping someone take a bunch of scattered Excel files and CSVs and input them all into a MySQL database.  This is a very iterative, trial &amp; error process.  We certainly don't want to be re-entering a bunch of boilerplate.  Pandas to the rescue!  We can painlessly load those files into a DataFrame, then just export them to the db!</p><p>Well, not so fast  First off, loading stuff into a DB is a task all its own - Pandas and your RDBMS have different kinds of tolerance for mistakes, and differ in often-unpredictable ways.  For example, one time I was performing a task similar to the one described here (taking scattered files and loading them into a DB) - I was speeding along nicely, but then ran into a speedbump: turns out Pandas generally doesn't infer that a column is a date unless you tell it specifically, and will generally parse dates as strings.  Now, this was fine when the dates were present - MySQL is pretty smart about accepting different forms of dates &amp; times.  But one thing it doesn't like is accepting an empty string <code>''</code> into a date or time column.  Not a huge deal, just had to cast the column as a date:</p><p><code>df['date'] = pd.to_datetime(df['date'])</code></p><p>Now the blank strings are <code>NaT</code>, which MySQL knows how to handle!</p><p>This was simple enough, but there's all kinds of little hiccups that can happen.  And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing affair - if there's one error, that means none of the rows will write.  Which can get pretty annoying if you were trying to write a decent-sized DataFrame, especially if the first error doesn't show up until one of the later rows.  Waiting sucks.  And it's not just about being impatient - long waiting times can disrupt your flow.</p><p>Rapid prototyping &amp; highly-interactive development are some of Python's greatest strengths, and they are great strengths indeed!  Paul Graham (one of the guys behind Y Combinator) once made the comparison between REPL-heavy development and the popularizing of oil paints (he was talking about LISP, but it's also quite true of Python, as Python took a lot of its cues from LISP):</p><blockquote>Before oil paint became popular, painters used a medium, called tempera , that cannot be blended or over-painted. The cost of mistakes was high, and this tended to make painters conservative. Then came oil paint, and with it a great change in style. Oil \"allows for second thoughts\". This proved a decisive advantage in dealing with difficult subjects like the human figure.The new medium did not just make painters' lives easier. It made possible a new and more ambitious kind of painting. Janson writes:Without oil, the Flemish Masters'conquest of visible reality would have been much more limited. Thus, from a technical point of view, too, they deserve to be called the \"fathers of modern painting\" , for oil has been the painter's basic medium ever since. As a material, tempera is no lesss beautiful than oil. But the flexibility of oil paint gives greater scope to the imagination--that was the deciding factor.<br>Programming is now undergoing a similar change...Meanwhile, ideas borrowed from Lisp increasingly turn up in the mainstream: <strong>interactive programming environments, garbage collection, and run-time typing</strong> to name a few.More powerful tools are taking the risk out of exploration. That's good news for programmers, because it means that we will be able to undertake more ambitious projects. The use of oil paint certainly had this effect. The period immediately following its adoption was a golden age for painting. There are signs already that something similar is happening in programming.<br>(Emphasis mine)<br>From here: <a href=\"http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\">http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html</a></blockquote><p>A little scenario to demonstrate:</p><p>Let's pretend we have a MySQL instance running, and have already created a database named <code>items</code></p><pre><code class=\"language-python\">import pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(&quot;&quot;&quot;CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)&quot;&quot;&quot;, cnx)\n\ndf = pd.DataFrame({\n    &quot;author&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],\n    &quot;copies&quot;: [2, &quot;&quot;, 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n</code></pre>\n<p>Soo, let's tighten this feedback loop, shall we?</p><p>We'll iterate through the DataFrame with the useful <code>iterrows()</code> method.  This gives us essentially an <code>enum</code> made from our DataFrame - we'll get a bunch of tuples giving us the index as the first element and the row as its own Pandas Series as the second.</p><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n</code></pre>\n<p>Let's unpack that a bit.</p><p>Remember that we're getting a two-element tuple, with the good stuff in the second element, so</p><p><code>x[1]</code></p><p>Next, we convert the Series to a one-entry DataFrame, because the Series doesn't have the DataFrame's <code>to_sql()</code> method.</p><p><code>pd.DataFrame(x[1])</code></p><p>The default behavior will assume this is a single column with, each variable being the address of a different row.  MySQL isn't going to be having it.  Sooo, we transpose!</p><p><code>pd.DataFrame(x[1]).transpose()</code></p><p>And finally, we use our beloved <code>to_sql</code> method on that.</p><p>Let's check our table now!</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n</code></pre>\n<p>It wrote the first row!  Not much of a difference with this toy example, but once you were writing a few thousand rows and the error didn't pop up until the 3000th, this would make a pretty noticeable difference in your ability to quickly experiment with different cleaning schemes.</p><p>Note that this will still short-circuit as soon as we hit the error.  If we wanted to make sure we got all the valid input before working on our tough cases, we could make a little <code>try/except</code> block.</p><pre><code class=\"language-python\">\n</code></pre>\n<pre><code>for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n</code></pre><p>This will <code>try</code> to write each line, and if it encounters an <code>Exception</code> it'll <code>continue</code> the loop.</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n</code></pre>\n<p>Alright, now the bulk of our data's in the db!  Whatever else happens, you've done that much!  Now you can relax a bit, which is useful for stimulating the creativity you'll need for the more complicated edge cases.</p><p>So, we're ready to start testing new cleaning schemes?  Well, not quite yet...</p><p>Let's say we went and tried to think up a fix.  We go to test it out and...</p><pre><code class=\"language-python\">#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '1' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '3' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n</code></pre>\n<p>The error we're interested is in there, but what's all this other nonsense crowding it?</p><p>Well, one of the handy things about a database is that it'll enforce uniqueness based on the constraints you give it.  It's already got an entry with an <code>id</code> value of 1, so it's going to complain if you try to put another one.  In addition to providing a lot of distraction, this'll also slow us down considerably - after all, part of the point was to make our experiments with data-cleaning go faster!</p><p>Luckily, Pandas' wonderful logical indexing will make it a snap to ensure that we only bother with entries that aren't in the database yet.</p><pre><code class=\"language-python\">#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n</code></pre>\n<p>This will also be shockingly quick - Pandas' logical indexing takes advantage of all that magic going on under the hood.  Using it, instead of manually iteration, can literally bring you from waiting minutes to waiting seconds.</p><p>Buuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so how about we just turn it into a function?</p><pre><code class=\"language-python\">#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n    return df[~df.index.isin(usedIDs)]\n</code></pre>\n<p>So, next time we think we've made some progress on an edge case, we just call...</p><pre><code class=\"language-python\">#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>Actually, let's clean that up even more - the more keys we hit, the more opportunities to make a mistake!  The most bug-free code is the code you don't write.</p><pre><code class=\"language-python\">def writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>So, finally, we can work on our new cleaning scheme, and whenever we think we're done...</p><p><code>writeNewRows(cleanedDF)</code></p><p>And boom!  Instant feedback!</p>","url":"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/","uuid":"9788b54d-ef44-4a35-9ec6-6a8678038480","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b46b4bcc6a9e951f8a6cc32"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a4","title":"Lynx Roundup, July 16th","slug":"lynx-roundup-july-16th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","custom_excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","created_at_pretty":"13 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"28 July, 2018","created_at":"2018-07-13T04:18:17.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-07-28T16:26:42.000-04:00","meta_title":"Lynx Roundup, July 16th | Hackers and Slackers","meta_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","og_title":"Lynx Roundup, July 16th","twitter_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","twitter_title":"Lynx Roundup, July 16th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\n\n\n\nhttp://mattturck.com/bigdata2018\n[http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&utm_medium=email&utm_source=Data_Elixir_189]\n\n\n\nOne of the best-written tutorials I've ever seen:\nhttps://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\n\n\n\nhttps://robertheaton.com/2018/06/25/how-to-read/\n\n\n\nhttp://explained.ai/matrix-calculus/index.html","html":"<p><a href=\"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\">https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is</a></p><p></p><p><a href=\"http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&amp;utm_medium=email&amp;utm_source=Data_Elixir_189\">http://mattturck.com/bigdata2018</a></p><p></p><p>One of the best-written tutorials I've ever seen:<br><a href=\"https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\">https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</a></p><p></p><p><a href=\"https://robertheaton.com/2018/06/25/how-to-read/\">https://robertheaton.com/2018/06/25/how-to-read/</a></p><p></p><p><a href=\"http://explained.ai/matrix-calculus/index.html\">http://explained.ai/matrix-calculus/index.html</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-16th/","uuid":"f886dfa1-1d10-423e-a926-2f5c8bbe7085","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486049c6a9e951f8a6cc54"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a3","title":"Lynx Roundup, July 15th","slug":"lynx-roundup-july-15th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/72@2x.jpg","excerpt":"Routing with Neo4j, Crazy new deep learning chip, and what even is bytecode anyway?","custom_excerpt":"Routing with Neo4j, Crazy new deep learning chip, and what even is bytecode anyway?","created_at_pretty":"13 July, 2018","published_at_pretty":"15 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:13:55.000-04:00","published_at":"2018-07-15T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 15th | Hackers and Slackers","meta_description":"Routing with Neo4j, Crazy new deep learning chip, and what even is bytecode anyway?","og_description":"Routing with Neo4j, Crazy new deep learning chip, and what even is bytecode anyway?","og_image":"https://hackersandslackers.com/content/images/lynx/72@2x.jpg","og_title":"Lynx Roundup, July 15th","twitter_description":"Routing with Neo4j, Crazy new deep learning chip, and what even is bytecode anyway?","twitter_image":"https://hackersandslackers.com/content/images/lynx/72@2x.jpg","twitter_title":"Lynx Roundup, July 15th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"https://tbgraph.wordpress.com/2018/06/28/finding-alternative-routes-in-california-road-network-with-neo4j/\n\n\n\nhttps://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster\n[https://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda?share=9a4e8a42&srid=JXMt]\n\n\n\n-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda\n[https://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda?share=9a4e8a42&srid=JXMt]\n\n\n\nhttps://coachmancini.com/the-data-scientist-of-the-future/\n\n\n\nhttps://spectrum.ieee.org/tech-talk/semiconductors/processors/ibms-new-doitall-deep-learning-chip","html":"<p><a href=\"https://tbgraph.wordpress.com/2018/06/28/finding-alternative-routes-in-california-road-network-with-neo4j/\">https://tbgraph.wordpress.com/2018/06/28/finding-alternative-routes-in-california-road-network-with-neo4j/</a></p><p></p><p><a href=\"https://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda?share=9a4e8a42&amp;srid=JXMt\">https://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster</a></p><p></p><p><a href=\"https://www.quora.com/If-Python-and-Java-both-compile-to-bytecode-why-is-Java-so-much-faster-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda?share=9a4e8a42&amp;srid=JXMt\">-Whats-the-JIT-doing-with-bytecode-that-CPython-isnt/answer/Eliot-Miranda</a></p><p></p><p><a href=\"https://coachmancini.com/the-data-scientist-of-the-future/\">https://coachmancini.com/the-data-scientist-of-the-future/</a></p><p></p><p><a href=\"https://spectrum.ieee.org/tech-talk/semiconductors/processors/ibms-new-doitall-deep-learning-chip\">https://spectrum.ieee.org/tech-talk/semiconductors/processors/ibms-new-doitall-deep-learning-chip</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-15th/","uuid":"c24a1a11-617f-42f0-8f6c-104429f11608","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b485f43c6a9e951f8a6cc4f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736aa","title":"Create a VPS with Google Cloud: Introducing Compute Engine","slug":"setting-up-dns-with-google-cloud-platform","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/gcp@2x.jpg","excerpt":"Spin up a VPS and configure DNS with relative ease.","custom_excerpt":"Spin up a VPS and configure DNS with relative ease.","created_at_pretty":"14 July, 2018","published_at_pretty":"14 July, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-07-14T10:28:34.000-04:00","published_at":"2018-07-14T14:55:03.000-04:00","updated_at":"2019-02-14T02:29:40.000-05:00","meta_title":"Google Cloud Platform: Creating a VPS | Hackers and Slackers","meta_description":"Google Cloud Platform is a compelling choice for respectable enterprises, particularly those with a sense of style and curiosity.","og_description":"The ease of plug-and-play these plug-and-play services make GCP a compelling choice for a respectable enterprise which hasn't lost the gift of curiosity just yet. Let's set up a server to see what the hype is about. Today'll we'll be creating a standard Linux instance and walking through the steps to configure an existing DNS for said instance.","og_image":"https://hackersandslackers.com/content/images/2018/07/gcp@2x.jpg","og_title":"Create a VPS with Google Cloud: Compute Engine","twitter_description":"The ease of plug-and-play these plug-and-play services make GCP a compelling choice for a respectable enterprise which hasn't lost the gift of curiosity just yet. Let's set up a server to see what the hype is about. Today'll we'll be creating a standard Linux instance and walking through the steps to configure an existing DNS for said instance.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/gcp@2x.jpg","twitter_title":"Create a VPS with Google Cloud: Compute Engine","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"For the last few weeks I've been enamored with Google's cloud platform, aptly\nnamed Google Cloud Platform. GCP contains the things you might expect from a\nyoung player in the 'screw AWS' space: much of what exists on AWS has an\nequivalent on GPC, but certain subtleties exist, such as the lack of Python\nserverless functions and so forth. That said, GCP makes up for any shortcomings\nby leveraging services exclusive to Google.\n\nIn my opinion, GCP is the first contender in the market to package enterprise\ncloud computing in a satisfying way. It's clear GCP has assigned UI and Product\nManagement resources to their platform, where Amazon clearly did not. while not\nwithout its shortcomings, it's obvious Google has chosen usability as a key\ndifferentiator from AWS.\n\nAside from the UI, GCP offers plenty of fun functionality such as their cloud\nlauncher. This is the equivalent of one-click deploys for cool stuff, whether\nthey be services to add to your VPC, Google APIs, Datasets, or what have you.\nThe ease of plug-and-play these plug-and-play services make GCP a compelling\nchoice for a respectable enterprise which hasn't lost the gift of curiosity.\n\nIt's like product hunt... on crack.The best way to get a feel for what value a\nproduct has would be use it, of course. In the interest of becoming familiar\nwith Google Cloud, we'll execute the most basic task of setting up a VPS to\ndeploy code to. This practice will end up touching on many of GCP's core\nservices which will allow us to grasp the basic offerings of the product, as\nwell as its strengths and weakness.\n\nDoes in Fact Compute \nGCP cutely names their server's Compute Engines,  which are at least more\ntolerable than, say, EC2 instances. I'm just going to call them servers because\nI'm not the type of person who orders a \"tall\" at Starbucks.\n\nCreate a \"project\" in Google Cloud, and you'll immediately land at a dashboard.\nAll Google's services are tucked away in the left-hand menu. Open that bad boy\nup and find Compute Engine.\n\nShhh, it's thinking.Select create. As opposed to the preset choices of VPCS you might be used to,\nGoogle allows us to customize our VPS to our exact technical specifications on a\nsliding scale. Want 96 processing cores, but only a single GB of RAM? No\nproblem, if that's what you're into. Weirdo.\n\nAs well as picking between the usual Linux distributions, Compute Engine also\nallows customers to select their number of GPUs, as well as the generation of\nIntel CPU their instance will run on.\n\nDat customization thoWe want traffic to hit this instance, so make sure you\ncheck Allow HTTP  traffic  and Allow HTTPS traffic  before continuing. Once your\ninstance is created, you should immediately able to SSH into your server via\nGCP's browser client.\n\nThe App Engine\nGCP is not without its own fair share of arbitrary product classifications. DNS\nrecords and hosts are contained within the App Engine  service of the platform.\nFind the App Engine service in the left hand navigation, and scroll down to the \nsettings  link:\n\nAllllll the way at the bottom.Here's we'll be able to see a \"custom domains\" tab\nwhich allows us to point a domain we've purchased from a service like Namecheap \n or what-have-you to Google Cloud. I'll personally be walking though this\nexample by directing a pointless domain called memegenerator.io  I purchased on\nNamecheap for no good reason.\n\nWhen you add a custom domain via this screen, you'll immediately be asked to\nverify ownership over the domain via the familiar Google Webmaster tool, which\nyou'll be redirected to automatically.\n\nBack to Your Registrar\nChances are you've dealt with verification via Google webmaster before, but this\ntime we've only given the option to do this via DNS. Select your registrar in\nthe dropdown in order to reveal a Google-generated record used to verify your\ndomain. \n\nPlease don't tell me you use GoDaddy.The resulting value will need to be added\nas a .txt record before we can actually point your domain to Google's servers.\n\nIf you're using Namecheap like I am, log in to your dashboard and find your\ndomain by clicking \"manage domain\". Make sure you're under the \"advanced DNS\"\ntab:\n\nEven if you're not using Namecheap, this shouldn't be much different.Delete all\nexisting records. Then create a TXT record (with @ as the host) and input the\nvalue that Google provided you earlier. Now, when you return to the webmaster\ntool, clicking verify tool should  pick up on this change. \n\nIf the webmaster tool does not pick up on this verification right away, don't\npanic. This happens fairly often - just frantically keep making sure everything\nis set up correctly while mashing the verify button.Navigate back to the Custom\nDomains tab in GCP and continue the process- you should see that your domain is\nverified. You'll be prompted to enter any subdomains you'd GCP to pick up on\nhere. Wrap that up and save.\n\n\n\nMake it Rain with Records\nOh, we're far from over buddy. We need to go back to update our A and AAAA\nrecords, now that Google as bestowed that privilege upon us. You should see a\ntable such as the one below:\n\nType\n Data\n Alias\n A\n 216.239.32.21\n A\n 216.239.34.21\n A\n 216.239.36.21\n A\n 216.239.38.21\n AAAA\n 2001:4860:4802:32::15\n AAAA\n 2001:4860:4802:34::15\n AAAA\n 2001:4860:4802:36::15\n AAAA\n 2001:4860:4802:38::15\n CNAME\n ghs.googlehosted.com\n www\n Copy that into your registrar's custom DNS records. Have fun with that.\n\nCloud DNS\nYou may have noticed that we haven't actually specified our Nameservers yet.\nNobody said this was going to be fun; if it were, we probably wouldn't need this\ntutorial. In the GCP search bar, search for Cloud DNS. Create a Zone, and leave\nDNSSEC off.\n\n\n\nBefore we do this next part, I'd like to interject and mention that you did a\nspectacular job of creating all those records and pasting all those values\nearlier. Considering you seem to have a knack for this, it probably won't hurt\nto know that we need to go back into our registrar a third time to paste some\nvalues. You got this. \n\nGoogle's nameservers have now been generated and exposed to you so we can \nactually  point our domain to something meaningful now. You should have 4\nnameservers like the following:\n\nName\n Type\n TTL\n Data\n memegenerator.io.\n NS\n 21600\n ns-cloud-b1.googledomains.com.\n ns-cloud-b2.googledomains.com.\n ns-cloud-b3.googledomains.com.\n ns-cloud-b4.googledomains.com.\n Assign a Static IP\nOkay, we're officially done messing around with our registrar. In the GCP search\nbar, search for External IP addresses.  From there, click the \"Reserve static\nAddress\" button at the top of the screen. This will prompt you with a short\nform: the only important field to fill out here is the \"Attached to\"  dropdown,\nwhich denotes which server instance the IP will be assigned to.\n\nCompute Engine Instance Settings\nShit, is there even more? OK, we're almost done here. Go to your Compute Engine\ninstance you set up earlier. Click \"Edit\". Scroll to the Network Interface \nsection and map the Static IP we created from earlier. Also, go ahead and enter\nyour PTR record:\n\nWhen will it end... please send help.FINAL CHAPTER: Firewall Settings\nLook, I just want to say you're all doing a great job so far. All of you. We're\nall a team here; let's stick together and see this through. Search for Firewall\nRules  and selected Create a Firewall Rule. Name it whatever you want.\n\n * Targets  - This will be where our traffic routes. We want to route to our\n   instance, which is a specified service account.\n * Target service account  - Referring to the above, this is where we select the\n   computer instance we want to hit.\n * Target service account  scope  - Select \"in this project\".\n * Source Filter - Once again, select specified service account.\n * Source service account  scope - Select \"in this project\"\n * Source service account  - This is where we say where the traffic is coming\n   from. It's coming from the App engine, as this is where we specified our DNS.\n * For IPs  and ports, well, do what you want. It's your server. \n\nGet at it\nWell, there you have it. Hopefully by now the domain you've painstaking\nconfigured now points to your server, so you can go ahead and configure your\nwebserver settings or whatever it is you do.\n\nAlright fine, so GCP isn't completely free of its own redundancies. As much as I\nlove to hate on AWS, it seems almost inevitable at this point that any\nenterprise cloud service will maintain a certain level of obscure processes.\nThis is great for flexibility when scaling, but let's be honest: if these\nplatforms were easy to use, who would pay for the certifications?\n\nCheekiness aside, I've become a strong fan of GCP. Google seems to have hit a\nmiddle ground between being user-friendly and powerful, which fills a niché\nwe'll realize was desperately needed. For a fair review of the platform itself,\nI find myself agreeing mostly with this stranger from the internet: \nhttps://www.deps.co/blog/google-cloud-platform-good-bad-ugly/","html":"<p>For the last few weeks I've been enamored with Google's cloud platform, aptly named Google Cloud Platform. GCP contains the things you might expect from a young player in the 'screw AWS' space: much of what exists on AWS has an equivalent on GPC, but certain subtleties exist, such as the lack of Python serverless functions and so forth. That said, GCP makes up for any shortcomings by leveraging services exclusive to Google.</p><p>In my opinion, GCP is the first contender in the market to package enterprise cloud computing in a satisfying way. It's clear GCP has assigned UI and Product Management resources to their platform, where Amazon clearly did not. while not without its shortcomings, it's obvious Google has chosen usability as a key differentiator from AWS.</p><p>Aside from the UI, GCP offers plenty of fun functionality such as their cloud launcher. This is the equivalent of one-click deploys for cool stuff, whether they be services to add to your VPC, Google APIs, Datasets, or what have you. The ease of plug-and-play these plug-and-play services make GCP a compelling choice for a respectable enterprise which hasn't lost the gift of curiosity.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/2018-07-14_10-36-02.gif\" class=\"kg-image\"><figcaption>It's like product hunt... on crack.</figcaption></figure><p>The best way to get a feel for what value a product has would be use it, of course. In the interest of becoming familiar with Google Cloud, we'll execute the most basic task of setting up a VPS to deploy code to. This practice will end up touching on many of GCP's core services which will allow us to grasp the basic offerings of the product, as well as its strengths and weakness.</p><h2 id=\"does-in-fact-compute\">Does in Fact Compute </h2><p>GCP cutely names their server's <em>Compute Engines,</em> which are at least more tolerable than, say, EC2 instances. I'm just going to call them servers because I'm not the type of person who orders a \"tall\" at Starbucks.</p><p>Create a \"project\" in Google Cloud, and you'll immediately land at a dashboard. All Google's services are tucked away in the left-hand menu. Open that bad boy up and find Compute Engine.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/2018-07-14_10-44-14.gif\" class=\"kg-image\"><figcaption>Shhh, it's thinking.</figcaption></figure><p>Select <em>create</em>. As opposed to the preset choices of VPCS you might be used to, Google allows us to customize our VPS to our exact technical specifications on a sliding scale. Want 96 processing cores, but only a single GB of RAM? No problem, if that's what you're into. Weirdo.</p><p>As well as picking between the usual Linux distributions, Compute Engine also allows customers to select their number of GPUs, as well as the generation of Intel CPU their instance will run on.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-10.48.02-AM.png\" class=\"kg-image\"><figcaption>Dat customization tho</figcaption></figure><p>We want traffic to hit this instance, so make sure you check <strong>Allow HTTP</strong> <strong>traffic</strong> and <strong>Allow HTTPS traffic</strong> before continuing. Once your instance is created, you should immediately able to SSH into your server via GCP's browser client.</p><h2 id=\"the-app-engine\">The App Engine</h2><p>GCP is not without its own fair share of arbitrary product classifications. DNS records and hosts are contained within the <strong>App Engine</strong> service of the platform. Find the App Engine service in the left hand navigation, and scroll down to the <em>settings</em> link:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-10.58.19-AM.png\" class=\"kg-image\"><figcaption>Allllll the way at the bottom.</figcaption></figure><p>Here's we'll be able to see a \"custom domains\" tab which allows us to point a domain we've purchased from a service like <strong>Namecheap</strong><em> </em>or what-have-you to Google Cloud. I'll personally be walking though this example by directing a pointless domain called <em>memegenerator.io</em> I purchased on Namecheap for no good reason.</p><p>When you add a custom domain via this screen, you'll immediately be asked to verify ownership over the domain via the familiar Google Webmaster tool, which you'll be redirected to automatically.</p><h2 id=\"back-to-your-registrar\">Back to Your Registrar</h2><p>Chances are you've dealt with verification via Google webmaster before, but this time we've only given the option to do this via DNS. Select your registrar in the dropdown in order to reveal a Google-generated record used to verify your domain. </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-11.04.07-AM.png\" class=\"kg-image\"><figcaption>Please don't tell me you use GoDaddy.</figcaption></figure><p>The resulting value will need to be added as a .txt record before we can actually point your domain to Google's servers.</p><p>If you're using Namecheap like I am, log in to your dashboard and find your domain by clicking \"manage domain\". Make sure you're under the \"advanced DNS\" tab:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-11.08.58-AM.png\" class=\"kg-image\"><figcaption>Even if you're not using Namecheap, this shouldn't be much different.</figcaption></figure><p>Delete all existing records. Then create a TXT record (with @ as the host) and input the value that Google provided you earlier. Now, when you return to the webmaster tool, clicking verify tool <em>should</em> pick up on this change. </p><div class=\"protip\">\nIf the webmaster tool does not pick up on this verification right away, don't panic. This happens fairly often - just frantically keep making sure everything is set up correctly while mashing the verify button.\n</div><p>Navigate back to the Custom Domains tab in GCP and continue the process- you should see that your domain is verified. You'll be prompted to enter any subdomains you'd GCP to pick up on here. Wrap that up and save.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-11.13.27-AM.png\" class=\"kg-image\"></figure><p></p><h2 id=\"make-it-rain-with-records\">Make it Rain with Records</h2><p>Oh, we're far from over buddy. We need to go back to update our A and AAAA records, now that Google as bestowed that privilege upon us. You should see a table such as the one below:</p><div class=\"tablecontainer\">\n<table>\n  <thead>\n    <tr>\n      <th>Type</th>\n      <th>Data</th>\n      <th>Alias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>A</td>\n      <td>216.239.32.21</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>216.239.34.21</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>216.239.36.21</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>216.239.38.21</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>AAAA</td>\n      <td>2001:4860:4802:32::15</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>AAAA</td>\n      <td>2001:4860:4802:34::15</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>AAAA</td>\n      <td>2001:4860:4802:36::15</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>AAAA</td>\n      <td>2001:4860:4802:38::15</td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>CNAME</td>\n      <td>ghs.googlehosted.com</td>\n      <td>www</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Copy that into your registrar's custom DNS records. Have fun with that.</p><h2 id=\"cloud-dns\">Cloud DNS</h2><p>You may have noticed that we haven't actually specified our Nameservers yet. Nobody said this was going to be fun; if it were, we probably wouldn't need this tutorial. In the GCP search bar, search for <em>Cloud DNS</em>. Create a Zone, and leave DNSSEC off.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-11.25.33-AM.png\" class=\"kg-image\"></figure><p></p><p>Before we do this next part, I'd like to interject and mention that you did a spectacular job of creating all those records and pasting all those values earlier. Considering you seem to have a knack for this, it probably won't hurt to know that we need to go back into our registrar a third time to paste some values. You got this. </p><p>Google's nameservers have now been generated and exposed to you so we can <em>actually</em> point our domain to something meaningful now. You should have 4 nameservers like the following:</p><table>\n  <thead>\n    <tr>\n      <th>Name</th>\n      <th>Type</th>\n      <th>TTL</th>\n      <th>Data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>memegenerator.io.</td>\n      <td>NS</td>\n      <td>21600</td>\n      <td>ns-cloud-b1.googledomains.com.</td>\n    </tr>\n    <tr>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>ns-cloud-b2.googledomains.com.</td>\n    </tr>\n    <tr>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>ns-cloud-b3.googledomains.com.</td>\n    </tr>\n    <tr>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>ns-cloud-b4.googledomains.com.</td>\n    </tr>\n  </tbody>\n</table><h2 id=\"assign-a-static-ip\">Assign a Static IP</h2><p>Okay, we're officially done messing around with our registrar. In the GCP search bar, search for <strong>External IP addresses.</strong> From there, click the \"Reserve static Address\" button at the top of the screen. This will prompt you with a short form: the only important field to fill out here is the <em>\"Attached to\"</em> dropdown, which denotes which server instance the IP will be assigned to.</p><h2 id=\"compute-engine-instance-settings\">Compute Engine Instance Settings</h2><p>Shit, is there even more? OK, we're almost done here. Go to your Compute Engine instance you set up earlier. Click \"Edit\". Scroll to the <em>Network Interface</em> section and map the Static IP we created from earlier. Also, go ahead and enter your PTR record:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-07-14-at-11.47.24-AM.png\" class=\"kg-image\"><figcaption>When will it end... please send help.</figcaption></figure><h2 id=\"final-chapter-firewall-settings\">FINAL CHAPTER: Firewall Settings</h2><p>Look, I just want to say you're all doing a great job so far. All of you. We're all a team here; let's stick together and see this through. Search for <strong>Firewall Rules</strong> and selected <em>Create a Firewall Rule. </em>Name it whatever you want.</p><ul><li><strong>Targets</strong> - This will be where our traffic routes. We want to route to our instance, which is a <em>specified service account.</em></li><li><strong>Target service account</strong> - Referring to the above, this is where we select the computer instance we want to hit.</li><li><strong>Target service account</strong> <strong>scope</strong> - Select \"in this project\".</li><li><strong>Source Filter </strong>- Once again, select <em>specified service account.</em></li><li><strong>Source service account</strong> <strong>scope </strong>- Select \"in this project\"</li><li><strong>Source service account</strong> - This is where we say where the traffic is coming from. It's coming from the <em>App engine</em>, as this is where we specified our DNS.</li><li>For <strong>IPs</strong> and <strong>ports, </strong>well, do what you want. It's your server. </li></ul><h2 id=\"get-at-it\">Get at it</h2><p>Well, there you have it. Hopefully by now the domain you've painstaking configured now points to your server, so you can go ahead and configure your webserver settings or whatever it is you do.</p><p>Alright fine, so GCP isn't completely free of its own redundancies. As much as I love to hate on AWS, it seems almost inevitable at this point that any enterprise cloud service will maintain a certain level of obscure processes. This is great for flexibility when scaling, but let's be honest: if these platforms were easy to use, who would pay for the certifications?</p><p>Cheekiness aside, I've become a strong fan of GCP. Google seems to have hit a middle ground between being user-friendly and powerful, which fills a niché we'll realize was desperately needed. For a fair review of the platform itself, I find myself agreeing mostly with this stranger from the internet: <a href=\"https://www.deps.co/blog/google-cloud-platform-good-bad-ugly/\">https://www.deps.co/blog/google-cloud-platform-good-bad-ugly/</a></p>","url":"https://hackersandslackers.com/setting-up-dns-with-google-cloud-platform/","uuid":"ed22ae1a-b636-48dd-8502-141ae08fa9d9","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b4a08921c20005e9422c108"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a2","title":"Lynx Roundup, July 14th","slug":"lynx-roundup-july-14th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","excerpt":"Blockchain algos, SQL Cheat Sheet, Convex Optimization.","custom_excerpt":"Blockchain algos, SQL Cheat Sheet, Convex Optimization.","created_at_pretty":"13 July, 2018","published_at_pretty":"14 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:11:32.000-04:00","published_at":"2018-07-14T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 14th | Hackers and Slackers","meta_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","og_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","og_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","og_title":"Lynx Roundup, July 14th","twitter_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","twitter_title":"Lynx Roundup, July 14th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"}],"plaintext":"https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f\n\n\n\nhttps://www.kdnuggets.com/2018/07/sql-cheat-sheet.html\n\n\n\nhttps://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization\n\n\n\nhttps://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi\n\n\n\nhttps://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization","html":"<p></p><p><a href=\"https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f\">https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f</a></p><p></p><p><a href=\"https://www.kdnuggets.com/2018/07/sql-cheat-sheet.html\">https://www.kdnuggets.com/2018/07/sql-cheat-sheet.html</a></p><p></p><p><a href=\"https://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization\">https://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization</a></p><p></p><p><a href=\"https://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi\">https://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi</a></p><p></p><p><a href=\"https://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization\">https://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-14th/","uuid":"16d7a70b-ae09-4917-8c78-478523e1c93a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b485eb4c6a9e951f8a6cc4b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673699","title":"Lynx Roundup, July 13th","slug":"lynx-roundup-july-13th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","excerpt":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff.","custom_excerpt":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff.","created_at_pretty":"02 July, 2018","published_at_pretty":"13 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:20:31.000-04:00","published_at":"2018-07-13T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 13th | Hackers and Slackers","meta_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","og_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","og_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","og_title":"Lynx Roundup, July 13th","twitter_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","twitter_title":"Lynx Roundup, July 13th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Way of thinking of regexes in a more modular, chunked way.  In Clojure - someone\nshould make a Python version with Toolz.  Maybe me.\nhttps://github.com/fhur/regie/blob/master/README.md\n\n\n\nhttps://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/\n\n\n\nhttps://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de\n\n\n\nhttps://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/\n\n\n\nhttps://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato","html":"<p></p><p>Way of thinking of regexes in a more modular, chunked way.  In Clojure - someone should make a Python version with Toolz.  Maybe me.  <a href=\"https://github.com/fhur/regie/blob/master/README.md\">https://github.com/fhur/regie/blob/master/README.md</a></p><p></p><p><a href=\"https://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/\">https://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/</a></p><p></p><p><a href=\"https://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de\">https://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de</a></p><p></p><p><a href=\"https://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/\">https://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/</a></p><p></p><p><a href=\"https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato\">https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-13th/","uuid":"4d664425-8ee2-4308-afc2-3dc55a1856b9","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c42fd0ac8a143588f355"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673698","title":"Lynx Roundup, July 12th","slug":"lynx-roundup-july-12th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/12/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/12/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"12 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:11:17.000-04:00","published_at":"2018-07-12T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 12th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","og_title":"Lynx Roundup, July 12th","twitter_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","twitter_title":"Lynx Roundup, July 12th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/\n\n\n\nhttps://www.interviewcake.com/data-structures-reference\n\n\n\nhttps://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6\n\n\n\nhttps://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/\n\n\n\nhttps://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values","html":"<p></p><p><a href=\"https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/\">https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/</a></p><p></p><p><a href=\"https://www.interviewcake.com/data-structures-reference\">https://www.interviewcake.com/data-structures-reference</a></p><p></p><p><a href=\"https://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6\">https://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6</a></p><p></p><p><a href=\"https://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/\">https://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/</a></p><p></p><p><a href=\"https://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values\">https://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-12th/","uuid":"347c148f-b80f-4e98-aec1-42557c7ea37b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c205d0ac8a143588f34f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673697","title":"Lynx Roundup, July 11th","slug":"lynx-roundup-july-11th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/11/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/11/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"11 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:09:25.000-04:00","published_at":"2018-07-11T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 11th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","og_title":"Lynx Roundup, July 11th","twitter_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","twitter_title":"Lynx Roundup, July 11th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/\n\n\n\nhttps://www.algorithm-archive.org/\n\n\n\nhttps://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/\n\n\n\nhttps://news.ycombinator.com/item?id=17372497\n\n\n\nhttps://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3","html":"<p></p><p><a href=\"https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/\">https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/</a></p><p></p><p><a href=\"https://www.algorithm-archive.org/\">https://www.algorithm-archive.org/</a></p><p></p><p><a href=\"https://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/\">https://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/</a></p><p></p><p><a href=\"https://news.ycombinator.com/item?id=17372497\">https://news.ycombinator.com/item?id=17372497</a></p><p></p><p><a href=\"https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3\">https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-11th/","uuid":"6169f835-54d7-4ec1-812e-d990e7f87d9a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c195d0ac8a143588f34a"}}]}},"pageContext":{"pageNumber":23,"humanPageNumber":24,"skip":276,"limit":12,"numberOfPages":33,"previousPagePath":"/page/23","nextPagePath":"/page/25"}}