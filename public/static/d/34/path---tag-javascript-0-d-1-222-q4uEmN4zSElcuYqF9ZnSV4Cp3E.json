{"data":{"ghostTag":{"slug":"javascript","name":"JavaScript","visibility":"public","feature_image":null,"description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c838ee05af763016e85085b","title":"Building a Client For Your GraphQL API","slug":"interacting-with-your-graphql-api","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","excerpt":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","custom_excerpt":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","created_at_pretty":"09 March, 2019","published_at_pretty":"09 March, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-03-09T05:01:04.000-05:00","published_at":"2019-03-09T15:43:14.000-05:00","updated_at":"2019-04-14T05:36:35.000-04:00","meta_title":"Building a Client For Your GraphQL API | Hackers and Slackers","meta_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","og_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","og_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","og_title":"Building a Client For Your GraphQL API","twitter_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","twitter_title":"Building a Client For Your GraphQL API","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"If you had the pleasure of joining us last time, we had just completed a crash\ncourse in structuring GraphQL Queries\n[https://hackersandslackers.com/writing-your-first-graphql-queries/]. As much we\nall love studying abstract queries within the confines of a playground\nenvironment, the only real way to learn anything to overzealously attempt to\nbuild something way out of our skill level. Thus, we're going to shift gears and\nactually make something  with all the dry technical knowledge we've accumulated\nso far. Hooray!\n\nData Gone Wild: Exposing Your GraphQL Endpoint\nIf you're following along with Prisma as your GraphQL service, the endpoint for\nyour API defaults to [your_ip_address]:4466. What's more, you've probably\nnoticed it is publicly accessible. THIS IS VERY BAD.  Your server has full\nread/write access to whichever database you configured with it... if anybody\nfinds your endpoint hanging out in a Github commit somewhere, you've just lost\nyour database and everything in it. You're pretty much Equifax, and you should\nfeel bad.\n\nPrisma has a straightforward solution. While SSHed into\nwherever-you-set-up-your-server, check out the prisma.yaml  file which was\ngenerated as a result of when we first started getting set up. You know, this\ndirectory:\n\nmy-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n\n\nprisma.yaml  seems inglorious, but that's because it's hiding a secret; or\nshould I say, it's not  hiding a secret! Hah!... (you know, like, credentials).\nAnyway. \n\nIn order to enable authorization on our endpoint, we need to add a secret to our\n prisma.yaml  file. The secret can be anything you like; this is simply a string\nwhich will be used to generate a token. Add a line which defines secret  like\nthis:\n\nendpoint: http://localhost:4466\ndatamodel: datamodel.prisma\nsecret: HIIHGUTFTUY$VK$G$YI&TUYCUY$DT$\n\ngenerate:\n  - generator: typescript-client\n    output: ./generated/prisma-client/\n\n\nWith your secret stashed away safely, the Prisma CLI can now use this secret to\ncreate the authentication token. This will be the value we pass in the headers\nof our requests to actually interact with our Prisma server remotely.\n\nType $ prisma token  in your project directory to get the work of art:\n\n$ prisma token\neyJhbGciOiJIUzI1NiIsInUYGFUJGSFKHFGSJFKSFJKSFGJdfSwiaWF0IjoxNTUyMTYwMDQ5LCJleHAiOjE1NTI3NjQ4NDl9.xrubUg_dRc93bqqR4f6jGt-KvQRS2Xq6lRi0a0uw-C0\n\n\nNice; believe it or not, that was the \"hard\" part.\n\nEXTRA CREDIT: Assign a DNS Record and Apply a Security Certificate\nIf really want to, you could already query against your insecure IP address and\nstart receiving some information. That said, making HTTP  requests as such from \nHTTPS  origins will fail. Not only that, but you kind of look shitty for not\neven bothering to name your API, much less apply a free SSL certificate. For the\neasiest possible way to do this, see our post on using Caddy as an HTTP server\n[https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/]\n.\n\nBuilding a Javascript Client to Consume Our API\nWith our API nice and secure, we can start hitting this baby from wherever we\nwant... as long as it's a Node app. We'll start by requiring two packages:\n\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\n\nGraphQLClient  is the magic behind our client- it's everything. It also happens\nto be very similar to existing npm  libraries for making requests, such as \nnode-fetch [https://hackersandslackers.com/making-api-requests-with-nodejs/].\n\nWe'll also leverage the dotenv  library to make sure our API endpoint  and \nBearer token  stay out of source code. Try not to be Equifax whenever possible. \ndotenv  allows us to load sensitive values from a .env  file. Just in case you\nneed a refresher, that file should look like this:\n\nNODE_ENV=Production\nENDPOINT=https://yourapiendpoint.com\nAUTH=Bearer eyJhbGciOBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHGUYFIERIBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHZl-UGnMrOk3w\n\nInitialize The GraphQL Client\nI like to set up a one-time client for our API that we can go back and reuse if\nneed be. After pulling the API endpoint and token from our .env  file, setting\nup the client is easy:\n\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n\nEMERGENCY MEETING: EVERYBODY HUDDLE UP\nOh I'm sorry, were you focusing on development? Unfortunately for you, I spent 8\nyears as a product manager, and I love  stopping everything suddenly to call\nemergency meetings.\n\nReal talk though, let's think back to the JIRA Kanban board example we've been\nusing for the last two posts. If you recall, we're going to write a query that\npopulates a 4-column Kanban board. The board represents a project (in this case,\n Hackers and Slackers) and each column represents a status  of ticket, like\nthis:\n\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n\nWe've previously established that GraphQL queries are friendly to drop-in\nvariables. Let's use this to build some logic into our client, as opposed to\nhardcoding a massive query, which is really just the same 4 queries stitched\ntogether. Here's what a query to populate a single JIRA column looks like:\n\n// Structured query\nconst query = `\n    query JiraIssuesByStatus($project: String, $status: String) {\n         jiraIssues(where: {project: $project, status: $status}, \n         orderBy: timestamp_DESC, \n         first: 6) {\n            key\n            summary\n            epic\n            status\n            project\n            priority\n            issuetype\n            timestamp\n            }\n         }\n       `\n\nWe're passing both the project  and the issue status  as variables to our query.\nWe can make things a bit dynamic here by looping through our statuses and\nexecuting this query four times: each time resulting in a callback filling the\nappropriate columns with JIRA issues.\n\nThis approach is certainly less clunky and more dynamic than a hardcoded query.\nThat said, this still  isn't the best solution. Remember: the strength of\nGraphQL is the ability to get obscene amounts of data across complex\nrelationships in a single call. The best approach here would probably be to\nbuild the query string itself dynamically using fragments,  which we'll review\nin the next post.Game On: Our Client in Action\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n// Structured query\nconst query = `\n   query JiraIssuesByStatus($project: String, $status: String) {\n      jiraIssues(where: {project: $project, status: $status}, orderBy: timestamp_DESC, first: 6) {\n         key\n         summary\n         epic\n         status\n         project\n         priority\n         issuetype\n         timestamp\n        }\n      }\n    `;\n\n// All Possible Issue Statuses\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n// Execute a query per issue status\nfor(var i = 0; i < statuses.length; i++){\n  var variables = {\n    project: \"Hackers and Slackers\",\n    status: statuses[i]\n  }\n\n  client.request(query, variables).then((data) => {\n    console.log(data)\n  }).catch(err => {\n    console.log(err.response.errors) // GraphQL response errors\n    console.log(err.response.data) // Response data if available\n  });\n}\n\n\nWorks like a charm. We only had one endpoint, only had to set one header, and\ndidn't spend any time reading through hundreds of pages of documentation to\nfigure out which combination of REST API endpoint, parameters, and methods\nactually get us what we want. It's almost as if we're writing SQL now, except...\nit looks a lot more like... NoSQL. Thanks for the inspiration, MongoDB! Hope\nthat whole selling-open-source-software  thing works out.\n\nOh, and of course, here were the results of my query:\n\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550194791,\n       project: 'Hackers and Slackers',\n       key: 'HACK-778',\n       epic: 'Code snippets',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'HLJS: set indentation level' },\n     { priority: 'Medium',\n       timestamp: 1550194782,\n\n       project: 'Hackers and Slackers',\n       key: 'HACK-555',\n       epic: 'Optimization',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Minify Babel' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-785',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Unix commands for data' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-251',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Using Ghost\\'s content filtering' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-302',\n       epic: 'Widgets',\n       status: 'Backlog',\n       issuetype: 'Integration',\n       summary: 'Discord channel signups ' },\n     { priority: 'Low',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-336',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Linux: Configuring your server to send SMTP emails' } ] }\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550224412,\n       project: 'Hackers and Slackers',\n       key: 'HACK-769',\n       epic: 'Projects Page',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'Fix projects dropdown' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-710',\n       epic: 'Lynx',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Implement auto text synopsis for Lynx posts' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-777',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Redesign footer to be informative; link-heavy' },\n     { priority: 'Highest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-779',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Changeover from cloudinary to DO' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-780',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Make mobile post title bold' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-781',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'This post consistently doesn’t work on mobile' } ] }\n{ jiraIssues:\n   [ { priority: 'Low',\n       timestamp: 1550223282,\n       project: 'Hackers and Slackers',\n       key: 'HACK-782',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary:\n        'Lynx: on mobile, instead of full link, show domainname.com/...' },\n     { priority: 'High',\n       timestamp: 1550194799,\n       project: 'Hackers and Slackers',\n       key: 'HACK-774',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'New Widget: Next/Previous article in series' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-395',\n       epic: 'Page Templates',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Create fallback image for posts with no image' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-756',\n       epic: 'Newsletter',\n       status: 'To Do',\n       issuetype: 'Major Functionality',\n       summary: 'Automate newsletter' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-775',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Data & Analytics',\n       summary: 'Update issuetype icons' },\n     { priority: 'Lowest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-776',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Add fork icon to repos' } ] }\n{ jiraIssues:\n   [ { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-784',\n       epic: 'New Post',\n       status: 'In Progress',\n       issuetype: 'Content',\n       summary: 'Welcome to SQL part1' } ] }\n\n\nBefore we say \"GG, 2ez, 1v1 me,\" know that we're  only getting started \nuncovering what GraphQL can do. It's not all just creating and deleting records\neither; we're talking full-on database JOIN equivalent type shit here. Stick\naround folks, the bandwagon's just getting warmed up.","html":"<p>If you had the pleasure of joining us last time, we had just completed a <a href=\"https://hackersandslackers.com/writing-your-first-graphql-queries/\">crash course in structuring GraphQL Queries</a>. As much we all love studying abstract queries within the confines of a playground environment, the only real way to learn anything to overzealously attempt to build something way out of our skill level. Thus, we're going to shift gears and actually <em>make something</em> with all the dry technical knowledge we've accumulated so far. Hooray!</p><h2 id=\"data-gone-wild-exposing-your-graphql-endpoint\">Data Gone Wild: Exposing Your GraphQL Endpoint</h2><p>If you're following along with Prisma as your GraphQL service, the endpoint for your API defaults to <code>[your_ip_address]:4466</code>. What's more, you've probably noticed it is publicly accessible. <strong>THIS IS VERY BAD.</strong> Your server has full read/write access to whichever database you configured with it... if anybody finds your endpoint hanging out in a Github commit somewhere, you've just lost your database and everything in it. You're pretty much Equifax, and you should feel bad.</p><p>Prisma has a straightforward solution. While SSHed into wherever-you-set-up-your-server, check out the <code>prisma.yaml</code> file which was generated as a result of when we first started getting set up. You know, this directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n</code></pre>\n<!--kg-card-end: markdown--><p><code>prisma.yaml</code> seems inglorious, but that's because it's hiding a secret; or should I say, it's <em>not</em> hiding a secret! Hah!... (you know, like, credentials). Anyway. </p><p>In order to enable authorization on our endpoint, we need to add a secret to our <code>prisma.yaml</code> file. The secret can be anything you like; this is simply a string which will be used to generate a token. Add a line which defines <code>secret</code> like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">endpoint: http://localhost:4466\ndatamodel: datamodel.prisma\nsecret: HIIHGUTFTUY$VK$G$YI&amp;TUYCUY$DT$\n\ngenerate:\n  - generator: typescript-client\n    output: ./generated/prisma-client/\n</code></pre>\n<!--kg-card-end: markdown--><p>With your secret stashed away safely, the <strong>Prisma CLI </strong>can now use this secret to create the authentication token. This will be the value we pass in the headers of our requests to actually interact with our Prisma server remotely.</p><p>Type <code>$ prisma token</code> in your project directory to get the work of art:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ prisma token\neyJhbGciOiJIUzI1NiIsInUYGFUJGSFKHFGSJFKSFJKSFGJdfSwiaWF0IjoxNTUyMTYwMDQ5LCJleHAiOjE1NTI3NjQ4NDl9.xrubUg_dRc93bqqR4f6jGt-KvQRS2Xq6lRi0a0uw-C0\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice; believe it or not, that was the \"hard\" part.</p><h3 id=\"extra-credit-assign-a-dns-record-and-apply-a-security-certificate\">EXTRA CREDIT: Assign a DNS Record and Apply a Security Certificate</h3><p>If really want to, you could already query against your insecure IP address and start receiving some information. That said, making <strong>HTTP</strong> requests as such from <strong>HTTPS</strong> origins will fail. Not only that, but you kind of look shitty for not even bothering to name your API, much less apply a free SSL certificate. For the easiest possible way to do this, see our post on <a href=\"https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/\">using Caddy as an HTTP server</a>.</p><h2 id=\"building-a-javascript-client-to-consume-our-api\">Building a Javascript Client to Consume Our API</h2><p>With our API nice and secure, we can start hitting this baby from wherever we want... as long as it's a Node app. We'll start by requiring two packages:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n</code></pre>\n<!--kg-card-end: markdown--><p><code>GraphQLClient</code> is the magic behind our client- it's everything. It also happens to be very similar to existing <strong>npm</strong> libraries for making requests, such as <a href=\"https://hackersandslackers.com/making-api-requests-with-nodejs/\">node-fetch</a>.</p><p>We'll also leverage the <code>dotenv</code> library to make sure our <strong>API endpoint</strong> and <strong>Bearer token</strong> stay out of source code. Try not to be Equifax whenever possible. <code>dotenv</code> allows us to load sensitive values from a <code>.env</code> file. Just in case you need a refresher, that file should look like this:</p><!--kg-card-begin: code--><pre><code>NODE_ENV=Production\nENDPOINT=https://yourapiendpoint.com\nAUTH=Bearer eyJhbGciOBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHGUYFIERIBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHZl-UGnMrOk3w</code></pre><!--kg-card-end: code--><h3 id=\"initialize-the-graphql-client\">Initialize The GraphQL Client</h3><p>I like to set up a one-time client for our API that we can go back and reuse if need be. After pulling the API endpoint and token from our <code>.env</code> file, setting up the client is easy:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"emergency-meeting-everybody-huddle-up\">EMERGENCY MEETING: EVERYBODY HUDDLE UP</h2><p>Oh I'm sorry, were you focusing on development? Unfortunately for you, I spent 8 years as a product manager, and I <em>love</em> stopping everything suddenly to call emergency meetings.</p><p>Real talk though, let's think back to the JIRA Kanban board example we've been using for the last two posts. If you recall, we're going to write a query that populates a 4-column Kanban board. The board represents a <em>project </em>(in this case, <strong>Hackers and Slackers</strong>) and each column represents a <em>status</em> of ticket, like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n</code></pre>\n<!--kg-card-end: markdown--><p>We've previously established that GraphQL queries are friendly to drop-in variables. Let's use this to build some logic into our client, as opposed to hardcoding a massive query, which is really just the same 4 queries stitched together. Here's what a query to populate a single JIRA column looks like:</p><!--kg-card-begin: code--><pre><code>// Structured query\nconst query = `\n    query JiraIssuesByStatus($project: String, $status: String) {\n         jiraIssues(where: {project: $project, status: $status}, \n         orderBy: timestamp_DESC, \n         first: 6) {\n            key\n            summary\n            epic\n            status\n            project\n            priority\n            issuetype\n            timestamp\n            }\n         }\n       `</code></pre><!--kg-card-end: code--><p>We're passing both the <em>project</em> and the <em>issue status</em> as variables to our query. We can make things a bit dynamic here by looping through our statuses and executing this query four times: each time resulting in a callback filling the appropriate columns with JIRA issues.</p><!--kg-card-begin: html--><div class=\"protip\">\nThis approach is certainly less clunky and more dynamic than a hardcoded query. That said, this <i>still</i> isn't the best solution. Remember: the strength of GraphQL is the ability to get obscene amounts of data across complex relationships in a single call. The best approach here would probably be to build the query string itself dynamically using <strong>fragments,</strong> which we'll review in the next post.\n</div><!--kg-card-end: html--><h2 id=\"game-on-our-client-in-action\">Game On: Our Client in Action</h2><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n// Structured query\nconst query = `\n   query JiraIssuesByStatus($project: String, $status: String) {\n      jiraIssues(where: {project: $project, status: $status}, orderBy: timestamp_DESC, first: 6) {\n         key\n         summary\n         epic\n         status\n         project\n         priority\n         issuetype\n         timestamp\n        }\n      }\n    `;\n\n// All Possible Issue Statuses\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n// Execute a query per issue status\nfor(var i = 0; i &lt; statuses.length; i++){\n  var variables = {\n    project: &quot;Hackers and Slackers&quot;,\n    status: statuses[i]\n  }\n\n  client.request(query, variables).then((data) =&gt; {\n    console.log(data)\n  }).catch(err =&gt; {\n    console.log(err.response.errors) // GraphQL response errors\n    console.log(err.response.data) // Response data if available\n  });\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Works like a charm. We only had one endpoint, only had to set one header, and didn't spend any time reading through hundreds of pages of documentation to figure out which combination of REST API endpoint, parameters, and methods actually get us what we want. It's almost as if we're writing SQL now, except... it looks a lot more like... NoSQL. Thanks for the inspiration, <strong>MongoDB</strong>! Hope that whole <em>selling-open-source-software</em> thing works out.</p><p>Oh, and of course, here were the results of my query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550194791,\n       project: 'Hackers and Slackers',\n       key: 'HACK-778',\n       epic: 'Code snippets',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'HLJS: set indentation level' },\n     { priority: 'Medium',\n       timestamp: 1550194782,\n\n       project: 'Hackers and Slackers',\n       key: 'HACK-555',\n       epic: 'Optimization',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Minify Babel' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-785',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Unix commands for data' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-251',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Using Ghost\\'s content filtering' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-302',\n       epic: 'Widgets',\n       status: 'Backlog',\n       issuetype: 'Integration',\n       summary: 'Discord channel signups ' },\n     { priority: 'Low',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-336',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Linux: Configuring your server to send SMTP emails' } ] }\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550224412,\n       project: 'Hackers and Slackers',\n       key: 'HACK-769',\n       epic: 'Projects Page',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'Fix projects dropdown' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-710',\n       epic: 'Lynx',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Implement auto text synopsis for Lynx posts' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-777',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Redesign footer to be informative; link-heavy' },\n     { priority: 'Highest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-779',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Changeover from cloudinary to DO' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-780',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Make mobile post title bold' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-781',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'This post consistently doesn’t work on mobile' } ] }\n{ jiraIssues:\n   [ { priority: 'Low',\n       timestamp: 1550223282,\n       project: 'Hackers and Slackers',\n       key: 'HACK-782',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary:\n        'Lynx: on mobile, instead of full link, show domainname.com/...' },\n     { priority: 'High',\n       timestamp: 1550194799,\n       project: 'Hackers and Slackers',\n       key: 'HACK-774',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'New Widget: Next/Previous article in series' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-395',\n       epic: 'Page Templates',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Create fallback image for posts with no image' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-756',\n       epic: 'Newsletter',\n       status: 'To Do',\n       issuetype: 'Major Functionality',\n       summary: 'Automate newsletter' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-775',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Data &amp; Analytics',\n       summary: 'Update issuetype icons' },\n     { priority: 'Lowest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-776',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Add fork icon to repos' } ] }\n{ jiraIssues:\n   [ { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-784',\n       epic: 'New Post',\n       status: 'In Progress',\n       issuetype: 'Content',\n       summary: 'Welcome to SQL part1' } ] }\n</code></pre>\n<!--kg-card-end: markdown--><p>Before we say \"GG, 2ez, 1v1 me,\" know that we're<em> only getting started</em> uncovering what GraphQL can do. It's not all just creating and deleting records either; we're talking full-on database JOIN equivalent type shit here. Stick around folks, the bandwagon's just getting warmed up.</p>","url":"https://hackersandslackers.com/interacting-with-your-graphql-api/","uuid":"34fef193-6a56-4754-a329-3d34571fcd15","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c838ee05af763016e85085b"}},{"node":{"id":"Ghost__Post__5c6dd08fa624d869fba41325","title":"Making API Requests With node-fetch","slug":"making-api-requests-with-nodejs","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","excerpt":"Using the lightweight node-fetch library for REST API requests in NodeJS.","custom_excerpt":"Using the lightweight node-fetch library for REST API requests in NodeJS.","created_at_pretty":"20 February, 2019","published_at_pretty":"21 February, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-02-20T17:11:27.000-05:00","published_at":"2019-02-20T20:22:20.000-05:00","updated_at":"2019-04-08T23:21:34.000-04:00","meta_title":"Making API Requests with node-fetch | Hackers and Slackers","meta_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","og_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","og_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","og_title":"Making API Requests with node-fetch","twitter_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","twitter_title":"Making API Requests with node-fetch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"If you're the type of person to read technical Javascript posts in your free\ntime (you are), you don't need me to tell you that JQuery is dead. JQuery\nthemselves have proclaimed JQuery to be dead. The only cool thing about JQuery\nis who can remove it from their legacy stack the fastest, which begs the\nquestion: why is the third most popular page on this site an old post about\nJQuery?\n\nMaintaining a blog of tutorials has taught me a lot about the gap between\nperception and reality. While we content publishers sling Medium posts from our\nivory towers, we quickly create a perception of what \"everybody\" is doing, but\nit turns out \"everybody\" only includes individuals who are exceptionally\nvisible. That demographic makes up significantly less than 10-20% of the active\nworkforce. I would have assumed any post with the word \"React\" would immediately\nexplode, when in reality people are more interested in using Handlebars with\nExpressJS [https://hackersandslackers.com/handlebars-templating-in-expressjs/] \n(I'm not proud of that post by the way, please don't read it).\n\nI want to provide an alternative to using AJAX calls when interacting with REST\nAPIs to clear my conscious of ever enabling bad behavior in the first place.\nHopefully, those who have lost their way might find something to take from it.\nConsidering how deep I've gone down the GraphQL rabbit hole myself, this may be\nthe last chance to bother writing about REST at all.\n\nLibrary of Choice: node-fetch\nLike everything in Javascript, there are way too many packages doing the same\nthing and solving the same problem. Making API requests is no exception. http\n[https://www.npmjs.com/package/http]  is a bit primitive, request\n[https://www.npmjs.com/package/request]  breaks when building with Webpack, r2\n[https://www.npmjs.com/package/r2]  seems like a pointless clone, and so on.\nDon't get me started with async libraries with 40 different methods for chaining\nrequests. Who is gunslinging API requests to the point where we need this many\noptions to pipe or parallel API requests anyway?\n\nAfter using all of these libraries, node-fetch\n[https://www.npmjs.com/package/node-fetch]  is the weapon of choice for today.\nTo put it simply: it's straightforward, and the only one that actually works out\nof the box with Webpack without absurd configuration nonsense. \n\nThe other request library worth mentioning is isomorphic-fetch\n[https://www.npmjs.com/package/isomorphic-fetch], which is intended to be a\ndrop-in replacement for node-fetch. isometric-fetch  mimics the syntax of \nnode-fetch, but impressively works on both  the client and server-side. When\nused on the client side, isomorphicfetch works by first importing the \nes6-promise  polyfill.\n\nGetting Set Up\nStart a Node project and install node-fetch:\n\nnpm install --save node-fetch\n\n\nIn the JS file we'd like to make a request, we can reference node-fetch  using \nrequire():\n\nconst fetch = require('node-fetch');\n\n\nCreating a node-fetch Request\nWe'll start with the most basic GET request possible:\n\nfetch('https://example.com')\n  .then(response => response.json())\n  .then(data => {\n    console.log(data)\n  })\n  .catch(err => ...)\n\n\nIndeed, that's all it takes a base level. Without specifying a method,\nnode-fetch assumes we're making a GET request. From we generate JSON from the\nrequest body and print the result to the console.\n\nChances are you're not going to get much value out of any request without\npassing headers, parameters, or a body to the target endpoint. Here's how we'd\nmake a more complicated (and realistic) POST call:\n\nvar url ='https://example.com';\nvar headers = {\n  \"Content-Type\": \"application/json\",\n  \"client_id\": \"1001125\",\n  \"client_secret\": \"876JHG76UKFJYGVHf867rFUTFGHCJ8JHV\"\n}\nvar data = {\n  \"name\": \"Wade Wilson\",\n  \"occupation\": \"Murderer\",\n  \"age\": \"30 (forever)\"\n}\nfetch(url, { method: 'POST', headers: headers, body: data})\n  .then((res) => {\n     return res.json()\n})\n.then((json) => {\n  console.log(json);\n  // Do something with the returned data.\n});\n\n\nThat's more like it: now we're passing headers and a JSON body. If needed, the \nfetch()  method also accepts a credentials  parameter for authentication.\n\nNote that we are avoiding callback hell by keeping logic that utilizes the\nresponse JSON in our then()  arrow functions. We can chain together as many of\nthese statements as we want.\n\nProperties of a Response\nThe response object contains much more than just the response body JSON:\n\nfetch('https://example.com')\n.then(res => {\n  res.text()       // response body (=> Promise)\n  res.json()       // parse via JSON (=> Promise)\n  res.status       //=> 200\n  res.statusText   //=> 'OK'\n  res.redirected   //=> false\n  res.ok           //=> true\n  res.url          //=> 'https://example.com'\n  res.type         //=> 'basic'\n                   //   ('cors' 'default' 'error'\n                   //    'opaque' 'opaqueredirect')\n\n  res.headers.get('Content-Type')\n})\n\n\nres.status  is particularly handy when building functionality around catching\nerrors:\n\nfetch('https://example.com')\n  .then(reportStatus)\n  \nfunction checkStatus (res) {\n  if (res.status >= 200 && res.status < 300) {\n    return res\n  } else {\n    let err = new Error(res.statusText)\n    err.response = res\n    throw err\n  }\n}\n\n\nMaking Asynchronous Requests\nChances are that when we make an API request, we're planning to do something\nwith the resulting data. Once we start building logic which depends on the\noutcome of a request, this is when we start running into Callback Hell: perhaps\nthe worst  part of JavaScript. In a nutshell, JavaScript will not wait for a\nrequest to execute the next line of code, therefore making a request and\nreferencing it immediately will result in no data returned. We can get around\nthis by using a combination of async  and await.\n\nasync  is a keyword which denotes that a function is to be executed\nasynchronously (as in async function my_func(){...}). await  can be used when\ncalling async  functions to wait on the result of an async function to be\nreturned (ie: const response = await my_func()).\n\nHere's an example of async/await  in action:\n\nconst fetch = require(\"node-fetch\");\n\nconst url = \"https://example.com\";\n\nconst get_data = async url => {\n  try {\n    const response = await fetch(url);\n    const json = await response.json();\n    console.log(json);\n  } catch (error) {\n    console.log(error);\n  }\n};\n\ngetData(url);","html":"<p>If you're the type of person to read technical Javascript posts in your free time (you are), you don't need me to tell you that JQuery is dead. JQuery themselves have proclaimed JQuery to be dead. The only cool thing about JQuery is who can remove it from their legacy stack the fastest, which begs the question: why is the third most popular page on this site an old post about JQuery?</p><p>Maintaining a blog of tutorials has taught me a lot about the gap between perception and reality. While we content publishers sling Medium posts from our ivory towers, we quickly create a perception of what \"everybody\" is doing, but it turns out \"everybody\" only includes individuals who are exceptionally visible. That demographic makes up significantly less than 10-20% of the active workforce. I would have assumed any post with the word \"React\" would immediately explode, when in reality people are more interested in using <a href=\"https://hackersandslackers.com/handlebars-templating-in-expressjs/\">Handlebars with ExpressJS</a> (I'm not proud of that post by the way, please don't read it).</p><p>I want to provide an alternative to using AJAX calls when interacting with REST APIs to clear my conscious of ever enabling bad behavior in the first place. Hopefully, those who have lost their way might find something to take from it. Considering how deep I've gone down the GraphQL rabbit hole myself, this may be the last chance to bother writing about REST at all.</p><h2 id=\"library-of-choice-node-fetch\">Library of Choice: node-fetch</h2><p>Like everything in Javascript, there are way too many packages doing the same thing and solving the same problem. Making API requests is no exception. <strong><a href=\"https://www.npmjs.com/package/http\">http</a> </strong>is a bit primitive, <strong><a href=\"https://www.npmjs.com/package/request\">request</a></strong> breaks when building with Webpack, <strong><a href=\"https://www.npmjs.com/package/r2\">r2</a></strong> seems like a pointless clone, and so on. Don't get me started with async libraries with 40 different methods for chaining requests. Who is gunslinging API requests to the point where we need this many options to pipe or parallel API requests anyway?</p><p>After using all of these libraries, <strong><a href=\"https://www.npmjs.com/package/node-fetch\">node-fetch</a></strong> is the weapon of choice for today. To put it simply: it's straightforward, and the only one that actually works out of the box with Webpack without absurd configuration nonsense. </p><p>The other request library worth mentioning is <strong><a href=\"https://www.npmjs.com/package/isomorphic-fetch\">isomorphic-fetch</a>, </strong>which is intended to be a drop-in replacement for <em>node-fetch</em>. <strong>isometric-fetch</strong> mimics the syntax of <strong>node-fetch</strong>, but impressively works on <em>both</em> the client and server-side. When used on the client side, <em>isomorphicfetch </em>works by first importing the <code>es6-promise</code> polyfill.</p><h3 id=\"getting-set-up\">Getting Set Up</h3><p>Start a Node project and install node-fetch:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">npm install --save node-fetch\n</code></pre>\n<!--kg-card-end: markdown--><p>In the JS file we'd like to make a request, we can reference <strong>node-fetch</strong> using <strong>require():</strong></p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const fetch = require('node-fetch');\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-a-node-fetch-request\">Creating a node-fetch Request</h2><p>We'll start with the most basic GET request possible:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n  .then(response =&gt; response.json())\n  .then(data =&gt; {\n    console.log(data)\n  })\n  .catch(err =&gt; ...)\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, that's all it takes a base level. Without specifying a method, node-fetch assumes we're making a GET request. From we generate JSON from the request body and print the result to the console.</p><p>Chances are you're not going to get much value out of any request without passing headers, parameters, or a body to the target endpoint. Here's how we'd make a more complicated (and realistic) POST call:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var url ='https://example.com';\nvar headers = {\n  &quot;Content-Type&quot;: &quot;application/json&quot;,\n  &quot;client_id&quot;: &quot;1001125&quot;,\n  &quot;client_secret&quot;: &quot;876JHG76UKFJYGVHf867rFUTFGHCJ8JHV&quot;\n}\nvar data = {\n  &quot;name&quot;: &quot;Wade Wilson&quot;,\n  &quot;occupation&quot;: &quot;Murderer&quot;,\n  &quot;age&quot;: &quot;30 (forever)&quot;\n}\nfetch(url, { method: 'POST', headers: headers, body: data})\n  .then((res) =&gt; {\n     return res.json()\n})\n.then((json) =&gt; {\n  console.log(json);\n  // Do something with the returned data.\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>That's more like it: now we're passing headers and a JSON body. If needed, the <strong>fetch()</strong> method also accepts a <code>credentials</code> parameter for authentication.</p><p>Note that we are avoiding callback hell by keeping logic that utilizes the response JSON in our <strong>then()</strong> arrow functions. We can chain together as many of these statements as we want.</p><h3 id=\"properties-of-a-response\">Properties of a Response</h3><p>The response object contains much more than just the response body JSON:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n.then(res =&gt; {\n  res.text()       // response body (=&gt; Promise)\n  res.json()       // parse via JSON (=&gt; Promise)\n  res.status       //=&gt; 200\n  res.statusText   //=&gt; 'OK'\n  res.redirected   //=&gt; false\n  res.ok           //=&gt; true\n  res.url          //=&gt; 'https://example.com'\n  res.type         //=&gt; 'basic'\n                   //   ('cors' 'default' 'error'\n                   //    'opaque' 'opaqueredirect')\n\n  res.headers.get('Content-Type')\n})\n</code></pre>\n<!--kg-card-end: markdown--><p><code>res.status</code> is particularly handy when building functionality around catching errors:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n  .then(reportStatus)\n  \nfunction checkStatus (res) {\n  if (res.status &gt;= 200 &amp;&amp; res.status &lt; 300) {\n    return res\n  } else {\n    let err = new Error(res.statusText)\n    err.response = res\n    throw err\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"making-asynchronous-requests\">Making Asynchronous Requests</h2><p>Chances are that when we make an API request, we're planning to do something with the resulting data. Once we start building logic which depends on the outcome of a request, this is when we start running into <strong>Callback Hell</strong>: perhaps the <em>worst</em> part of JavaScript. In a nutshell, JavaScript will not wait for a request to execute the next line of code, therefore making a request and referencing it immediately will result in no data returned. We can get around this by using a combination of <strong>async</strong> and <strong>await.</strong></p><p><code>async</code> is a keyword which denotes that a function is to be executed asynchronously (as in <code>async function my_func(){...}</code>). <code>await</code> can be used when calling <code>async</code> functions to wait on the result of an async function to be returned (ie: <code>const response = await my_func()</code>).</p><p>Here's an example of <strong>async/await</strong> in action:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const fetch = require(&quot;node-fetch&quot;);\n\nconst url = &quot;https://example.com&quot;;\n\nconst get_data = async url =&gt; {\n  try {\n    const response = await fetch(url);\n    const json = await response.json();\n    console.log(json);\n  } catch (error) {\n    console.log(error);\n  }\n};\n\ngetData(url);\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/making-api-requests-with-nodejs/","uuid":"9b46eb2c-0339-44f9-8909-13474dff9377","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6dd08fa624d869fba41325"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673758","title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","slug":"complex-features-in-mongodb-cloud","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","excerpt":"Using functions, webhooks, and values to utilize external APIs.","custom_excerpt":"Using functions, webhooks, and values to utilize external APIs.","created_at_pretty":"12 December, 2018","published_at_pretty":"14 December, 2018","updated_at_pretty":"01 January, 2019","created_at":"2018-12-12T18:26:09.000-05:00","published_at":"2018-12-14T08:00:00.000-05:00","updated_at":"2019-01-01T09:36:10.000-05:00","meta_title":"Building Complex Features in MongoDB Cloud | Hackers and Slackers","meta_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","og_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","og_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","og_title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","twitter_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","twitter_title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Friends, family, and most importantly, strangers: I approach you today with a\ntale of renewed inspiration. After loudly broadcasting my own confusion and\nmediocre ability to actually implement an effective cloud via MongoDB Stitch, my\nineptitude has been answered with an early Christmas gift. \n\nMy incessant complaining gained some acknowledgement from a couple of folks over\nat MongoDB. Perhaps the timing is simply by chance, but since then I've begun\nnoticing something some subtleties in the Stitch documentation; namely that if\nyou look hard enough, some of it begins to make sense. Either way, I'm chalking\nthis one up as a Christmas Miracle.\n\nLet's Automate Stuff: More Webhooks, Less Labor\nTo demonstrate what building an end-to-end sexy feature looks like in MongoDB\nStitch, I'm going to borrow some help from some old friends: the team behind \nClarifai. \n\nClarifai is one of the early players in the field of what I'm sure we'll\ncreatively refer to as AI as a service. More specifically, they provide an API\nfor image recognition which returns impressive metadata simply by passing an\nimage URL. Best part is, unless you're abusing the shit out of 5000 requests per\nmonth, the API is essentially free:\n\nPredict\n Search\n Custom Model Training\n Add or Edit Input Images\n 5,000  free operations\n 5,000  free operations\n 5,000  free operations\n 5,000  free operations\n Pre-Built Models: \n$1.20 / 1,000  operations\n\nCustom Models:\n$3.20 / 1,000  operations\n$1.20 / 1,000  operations\n $1.20 / 1,000  operations\n $1.20 / 1,000  operations\n If we were to try to fit any more instances of the words \"AI\" and \"Cloud\" into\nthis post, things could quickly derail into a shitty  IBM Watson commercial.\n\n(PS: Blockchain.)\n\nStoring Our Clarifai API Key\nIf you're following along, hit up Clarifai [https://clarifai.com/]  to grab your\nAPI key, no strings attached.\n\nAnd no, nobody is paying me to me to write about their SaaS products.Copy and\npaste your brand new key and head over to the MongoDB Stitch Console\n[https://stitch.mongodb.com]. In our Stitch project, we're going to store our\nkey as a value (you might recall this as being a convenient way to store\nsecrets).\n\nCopy and paste your key as a string in a new value. The only catch is we'll be\nformatting our key as Key #####################, simply because this is the\nformat the API expects to receive when we pass our key as a header to the\nClarifai API.\n\nWarning: Mild Architecting Ahead\nBefore going too far into code, let's recap how this functionality will probably\nwork.\n\nIn our actual application, we'll be identifying images needing alt  tags (either\nvia frontend or backend logic). At that point, we should find the src  attribute\nof said <img>  tags and pass it to a Stitch function; preferably one that makes\na post request to Clarifai. \n\nThis is in fact too simple to be true, as there is one gotcha: Stitch functions \ncannot make http requests on their own. They can,  however, invoke Stitch \nWebhooks. These webhooks share nearly identical syntax and structure to \nfunctions, with a few exceptions:\n\n * Webhooks have endpoints (duh).\n * They have explicit inbound/outbound rules restricting what can invoke them.\n * There are options to set authorization via key or otherwise.\n\nWith all that in mind, our end-to-end flow will end up looking something like\nthis:\n\n 1. Our application identifies an image needing tags an invokes a serverless \n    function.\n 2. The function  constructs the body of the request we'll be making to Clarifai \n     with containing the URL of the image.\n 3. As crazy as it sounds, we then POST to a Stitch endpoint, which in turns\n    makes the actual  POST request to Clarifai. The request is made with the\n    body passed from our function, as well as the API key we stored earlier.\n 4. We'll receive a response of tags which we can do something with on the\n    application-side.\n\nWriting our Function\nWe'll start by writing a simple function as our go-between for our app and our\nservice:\n\nexports = function(img){\n   const http = context.services.get(\"GetClarifaiTags\");\n   var data = {\n        \"inputs\": [\n          {\n            \"data\": {\n              \"image\": {\n                \"url\": img\n              }\n            }\n          }\n        ]\n      };\n      \n    var header_data = {\"Content-Type\": [ \"application/json\" ]};\n   \n    return http.post({\n        url: \"https://webhooks.mongodb-stitch.com/api/client/v2.0/app/hackers-uangn/service/GetClarifaiTags/incoming_webhook/GetTagsForNewImage\",\n        headers: header_data,\n        body: JSON.stringify(data)\n      })\n      .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n\n\nThe first thing we do is reference our webhook (which we haven't created yet)\nwith this line:\n\nconst http = context.services.get(\"GetClarifaiTags\");\n\n\nIn the context of a function, context.services.get()  allows us to reference and\ninteract with other services we've created in Stitch. It's important to note\nthat we pass the user-created name of service we want to interact with. This is\none of the reasons why Stitch's documentation is so confusing - they\nconsistently use \"http\"  as an example service name. This seems to imply that\nwe'd want to import a type  of service as opposed to an instance  of a service,\nwhich is wrong.  \n\ndata  is the body of our request, which abides by Clarifai's documentation on\nhow to user their predict API. We need to pass this as a string to our webhook,\nthus we use JSON.stringify(data).\n\nIt's also important to note the structure of Mongo's headers when making\nrequests; notice that the value of each key pair is a list, as exemplified by \n\"Content-Type\": [ \"application/json\" ].\n\nAs you might imagine, these things in combination can cause a whole lot of\nconfusion. Hopefully you know a good blog to point these things out to you\nbeforehand.\n\nCreate a Webhook via 'HTTP Services'\nMove into the \"Services\" tab to create our webhook. Select HTTP  from the list\nof options:\n\nKind of a weird mix of services imho.Set your webhook to be a POST request.\nAuthentication shouldn't be a problem for us since we're only exposing this hook\nto our function, plus there are other ways to handle this.\n\nTIP: Don't post screenshots of sensitive endpoint URLs on the internet.The\nsyntax and methods available for writing a webhook are almost exactly the same\nas when writing regular functions. The one thing to note would be the presence\nof payload  being passed into the function; this object contains both the\nparameters and the body of requests being received by this endpoint. \npayload.body  gives us the body, whereas payload.query.arg  will give us the\nparameters.\n\nexports = function(payload){\n  const http = context.services.get(\"GetClarifaiTags\");\n  const token = context.values.get(\"clarifai_key\");\n  \n  var data = {};\n  if (payload.body) {\n    data = payload.body;\n  }\n  var header_data = {\n    \"Authorization\": [token], \n    \"Content-Type\": [\"application/json\"]\n  };\n\n    return http.post({\n        url: \"https://api.clarifai.com/v2/models/aaa03c23b3724a16a56b629203edc62c/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs\",\n        body: data,\n        headers: header_data\n      })\n      .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n\n\nJust as we can access services within functions, we can similarly access values\nvia context.values.get(\"myValue\").\n\nNow that we have both the body and our API key ready, we can actually go ahead\nand construct a valid request to Clarifai. The syntax should be\nself-explanatory, but here's the Stitch http service documentation\n[https://docs.mongodb.com/stitch/services/http-actions/http.post/]  just in\ncase.\n\nWhy Did we Have to Make a Weird Webhook which is both Receiving and Posting\nInformation?\nThis is an excellent question and served to be a huge source of confusion for\nwhat must have been months. Go back to the \"Services\" tab, and pay close\nattention: for each service we create, a set of Rules  are automatically created\nand attached to our service. HTTP services have all functionality disabled room\ndefault, with little to no mention of the existence of rules in the first place. \n This is important for two reasons:\n\n 1. It's a silly UI hiccup that can waste the majority of your waking life.\n 2. This means that only  services can do things like post to external APIs.\n    This is why we didn't simply keep our logic in one function.\n\nOur Workflow in Practice\nAssuming you've added some logic to your app to pick out image URLs needing\ntags, our chain of events should be complete and return results to our\napplication. The POST request we make will return a response to the POST request\nof our function, and our function will return the results to our application.\nWe've successfully created a complex, albeit confusing, cloud architecture or\nexternal services.\n\nThis is where your imagination should hopefully kick in. You'll notice I have a\nfew services such as the endpoints which receive updates every time a JIRA issue\nis created or updated. This is what powers our public-facing kanban board.\n[https://hackersandslackers.com/projects/]","html":"<p>Friends, family, and most importantly, strangers: I approach you today with a tale of renewed inspiration. After loudly broadcasting my own confusion and mediocre ability to actually implement an effective cloud via MongoDB Stitch, my ineptitude has been answered with an early Christmas gift. </p><p>My incessant complaining gained some acknowledgement from a couple of folks over at MongoDB. Perhaps the timing is simply by chance, but since then I've begun noticing something some subtleties in the Stitch documentation; namely that if you look hard enough, some of it begins to make sense. Either way, I'm chalking this one up as a Christmas Miracle.</p><h2 id=\"let-s-automate-stuff-more-webhooks-less-labor\">Let's Automate Stuff: More Webhooks, Less Labor</h2><p>To demonstrate what building an end-to-end sexy feature looks like in MongoDB Stitch, I'm going to borrow some help from some old friends: the team behind <strong>Clarifai</strong>. </p><p>Clarifai is one of the early players in the field of what I'm sure we'll creatively refer to as <em>AI as a service. </em>More specifically, they provide an API for image recognition which returns impressive metadata simply by passing an image URL. Best part is, unless you're abusing the shit out of 5000 requests per month, the API is essentially free:</p><style>\n    table td {\n        text-align:left;\n        font-size: .95em;\n    }\n</style>\n\n<div class=\"tableContainer\">\n  <table>\n    <thead>\n      <th>Predict</th>\n      <th>Search</th>\n      <th>Custom Model Training</th>\n      <th>Add or Edit Input Images</th>\n    </thead>\n    <tbody>\n      <tr>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n      </tr>\n      <tr>\n        <td>\n          <strong>Pre-Built Models: </strong><br>\n          <small><strong>$1.20 / 1,000</strong> operations</small><br><br>\n          <strong>Custom Models:</strong><br>\n          <small><strong>$3.20 / 1,000</strong> operations</small><br>\n        </td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<p>If we were to try to fit any more instances of the words \"AI\" and \"Cloud\" into this post, things could quickly derail into a shitty  IBM Watson commercial.</p><p><em>(PS: Blockchain.)</em></p><h2 id=\"storing-our-clarifai-api-key\">Storing Our Clarifai API Key</h2><p>If you're following along, hit up <a href=\"https://clarifai.com/\">Clarifai</a> to grab your API key, no strings attached.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/clarifaiapi_o.png\" class=\"kg-image\"><figcaption>And no, nobody is paying me to me to write about their SaaS products.</figcaption></figure><p>Copy and paste your brand new key and head over to the <a href=\"https://stitch.mongodb.com\">MongoDB Stitch Console</a>. In our Stitch project, we're going to store our key as a <strong>value </strong>(you might recall this as being a convenient way to store secrets).</p><p>Copy and paste your key as a string in a new value. The only catch is we'll be formatting our key as <code>Key #####################</code>, simply because this is the format the API expects to receive when we pass our key as a header to the Clarifai API.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mongovalues_o-1.png\" class=\"kg-image\"></figure><h2 id=\"warning-mild-architecting-ahead\">Warning: Mild Architecting Ahead</h2><p>Before going too far into code, let's recap how this functionality will probably work.</p><p>In our actual application, we'll be identifying images needing <code>alt</code> tags (either via frontend or backend logic). At that point, we should find the <code>src</code> attribute of said <code>&lt;img&gt;</code> tags and pass it to a Stitch function; preferably one that makes a post request to <strong>Clarifai</strong>. </p><p>This is in fact too simple to be true, as there is one gotcha: Stitch <strong>functions</strong> cannot make http requests on their own. They <em>can,</em> however, invoke Stitch <strong>Webhooks. </strong>These webhooks share nearly identical syntax and structure to <strong>functions</strong>, with a few exceptions:</p><ul><li>Webhooks have endpoints (duh).</li><li>They have explicit inbound/outbound rules restricting what can invoke them.</li><li>There are options to set authorization via key or otherwise.</li></ul><p>With all that in mind, our end-to-end flow will end up looking something like this:</p><ol><li>Our application identifies an image needing tags an invokes a serverless <strong>function.</strong></li><li>The <strong>function</strong> constructs the body of the request we'll be making to <strong>Clarifai</strong> with containing the URL of the image.</li><li>As crazy as it sounds, we then POST to a Stitch endpoint, which in turns makes the <em>actual</em> POST request to Clarifai. The request is made with the body passed from our function, as well as the API key we stored earlier.</li><li>We'll receive a response of tags which we can do something with on the application-side.</li></ol><h2 id=\"writing-our-function\">Writing our Function</h2><p>We'll start by writing a simple function as our go-between for our app and our service:</p><pre><code class=\"language-javascript\">exports = function(img){\n   const http = context.services.get(&quot;GetClarifaiTags&quot;);\n   var data = {\n        &quot;inputs&quot;: [\n          {\n            &quot;data&quot;: {\n              &quot;image&quot;: {\n                &quot;url&quot;: img\n              }\n            }\n          }\n        ]\n      };\n      \n    var header_data = {&quot;Content-Type&quot;: [ &quot;application/json&quot; ]};\n   \n    return http.post({\n        url: &quot;https://webhooks.mongodb-stitch.com/api/client/v2.0/app/hackers-uangn/service/GetClarifaiTags/incoming_webhook/GetTagsForNewImage&quot;,\n        headers: header_data,\n        body: JSON.stringify(data)\n      })\n      .then(response =&gt; {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n</code></pre>\n<p>The first thing we do is reference our webhook (which we haven't created yet) with this line:</p><pre><code class=\"language-javascript\">const http = context.services.get(&quot;GetClarifaiTags&quot;);\n</code></pre>\n<p>In the context of a function, <code>context.services.get()</code> allows us to reference and interact with other services we've created in Stitch. It's important to note that we pass <strong>the user-created name of service </strong>we want to interact with. This is one of the reasons why Stitch's documentation is so confusing - they consistently use <em>\"http\"</em> as an example service name. This seems to imply that we'd want to import a <em>type</em> of service as opposed to an <em>instance</em> of a service, which is <strong>wrong.</strong> </p><p><code>data</code> is the body of our request, which abides by <a href=\"https://clarifai.com/developer/guide/predict#predict\">Clarifai's documentation on how to user their <em>predict</em> API</a>. We need to pass this as a string to our webhook, thus we use <code>JSON.stringify(data)</code>.</p><p>It's also important to note the structure of Mongo's headers when making requests; notice that the value of each key pair is a <strong>list, </strong>as exemplified by <code>\"Content-Type\": [ \"application/json\" ]</code>.</p><p>As you might imagine, these things in combination can cause a whole lot of confusion. Hopefully you know a good blog to point these things out to you beforehand.</p><h2 id=\"create-a-webhook-via-http-services-\">Create a Webhook via 'HTTP Services'</h2><p>Move into the \"Services\" tab to create our webhook. Select <strong>HTTP</strong> from the list of options:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mongoservices.png\" class=\"kg-image\"><figcaption>Kind of a weird mix of services imho.</figcaption></figure><p>Set your webhook to be a POST request. Authentication shouldn't be a problem for us since we're only exposing this hook to our function, plus there are other ways to handle this.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-12-13-at-4.23.27-PM_o.png\" class=\"kg-image\"><figcaption>TIP: Don't post screenshots of sensitive endpoint URLs on the internet.</figcaption></figure><p>The syntax and methods available for writing a webhook are almost exactly the same as when writing regular functions. The one thing to note would be the presence of <strong>payload</strong> being passed into the function; this object contains <em><strong>both the parameters and the body </strong></em>of requests being received by this endpoint. <code>payload.body</code> gives us the body, whereas <code>payload.query.arg</code> will give us the parameters.</p><pre><code class=\"language-javascript\">exports = function(payload){\n  const http = context.services.get(&quot;GetClarifaiTags&quot;);\n  const token = context.values.get(&quot;clarifai_key&quot;);\n  \n  var data = {};\n  if (payload.body) {\n    data = payload.body;\n  }\n  var header_data = {\n    &quot;Authorization&quot;: [token], \n    &quot;Content-Type&quot;: [&quot;application/json&quot;]\n  };\n\n    return http.post({\n        url: &quot;https://api.clarifai.com/v2/models/aaa03c23b3724a16a56b629203edc62c/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs&quot;,\n        body: data,\n        headers: header_data\n      })\n      .then(response =&gt; {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n</code></pre>\n<p>Just as we can access services within functions, we can similarly access values via <code>context.values.get(\"myValue\")</code>.</p><p>Now that we have both the body and our API key ready, we can actually go ahead and construct a valid request to Clarifai. The syntax should be self-explanatory, but here's the <a href=\"https://docs.mongodb.com/stitch/services/http-actions/http.post/\">Stitch http service documentation</a> just in case.</p><h3 id=\"why-did-we-have-to-make-a-weird-webhook-which-is-both-receiving-and-posting-information\">Why Did we Have to Make a Weird Webhook which is both Receiving and Posting Information?</h3><p>This is an excellent question and served to be a huge source of confusion for what must have been months. Go back to the \"Services\" tab, and pay close attention: for each service we create, a set of <strong>Rules</strong> are automatically created and attached to our service. <strong>HTTP services have all functionality disabled room default, with little to no mention of the existence of rules in the first place.</strong> This is important for two reasons:</p><ol><li>It's a silly UI hiccup that can waste the majority of your waking life.</li><li>This means that <em>only</em> services can do things like post to external APIs. This is why we didn't simply keep our logic in one function.</li></ol><h2 id=\"our-workflow-in-practice\">Our Workflow in Practice</h2><p>Assuming you've added some logic to your app to pick out image URLs needing tags, our chain of events should be complete and return results to our application. The POST request we make will return a response to the POST request of our function, and our function will return the results to our application. We've successfully created a complex, albeit confusing, cloud architecture or external services.</p><p>This is where your imagination should hopefully kick in. You'll notice I have a few services such as the endpoints which receive updates every time a <strong>JIRA </strong>issue is created or updated. This is what powers our <a href=\"https://hackersandslackers.com/projects/\">public-facing kanban board.</a></p>","url":"https://hackersandslackers.com/complex-features-in-mongodb-cloud/","uuid":"91acc3b3-88c2-4313-aedd-adf1eac1dc36","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c1199114b9896120b3c1b34"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c9","title":"MongoDB Stitch Serverless Functions","slug":"mongodb-stitch-serverless-functions","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/stitch3@2x.jpg","excerpt":"A crash course in MongoDB Stitch serverless functions: the bread and butter of MongoDB Cloud.","custom_excerpt":"A crash course in MongoDB Stitch serverless functions: the bread and butter of MongoDB Cloud.","created_at_pretty":"06 August, 2018","published_at_pretty":"26 November, 2018","updated_at_pretty":"05 April, 2019","created_at":"2018-08-06T19:35:37.000-04:00","published_at":"2018-11-26T08:00:00.000-05:00","updated_at":"2019-04-04T21:42:58.000-04:00","meta_title":"Using Serverless Functions in MongoDB Stitch  | Hackers And Slackers","meta_description":"You have a database, and you want to get data out of it. MongoDB Stitch can achieve this without building an API and can do it securely via frontend code.","og_description":"A crash course in MongoDB Stitch serverless functions: the bread and butter of MongoDB Cloud.\n","og_image":"https://hackersandslackers.com/content/images/2018/08/stitch3@2x.jpg","og_title":"MongoDB Stitch Serverless Functions","twitter_description":"A crash course in MongoDB Stitch serverless functions: the bread and butter of MongoDB Cloud.\n","twitter_image":"https://hackersandslackers.com/content/images/2018/08/stitch3@2x.jpg","twitter_title":"MongoDB Stitch Serverless Functions","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"At times, I've found my opinion of MongoDB Atlas  and MongoDB Stitch  to waver\nbetween two extremes. Sometimes I'm struck by the allure of a cloud which\nfundamentally disregards schemas (wooo no schema party!). Other times, such as\nwhen Mongo decides to upgrade to a new version and you find all your production\ninstances broken, I like the ecosystem a bit less. \n\nMy biggest qualm with MongoDB is poor documentation. The \"tutorials\" and sample\ncode seems hacked-together, unmaintained, and worst of all, inconsistent with\nitself. Reading through the docs seems to always end up with Mongo forcing\nTwilio down my throat my for some miserable reason. \n\nJust to illustrate how bad things can get, below are two totally sets of\ndocumentation for what is supposed to be the same product. Mongo's main\ndocumentation on the left frequently references the bastardized documentation on\nthe right. What is the documentation on the right? It's a collection of\nnonsense\nliving on an S3 bucket\n[https://s3.amazonaws.com/stitch-sdks/js/docs/4.0.0/index.html]  which lists the\nmethods black-boxed into Stitch, often with zero explanation on how to actually\nutilize functionality.\n\nWhich one is real? And WHY?!How frustrating is this? I've had email user\nauthentication \"working\" for weeks as far as Stitch's logs say, although not a\nsingle user has actually been registered in that time. Anyways, I digress.\n\nMaking a Serverless Function\nStitch Serverless functions are of course strictly Javascript (MongoDB abides by\nECMA2015 features). In your Stitch console, check out the \"functions\" link in\nthe left hand nav:\n\nGo ahead and create a new function.There are just a few things we need to\nspecify when creating a new function:\n\n * The name of the function (duh).\n * Whether or not the function can be accessed \"publicly\". A \"Private\" function\n   is the equivalent of a function that only accessible to the VPC it belongs to\n   (although technically MongoDB Cloud doesn't use this terminology).\n * A condition which needs to be met in order for the function to execute.\n\nHere's a screenshot of everything we just went over. Because whatever.Switch\nover to the function editor to start really F*&king Sh!t up.\n\nMongo's Serverless Function Editor\nWe can call a Serverless function in a number of ways, with one of those ways\nbeing directly from our frontend code. In this case, we're basically just taking\na Javascript function which could  live in our frontend codebase and moving it\nto the cloud, thus functions can be passed any number of arguments (just like a\nnormal function).\n\nLuckily for us, Mongo provides some commented out boilerplate code when creating\na new function, which gives us an idea of what we might want to use these\nfunctions for:\n\nexports = function(arg){\n  /*\n    Accessing application's values:\n    var x = context.values.get(\"value_name\");\n\n    Accessing a mongodb service:\n    var collection = context.services.get(\"mongodb-atlas\").db(\"dbname\").collection(\"coll_name\");\n    var doc = collection.findOne({owner_id: context.user.id});\n\n    To call other named functions:\n    var result = context.functions.execute(\"function_name\", arg1, arg2);\n\n    Try running in the console below.\n  */\n  return {arg: arg};\n};\n\n\nPay special attention to context.services  here. When using a serverless\nfunction to access MongoDB services such as our database or endpoints, we can\naccess these via context.services  along with whichever service we're trying to\nmess with.\n\nQuerying our Database Within a Function\nLet's grab a single record from a collection in our Atlas collection:\n\nexports = function(arg){\n      const mongodb = context.services.get(\"mongodb-atlas\");\n      const collection = mongodb.db(\"blog\").collection(\"authors\");\n      var result = collection.findOne({\"author\": arg});\n      return result;\n};\n\n\nWe use findOne here to return an object, whereas we'd probably use toArray  if\nwe'd be expecting multiple results. The query we're running is contained within \nfindOne({\"author\": arg}). Our function takes an argument and returns a record\nwhere the value matches the argument: this makes our functions highly reusable,\nof course.\n\nCalling Our Function via Our App\nAs a recap, you have the option of including Stitch in your app either via a\nlink to a script or by installing the appropriate NPM modules. It's preferable\nto do the latter, but for the sake of this post, my patience with dealing with\nJavascript's babel browserify webpack gulp yarn npm requires package-lock .env\npipify facepunch  ecosystem has reached its limit. \n\nFeel free to follow in my footsteps of worst practices by embedding stitch\ndirectly:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n\n\nAuthenticating Before Calling Functions\nBefore making queries or interacting with any serverless functions of any kind,\nwe need to authenticate a 'user' with the server; even if that user is an\nanonymous one (it's in our own best benefit to know which user crashed the\nserver, even if that 'users' is a random string of numbers). Because we allowed\nanonymous users to peruse through our data, this is easy:\n\n// Authenticates anonymous user\nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n  console.log('logged in anonymously as user')\n});\n\n\nCalling our Function\nNow that that's done, we can call our function immediately after:\n\n// Authenticates anonymous user\nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n  console.log('logged in anonymously as user')\n});\n\n// Calls function\nclient.callFunction(\"getUsers\", [\"{{author}}\"]).then(result => {\n  console.log(result)\n});\n\n\nOur function is called get users  and we're passing a single parameter of \n{{author}}. Even though one parameter is being passed, we pass parameters as\nlists as Mongo Serverless functions, as these functions are agnostic to what\nmight be coming their way.\n\nUsing Functions to Grab Stored Values\nLet's look at one more use case where calling a Stitch Serverless function might\ncome in handy.\n\nBack in the Stitch UI, check out the \"values\" tab in the left-hand nav. This is\na place where we can store constant values which should accessible through our\napplication, or even a place to retrieve secrets:\n\n2secret4uValues can only be retrieved by functions, and this would be a good\ntime to ensure those particular functions are marked \"private\" For instance, if\nyou have an API call you need to make, It would be best to create a function\nthat handles the logic of that API call, and within that function, invoke\nanother private function whose job it is simply to retrieve the key in question.\nMake sense?  Ah well, you'll figure it out.\n\nMaking a Serverless Function that Does Something\nAnyway, let's apply our knowledge of functions to actually do something. On our\nsite we currently use a third party Medium widget which fetches stories from a\nuser's Medium account. Here's how that would look in its entirety:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n\n<script src=\"https://medium-widget.pixelpoint.io/widget.js\"></script>\n\n<script>\nfunction createMediumCard(medium){\n  console.log('medium= ' + medium);\n  MediumWidget.Init({\n    renderTo: '#medium-widget',\n    params: {\n      \"resource\": 'https://medium.com/' + medium,\n      \"postsPerLine\": 1,\n      \"limit\": 3,\n      \"picture\": \"small\",\n      \"fields\": [\"description\", \"publishAt\"],\n      \"ratio\": \"square\"\n    }\n  })\n  $('#medium').css('display', 'block');\n}\n    \nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n  console.log('logged in anonymously as user')\n});\n    \nclient.callFunction(\"getUsers\", [\"{{author}}\"]).then(result => {\n  console.log(result)\n});\n</script>\n\n\nNormally, \"resource\": medium,  would actually read the URL of the Medium profile\nwe're trying to embed. However, when you blog on a platform like Ghost which\nonly allows your authors to have either Facebook or Twitter profiles, we need to\nessentially go out of our way to build a second, nonintrusive database to pull\ndata from to add functionality like this. Yeah - I'll have to show you what MY\n\"stack\" looks like for a single blog theme some day. It's ridiculous.\n\nAnyway, that’s all I’ve got for now. I hope these ramblings help you assess\nMongoDB Cloud for yourself. No matter the provider, Enterprise Clouds target fat\nbudgets and are designed to rake in big money. It almost makes you wonder why\nsomebody would pay out of pocket for three of them just to write a stupid blog.","html":"<p>At times, I've found my opinion of <strong>MongoDB Atlas</strong> and <strong>MongoDB Stitch</strong> to waver between two extremes. Sometimes I'm struck by the allure of a cloud which fundamentally disregards schemas (wooo no schema party!). Other times, such as when Mongo decides to upgrade to a new version and you find all your production instances broken, I like the ecosystem a bit less. </p><p><strong>My biggest qualm with MongoDB is poor documentation. </strong>The \"tutorials\" and sample code seems hacked-together, unmaintained, and worst of all, inconsistent with itself. Reading through the docs seems to always end up with Mongo forcing Twilio down my throat my for some miserable reason. </p><p>Just to illustrate how bad things can get, below are two totally sets of documentation for what is supposed to be the same product. Mongo's main documentation on the left frequently references the bastardized documentation on the right. What is the documentation on the right? It's a <a href=\"https://s3.amazonaws.com/stitch-sdks/js/docs/4.0.0/index.html\">collection of nonsense living on an S3 bucket</a> which lists the methods black-boxed into Stitch, often with zero explanation on how to actually utilize functionality.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mongodocs.jpg\" class=\"kg-image\"><figcaption>Which one is real? And WHY?!</figcaption></figure><!--kg-card-end: image--><p>How frustrating is this? I've had email user authentication \"working\" for weeks as far as Stitch's logs say, although not a single user has actually been registered in that time. Anyways, I digress.</p><h2 id=\"making-a-serverless-function\">Making a Serverless Function</h2><p>Stitch Serverless functions are of course strictly Javascript (MongoDB abides by ECMA2015 features). In your Stitch console, check out the \"functions\" link in the left hand nav:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-11-25-at-10.28.24-PM.png\" class=\"kg-image\"><figcaption>Go ahead and create a new function.</figcaption></figure><!--kg-card-end: image--><p>There are just a few things we need to specify when creating a new function:</p><ul><li>The name of the function (duh).</li><li>Whether or not the function can be accessed \"publicly\". A \"Private\" function is the equivalent of a function that only accessible to the VPC it belongs to (although technically MongoDB Cloud doesn't use this terminology).</li><li>A condition which needs to be met in order for the function to execute.</li></ul><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-11-25-at-10.31.05-PM_o.png\" class=\"kg-image\"><figcaption>Here's a screenshot of everything we just went over. Because whatever.</figcaption></figure><!--kg-card-end: image--><p>Switch over to the function editor to start really F*&amp;king Sh!t up.</p><h2 id=\"mongo-s-serverless-function-editor\">Mongo's Serverless Function Editor</h2><p>We can call a Serverless function in a number of ways, with one of those ways being directly from our frontend code. In this case, we're basically just taking a Javascript function which <em>could</em> live in our frontend codebase and moving it to the cloud, thus functions can be passed any number of arguments (just like a normal function).</p><p>Luckily for us, Mongo provides some commented out boilerplate code when creating a new function, which gives us an idea of what we might want to use these functions for:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">exports = function(arg){\n  /*\n    Accessing application's values:\n    var x = context.values.get(&quot;value_name&quot;);\n\n    Accessing a mongodb service:\n    var collection = context.services.get(&quot;mongodb-atlas&quot;).db(&quot;dbname&quot;).collection(&quot;coll_name&quot;);\n    var doc = collection.findOne({owner_id: context.user.id});\n\n    To call other named functions:\n    var result = context.functions.execute(&quot;function_name&quot;, arg1, arg2);\n\n    Try running in the console below.\n  */\n  return {arg: arg};\n};\n</code></pre>\n<!--kg-card-end: markdown--><p>Pay special attention to <code>context.services</code> here. When using a serverless function to access MongoDB services such as our database or endpoints, we can access these via <code>context.services</code> along with whichever service we're trying to mess with.</p><h3 id=\"querying-our-database-within-a-function\">Querying our Database Within a Function</h3><p>Let's grab a single record from a collection in our Atlas collection:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">exports = function(arg){\n      const mongodb = context.services.get(&quot;mongodb-atlas&quot;);\n      const collection = mongodb.db(&quot;blog&quot;).collection(&quot;authors&quot;);\n      var result = collection.findOne({&quot;author&quot;: arg});\n      return result;\n};\n</code></pre>\n<!--kg-card-end: markdown--><p>We use <strong>findOne </strong>here to return an object, whereas we'd probably use <strong>toArray</strong> if we'd be expecting multiple results. The query we're running is contained within <code>findOne({\"author\": arg})</code>. Our function takes an argument and returns a record where the value matches the argument: this makes our functions highly reusable, of course.</p><h2 id=\"calling-our-function-via-our-app\">Calling Our Function via Our App</h2><p>As a recap, you have the option of including Stitch in your app either via a link to a script or by installing the appropriate NPM modules. It's preferable to do the latter, but for the sake of this post, my patience with dealing with Javascript's <strong>babel browserify webpack gulp yarn npm requires package-lock .env pipify facepunch</strong> ecosystem has reached its limit. </p><p>Feel free to follow in my footsteps of worst practices by embedding stitch directly:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"authenticating-before-calling-functions\">Authenticating Before Calling Functions</h3><p>Before making queries or interacting with any serverless functions of any kind, we need to authenticate a 'user' with the server; even if that user is an anonymous one (it's in our own best benefit to know which user crashed the server, even if that 'users' is a random string of numbers). Because we allowed anonymous users to peruse through our data, this is easy:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">// Authenticates anonymous user\nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n  console.log('logged in anonymously as user')\n});\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"calling-our-function\">Calling our Function</h3><p>Now that that's done, we can call our function immediately after:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">// Authenticates anonymous user\nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n  console.log('logged in anonymously as user')\n});\n\n// Calls function\nclient.callFunction(&quot;getUsers&quot;, [&quot;{{author}}&quot;]).then(result =&gt; {\n  console.log(result)\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Our function is called <code>get users</code> and we're passing a single parameter of <code>{{author}}</code>. Even though one parameter is being passed, we pass parameters as lists as Mongo Serverless functions, as these functions are agnostic to what might be coming their way.</p><h2 id=\"using-functions-to-grab-stored-values\">Using Functions to Grab Stored Values</h2><p>Let's look at one more use case where calling a Stitch Serverless function might come in handy.</p><p>Back in the Stitch UI, check out the \"values\" tab in the left-hand nav. This is a place where we can store constant values which should accessible through our application, or even a place to retrieve secrets:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-11-26-at-8.04.25-AM_o.png\" class=\"kg-image\"><figcaption>2secret4u</figcaption></figure><!--kg-card-end: image--><p>Values can only be retrieved by functions, and this would be a good time to ensure those particular functions are marked \"private\" For instance, if you have an API call you need to make, It would be best to create a function that handles the logic of that API call, and within that function, invoke another private function whose job it is simply to retrieve the key in question. Make sense?  Ah well, you'll figure it out.</p><h2 id=\"making-a-serverless-function-that-does-something\">Making a Serverless Function that Does Something</h2><p>Anyway, let's apply our knowledge of functions to actually do something. On our site we currently use a third party Medium widget which fetches stories from a user's Medium account. Here's how that would look in its entirety:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n\n&lt;script src=&quot;https://medium-widget.pixelpoint.io/widget.js&quot;&gt;&lt;/script&gt;\n\n&lt;script&gt;\nfunction createMediumCard(medium){\n  console.log('medium= ' + medium);\n  MediumWidget.Init({\n    renderTo: '#medium-widget',\n    params: {\n      &quot;resource&quot;: 'https://medium.com/' + medium,\n      &quot;postsPerLine&quot;: 1,\n      &quot;limit&quot;: 3,\n      &quot;picture&quot;: &quot;small&quot;,\n      &quot;fields&quot;: [&quot;description&quot;, &quot;publishAt&quot;],\n      &quot;ratio&quot;: &quot;square&quot;\n    }\n  })\n  $('#medium').css('display', 'block');\n}\n    \nclient.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n  console.log('logged in anonymously as user')\n});\n    \nclient.callFunction(&quot;getUsers&quot;, [&quot;{{author}}&quot;]).then(result =&gt; {\n  console.log(result)\n});\n&lt;/script&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Normally, <code>\"resource\": medium,</code> would actually read the URL of the Medium profile we're trying to embed. However, when you blog on a platform like Ghost which only allows your authors to have either Facebook or Twitter profiles, we need to essentially go out of our way to build a second, nonintrusive database to pull data from to add functionality like this. Yeah - I'll have to show you what MY \"stack\" looks like for a single blog theme some day. It's ridiculous.</p><p>Anyway, that’s all I’ve got for now. I hope these ramblings help you assess MongoDB Cloud for yourself. No matter the provider, Enterprise Clouds target fat budgets and are designed to rake in big money. It almost makes you wonder why somebody would pay out of pocket for three of them just to write a stupid blog.</p>","url":"https://hackersandslackers.com/mongodb-stitch-serverless-functions/","uuid":"96e26ca1-02d4-41d6-afa8-db92b2e9c171","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b68db4904d65d1246ebd1eb"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373e","title":"Image Compression Using Gulp and Imagemin","slug":"simple-image-size-optimization-using-imagemin-and-gulp","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/imagemin-6@2x.jpg","excerpt":"The simplest way to optimize page speed without breaking everything.\n","custom_excerpt":"The simplest way to optimize page speed without breaking everything.\n","created_at_pretty":"16 November, 2018","published_at_pretty":"22 November, 2018","updated_at_pretty":"27 December, 2018","created_at":"2018-11-16T18:46:04.000-05:00","published_at":"2018-11-21T20:49:01.000-05:00","updated_at":"2018-12-26T23:25:33.000-05:00","meta_title":"Image Compression Using Gulp and Imagemin | Hackers and Slackers","meta_description":"The simplest way to optimize page speed without breaking everything.\n","og_description":"The simplest way to optimize page speed without breaking everything\n","og_image":"https://hackersandslackers.com/content/images/2018/11/imagemin-6@2x.jpg","og_title":"Image Compression Using Gulp and Imagemin | Hackers and Slackers","twitter_description":"The simplest way to optimize page speed without breaking everything\n","twitter_image":"https://hackersandslackers.com/content/images/2018/11/imagemin-6@2x.jpg","twitter_title":"Image Compression Using Gulp and Imagemin | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},"tags":[{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"}],"plaintext":"I promised myself I wouldn’t get involved in any more Gulp tutorials; task\nrunners aren’t exactly the sexiest topic in the world, and chances are if you’ve\nmade it to this blog, you’ve either solidified a CI/CD pipeline for going live\nwith software, or you simply don’t need one. We’ll make an exception this time,\nbecause gulp-imagemin  is particularly dope.\n\nImagemin [https://github.com/imagemin/imagemin]  is a standalone Node library\nwhich also comes with a CLI [https://github.com/imagemin/imagemin-cli], and of\ncourse, a Gulp plugin [https://github.com/sindresorhus/gulp-imagemin]. In short,\n imagemin  compresses images in a given directory and is intelligent enough to\nrecognize images it has already compressed. This is huge because it means we can\nrecklessly tell imagemin  to compress the same folder of images hundreds of\ntimes, and each image will only be compressed exactly once.\n\nFor this tutorial, we’ll be taking gulp-imagemin and creating a task to compress\nimages in complex folder structures.\n\nUsing Imagemin on Complex Folder Structures\nWe’ve probably mentioned this once or twice before, but this blog is a theme\nrunning on a Ghost [https://ghost.org/]  stack. The thing about Ghost (and\nprobably any other blogging platform) is that it stores content in a date-based\nfolder hierarchy. /images  looks like this:\n\n/images\n├─ /2017\n│  └─ 01\n│  └─ 02\n│  └─ 03\n│  └─ 04\n│  └─ 05\n│  └─ 06\n│  └─ 07\n│  └─ 08\n│  └─ 09\n│  └─ 10\n│  └─ 11\n│  └─ 12\n└─ /2018\n   └─ 01\n   └─ 02\n   └─ 03\n   └─ 04\n   └─ 05\n   └─ 06\n   └─ 07\n   └─ 08\n   └─ 09\n   └─ 10\n   └─ 11\n\n\nImagemin  does not  work recursively, so we’ll need to handle looping through\nthis file structure ourselves.\n\nStarting our Gulpfile\nLet’s get started by going through the barebones of the libraries required to\nmake this happen:\n\nvar gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  fs = require('fs'),\n  path = require('path');\n\n\ngulp-imagemin is the core Gulp plugin we need to compress our images, but is\nactually useless on it’s own — we need to also import plugins-for-a-plugin; \ngulp-imagemin requires a separate plugin for each image type we need to express.\n\nWe’re also requiring fs and path  here, which will let us walk through folder\nstructures programmatically.\n\nImagemin Plugins\nAs mentioned imagemin itself has plugins per image type: only require the ones\nyou think you’ll need:\n\nvar gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminWebp = require('imagemin-webp'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  imageminPngquant = require('imagemin-pngquant'),\n  imageminGifSicle = require('imagemin-gifsicle'),\n  imageminOptiPng = require('imagemin-optipng'),\n  imageminSvgo = require('imagemin-svgo'),\n  fs = require('fs'),\n  path = require('path');\n\n\nFor the sake of keeping this tutorial simple, we’ll limit our use case to JPGs.\n\nA particular standout here worth mentioning here is WebP\n[https://developers.google.com/speed/webp/]: a “next-gen” image compression for\nthe web which supposedly offers the best image quality for the smallest file\nsize available.\n\nLet’s Get This Going\nSome people (myself included) like to specify paths to their assets as a single\nvariable in their Gulpfile. This is even more relevant in the case of anybody\nusing Ghost, where images are in a totally different file structure from where\nour Gulpfile lives.\n\nvar gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  fs = require('fs'),\n  path = require('path');\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  },\n  images: {\n    src: '/var/www/my-theme/content/images/2018/',\n    dest: '/var/www/my-theme/content/images/2018/'\n  }\n};\n\n\nLooping Through Folders\nWe need to look in our /images  folder are recursively find all folders\ncontaining images. Referencing the image path we set in paths, we’ll build an\narray of targeted folders:\n\nfunction image_loop() {\n  var folder_arr = []\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i < folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      folder_arr.push(folder_path);\n    }\n    for(var i =0; i < folder_arr.length; i++) {\n        images(folder_arr[i]);\n    }\n  });\n}\n\n\nfs.readdir()  is a method that returns the contents of any directory. We'll\ncreate a function called image_loop which loops through all folders in the\ntarget directory, and will then call another function to compress the contents:\n\nfunction image_loop() { \n   fs.readdir(paths.images.src, function(err, folders) { \n      for(var i =0; i < folders.length; i++){ \n         var folder_path = path.join(paths.images.src, folders[i]);   \n         images(folders[i]); \n       } \n   }); \n }\n\n\nCompressing Images in Each Folder\nimage_loop  calls function images  once per folder to compress the contents of\neach folder. Here’s where we actually get to use imagemin:\n\nfunction image_loop() {\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i < folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      images(folders[i]);\n    }\n  });\n}\n\n\nSimple enough, all we’re doing is:\n\n * Looking for files ending in .jpg  in each folder\n * Running imageminJpegtran to compress each JPG file\n * Specifying verbose, which prints the result to the console (for example: \n   “Minified 0 images”)\n * Writing files to the destination (which is the same as the source, thus\n   overwriting our files)\n\nPut it All Together\nvar gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  fs = require('fs'),\n  path = require('path');\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  },\n  images: {\n    src: '/var/www/my-theme/content/images/2018/',\n    dest: '/var/www/my-theme/content/images/2018/'\n  }\n};\nfunction images(folder_path) {\n  return gulp.src(folder_path + '/*.jpg')\n  .pipe(imagemin(\n    [imageminJpegtran({progressive: true})],\n    {verbose: true}\n  ))\n  .pipe(gulp.dest(paths.images.dest));\n}\nfunction image_loop() {\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i < folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      images(folders[i]);\n    }\n  });\n}\nvar build = gulp.parallel(styles, scripts, image_loop);\ngulp.task('default', build);\n\n\nAnd there you have it; a Gulpfile which compresses your images without intruding\nrequiring any sort of relinking.\n\nIf you’re interested in imagemin  or further optimizing your site, I highly\nrecommend Google’s recently announced beta of https://web.dev [https://web.dev/]\n. This is an excellent resource for auditing your site for opportunities on\nspeed, SEO, and more.","html":"<p>I promised myself I wouldn’t get involved in any more Gulp tutorials; task runners aren’t exactly the sexiest topic in the world, and chances are if you’ve made it to this blog, you’ve either solidified a CI/CD pipeline for going live with software, or you simply don’t need one. We’ll make an exception this time, because <strong><strong>gulp-imagemin</strong></strong> is particularly dope.</p><p><a href=\"https://github.com/imagemin/imagemin\" rel=\"noopener\"><strong><strong>Imagemin</strong></strong></a> is a standalone Node library which also comes with a <a href=\"https://github.com/imagemin/imagemin-cli\" rel=\"noopener\">CLI</a>, and of course, a <a href=\"https://github.com/sindresorhus/gulp-imagemin\" rel=\"noopener\">Gulp plugin</a>. In short, <em>imagemin</em> compresses images in a given directory and is intelligent enough to recognize images it has already compressed. This is huge because it means we can recklessly tell <em>imagemin</em> to compress the same folder of images hundreds of times, and each image will only be compressed exactly once.</p><p>For this tutorial, we’ll be taking <strong><strong>gulp-imagemin </strong></strong>and creating a task to compress images in complex folder structures.</p><h3 id=\"using-imagemin-on-complex-folder-structures\">Using Imagemin on Complex Folder Structures</h3><p>We’ve probably mentioned this once or twice before, but this blog is a theme running on a <a href=\"https://ghost.org/\" rel=\"noopener\"><strong><strong>Ghost</strong></strong></a><strong><strong> </strong></strong>stack. The thing about Ghost (and probably any other blogging platform) is that it stores content in a date-based folder hierarchy. <strong><strong>/images</strong></strong> looks like this:</p><pre><code class=\"language-bash\">/images\n├─ /2017\n│  └─ 01\n│  └─ 02\n│  └─ 03\n│  └─ 04\n│  └─ 05\n│  └─ 06\n│  └─ 07\n│  └─ 08\n│  └─ 09\n│  └─ 10\n│  └─ 11\n│  └─ 12\n└─ /2018\n   └─ 01\n   └─ 02\n   └─ 03\n   └─ 04\n   └─ 05\n   └─ 06\n   └─ 07\n   └─ 08\n   └─ 09\n   └─ 10\n   └─ 11\n</code></pre>\n<p><strong><strong>Imagemin</strong></strong> does <em>not</em> work recursively, so we’ll need to handle looping through this file structure ourselves.</p><h3 id=\"starting-our-gulpfile\">Starting our Gulpfile</h3><p>Let’s get started by going through the barebones of the libraries required to make this happen:</p><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  fs = require('fs'),\n  path = require('path');\n</code></pre>\n<p><strong><strong>gulp-imagemin </strong></strong>is the core Gulp plugin we need to compress our images, but is actually useless on it’s own — we need to also import <em>plugins-for-a-plugin; </em>gulp-imagemin requires a separate plugin for each image type we need to express.</p><p>We’re also requiring <strong><strong>fs </strong></strong>and <strong><strong>path</strong></strong> here, which will let us walk through folder structures programmatically.</p><h3 id=\"imagemin-plugins\">Imagemin Plugins</h3><p>As mentioned imagemin itself has plugins per image type: only require the ones you think you’ll need:</p><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminWebp = require('imagemin-webp'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  imageminPngquant = require('imagemin-pngquant'),\n  imageminGifSicle = require('imagemin-gifsicle'),\n  imageminOptiPng = require('imagemin-optipng'),\n  imageminSvgo = require('imagemin-svgo'),\n  fs = require('fs'),\n  path = require('path');\n</code></pre>\n<p>For the sake of keeping this tutorial simple, we’ll limit our use case to JPGs.</p><p>A particular standout here worth mentioning here is <a href=\"https://developers.google.com/speed/webp/\" rel=\"noopener\">WebP</a>: a “next-gen” image compression for the web which supposedly offers the best image quality for the smallest file size available.</p><h3 id=\"let-s-get-this-going\">Let’s Get This Going</h3><p>Some people (myself included) like to specify paths to their assets as a single variable in their Gulpfile. This is even more relevant in the case of anybody using <strong><strong>Ghost, </strong></strong>where images are in a totally different file structure from where our Gulpfile lives.</p><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  fs = require('fs'),\n  path = require('path');\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  },\n  images: {\n    src: '/var/www/my-theme/content/images/2018/',\n    dest: '/var/www/my-theme/content/images/2018/'\n  }\n};\n</code></pre>\n<h3 id=\"looping-through-folders\">Looping Through Folders</h3><p>We need to look in our <strong><strong>/images</strong></strong> folder are recursively find all folders containing images. Referencing the image path we set in <strong><strong>paths</strong></strong>, we’ll build an array of targeted folders:</p><pre><code class=\"language-javascript\">function image_loop() {\n  var folder_arr = []\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i &lt; folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      folder_arr.push(folder_path);\n    }\n    for(var i =0; i &lt; folder_arr.length; i++) {\n        images(folder_arr[i]);\n    }\n  });\n}\n</code></pre>\n<p><code>fs.readdir()</code> is a method that returns the contents of any directory. We'll create a function called <strong><strong>image_loop </strong></strong>which loops through all folders in the target directory, and will then call another function to compress the contents:</p><pre><code class=\"language-javascript\">function image_loop() { \n   fs.readdir(paths.images.src, function(err, folders) { \n      for(var i =0; i &lt; folders.length; i++){ \n         var folder_path = path.join(paths.images.src, folders[i]);   \n         images(folders[i]); \n       } \n   }); \n }\n</code></pre>\n<h3 id=\"compressing-images-in-each-folder\">Compressing Images in Each Folder</h3><p><strong><strong>image_loop</strong></strong> calls function <strong><strong>images</strong></strong> once per folder to compress the contents of each folder. Here’s where we actually get to use <strong><strong>imagemin:</strong></strong></p><pre><code class=\"language-javascript\">function image_loop() {\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i &lt; folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      images(folders[i]);\n    }\n  });\n}\n</code></pre>\n<p>Simple enough, all we’re doing is:</p><ul><li>Looking for files ending in <strong><strong>.jpg</strong></strong> in each folder</li><li>Running <strong><strong>imageminJpegtran </strong></strong>to compress each JPG file</li><li>Specifying <strong><strong>verbose</strong></strong>, which prints the result to the console (for example: <em>“Minified 0 images”</em>)</li><li>Writing files to the destination (which is the same as the source, thus overwriting our files)</li></ul><h2 id=\"put-it-all-together\">Put it All Together</h2><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n  imagemin = require('gulp-imagemin'),\n  imageminJpegtran = require('imagemin-jpegtran'),\n  fs = require('fs'),\n  path = require('path');\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  },\n  images: {\n    src: '/var/www/my-theme/content/images/2018/',\n    dest: '/var/www/my-theme/content/images/2018/'\n  }\n};\nfunction images(folder_path) {\n  return gulp.src(folder_path + '/*.jpg')\n  .pipe(imagemin(\n    [imageminJpegtran({progressive: true})],\n    {verbose: true}\n  ))\n  .pipe(gulp.dest(paths.images.dest));\n}\nfunction image_loop() {\n  fs.readdir(paths.images.src, function(err, folders) {\n    for(var i =0; i &lt; folders.length; i++){\n      var folder_path = path.join(paths.images.src, folders[i]);\n      images(folders[i]);\n    }\n  });\n}\nvar build = gulp.parallel(styles, scripts, image_loop);\ngulp.task('default', build);\n</code></pre>\n<p>And there you have it; a Gulpfile which compresses your images without intruding requiring any sort of relinking.</p><p>If you’re interested in <strong><strong>imagemin</strong></strong> or further optimizing your site, I highly recommend Google’s recently announced beta of <a href=\"https://web.dev/\" rel=\"noopener\">https://web.dev</a>. This is an excellent resource for auditing your site for opportunities on speed, SEO, and more.</p>","url":"https://hackersandslackers.com/simple-image-size-optimization-using-imagemin-and-gulp/","uuid":"e80f7a95-da5e-417f-8a71-683772fd93a9","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bef56bc5bebbe659bef57c0"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373d","title":"MongoDB Cloud: \"Backend as a Service\" with Atlas & Stitch","slug":"mongodb-cloud-backend-as-a-service-with-atlas-and-stitch","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","custom_excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","created_at_pretty":"13 November, 2018","published_at_pretty":"15 November, 2018","updated_at_pretty":"15 February, 2019","created_at":"2018-11-13T16:05:20.000-05:00","published_at":"2018-11-15T08:00:00.000-05:00","updated_at":"2019-02-15T12:49:05.000-05:00","meta_title":"MongoDB Cloud: \"Backend as a Service\" | Hackers and Slackers","meta_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","og_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","twitter_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","twitter_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Unless you've been living under a rock (or only visit this site via work-related\nGoogle Searches, like most people) you've probably heard me drone on here and\nthere about MongoDB Atlas  and MongoDB Stitch. I even went so far as to hack\ntogether an awful workflow that somehow utilized Tableau as an ETL tool to feed\nJIRA information into Mongo. I'd like to formally apologize for that entire\nseries: I can't imagine there's a single soul on this planet interested in\nlearning about all of those things simultaneously. Such hobbies reserved for\nmasochists with blogging addictions. I apologize. Let's start over.\n\nFirst off, this is not a tutorial on how to use MongoDB: the database. I have\nzero interest cluttering the internet by reiterating what a MEAN stack is for\nthe ten thousandth time, nor will I bore you with core NoSQL concepts you\nalready understand. I'm here to talk about the giant on the horizon we didn't\nsee coming, where MongoDB the database decided to become MongoDB Inc\n[https://en.wikipedia.org/wiki/MongoDB_Inc.]:  the enterprise cloud provider.\nThe same MongoDB that recently purchased mLab\n[https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab]\n, the other  cloud-hosted solution for Mongo databases. MongoDB the company is\nbold enough to place its bets on building a cloud far  simpler and restricted\nthan either AWS or GCloud. The core of that bet implies that most of us aren't\nexactly building unicorn products as much as we're reinventing the wheel: and\nthey're probably right.\n\nWelcome to our series on MongoDB cloud, where we break down every service\nMongoDB has to offer; one by one.\n\nWhat is MongoDB Cloud, and Does it Exist?\nWhat I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual\nname of the suite MongoDB offers) is actually two products:\n\n * MongoDB Atlas: A cloud-hosted MongoDB cluster with a beefy set of features.\n   Real-time dashboards, high-availability, security features,  an awesome\n   desktop client, and a CLI to top it all off.\n * MongoDB Stitch: A group of services designed to interact with Atlas in every\n   conceivable way, including creating endpoints, triggers, user authentication\n   flows, serverless functions, and a UI to handle all of this.\n\nI'm spying on you and every query you make.Atlas as a Standalone Database\nThere are plenty of people who simply want an instance of MongoDB hosted in the\ncloud as-is: just ask the guys at mLab. This was in fact how I got pulled into\nMongo's cloud myself.\n\nMongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo,\nwhich Mongo itself is confident in by offering a free tier\n[https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/]  of Atlas to\nprospective buyers. If you're a company or enterprise, the phrases High\nAvailability, Horizontal Scalability, relatively Higher Performance  will\nprobably be enough for you. But for us hobbyists, why pay for a Mongo cloud\ninstance?\n\nMongo themselves gives this comparison:\n\nOverview\n MongoDB Atlas\n Compose\n ObjectRocket\n Free Tier\n Yes\nStorage: 512 MB\nRAM: Variable\n No\n30-day free trial\n No\n30-day free trial\n Live migration\n Yes\nNo\nNo\nChoice of cloud providers\n AWS, Azure & GCP\n AWS, Softlayer & GCP\nAvailable in 2 regions for each provider\n Rackspace\n Choice of instance configuration\n Yes\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n Availability of latest MongoDB version\n Yes\nNew versions of the database are available on MongoDB Atlas as soon as they are\nreleased\n No\nNew versions typically available 1-2 quarters following database release\nNo\nNew versions typically available 1-2 quarters following database release\nReplica Set Configuration\n Up to 7 replicas\nAll replicas configured as data-bearing nodes\n 3 data-bearing nodes\nOne of the data-bearing nodes is hidden and used for backups only\n 3 data-bearing nodes\nAutomatic Sharding Support\n Yes\nNo\nYes\nData explorer\n Yes\nYes\nNo\nSQL-based BI Connectivity\n Yes\nNo\nNo\nPause and resume clusters\n Yes\nNo\nNo\nDatabase supported in on-premise deployments\n Yes\nMongoDB Enterprise Advanced [/products/mongodb-enterprise-advanced]\n No\nNo\nGlobal writes Low-latency writes from anywhere in the world Yes\n No\n No\n Cross-region replication Distribute data around the world for multi-region\nfault tolerance and local reads Yes\n No\n No\n Monitoring of database health with automated alerting\n Yes\nMongoDB Atlas UI & support for APM platforms (New Relic)\n Yes\nNew Relic\n Yes\nNew Relic\n Continuous backup\n Yes\nBackups maintained\nseconds behind production cluster\n No\nBackups taken with mongodump against hidden replica set member\n No\nBackups taken with mongodump\n Queryable backups\n Yes\nNo\nNo\nAutomated & consistent snapshots of sharded clusters\n Yes\nNot Applicable\nNo support for auto-sharding\n No\nRequires manually coordinating the recovery of mongodumps across shards\n Access control & IP whitelisting\n Yes\nYes\nYes\nAWS VPC Peering\n Yes\nBeta Release\nYes\nAdditional Charge\n Encryption of data in-flight\n Yes\nTLS/SSL as standard\n Yes\nYes\nEncryption of data at-rest\n Yes\nAvailable for AWS deployments; always on with Azure and GCP\n No\nYes\nAvailable only with specific pricing plans and data centers\n LDAP Integration\n Yes\n No\nNo\n Database-level auditing\nTrack DDL, DML, DCL operations\n Yes\n No\nNo\n Bring your own KMS\n Yes\n No\nNo\n Realistically there are probably only a number of items that stand out on the\ncomparison list when we go strictly database-to-database. Freedom over instance\nconfiguration sounds great, but in practice is more similar to putting a cap on\nhow much MongoDB decides to charge you that month (by the way, it's usually a\nlot; keep this mind). Having the Latest Version  seems great, but this can just\nas easily mean breaking production unannounced as much as it means new features.\n\nMongoDB clearly wins over the enterprise space with Continuous & queryable\nbackups, integration with LDAP, and automatic sharding support. Truthfully if\nthis were merely a database-level feature and cost comparison, the decision to\ngo with  MongoDB Atlas  would come down to how much you like their pretty\ndesktop interface:\n\nA perfectly legitimate reason to pay up, imho.So let's say MongoDB Atlas is\nmarginally better than a competitor in the confined realm of \"being a database.\"\nAre Stitch microservices enough to justify keeping your instance with the\nMongoDB team?\n\nService-by-Service Breakdown of Stitch\nStitch is kind of like if AWS exited in an alternative universe, where JSON and\nJavaScript were earth's only technologies. Thinking back to how we create APIs\nin AWS, the status quo almost always involves spinning up a Dynamo  (NoSQL)\ndatabase to put behind Lambda functions, accessible by API Gateway endpoints.\nStitch's core use case revolves around this use-case of end-user-accessing-data,\nwith a number of services dedicated specifically to supporting or improving this\nflow. The closest comparison to Stitch would be GCloud's Firebase. \n\nSo what makes Stitch so special?\n\nService 1: Querying Atlas Securely via Frontend Code\nSomething that cannot be understated is the ability to query Atlas via frontend\nJavascript. We're not passing API keys, Secrets, or any sort of nonsense;\nbecause you're configured things correctly, whitelisted domains can run queries\nof any complexity without ever interacting with an app's backend.  This is not a\ncrazy use case: consider this blog for example, or more so lately, mobile\napplications:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n<script>\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('<DATABASE>');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => \n    db.collection('<COLLECTION>').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() => \n    db.collection('<COLLECTION>').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs => {\n      console.log(\"Found docs\", docs)\n      console.log(\"[MongoDB Stitch] Connected to Stitch\")\n  }).catch(err => {\n    console.error(err)\n  });\n</script>\n\n\nThis isn't to say we're allowing any user to query any data all willy-nilly just\nbecause they're on our whitelisted IP: all data stored in Atlas is restricted to\nspecified Users  by defining User Roles. Joe Schmoe can't just inject a query\ninto any presumed database and wreak havoc, because Joe Schmoe can only access\ndata we've permitted his user account to view or write to. What is this \"user\naccount\" you ask? This brings us to the next big feature...\n\nService 2: End-User Account Creation & Management\nStitch will handle user account creation for you without the boilerplate.\nCreating an app with user accounts is a huge pain in the ass. Cheeky phrases\nlike 'Do the OAuth Dance'  can't ever hope to minimize the agonizing repetitive\npain of creating user accounts or managing relationships between users and data\n(can user X  see a comment from user Y?). Stitch allows most of the intolerably\nbenign logic behind these features to be handled via a UI.\n\nIt would be a far cry to say these processes have been \"trivialized\", but the\ntime saved is perhaps just enough to keep a coding hobbyist interested in their\nside projects as opposed to giving up and playing Rocket League.\n\nAs far as the permissions to read comments go... well, here's a self-explanatory\nscreenshot of how Stitch handles read/write document permission in its simplest\nform:\n\nOwners of comments can write their comments. Everybody else reads. Seems simple.\nService 3: Serverless Functions\nStitch functions are akin to AWS Lambda functions, but much easier to configure\nfor cross-service integration (and also limited to JavaScript ECMA 2015 or\nsomething). Functions benefit from the previous two features, in that they too\ncan be triggered from a whitelisted app's frontend, and are governed by a simple\n\"rules\" system, eliminating the need for security group configurations etc.\n\nThis is what calling a function from an app's frontend looks like:\n\n<script>\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n     client.callFunction(\"numCards\", [\"In Progress\"]).then(results => {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n</script>\n\n\nFunctions can run any query against Atlas, retrieve values  (such as environment\nvariables), and even call other functions. Functions can also be fired by\ndatabase triggers,  where a change to a collection will prompt an action such as\nan alert.\n\nService 4: HTTP Webhooks\nWebhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to\none another in that they are one-off URLs to perform single tasks. We could\nnever build a well-designed API using Stitch Webhooks, as we could with API\nGateway; this simply isn't the niche MongoDB is trying to hit (the opposite, in\nfact). \n\nConfiguration for a single Webhook.This form with a mere 6 fields clearly\nillustrates what Stitch intends to do: trivializing the creation of\ntraditionally non-trivial features.\n\nService 5: Storing 'Values' in Stitch\nA \"value\" is equivalent to an environment variable. These can be used to store\nAPI keys, secrets, or whatever. Of course, values are retrieved via functions.\n\nShhh, it's a secret ;)Service 6+: A Bunch of Mostly Bloated Extras\nFinally, Stitch has thrown in a few third-party integrations for good measure.\nSome integrations like S3 Integration could definitely come in handy, but it's\nworth asking why Mongo constantly over advertises their integrations with Github \n and Twilio. We've already established that we can create endpoints which accept\ninformation, and we can make functions which GET  information... so isn't\nanything with an API pretty easy to 'integrate' with?\n\nThis isn't to say the extra services aren't useful, they just seem a bit... odd.\nIt feels a lot like bloating the catalog, but the catalog isn't nearly bloated\nenough where it feels normal (like Heroku add-ons, for example). The choice to\nlaunch Stitch with a handful of barely-useful integrations only comes off as\nmore and more aimless as time passes; as months turn to years and no additions\nor updates are made to service offerings, it's worth questioning what the vision\nhad been for the product in the first place. In my experience, feature sets like\nthese happen when Product Managers are more powerful than they are useful.\n\nThe Breathtaking Climax: Is Stitch Worth It?\nI've been utilizing Stitch to fill in the blanks in development for months now,\nperhaps nearly a year. Each time I find myself working with Stitch or looking at\nthe bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive\ntoy with an infuriating lack of accurate documentation.\n\n  Stitch is very much a copy-and-paste-cookie-cutter-code  type of product,\nwhich begs the question of why their tutorials are recklessly outdated;\nsometimes to the point where MongoDB's own tutorial source code doesn't work. \nThere are so many use cases and potential benefits to Stitch, so why is the \nGithub repo [https://github.com/mongodb/stitch-examples]  containing example\ncode snippets so unmaintained, and painfully irrelevant? Lastly, why am I\nselling this product harder than their own internal team?\n\nStitch is a good product with a lot of unfortunate oversight. That said, Google\nFirebase still doesn't even have an \"import data\" feature, so I suppose it's\ntime to dig deep into this vendor lock and write a 5-post series about it before\nSilicon Valley's best and brightest get their shit together enough to actually\ncreate something useful and intuitive for other human beings to use. In the\nmeantime, feel free to steal source from tutorials I'll be posting, because\nthey'll be sure to, you know, actually work.","html":"<p>Unless you've been living under a rock (or only visit this site via work-related Google Searches, like most people) you've probably heard me drone on here and there about <strong>MongoDB Atlas</strong> and <strong>MongoDB Stitch</strong>. I even went so far as to hack together an awful workflow that somehow utilized Tableau as an ETL tool to feed JIRA information into Mongo. I'd like to formally apologize for that entire series: I can't imagine there's a single soul on this planet interested in learning about all of those things simultaneously. Such hobbies reserved for masochists with blogging addictions. I apologize. Let's start over.</p><p>First off, this is not a tutorial on how to use <em>MongoDB: the database</em>. I have zero interest cluttering the internet by reiterating what a MEAN stack is for the ten thousandth time, nor will I bore you with core NoSQL concepts you already understand. I'm here to talk about the giant on the horizon we didn't see coming, where MongoDB the database decided to become <a href=\"https://en.wikipedia.org/wiki/MongoDB_Inc.\"><strong>MongoDB Inc</strong></a><strong>:</strong> the enterprise cloud provider. The same MongoDB that recently purchased <a href=\"https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab\">mLab</a>, the <em>other</em> cloud-hosted solution for Mongo databases. MongoDB the company is bold enough to place its bets on building a cloud <em>far</em> simpler and restricted than either AWS or GCloud. The core of that bet implies that most of us aren't exactly building unicorn products as much as we're reinventing the wheel: and they're probably right.</p><p>Welcome to our series on MongoDB cloud, where we break down every service MongoDB has to offer; one by one.</p><h2 id=\"what-is-mongodb-cloud-and-does-it-exist\">What is MongoDB Cloud, and Does it Exist?</h2><p>What I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual name of the suite MongoDB offers) is actually two products:</p><ul><li><strong>MongoDB Atlas</strong>: A cloud-hosted MongoDB cluster with a beefy set of features. Real-time dashboards, high-availability, security features,  an awesome desktop client, and a CLI to top it all off.</li><li><strong>MongoDB Stitch: </strong>A group of services designed to interact with Atlas in every conceivable way, including creating endpoints, triggers, user authentication flows, serverless functions, and a UI to handle all of this.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/metrics.gif\" class=\"kg-image\"><figcaption>I'm spying on you and every query you make.</figcaption></figure><h3 id=\"atlas-as-a-standalone-database\">Atlas as a Standalone Database</h3><p>There are plenty of people who simply want an instance of MongoDB hosted in the cloud as-is: just ask the guys at mLab. This was in fact how I got pulled into Mongo's cloud myself.</p><p>MongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo, which Mongo itself is confident in by offering a <a href=\"https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/\">free tier</a> of Atlas to prospective buyers. If you're a company or enterprise, the phrases <strong>High Availability</strong>, <strong>Horizontal Scalability, </strong>relatively <strong>Higher Performance</strong> will probably be enough for you. But for us hobbyists, why pay for a Mongo cloud instance?</p><p>Mongo themselves gives this comparison:</p><div class=\"tableContainer\">\n<table class=\"table left\">\n  <thead>\n    <tr>\n      <th>\n        <strong>Overview</strong>\n      </th>\n      <th>\n        <strong>MongoDB Atlas</strong>\n      </th>\n      <th>\n        <strong>Compose</strong>\n      </th>\n      <th>\n        <strong>ObjectRocket</strong>\n      </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\n        Free Tier\n      </td>\n      <td>\n        Yes<br><small>Storage: 512 MB<br>RAM: Variable</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Live migration\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of cloud providers\n      </td>\n      <td>\n        AWS, Azure &amp; GCP\n      </td>\n      <td>\n        AWS, Softlayer &amp; GCP<br><small>Available in 2 regions for each provider</small>\n      </td>\n      <td>\n        Rackspace\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of instance configuration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Availability of latest MongoDB version\n      </td>\n      <td>\n        Yes<br><small>New versions of the database are available on MongoDB Atlas as soon as they are released</small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Replica Set Configuration\n      </td>\n      <td>\n        Up to 7 replicas<br><small>All replicas configured as data-bearing nodes</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small>One of the data-bearing nodes is hidden and used for backups only</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automatic Sharding Support\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Data explorer\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        SQL-based BI Connectivity\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Pause and resume clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database supported in on-premise deployments\n      </td>\n      <td>\n        Yes<br><small><a href=\"/products/mongodb-enterprise-advanced\">MongoDB Enterprise Advanced</a></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Global writes <small>Low-latency writes from anywhere in the world </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Cross-region replication <small>Distribute data around the world for multi-region fault tolerance and local reads </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr><tr>\n      <td>\n        Monitoring of database health with automated alerting\n      </td>\n      <td>\n        Yes<br><small>MongoDB Atlas UI &amp; support for APM platforms (New Relic)</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Continuous backup\n      </td>\n      <td>\n        Yes<br><small>Backups maintained<br>seconds behind production cluster</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump against hidden replica set member</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Queryable backups\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automated &amp; consistent snapshots of sharded clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Not Applicable<br><small>No support for auto-sharding</small>\n      </td>\n      <td>\n        No<br><small>Requires manually coordinating the recovery of mongodumps across shards</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Access control &amp; IP whitelisting\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        AWS VPC Peering\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Beta Release<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Additional Charge</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data in-flight\n      </td>\n      <td>\n        Yes<br><small>TLS/SSL as standard</small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data at-rest\n      </td>\n      <td>\n        Yes<br><small>Available for AWS deployments; always on with Azure and GCP</small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Available only with specific pricing plans and data centers</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        LDAP Integration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database-level auditing<br><small>Track DDL, DML, DCL operations</small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Bring your own KMS\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Realistically there are probably only a number of items that stand out on the comparison list when we go strictly database-to-database. Freedom over <strong>instance configuration </strong>sounds great, but in practice is more similar to putting a cap on how much MongoDB decides to charge you that month (by the way, it's usually a lot; keep this mind). Having the <strong>Latest Version</strong> seems great, but this can just as easily mean breaking production unannounced as much as it means new features.</p><p>MongoDB clearly wins over the enterprise space with <strong>Continuous &amp; queryable backups</strong>, integration with <strong>LDAP, </strong>and <strong>automatic sharding support. </strong>Truthfully if this were merely a database-level feature and cost comparison, the decision to go with<strong> MongoDB Atlas</strong> would come down to how much you like their pretty desktop interface:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/compass.gif\" class=\"kg-image\"><figcaption>A perfectly legitimate reason to pay up, imho.</figcaption></figure><p>So let's say MongoDB Atlas is marginally better than a competitor in the confined realm of \"being a database.\" Are Stitch microservices enough to justify keeping your instance with the MongoDB team?</p><h2 id=\"service-by-service-breakdown-of-stitch\">Service-by-Service Breakdown of Stitch</h2><p>Stitch is kind of like if AWS exited in an alternative universe, where JSON and JavaScript were earth's only technologies. Thinking back to how we create APIs in AWS, the status quo almost always involves spinning up a <strong>Dynamo</strong> (NoSQL) database to put behind <strong>Lambda </strong>functions, accessible by <strong>API Gateway </strong>endpoints. Stitch's core use case revolves around this use-case of <em>end-user-accessing-data</em>, with a number of services dedicated specifically to supporting or improving this flow. The closest comparison to Stitch would be GCloud's <strong>Firebase</strong>. </p><p>So what makes Stitch so special?</p><h3 id=\"service-1-querying-atlas-securely-via-frontend-code\">Service 1: Querying Atlas Securely via Frontend Code</h3><p>Something that cannot be understated is the ability to query Atlas via frontend Javascript. We're not passing API keys, Secrets, or any sort of nonsense; because you're configured things correctly, whitelisted domains can run queries of any complexity <em>without ever interacting with an app's backend.</em> This is not a crazy use case: consider this blog for example, or more so lately, mobile applications:</p><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('&lt;DATABASE&gt;');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; \n    db.collection('&lt;COLLECTION&gt;').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() =&gt; \n    db.collection('&lt;COLLECTION&gt;').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs =&gt; {\n      console.log(&quot;Found docs&quot;, docs)\n      console.log(&quot;[MongoDB Stitch] Connected to Stitch&quot;)\n  }).catch(err =&gt; {\n    console.error(err)\n  });\n&lt;/script&gt;\n</code></pre>\n<p>This isn't to say we're allowing any user to query any data all willy-nilly just because they're on our whitelisted IP: all data stored in Atlas is restricted to specified <strong>Users</strong> by defining <strong>User Roles. </strong>Joe Schmoe can't just inject a query into any presumed database and wreak havoc, because Joe Schmoe can only access data we've permitted his user account to view or write to. What is this \"user account\" you ask? This brings us to the next big feature...</p><h3 id=\"service-2-end-user-account-creation-management\">Service 2: End-User Account Creation &amp; Management</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-5.59.03-PM.png\" class=\"kg-image\"><figcaption>Stitch will handle user account creation for you without the boilerplate.</figcaption></figure><p>Creating an app with user accounts is a huge pain in the ass. Cheeky phrases like '<strong>Do the OAuth Dance'</strong> can't ever hope to minimize the agonizing repetitive pain of creating user accounts or managing relationships between users and data (can <em>user X</em> see a comment from <em>user Y</em>?). Stitch allows most of the intolerably benign logic behind these features to be handled via a UI.</p><p>It would be a far cry to say these processes have been \"trivialized\", but the time saved is perhaps just enough to keep a coding hobbyist interested in their side projects as opposed to giving up and playing Rocket League.</p><p>As far as the permissions to read comments go... well, here's a self-explanatory screenshot of how Stitch handles read/write document permission in its simplest form:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-6.09.25-PM.png\" class=\"kg-image\"><figcaption>Owners of comments can write their comments. Everybody else reads. Seems simple.</figcaption></figure><h3 id=\"service-3-serverless-functions\">Service 3: Serverless Functions</h3><p>Stitch functions are akin to AWS Lambda functions, but much easier to configure for cross-service integration (and also limited to JavaScript ECMA 2015 or something). Functions benefit from the previous two features, in that they too can be triggered from a whitelisted app's frontend, and are governed by a simple \"rules\" system, eliminating the need for security group configurations etc.</p><p>This is what calling a function from an app's frontend looks like:</p><pre><code class=\"language-javascript\">&lt;script&gt;\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n     client.callFunction(&quot;numCards&quot;, [&quot;In Progress&quot;]).then(results =&gt; {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n&lt;/script&gt;\n</code></pre>\n<p>Functions can run any query against Atlas, retrieve <em>values</em> (such as environment variables), and even call other functions. Functions can also be fired by database <strong>triggers,</strong> where a change to a collection will prompt an action such as an alert.</p><h3 id=\"service-4-http-webhooks\">Service 4: HTTP Webhooks</h3><p>Webhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to one another in that they are one-off URLs to perform single tasks. We could never build a well-designed API using Stitch Webhooks, as we could with <strong>API Gateway</strong>; this simply isn't the niche MongoDB is trying to hit (the opposite, in fact). </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-4.33.27-PM.png\" class=\"kg-image\"><figcaption>Configuration for a single Webhook.</figcaption></figure><p>This form with a mere 6 fields clearly illustrates what Stitch intends to do: trivializing the creation of traditionally non-trivial features.</p><h3 id=\"service-5-storing-values-in-stitch\">Service 5: Storing 'Values' in Stitch</h3><p>A \"value\" is equivalent to an environment variable. These can be used to store API keys, secrets, or whatever. Of course, values are retrieved via functions.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-7.45.28-PM.png\" class=\"kg-image\"><figcaption>Shhh, it's a secret ;)</figcaption></figure><h3 id=\"service-6-a-bunch-of-mostly-bloated-extras\">Service 6+: A Bunch of Mostly Bloated Extras</h3><p>Finally, Stitch has thrown in a few third-party integrations for good measure. Some integrations like <strong>S3 Integration </strong>could definitely come in handy, but it's worth asking why Mongo constantly over advertises their integrations with <strong>Github</strong> and <strong>Twilio</strong>. We've already established that we can create endpoints which accept information, and we can make functions which <em>GET</em> information... so isn't anything with an API pretty easy to 'integrate' with?</p><p>This isn't to say the extra services aren't useful, they just seem a bit... odd. It feels a lot like bloating the catalog, but the catalog isn't nearly bloated enough where it feels normal (like Heroku add-ons, for example). The choice to launch Stitch with a handful of barely-useful integrations only comes off as more and more aimless as time passes; as months turn to years and no additions or updates are made to service offerings, it's worth questioning what the vision had been for the product in the first place. In my experience, feature sets like these happen when Product Managers are more powerful than they are useful.</p><h2 id=\"the-breathtaking-climax-is-stitch-worth-it\">The Breathtaking Climax: Is Stitch Worth It?</h2><p>I've been utilizing Stitch to fill in the blanks in development for months now, perhaps nearly a year. Each time I find myself working with Stitch or looking at the bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive toy with an infuriating lack of accurate documentation.</p><p> Stitch is very much a <em>copy-and-paste-cookie-cutter-code</em> type of product, which begs the question of why their tutorials are recklessly outdated; sometimes to the point where MongoDB's own tutorial source code <em>doesn't work. </em>There are so many use cases and potential benefits to Stitch, so why is the <a href=\"https://github.com/mongodb/stitch-examples\">Github repo</a> containing example code snippets so unmaintained, and painfully irrelevant? Lastly, why am I selling this product harder than their own internal team?</p><p>Stitch is a good product with a lot of unfortunate oversight. That said, Google Firebase still doesn't even have an \"import data\" feature, so I suppose it's time to dig deep into this vendor lock and write a 5-post series about it before Silicon Valley's best and brightest get their shit together enough to actually create something useful and intuitive for other human beings to use. In the meantime, feel free to steal source from tutorials I'll be posting, because they'll be sure to, you know, actually work.</p>","url":"https://hackersandslackers.com/mongodb-cloud-backend-as-a-service-with-atlas-and-stitch/","uuid":"5555fa6e-07f0-4f9a-8069-e1e68868e608","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5beb3c900dbec217f3ce801b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867371a","title":"Compile Frontend JavaScript with Babel and Gulp","slug":"babel-ecma-script","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","excerpt":"Using ECMAScript functions on the frontend the quick and dirty way.","custom_excerpt":"Using ECMAScript functions on the frontend the quick and dirty way.","created_at_pretty":"04 October, 2018","published_at_pretty":"06 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-03T23:05:08.000-04:00","published_at":"2018-10-06T06:20:00.000-04:00","updated_at":"2019-02-02T04:55:50.000-05:00","meta_title":"Frontend JavaScript with Babel | Hackers and Slackers","meta_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","og_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","og_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","og_title":"Compile Frontend JavaScript with Babel and Gulp","twitter_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","twitter_title":"Compile Frontend JavaScript with Babel and Gulp","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},"tags":[{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"NOTE: This post has not aged particularly well. This is made evident by the\npresence of the word \"Gulp\" present throughout the post.\n\n\n--------------------------------------------------------------------------------\n\nAs you may have already deduced from the occasional glaring holes in my\nprogramming knowledge, I haven’t spent as much of my life programming than,\nwell, anybody who haphazardly threw away an 8-year product management career.\nApparently, a lot can happen in 8 years... especially when it comes to\nJavaScript.\nPutting aside that whole server-side  NodeJS thing or whatever it’s called,\nthere’s years worth of mindfucks for those of us who fairly recently returned to\ndev. In our absence, the world has been Grunting, Gulping, Yarning and even\nNPMing. Let’s assume we’ve gotten past all of that... phew. Oh wait, I forgot to\nmention the monstrosity of “hey by way there’s this mess of a hardly-explained\n(protocol? organization? Fancy Github Repo?) called Babel that we all depend on\nto do meaningful shit in the browser these days.”\n\nIf you’re here because you’re trying to figure what the hell all these ‘const’\nand ‘import’ keywords are doing in modern-day browser JavaScript, consider this\nhalf focus-group and half getting-started tutorial. For the rest of you\nexperienced JS devs out there, perhaps you should stay. No, by all means, I’d\nlike you to sit here and think about the kind of ecosystem you’ve decided to\npatch together. Do it; otherwise, I might just give Kik a call to see what other\ntiny NPM modules they’d like to arbitrarily demand the removal of... remember\nthat? Exactly. Now think about what you’ve done.\n\nECMAScript and the Backwards-Compatible Web\nAny Actionscript veterans in the house? No? Just me? Great. Anyway, There’s\nsomething that ActionScript, JavaScript, and a bunch of Whatever-scripts have in\ncommon: they’re all implementations of ECMAScript: the ancestral equivalent of\ncomparing Latin to Spanish or French, but just one added detail: ECMAScript is\nstill a living language, and constantly improving. If some nation of cool cats\nstill spoke Latin as an active language and were constantly coming up with sick\nnew slang terms, those speaking French or Spanish would be powerless to come up\nwith phrases nearly as cool as “lit” or \"fam\"  (or whatever those Latin\nequivalents would be).\n\nThe problem is web browsers are inherently backward-compatible monstrosities.\nMost revenue-generating companies look at their >5% user base using Internet\nExplorer three and a half, and think “oh my god, that’s five percent of our\npotential revenue” (it isn’t, but whatever). That’s where we stand.\n\nNew features? For the INTERNET?\nI remember one particular weekend I rented a hotel room for the sole purpose of\nwrapping up an app based on MongoDB Stitch at the time. I was pretty confident I\nhad it all figured out, but found, something about Mongo’s quick start guides\nfelt off (I mean, more off than the rest of their nearly unreadable docs). Why\ndid Mongo insist on inline JavaScript? Haven’t we evolved past the days where\nPHP, HTML, CSS, and JS all lived on the same page?\n\nAs everybody-who-wasn’t-me at time already knows, JavaScript has been reaping\nthe benefits of new evolutions in ECMAScript over the years, but there’s a\ncatch: only the most modern browsers know what the hell to do with these new\nfeatures (duh), and they only do so with the dreaded  blocks thrown on HTML\npages. Attempting to use modern ECMA features in a linked JS file which hasn’t\nbeen pre-compiled is like dividing by zero. Seriously, you might die.\n\nBabel to the Rescue\nMore research only lead to more questions. Who is this Browserify wizard\n[http://browserify.org/]  and why is he dead? What is Webpack doing to fund all\ntheir highly produced branding, and why would anybody purchase a Webpack T-shirt\n[https://webpack.threadless.com/]? Finally, the biggest question of all: who the\nhell is this Babel guy who gives off as an electric explosion in your face on\nhard drugs?\n\nI can't even the difference between a compiler and a Marvel franchise anymore.I\nthink it was at this point in my journey of catching up on the last decade where\nI went truly mad and began pacing around my room, verbally assaulting inanimate\nobjects, and eventually even found myself washed up in a Wendys down the street,\nshivering and crying to myself. \n\nNow that I've been released from the psyche ward and are free to wander the\nstreets again, I've managed to piece together a bare-minimum understanding of\nwhat the hell is going on.\n\nWhat the Hell is Going On\nBecause Javascript as a language is destined to be backwards compatible to a\nyear when OJ Simpson was on trial and Windows 95 was considered cutting-edge\ntechnology, Babel  aims to \"improve\" browser Javascript. Because we can't\nactually improve or change the underlying technology, Babel looks at new-fangled\nECMA syntax (containing words like require,const  or promise) and compiles logic\nequivalent to these features with vanilla Javascript. If that sounds similar to\nhow Gulp  takes CSS preprocessors (like LESS  or SASS) and turns those files\ninto interpretable browser code, well, that's exactly what happens. \n\nGulp  is not be the only way to utilize new ECMAScript features. Babel syntax\ncan be compiled a number of ways, such as Babel's CLI\n[https://babeljs.io/docs/en/babel-cli], or from Server-side JS, or Webpack  if\nyou're Mister-Fancy-Pants. In fact, the trend across the board is that compiling\nyour site with modules via Webpack is winning dramatically. For all we know,\nthis could be the last Babel Gulp tutorial ever written.\n\nRight now we're focusing on the ability to use ECMAScript features on the\nfrontend; a common use-case for things like... well... theming the presentation\nlayer for a blog. If we can pull it off with Gulp, you probably won't struggle\nmuch with future methods.\n\nBabel NPM Packages\nAlright, so Babel isn't just a single NPM package. It's a lot. Let's see what's\nthere in a basic package.json:\n\n{\n  \"name\": \"example\",\n  \"scripts\": {\n    \"build\": \"babel src -d lib\"\n  },\n  \"presets\": [\n    \"env\"\n  ],\n  \"engines\": {\n    \"node\": \"8.12.0\"\n  },\n  \"dependencies\": {\n    \"@babel/polyfill\": \"^7.0.0\",\n    \"@babel/runtime\": \"^7.1.2\",\n    \"@babel/runtime-corejs2\": \"^7.1.2\",\n    \"babel-runtime\": \"^6.26.0\",\n    \"gulp-resolve-dependencies\": \"^2.2.0\",\n    \"gulp-sourcemaps\": \"^2.6.4\",\n    \"gulp-uglify-es\": \"^1.0.4\",\n  },\n  \"devDependencies\": {\n    \"@babel/cli\": \"^7.1.2\",\n    \"@babel/core\": \"^7.1.2\",\n    \"@babel/plugin-syntax-dynamic-import\": \"^7.0.0\",\n    \"@babel/plugin-transform-runtime\": \"^7.1.0\",\n    \"@babel/preset-env\": \"^7.1.0\",\n  }\n}\n\nLet's not go through all these in detail- in fact, let's take a look at our \ngulpfile.js:\n\n'use strict';\n\nvar gulp = require('gulp'),\n  concat = require('gulp-concat'),\n  autoprefixer = require('gulp-autoprefixer'),\n  sourcemaps = require('gulp-sourcemaps'),\n  babel = require('gulp-babel'),\n  resolveDependencies = require('gulp-resolve-dependencies'),\n\n\nvar paths = {\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  }\n};\n\nfunction scripts() {\n  return gulp.src(paths.scripts.src)\n    .pipe(babel({\n          presets: ['@babel/env'],\n          plugins: ['@babel/transform-runtime', '@babel/plugin-syntax-dynamic-import']\n    }))\n    .on('error', console.error.bind(console))\n    .pipe(resolveDependencies({\n            pattern: /\\* @requires [\\s-]*(.*\\.js)/g\n        }))\n    .pipe(concat('main.min.js'))\n    .pipe(gulp.dest(paths.scripts.dest));\n}\n\nNote that I've gone ahead and removed anything from the file that has nothing to\ndo with Babel - everything seen here is strictly relevant to building a\n'scripts' task. The gist is this:\n\nbabel  accepts two arrays:\n\n * Presets  refers to the version of ECMAScript we're targeting, and what we\n   have implies the latest.\n * Plugins  allow us to pass any plugins Babel supports; in this case, we're\n   rendering stuff at runtime. Let's not get into it too far.\n\nThe rest of the function is simple: we catch errors if any arise, we resolve\nrequired dependencies, and then we finally bunch everything into a single file\nand shove it in our destination folder.\n\nJust to be Clear\nWhen it comes to the best  way of achieving ECMAScript 2015 or 2016 etc in the\nbrowser,  I'm no expert. As previously mentioned, the understanding I've passed\non to you is a sort of bare minimum to start doing great things. \n\nAs far as what others are doing, I'd be willing to bet that the vast majority of\ndevs utilizing newer ECMAScript functions are running frameworks like React,\nwhile the rest have reached a consensus that Webpack is king and Gulp lame.\nThese things may be true.\n\nNo matter, this new found power of ours opens a lot of doors. Just wait until\nyou see what we do with MongoDB Stitch.","html":"<p><strong>NOTE</strong>: This post has not aged particularly well. This is made evident by the presence of the word \"Gulp\" present throughout the post.</p><hr><p>As you may have already deduced from the occasional glaring holes in my programming knowledge, I haven’t spent as much of my life programming than, well, anybody who haphazardly threw away an 8-year product management career. Apparently, a lot can happen in 8 years... especially when it comes to JavaScript.<br>Putting aside that whole server-side  NodeJS thing or whatever it’s called, there’s years worth of mindfucks for those of us who fairly recently returned to dev. In our absence, the world has been Grunting, Gulping, Yarning and even NPMing. Let’s assume we’ve gotten past all of that... phew. Oh wait, I forgot to mention the monstrosity of “hey by way there’s this mess of a hardly-explained (protocol? organization? Fancy Github Repo?) called Babel that we all depend on to do meaningful shit in the browser these days.”</p><p>If you’re here because you’re trying to figure what the hell all these ‘const’ and ‘import’ keywords are doing in modern-day browser JavaScript, consider this half focus-group and half getting-started tutorial. For the rest of you experienced JS devs out there, perhaps you should stay. No, by all means, I’d like you to sit here and think about the kind of ecosystem you’ve decided to patch together. Do it; otherwise, I might just give Kik a call to see what other tiny NPM modules they’d like to arbitrarily demand the removal of... remember that? Exactly. Now think about what you’ve done.</p><h2 id=\"ecmascript-and-the-backwards-compatible-web\">ECMAScript and the Backwards-Compatible Web</h2><p>Any Actionscript veterans in the house? No? Just me? Great. Anyway, There’s something that ActionScript, JavaScript, and a bunch of Whatever-scripts have in common: they’re all implementations of ECMAScript: the ancestral equivalent of comparing Latin to Spanish or French, but just one added detail: ECMAScript is still a living language, and constantly improving. If some nation of cool cats still spoke Latin as an active language and were constantly coming up with sick new slang terms, those speaking French or Spanish would be powerless to come up with phrases nearly as cool as “lit” or \"fam\"  (or whatever those Latin equivalents would be).</p><p>The problem is web browsers are inherently backward-compatible monstrosities. Most revenue-generating companies look at their &gt;5% user base using Internet Explorer three and a half, and think “oh my god, that’s five percent of our potential revenue” (it isn’t, but whatever). That’s where we stand.</p><h2 id=\"new-features-for-the-internet\">New features? For the INTERNET?</h2><p>I remember one particular weekend I rented a hotel room for the sole purpose of wrapping up an app based on MongoDB Stitch at the time. I was pretty confident I had it all figured out, but found, something about Mongo’s quick start guides felt off (I mean, more off than the rest of their nearly unreadable docs). Why did Mongo insist on inline JavaScript? Haven’t we evolved past the days where PHP, HTML, CSS, and JS all lived on the same page?</p><p>As everybody-who-wasn’t-me at time already knows, JavaScript has been reaping the benefits of new evolutions in ECMAScript over the years, but there’s a catch: only the most modern browsers know what the hell to do with these new features (duh), and they only do so with the dreaded  blocks thrown on HTML pages. Attempting to use modern ECMA features in a linked JS file which hasn’t been pre-compiled is like dividing by zero. Seriously, you might die.</p><h2 id=\"babel-to-the-rescue\">Babel to the Rescue</h2><p>More research only lead to more questions. Who is this <a href=\"http://browserify.org/\">Browserify wizard</a> and why is he dead? What is Webpack doing to fund all their highly produced branding, and why would anybody purchase a <a href=\"https://webpack.threadless.com/\">Webpack T-shirt</a>? Finally, the biggest question of all: who the hell is this Babel guy who gives off as an electric explosion in your face on hard drugs?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/babel2-1.jpg\" class=\"kg-image\"><figcaption><em>I can't even the difference between a compiler and a Marvel franchise anymore.&nbsp;</em></figcaption></figure><p>I think it was at this point in my journey of catching up on the last decade where I went truly mad and began pacing around my room, verbally assaulting inanimate objects, and eventually even found myself washed up in a Wendys down the street, shivering and crying to myself. </p><p>Now that I've been released from the psyche ward and are free to wander the streets again, I've managed to piece together a bare-minimum understanding of what the hell is going on.</p><h3 id=\"what-the-hell-is-going-on\">What the Hell is Going On</h3><p>Because Javascript as a language is destined to be backwards compatible to a year when OJ Simpson was on trial and Windows 95 was considered cutting-edge technology, <strong>Babel</strong> aims to \"improve\" browser Javascript. Because we can't actually improve or change the underlying technology, Babel looks at new-fangled ECMA syntax (containing words like <code>require</code>,  <code>const</code> or <code>promise</code>) and compiles logic equivalent to these features with vanilla Javascript. If that sounds similar to how <strong>Gulp</strong> takes CSS preprocessors (like <em>LESS</em> or <em>SASS</em>) and turns those files into interpretable browser code, well, that's exactly what happens. </p><p><strong>Gulp</strong> is not be the only way to utilize new ECMAScript features. Babel syntax can be compiled a number of ways, such as <a href=\"https://babeljs.io/docs/en/babel-cli\">Babel's CLI</a>, or from Server-side JS, or <strong>Webpack</strong> if you're Mister-Fancy-Pants. In fact, the trend across the board is that compiling your site with modules via Webpack is winning dramatically. For all we know, this could be the last Babel Gulp tutorial ever written.</p><p>Right now we're focusing on the ability to use ECMAScript features on the frontend; a common use-case for things like... well... theming the presentation layer for a blog. If we can pull it off with Gulp, you probably won't struggle much with future methods.</p><h2 id=\"babel-npm-packages\">Babel NPM Packages</h2><p>Alright, so Babel isn't just a single NPM package. It's a lot. Let's see what's there in a basic <code>package.json</code>:</p><pre><code>{\n  \"name\": \"example\",\n  \"scripts\": {\n    \"build\": \"babel src -d lib\"\n  },\n  \"presets\": [\n    \"env\"\n  ],\n  \"engines\": {\n    \"node\": \"8.12.0\"\n  },\n  \"dependencies\": {\n    \"@babel/polyfill\": \"^7.0.0\",\n    \"@babel/runtime\": \"^7.1.2\",\n    \"@babel/runtime-corejs2\": \"^7.1.2\",\n    \"babel-runtime\": \"^6.26.0\",\n    \"gulp-resolve-dependencies\": \"^2.2.0\",\n    \"gulp-sourcemaps\": \"^2.6.4\",\n    \"gulp-uglify-es\": \"^1.0.4\",\n  },\n  \"devDependencies\": {\n    \"@babel/cli\": \"^7.1.2\",\n    \"@babel/core\": \"^7.1.2\",\n    \"@babel/plugin-syntax-dynamic-import\": \"^7.0.0\",\n    \"@babel/plugin-transform-runtime\": \"^7.1.0\",\n    \"@babel/preset-env\": \"^7.1.0\",\n  }\n}</code></pre><p>Let's not go through all these in detail- in fact, let's take a look at our <strong>gulpfile.js</strong>:</p><pre><code>'use strict';\n\nvar gulp = require('gulp'),\n  concat = require('gulp-concat'),\n  autoprefixer = require('gulp-autoprefixer'),\n  sourcemaps = require('gulp-sourcemaps'),\n  babel = require('gulp-babel'),\n  resolveDependencies = require('gulp-resolve-dependencies'),\n\n\nvar paths = {\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  }\n};\n\nfunction scripts() {\n  return gulp.src(paths.scripts.src)\n    .pipe(babel({\n          presets: ['@babel/env'],\n          plugins: ['@babel/transform-runtime', '@babel/plugin-syntax-dynamic-import']\n    }))\n    .on('error', console.error.bind(console))\n    .pipe(resolveDependencies({\n            pattern: /\\* @requires [\\s-]*(.*\\.js)/g\n        }))\n    .pipe(concat('main.min.js'))\n    .pipe(gulp.dest(paths.scripts.dest));\n}</code></pre><p>Note that I've gone ahead and removed anything from the file that has nothing to do with Babel - everything seen here is strictly relevant to building a 'scripts' task. The gist is this:</p><p><strong>babel</strong> accepts two arrays:</p><ul><li><em>Presets</em> refers to the version of ECMAScript we're targeting, and what we have implies the latest.</li><li><em>Plugins</em> allow us to pass any plugins Babel supports; in this case, we're rendering stuff at runtime. Let's not get into it too far.</li></ul><p>The rest of the function is simple: we catch errors if any arise, we resolve required dependencies, and then we finally bunch everything into a single file and shove it in our destination folder.</p><h2 id=\"just-to-be-clear\">Just to be Clear</h2><p>When it comes to the <em>best</em> way of achieving ECMAScript 2015 or 2016 etc in the browser,  I'm no expert. As previously mentioned, the understanding I've passed on to you is a sort of bare minimum to start doing great things. </p><p>As far as what others are doing, I'd be willing to bet that the vast majority of devs utilizing newer ECMAScript functions are running frameworks like React, while the rest have reached a consensus that Webpack is king and Gulp lame. These things may be true.</p><p>No matter, this new found power of ours opens a lot of doors. Just wait until you see what we do with <strong>MongoDB Stitch</strong>.</p><p></p><p></p>","url":"https://hackersandslackers.com/babel-ecma-script/","uuid":"04fe6342-9462-4d3c-9e6d-bc000e17824a","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bb583642361b479aa119366"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867368d","title":"Upgrading to Gulp 4.0.0","slug":"upgrading-to-gulp-4","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/gulp2@2x.jpg","excerpt":"Upgrading to Gulp 4 and tackling the breaking changes that come with it.","custom_excerpt":"Upgrading to Gulp 4 and tackling the breaking changes that come with it.","created_at_pretty":"27 June, 2018","published_at_pretty":"28 June, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-06-27T11:06:39.000-04:00","published_at":"2018-06-28T07:30:00.000-04:00","updated_at":"2019-04-09T21:11:52.000-04:00","meta_title":"Upgrading to Gulp 4.0.0 | Hackers and Slackers","meta_description":"Upgrading to Gulp 4 and tackling the breaking changes that come with it.","og_description":"Upgrading to Gulp 4 and tackling the breaking changes that come with it.","og_image":"https://hackersandslackers.com/content/images/2018/06/gulp2@2x.jpg","og_title":"Upgrading to Gulp 4.0.0","twitter_description":"Upgrading to Gulp 4 and tackling the breaking changes that come with it.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/gulp2@2x.jpg","twitter_title":"Upgrading to Gulp 4.0.0","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in the day we touched on some of the wonderful upsides of implementing Gulp\n[https://hackersandslackers.com/using-gulp-to-make-frontend-tolerable/]  into\nyour workflow. If you have been following along and happened to install the\nlatest version of Gulp, you may has noticed something horrible happen: nothing\nworked. I probably should’ve mentioned that the latest major update to Gulp,\nGulp 4, is actually a breaking update. That's my bad.\n\nStrangely, documentation around upgrading from Gulp 3.X to Gulp 4 seems to be\nlagging way behind this release, with the closest thing to official\ndocumentation being some article a guy posted on Medium a while back.\nInteresting strategy, Gulp team.\n\nWhat's the Big Deal?\nThere are a couple architectural differences with Gulp 4, for the better.\nPreviously Gulp would allows us to hold off on executing tasks until dependent\ntask were completed. If task C were dependent on the completion of tasks both A\nand B, we would represent this as such:\n\ngulp.task('default', ['A', 'B'], function() {etc});\n\ngulp.task('A', ['C'], function() {etc});\ngulp.task('B', ['C'], function() {etc});\n\ngulp.task('C', function() {etc});\n\n\nWhile this was an effective way of handling such a workflow, Gulp has made the\nprocess a bit cleaner and easier to digest with the additions of gulp.parallel \nand gulp.series.\n\nParallel  denotes tasks which should be executed in parallel, aka the same time.\n\nSeries  defines a linear order of how tasks should be executed. Parallels can\nexist inside of series, effectively resulting in a fork of tasks before moving\nto the next task in the series:\n\ngulp.task('A', function() {etc});\ngulp.task('B', function() {etc});\n\ngulp.task('C', function() {etc});\n\ngulp.task('default', gulp.series('C', gulp.parallel('A', 'B'), function() {etc}));\n\n\nIn this case, Task C forks into Tasks A and B before converging on their merry\nway to the rest of the series.\n\nQuick Install\nTo get your hands on this, first uninstall your current versions of Gulp and\nGulp CLI:\n\nnpm uninstall gulp --save-dev\nnpm uninstall gulp -g\n\n\nThen we can go ahead and reinstall Gulp as normal. Feel free to force the\nversion:\n\nnpm install gulp-cli -g\nnpm install gulp@4.0.0 -D\n\n\nExample Gulp File\nEnough with all the jargon, we both know what you came here for. let me just\npost what I see to be the bare minimum Gulpfile for your copy and pasting\npleasure:\n\nvar gulp \t= require('gulp'),\n  \tless \t= require('gulp-less'),\n  \tconcat \t= require('gulp-concat'),\n  \tuglify \t= require('gulp-uglify'),\n  \trename \t= require('gulp-rename'),\n    handlebars = require('gulp-handlebars'),\n    declare = require('gulp-declare'),\n    cleanCSS = require('gulp-clean-css');\n\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  }\n};\n\nfunction styles() {\n  return gulp\n  \t.src(paths.styles.src, {\n      sourcemaps: true\n    })\n\t.pipe(less())\n\t.pipe(rename({\n\t  basename: 'main',\n\t  suffix: '.min'\n\t}))\n.pipe(cleanCSS({debug: true}))\n.pipe(concat('main.min.css'))\n.pipe(gulp.dest(paths.styles.dest));\n}\n\nfunction scripts() {\n  return gulp\n\t.src(paths.scripts.src, {\n\t\tsourcemaps: true\n\t})\n\t.pipe(uglify())\n\t.pipe(concat('main.min.js'))\n\t.pipe(gulp.dest(paths.scripts.dest));\n}\n\nfunction templates(){\n  gulp.src('views/*.hbs')\n    .pipe(handlebars())\n    //.pipe(wrap('Handlebars.template(<%= contents %>)'))\n    .pipe(declare({\n      namespace: 'MyApp.templates',\n      noRedeclare: true, // Avoid duplicate declarations\n    }))\n    .pipe(concat('templates.js'))\n    .pipe(gulp.dest('assets/js/'));\n}\n\nfunction watch() {\n  gulp.watch(paths.scripts.src, scripts);\n  gulp.watch(paths.styles.src, styles);\n}\n\nvar build = gulp.parallel(styles, scripts, templates, watch);\n\ngulp.task(build);\ngulp.task('default', build);\n\n\nNothing crazy here: just the typical concat and minification of source files.\n\nHopefully this helps somebody wondering why their latest Gulp installation broke\ntheir Gulpfile. Perhaps next time we'll dig down deep into the shadows of Gulp\nplugins to reveal secret elite legendary Gulp hacks for 1337 hax0rs only. Or\nnot, we could do whatever honestly.","html":"<p>Back in the day we touched on some of the wonderful upsides of <a href=\"https://hackersandslackers.com/using-gulp-to-make-frontend-tolerable/\">implementing Gulp</a> into your workflow. If you have been following along and happened to install the latest version of Gulp, you may has noticed something horrible happen: <em>nothing worked</em>. I probably should’ve mentioned that the latest major update to Gulp, Gulp 4, is actually a breaking update. That's my bad.</p><p>Strangely, documentation around upgrading from Gulp 3.X to Gulp 4 seems to be lagging way behind this release, with the closest thing to official documentation being some article a guy posted on Medium a while back. Interesting strategy, Gulp team.</p><h2 id=\"what-s-the-big-deal\">What's the Big Deal?</h2><p>There are a couple architectural differences with Gulp 4, for the better. Previously Gulp would allows us to hold off on executing tasks until dependent task were completed. If task C were dependent on the completion of tasks both A and B, we would represent this as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">gulp.task('default', ['A', 'B'], function() {etc});\n\ngulp.task('A', ['C'], function() {etc});\ngulp.task('B', ['C'], function() {etc});\n\ngulp.task('C', function() {etc});\n</code></pre>\n<!--kg-card-end: markdown--><p>While this was an effective way of handling such a workflow, Gulp has made the process a bit cleaner and easier to digest with the additions of <strong>gulp.parallel</strong> and <strong>gulp.series</strong>.</p><p><strong>Parallel</strong> denotes tasks which should be executed in parallel, aka the same time.</p><p><strong>Series</strong> defines a linear order of how tasks should be executed. Parallels can exist inside of series, effectively resulting in a fork of tasks before moving to the next task in the series:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">gulp.task('A', function() {etc});\ngulp.task('B', function() {etc});\n\ngulp.task('C', function() {etc});\n\ngulp.task('default', gulp.series('C', gulp.parallel('A', 'B'), function() {etc}));\n</code></pre>\n<!--kg-card-end: markdown--><p>In this case, Task C forks into Tasks A and B before converging on their merry way to the rest of the series.</p><h2 id=\"quick-install\">Quick Install</h2><p>To get your hands on this, first uninstall your current versions of Gulp and Gulp CLI:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">npm uninstall gulp --save-dev\nnpm uninstall gulp -g\n</code></pre>\n<!--kg-card-end: markdown--><p>Then we can go ahead and reinstall Gulp as normal. Feel free to force the version:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">npm install gulp-cli -g\nnpm install gulp@4.0.0 -D\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"example-gulp-file\">Example Gulp File</h2><p>Enough with all the jargon, we both know what you came here for. let me just post what I see to be the bare minimum Gulpfile for your copy and pasting pleasure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var gulp \t= require('gulp'),\n  \tless \t= require('gulp-less'),\n  \tconcat \t= require('gulp-concat'),\n  \tuglify \t= require('gulp-uglify'),\n  \trename \t= require('gulp-rename'),\n    handlebars = require('gulp-handlebars'),\n    declare = require('gulp-declare'),\n    cleanCSS = require('gulp-clean-css');\n\nvar paths = {\n  styles: {\n    src: 'src/less/*.less',\n    dest: 'assets/css'\n  },\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  },\n  html: {\n    src: 'views/*.hbs',\n    dest: 'assets/'\n  }\n};\n\nfunction styles() {\n  return gulp\n  \t.src(paths.styles.src, {\n      sourcemaps: true\n    })\n\t.pipe(less())\n\t.pipe(rename({\n\t  basename: 'main',\n\t  suffix: '.min'\n\t}))\n.pipe(cleanCSS({debug: true}))\n.pipe(concat('main.min.css'))\n.pipe(gulp.dest(paths.styles.dest));\n}\n\nfunction scripts() {\n  return gulp\n\t.src(paths.scripts.src, {\n\t\tsourcemaps: true\n\t})\n\t.pipe(uglify())\n\t.pipe(concat('main.min.js'))\n\t.pipe(gulp.dest(paths.scripts.dest));\n}\n\nfunction templates(){\n  gulp.src('views/*.hbs')\n    .pipe(handlebars())\n    //.pipe(wrap('Handlebars.template(&lt;%= contents %&gt;)'))\n    .pipe(declare({\n      namespace: 'MyApp.templates',\n      noRedeclare: true, // Avoid duplicate declarations\n    }))\n    .pipe(concat('templates.js'))\n    .pipe(gulp.dest('assets/js/'));\n}\n\nfunction watch() {\n  gulp.watch(paths.scripts.src, scripts);\n  gulp.watch(paths.styles.src, styles);\n}\n\nvar build = gulp.parallel(styles, scripts, templates, watch);\n\ngulp.task(build);\ngulp.task('default', build);\n</code></pre>\n<!--kg-card-end: markdown--><p>Nothing crazy here: just the typical concat and minification of source files.</p><p>Hopefully this helps somebody wondering why their latest Gulp installation broke their Gulpfile. Perhaps next time we'll dig down deep into the shadows of Gulp plugins to reveal secret elite legendary Gulp hacks for 1337 hax0rs only. Or not, we could do whatever honestly.</p>","url":"https://hackersandslackers.com/upgrading-to-gulp-4/","uuid":"deb7ac06-d159-4c66-8d8a-11442ffbb395","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b33a7ff1e2df4575e4c101c"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867368c","title":"Building Page Templates in ExpressJS With Handlebars","slug":"handlebars-templating-in-expressjs","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/handlebars.jpg","excerpt":"Building views in NodeJS by incorporating layouts, partials, and everything in between.","custom_excerpt":"Building views in NodeJS by incorporating layouts, partials, and everything in between.","created_at_pretty":"25 June, 2018","published_at_pretty":"26 June, 2018","updated_at_pretty":"05 March, 2019","created_at":"2018-06-25T16:39:57.000-04:00","published_at":"2018-06-26T18:52:57.000-04:00","updated_at":"2019-03-04T21:54:13.000-05:00","meta_title":"Handlebars Templating in ExpressJS | Hackers and Slackers","meta_description":"Building views in NodeJS by incorporating layouts, partials, and everything in between.","og_description":"Building views in NodeJS by incorporating layouts, partials, and everything in between.","og_image":"https://hackersandslackers.com/content/images/2019/03/handlebars.jpg","og_title":"Handlebars Templating in ExpressJS","twitter_description":"Building views in NodeJS by incorporating layouts, partials, and everything in between.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/handlebars.jpg","twitter_title":"Handlebars Templating in ExpressJS","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"Writing HTML sucks, thus we should do everything to minimize the time we spend\nwriting it as much as possible.  Thus, we have Handlebars\n[https://handlebarsjs.com/]: a lightweight templating system for Node.\nHandlebars allows us to avoid repetitive code by compiling the final DOM\nstructure of our site via logic, typically compiled by task runners such as\nGrunt or Gulp.\n\nIf you're involved in any sort of Node development, you're probably already\nfamiliar with Handlebars to a degree. I thought I was, but it isn't until we\nneed to start a new project from scratch that we realize that we totally forgot\nthe configuration process we took last time. That's why I'm here.\n\nLet's have a quick refresher on the parts that make up Handlebars\n\n * Layouts  are the most ambiguous high-level layer; these are commonly used to\n   set underlying page metadata as well as general layout (for lack of a better\n   term).\n * Pages  are templates which equate to one type  of page. For example, the\n   'post' page on this site is unique from, say, the homepage. Because all posts\n   share elements with one another, hundreds of posts share this same template.\n * Partials  are snippets which can be shared between pages, such as navigation.\n * A Context  is content which is passed to templates and result in being the\n   page's content\n * Helpers  are the closest we get to logic in Handlebars: these allow us to\n   display or omit content based on conditionals such as if  statements. For\n   example: showing an author's avatar only if they have uploaded an image.\n\nProject Setup\nWe're going to use the Express /views  folder to contain all of our handlebars\ngoodness. Our project should look something like this:\n\nmyapp\n├── bin\n├── build\n├── routes\n├── src\n├── views\n│   ├── layouts/\n│   ├── partials/\n│   └── error.hbs\n│   └── index.hbs\n│   └── login.hbs\n│   └── etc\n└── README.md\n└── app.js\n└── package.json\n\n\nIt's important to distinguish that we've separated our views folder into three\nclassifications for layouts, partials, and pages, where pages occupy the root \n/views  directory. It's important to keep this distinction as our structure\naffects how we serve up these templates.\n\nConfigure that Ish\nInstall handlebars:\n\nnpm install handlebars --save\n\n\nCrack open your app.js  file or whatever it is you call that thing. Require\nhandlebars:\n\nvar hbs = require( 'express-handlebars');\n\n\nNext we'll configure Express to use Handlebars as the view engine, and tell\nExpress where to find these files:\n\n// view engine setup\napp.set('view engine', 'hbs');\n\napp.engine( 'hbs', hbs( {\n  extname: 'hbs',\n  defaultView: 'default',\n  layoutsDir: __dirname + '/views/pages/',\n  partialsDir: __dirname + '/views/partials/'\n}));\n\n\nExpress assumes by default that we're storing our views in the '/views' folder,\nwhich we are. We take this a step further by specifying which subfolders our \npartials  and layouts  are in above. We can save pages  directly in /views.\n\nNotice that we're also setting a default layout. We can override this in our\nroutes if needed, but setting a default layout is useful for loading pages in an\nhtml wrapper container page metadata.\n\nKicks on Route 66\nLet's create our first route in routes/index.js. We're going to load a view\ncalled home  into a layout called default:\n\nvar express = require('express');\nvar router = express.Router();\n\nrouter.get('/', function(req, res, next) {\n  res.render('home', {layout: 'default', template: 'home-template'});\n});\n\n\nThis will render views/home.hbs  into views/layouts/default.hbs, provided are\nviews are set up correctly. We also pass a custom value template  which is\nuser-defined; more on that below.\n\nBasic Usage\nLet's finally take a look at our actual Handlebars views. Here's default.hbs:\n\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"utf-8\" />\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n  <title>Best Website</title>\n  <meta name=\"HandheldFriendly\" content=\"True\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, viewport-fit=cover\" />\n  <link rel=\"stylesheet\" href=\"/css/main.min.css\">\n</head>\n<body class=\"{{template}}\">\n  <div class=\"container\">\n\t  {{{body}}}\n  </div>\n  {{> footer}}\n</body>\n</html>\n\n\nWe have three values here: {{template}}  and {{{body}}}, and {{> footer}}.\n\n{{template}}  is a value with double brackets, thus is expecting linear data. We\npassed template  in our route: this sets the body class to equal home-template \non the chance that we'll want to apply page-specific styles or logic in the\nfuture.\n\n{{{body}}}  is rocking the triple brackets, and is reserved specifically to\nserve this purpose: loading templates into other templates.\n\nLastly we have {{> footer}}. This will load a partial named footer  from \nviews/partials/footer.hbs, provided that we create it. The difference between\nhow {{{body}}}  and {{> footer}}  are being loaded have to do with a general\nworkflow philosophy; pages are the main event and thus are loaded into layouts\nby their command. Partials can be called by pages at will whenever we please.\n\nThere's obviously a lot more to Handlebars- the fun doesn't truly begin until we\npull dynamic values from databases or wherever. We'll get there.","html":"<p>Writing HTML sucks, thus we should do everything to minimize the time we spend writing it as much as possible.  Thus, we have <a href=\"https://handlebarsjs.com/\">Handlebars</a>: a lightweight templating system for Node. Handlebars allows us to avoid repetitive code by compiling the final DOM structure of our site via logic, typically compiled by task runners such as Grunt or Gulp.</p><p>If you're involved in any sort of Node development, you're probably already familiar with Handlebars to a degree. I thought I was, but it isn't until we need to start a new project from scratch that we realize that we totally forgot the configuration process we took last time. That's why I'm here.</p><p>Let's have a quick refresher on the parts that make up Handlebars</p><ul><li><strong>Layouts</strong> are the most ambiguous high-level layer; these are commonly used to set underlying page metadata as well as general layout (for lack of a better term).</li><li><strong>Pages</strong> are templates which equate to one <em>type</em> of page. For example, the 'post' page on this site is unique from, say, the homepage. Because all posts share elements with one another, hundreds of posts share this same template.</li><li><strong>Partials</strong> are snippets which can be shared between pages, such as navigation.</li><li>A <strong>Context</strong> is content which is passed to templates and result in being the page's content</li><li><strong>Helpers</strong> are the closest we get to logic in Handlebars: these allow us to display or omit content based on conditionals such as <em>if</em> statements. For example: showing an author's avatar only if they have uploaded an image.</li></ul><h2 id=\"project-setup\">Project Setup</h2><p>We're going to use the Express <em>/views</em> folder to contain all of our handlebars goodness. Our project should look something like this:</p><pre><code class=\"language-bash\">myapp\n├── bin\n├── build\n├── routes\n├── src\n├── views\n│   ├── layouts/\n│   ├── partials/\n│   └── error.hbs\n│   └── index.hbs\n│   └── login.hbs\n│   └── etc\n└── README.md\n└── app.js\n└── package.json\n</code></pre>\n<p>It's important to distinguish that we've separated our views folder into three classifications for <strong>layouts</strong>, <strong>partials</strong>, and <strong>pages</strong>, where pages occupy the root <code>/views</code> directory. It's important to keep this distinction as our structure affects how we serve up these templates.</p><h2 id=\"configure-that-ish\">Configure that Ish</h2><p>Install handlebars:</p><pre><code class=\"language-bash\">npm install handlebars --save\n</code></pre>\n<p>Crack open your <code>app.js</code> file or whatever it is you call that thing. Require handlebars:</p><pre><code class=\"language-javascript\">var hbs = require( 'express-handlebars');\n</code></pre>\n<p>Next we'll configure Express to use Handlebars as the view engine, and tell Express where to find these files:</p><pre><code class=\"language-javascript\">// view engine setup\napp.set('view engine', 'hbs');\n\napp.engine( 'hbs', hbs( {\n  extname: 'hbs',\n  defaultView: 'default',\n  layoutsDir: __dirname + '/views/pages/',\n  partialsDir: __dirname + '/views/partials/'\n}));\n</code></pre>\n<p>Express assumes by default that we're storing our views in the '/views' folder, which we are. We take this a step further by specifying which subfolders our <strong>partials</strong> and <strong>layouts</strong> are in above. We can save <strong>pages</strong> directly in <code>/views</code>.</p><p>Notice that we're also setting a default layout. We can override this in our routes if needed, but setting a default layout is useful for loading pages in an html wrapper container page metadata.</p><h2 id=\"kicks-on-route-66\">Kicks on Route 66</h2><p>Let's create our first route in <code>routes/index.js</code>. We're going to load a view called <em>home</em> into a layout called <em>default</em>:</p><pre><code class=\"language-javascript\">var express = require('express');\nvar router = express.Router();\n\nrouter.get('/', function(req, res, next) {\n  res.render('home', {layout: 'default', template: 'home-template'});\n});\n</code></pre>\n<p>This will render <code>views/home.hbs</code> into <code>views/layouts/default.hbs</code>, provided are views are set up correctly. We also pass a custom value <strong>template</strong> which is user-defined; more on that below.</p><h2 id=\"basic-usage\">Basic Usage</h2><p>Let's finally take a look at our actual Handlebars views. Here's <strong>default.hbs</strong>:</p><pre><code class=\"language-handlebars\">&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n  &lt;meta charset=&quot;utf-8&quot; /&gt;\n  &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt;\n  &lt;title&gt;Best Website&lt;/title&gt;\n  &lt;meta name=&quot;HandheldFriendly&quot; content=&quot;True&quot; /&gt;\n  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0, viewport-fit=cover&quot; /&gt;\n  &lt;link rel=&quot;stylesheet&quot; href=&quot;/css/main.min.css&quot;&gt;\n&lt;/head&gt;\n&lt;body class=&quot;{{template}}&quot;&gt;\n  &lt;div class=&quot;container&quot;&gt;\n\t  {{{body}}}\n  &lt;/div&gt;\n  {{&gt; footer}}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>We have three values here: <strong>{{template}}</strong> and <strong>{{{body}}}</strong>, and <strong>{{&gt; footer}}</strong>.</p><p><strong>{{template}}</strong> is a value with double brackets, thus is expecting linear data. We passed <em>template</em> in our route: this sets the body class to equal <em>home-template</em> on the chance that we'll want to apply page-specific styles or logic in the future.</p><p><strong>{{{body}}}</strong> is rocking the triple brackets, and is reserved specifically to serve this purpose: loading templates into other templates.</p><p>Lastly we have <strong>{{&gt; footer}}</strong>. This will load a partial named <em>footer</em> from <code>views/partials/footer.hbs</code>, provided that we create it. The difference between how <code>{{{body}}}</code> and <code>{{&gt; footer}}</code> are being loaded have to do with a general workflow philosophy; pages are the main event and thus are loaded into layouts by their command. Partials can be called by pages at will whenever we please.</p><p>There's obviously a lot more to Handlebars- the fun doesn't truly begin until we pull dynamic values from databases or wherever. We'll get there.</p>","url":"https://hackersandslackers.com/handlebars-templating-in-expressjs/","uuid":"9258a456-aee9-4d91-a36b-b1db735270b7","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b31531d04c0af72fa9a7681"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673683","title":"Read and Write to S3 Buckets via NodeJS","slug":"accessing-private-s3-objects-with-node","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","custom_excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","created_at_pretty":"21 June, 2018","published_at_pretty":"22 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-21T19:26:44.000-04:00","published_at":"2018-06-22T07:30:00.000-04:00","updated_at":"2019-03-28T05:25:16.000-04:00","meta_title":"Accessing Private S3 Objects with Node | Hackers and Slackers","meta_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","og_title":"Accessing Private S3 Objects with Node","twitter_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","twitter_title":"Accessing Private S3 Objects with Node","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"We here at H+S are dedicated to one simple cause: creating posts about oddly\nspecific programming scenarios. Somewhere in the world as sad soul is looking to\nprogrammatically access files from an S3 server while keeping their bucket\nprivate. To that person: we heard you.\n\nThere are plenty of reasons you'd want to access files in S3. For example, let's\nsay you read that post\n[https://hackersandslackers.com/using-pandas-with-aws-lambda/]  about using\nPandas in a Lambda function. Since you're already familiar with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], you may hypothetically be in a\nposition to export data from a DB query to a CSV saved in S3. I bet you can\nguess what I've been doing lately.\n\nConfigure the AWS CLI on your VPS\nThe easiest and safest way to interact with other AWS services on your EC2\ninstance (or VPS of choice) is via the AWS CLI. This is easily installed as a\nglobal Python3 library:\n\n$ pip3 install awscli\n\n\nWith the CLI installed we'll be able to do something truly magical: set our AWS\nconfiguration globally. This means that any time we use interact with a\nmicroservice  (such as S3), the boto3  library will always look to the files\nstored in ~/.aws/  for our keys and secrets, without us specifying.  This\ncritical from a security perspective as it removes all  mentions of credentials\nfrom our codebase: including the location of said secrets.\n\nUse $ aws configure  to kickstart the process:\n\n$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n\n\nThis creates a couple config files for us. If we never need to modify these\nfiles, they can be found here:\n\n$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n\n\nNode Time\nWe'll assume you have an app set up with some basic routing, such as the\nbarebones ExpressJS set up.\n\nIn your app we'll need to add 2 dependencies:\n\n$ npm install --save aws-sdk\n$ npm install --save aws-config\n\n\nNow we'll create a route.\n\nvar awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n\n\nNotice the empty curly brackets in new AWS.S3({}). If we had decided to\nbarbarically hardcode our credentials into our source code, normally those\nvalues would live between those brackets as an object. When the brackets are\nempty, the AWS library automagically knows to look to our AWS credentials file\nfor our access and secret keys. \n\nThis is how you'd do things the wrong way, just in case you wanted to be\nentertained:\n\nvar s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n\n\nYeah, that totally won't get committed somewhere by accident. Shake-my-head fam.\n\nThat's pretty much it: this route will prompt a download of the target file upon\nhitting the route. As much as I'm sure we'd all love to sit here and go through\nmore complicated use cases, let's just avoid Callback Hell altogether and enjoy\nthe rest of our day.\n\nHell will have to wait until next time.","html":"<p>We here at H+S are dedicated to one simple cause: creating posts about oddly specific programming scenarios. Somewhere in the world as sad soul is looking to programmatically access files from an S3 server while keeping their bucket private. To that person: we heard you.</p><p>There are plenty of reasons you'd want to access files in S3. For example, let's say you read <a href=\"https://hackersandslackers.com/using-pandas-with-aws-lambda/\">that post</a> about using Pandas in a Lambda function. Since you're already familiar with <a href=\"https://hackersandslackers.com/using-pymysql/\">PyMySQL</a>, you may hypothetically be in a position to export data from a DB query to a CSV saved in S3. I bet you can guess what I've been doing lately.</p><h2 id=\"configure-the-aws-cli-on-your-vps\">Configure the AWS CLI on your VPS</h2><p>The easiest and safest way to interact with other AWS services on your EC2 instance (or VPS of choice) is via the AWS CLI. This is easily installed as a global Python3 library:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ pip3 install awscli\n</code></pre>\n<!--kg-card-end: markdown--><p>With the CLI installed we'll be able to do something truly magical: set our AWS configuration globally. This means that any time we use interact with a microservice  (such as S3), the <strong>boto3</strong> library will always look to the files stored in <code>~/.aws/</code> for our keys and secrets, without us specifying.  This critical from a security perspective as it removes <em>all</em> mentions of credentials from our codebase: including the location of said secrets.</p><p>Use <code>$ aws configure</code> to kickstart the process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n</code></pre>\n<!--kg-card-end: markdown--><p>This creates a couple config files for us. If we never need to modify these files, they can be found here:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"node-time\">Node Time</h2><p>We'll assume you have an app set up with some basic routing, such as the barebones ExpressJS set up.</p><p>In your app we'll need to add 2 dependencies:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install --save aws-sdk\n$ npm install --save aws-config\n</code></pre>\n<!--kg-card-end: markdown--><p>Now we'll create a route.</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Notice the empty curly brackets in <code>new AWS.S3({})</code>. If we had decided to barbarically hardcode our credentials into our source code, normally those values would live between those brackets as an object. When the brackets are empty, the AWS library automagically knows to look to our AWS credentials file for our access and secret keys. </p><p>This is how you'd do things the <strong><em>wrong </em></strong>way, just in case you wanted to be entertained:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Yeah, that totally won't get committed somewhere by accident. Shake-my-head fam.</p><p>That's pretty much it: this route will prompt a download of the target file upon hitting the route. As much as I'm sure we'd all love to sit here and go through more complicated use cases, let's just avoid Callback Hell altogether and enjoy the rest of our day.</p><p>Hell will have to wait until next time.</p>","url":"https://hackersandslackers.com/accessing-private-s3-objects-with-node/","uuid":"210f5e64-7599-43d0-a148-d68373a9d3c4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2c34345f0bc81011d7cfc6"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673660","title":"Using Gulp: Tasks to Make Frontend Tolerable","slug":"using-gulp-to-make-frontend-tolerable","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/gulp-1.jpg","excerpt":"Optimize your frontend code with Gulp: the task runner to make you production-ready.","custom_excerpt":"Optimize your frontend code with Gulp: the task runner to make you production-ready.","created_at_pretty":"30 May, 2018","published_at_pretty":"30 May, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-05-29T23:28:01.000-04:00","published_at":"2018-05-30T12:40:20.000-04:00","updated_at":"2019-03-28T05:58:57.000-04:00","meta_title":"Using Gulp to Make Frontend Tolerable | Hackers and Slackers","meta_description":"Automate tasks for production deployment such as compiling your CSS and JS","og_description":"Optimize your frontend code with Gulp: the task runner to make you production-ready.","og_image":"https://hackersandslackers.com/content/images/2019/03/gulp-1.jpg","og_title":"Using Gulp to Make Frontend Tolerable","twitter_description":"Automate the lame stuff","twitter_image":"https://hackersandslackers.com/content/images/2019/03/gulp-1.jpg","twitter_title":"Using Gulp to Make Frontend Tolerable","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"NOTE:  This tutorial was written for Gulp versions <4.0.0. Check out this post\n[https://hackersandslackers.com/upgrading-to-gulp-4/]  for Gulp >4.0.0\n\n\n--------------------------------------------------------------------------------\n\nPerhaps the whole obligatory-Gulp-tutorial on [Some Coding Blog] thing has\nbecome a bit cliché at this point. Haters may do as they will, but I 'll take\nany opportunity to jam as many SEO keywords I can get at this point. You know\nthe ones: Gulp, ExpressJS, NodeJS, or perhaps even React Vue frontend Ubuntu\nframework API social cloud data entrepreneur community. \n\nRegardless, we all need our own copy+paste references from time-to-time, or even\nworse: when we copy/paste our gulpfile.js from project to project and forget\nwhat they actually do. I won't tell anybody.\n\nQuick 101\nNodeJS developers use Gulp to automate necessary processes before moving their\nfrontend code to production. This includes minifying files to make them run\nfaster, and to also make them unreadable to people who would otherwise make fun\nof your mediocre Javascript which you were forced to crank out on a short\ntimeline.\n\nGeneral Workflow\nLet's say you're running a basic Express app. As opposed to developing and\nstoring files in a directory such as /public, Gulp enables us to develop in one\ndirectory, and compile to another. That means we can keep our horrible\nuncompressed and uncompiled  source in a folder such as /src, and output them to\na directory such as /dist, which would be our public-facing output. An Express\nfile structure utilizing this would look something like this:\n\n  |- src/\n      |- js/ \n      |- scss/\n  |- dist/\n      |- js/ \n      |- css/\n      |- img/\n      |- fonts/\n  |- views\n      |- index.hbs\n  |- routes\n      |- index.js\n  |- gulpfile.js\n  |- node_modules/\n  |- package.json\n  |- app.js\n\n\nInstallation\nFirst install the gulp CLI:\n\nnpm install --global gulp-cli\n\n\nNext, enter your project folder and install gulp while saving it as  a project\ndependency.\n\nnpm install --save gulp\n\n\nHow it Works\nGulp doesn't do much on its own; the true magic lies within its vast library of\nplugins. Each individual plugin typically covers a simple task, such as\ncompiling LESS or SASS files, or minifying client-side JavaScript and CSS. The\nlimited scope of plugins entails a bit of setup in our gulpfile to chain said\ntasks together, but it also makes Gulp highly customizable to cater to your\nspecific needs.\n\nThe Gulpfile\nGulp works by invoking a file called gulpfile.js in your main directory, which\nyou'll need to create and set up yourself (sorry). The file is divided into two\nmain parts: requiring (importing) plugins, and defining which tasks to run when\ngulp is invoked. A basic worthless gulpfile might look something like:\n\nvar gulp = require('gulp');\n\ngulp.task('default', function () {\n  console.log('Sup World!');\n});\n\n\nTo make this file useful, we'll need to install more plugins and set up tasks\nfor them.\n\nEssential Plugins\nLet's look at what each one does. Keep in mind there are thousands of Gulp\nplugins, so let's just touch on the big hitters here.\n\nKeep in mind to install any of these plugins, you'll simply need to run the npm\ninstallation in your project directory:\n\nnpm install --save [plugin name]\n\n\ngulp-uglify\nMinifies Javascript or CSS files, and outputs the result into the directory of\nyour choice. This plugin can be reused across filetypes, as we'll demonstrate in\na moment.\n\ngulp-concat\nCombines minified files into a single file. This is essential for browser\nperformance as it reduces the number of http requests being made every time your\npage loads.\n\ngulp-rename\nRenames files (such as those produced by gulp-concat).\n\ngulp-sass / gulp-less\nCompiles your Sass or Less files into CSS and outputs to the directory of your\nchoice.\n\ngulp-minify-css\nMinifies CSS, as you might imagine. This can chained to gulp-sass or gulp-less\nto minify the CSS files those tasks produce.\n\ngulp-autoprefixer\nThank god for this. Autoprefixer finds CSS styles and adds the browser-specific\nequivalents to your CSS, so you don't need to write the same style 10 times for\nevery horrible browser in existence. This means you can write styles such as:\n\nbackground: linear-gradient(to bottom, white, black);\n\n\nAnd have them output as:\n\nbackground: -webkit-gradient(linear, left top, left bottom, from(white), to(black));\nbackground: -webkit-linear-gradient(top, white, black);\nbackground: -o-linear-gradient(top, white, black);\nbackground: linear-gradient(to bottom, white, black);\n\n\ngulp-watch\nAllows Gulp to listen for changes being made to source files, so that it may\nfire an event upon file change, such as:\n\ngulp-livereload\nCompiles the changes made in directories being watched via gulp-watch\nautomatically while you work.\n\nNext Level Pro Shit\nWhile these plugins aren't 'essential', they are really cool and helpful.\n\ngulp-sourcemaps\nAn obnoxious side effect of minifying and concating your files is when it comes\ntime to debug errors on the frontend. Errors occurring at \"line 235\" are pretty\nuseless considering your error codes are referring to the compiled files,\nwithout granting a hint as to where the problematic code may have come from in\nthe first place. gulp-sourcemaps resolves this by adding commenting paths to\nwhich source files your code originated from.\n\ngulp-browser-sync\nBy leveraging BrowserSync [https://browsersync.io/], this plugin immediately\nrefreshes an open browser which links to files just changed by gulp. This means\nyou can code, compile, and see the results in real time. This takes a bit extra\neffort to set up, so be sure to check their documentation\n[https://browsersync.io/docs].\n\ngulp-load-plugins\nNormally when creating our gulpfile, we need to start off by requiring our\nplugins via something like this:\n\nvar gulp = require('gulp'),\n    del = require('del'),\n    concat = require('gulp-concat'),\n    rename = require('gulp-rename'),\n    uglify = require('gulp-uglify'),\n    sass = require('gulp-sass'),\n    watch = require('gulp-watch'),\n    livereload = require('gulp-livereload'),\n    minifyCss = require('gulp-minify-css'),\n    autoprefixer = require('gulp-autoprefixer');\n\n\ngulp-load-plugins  instead checks your package.json for any Gulp plugins and\nimmediately requires them, thus saving you a few precious minutes. The output\ninstead looks like:\n\nvar $ = require('gulp-load-plugins')();\n\n\nBuilding The Gulpfile\nNow that we have all these dope plugins, we can finally build our gulpfile.\nHere's an example (without using gulp-load-plugins  for now):\n\nvar gulp = require('gulp'),\n    del = require('del'),\n    concat = require('gulp-concat'),\n    rename = require('gulp-rename'),\n    uglify = require('gulp-uglify'),\n    sass = require('gulp-sass'),\n    watch = require('gulp-watch'),\n    livereload = require('gulp-livereload'),\n    minifyCss = require('gulp-minify-css'),\n    autoprefixer = require('gulp-autoprefixer');\n\ngulp.task('styles', function() {\n    return gulp.src('src/sass/*.scss')\n        .pipe(sass({outputStyle: 'expanded', unix_newlines: true, linefeed: \"lf\"}).on('error', sass.logError))\n        .pipe(autoprefixer())\n        .pipe(minifyCss({\n            keepSpecialComments: 1\n        }))\n        .pipe(rename(\"theme.min.css\"))\n        .pipe(gulp.dest('./assets/css/'));\n});\n\n\ngulp.task('scripts', function() {\n    return gulp.src(['src/js/plugin/*.js', 'src/js/base.js'])\n        .pipe(uglify())\n        .pipe(concat('theme.min.js'))\n        .pipe(gulp.dest('assets/js'));\n});\n\n\ngulp.task('ghost_config', ['scripts'], function() {\n    return gulp.src(['src/js/config.js', 'assets/js/theme.min.js'])\n        .pipe(concat('theme.min.js'))\n        .pipe(gulp.dest('assets/js'));\n});\n\n\ngulp.task('default', function() {\n    gulp.start('styles', 'scripts', 'ghost_config', 'watch');\n});\n\n\ngulp.task('watch', function() {\n    gulp.watch('src/sass/*.scss', ['styles']);\n    gulp.watch('src/js/*.js', ['scripts', 'ghost_config']);\n    livereload.listen();\n    gulp.watch(['*']).on('change', livereload.changed);\n});\n\n\n\nJust by looking at the file itself, you may be able to dissect what's happening.\nAfter we require our plugins, we define our tasks,  which are essentially\nindividual jobs consisting of one or more gulp plugins depending on how you've\nchained them.\n\nHere's the general terminology to help clear things up:\n\n * gulp.task: Defines a task consisting of one of more plugin actions.\n * gulp.src:  Specifies the folder containing source files.\n * gulp.dest: Defines the folder to output compiled files to.\n * pipe(): Allows multiple events to be chained together in a single task.\n\nWrapping up\nOnce your file is ready to go, simply run the grunt  command in your project\ndirectory. You should see Gulp output the status of each task you've set, as\nwell as any errors which may have occurred.\n\nIn short, just use Gulp. The scientific community has come to a consensus that\nGulp is objectively superior to its counterpart, Grunt. Ask Matt, he's a\nscientist.\n\nPeace fam!","html":"<p><em><strong>NOTE</strong>:  This tutorial was written for Gulp versions &lt;4.0.0. Check out <a href=\"https://hackersandslackers.com/upgrading-to-gulp-4/\">this post</a> for Gulp &gt;4.0.0</em></p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><p>Perhaps the whole obligatory-Gulp-tutorial on [Some Coding Blog] thing has become a bit cliché at this point. Haters may do as they will, but I 'll take any opportunity to jam as many SEO keywords I can get at this point. You know the ones: Gulp, ExpressJS, NodeJS, or perhaps even React Vue frontend Ubuntu framework API social cloud data entrepreneur community. </p><p>Regardless, we all need our own copy+paste references from time-to-time, or even worse: when we copy/paste our gulpfile.js from project to project and forget what they actually do. I won't tell anybody.</p><h2 id=\"quick-101\">Quick 101</h2><p>NodeJS developers use Gulp to automate necessary processes before moving their frontend code to production. This includes minifying files to make them run faster, and to also make them unreadable to people who would otherwise make fun of your mediocre Javascript which you were forced to crank out on a short timeline.</p><h3 id=\"general-workflow\">General Workflow</h3><p>Let's say you're running a basic Express app. As opposed to developing and storing files in a directory such as /public, Gulp enables us to develop in one directory, and compile to another. That means we can keep our horrible uncompressed and uncompiled  source in a folder such as /src, and output them to a directory such as /dist, which would be our public-facing output. An Express file structure utilizing this would look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">  |- src/\n      |- js/ \n      |- scss/\n  |- dist/\n      |- js/ \n      |- css/\n      |- img/\n      |- fonts/\n  |- views\n      |- index.hbs\n  |- routes\n      |- index.js\n  |- gulpfile.js\n  |- node_modules/\n  |- package.json\n  |- app.js\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"installation\">Installation</h3><p>First install the gulp CLI:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install --global gulp-cli\n</code></pre>\n<!--kg-card-end: markdown--><p>Next, enter your project folder and install gulp while saving it as  a project dependency.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install --save gulp\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"how-it-works\">How it Works</h2><p>Gulp doesn't do much on its own; the true magic lies within its vast library of plugins. Each individual plugin typically covers a simple task, such as compiling LESS or SASS files, or minifying client-side JavaScript and CSS. The limited scope of plugins entails a bit of setup in our gulpfile to chain said tasks together, but it also makes Gulp highly customizable to cater to your specific needs.</p><h3 id=\"the-gulpfile\">The Gulpfile</h3><p>Gulp works by invoking a file called gulpfile.js in your main directory, which you'll need to create and set up yourself (sorry). The file is divided into two main parts: requiring (importing) plugins, and defining which tasks to run when gulp is invoked. A basic worthless gulpfile might look something like:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var gulp = require('gulp');\n\ngulp.task('default', function () {\n  console.log('Sup World!');\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>To make this file useful, we'll need to install more plugins and set up tasks for them.</p><h2 id=\"essential-plugins\">Essential Plugins</h2><p>Let's look at what each one does. Keep in mind there are thousands of Gulp plugins, so let's just touch on the big hitters here.</p><p>Keep in mind to install any of these plugins, you'll simply need to run the npm installation in your project directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">npm install --save [plugin name]\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"gulp-uglify\">gulp-uglify</h3><p>Minifies Javascript or CSS files, and outputs the result into the directory of your choice. This plugin can be reused across filetypes, as we'll demonstrate in a moment.</p><h3 id=\"gulp-concat\">gulp-concat</h3><p>Combines minified files into a single file. This is essential for browser performance as it reduces the number of http requests being made every time your page loads.</p><h3 id=\"gulp-rename\">gulp-rename</h3><p>Renames files (such as those produced by gulp-concat).</p><h3 id=\"gulp-sass-gulp-less\">gulp-sass / gulp-less</h3><p>Compiles your Sass or Less files into CSS and outputs to the directory of your choice.</p><h3 id=\"gulp-minify-css\">gulp-minify-css</h3><p>Minifies CSS, as you might imagine. This can chained to gulp-sass or gulp-less to minify the CSS files those tasks produce.</p><h3 id=\"gulp-autoprefixer\">gulp-autoprefixer</h3><p>Thank god for this. Autoprefixer finds CSS styles and adds the browser-specific equivalents to your CSS, so you don't need to write the same style 10 times for every horrible browser in existence. This means you can write styles such as:</p><!--kg-card-begin: markdown--><pre><code class=\"language-css\">background: linear-gradient(to bottom, white, black);\n</code></pre>\n<!--kg-card-end: markdown--><p>And have them output as:</p><!--kg-card-begin: markdown--><pre><code class=\"language-css\">background: -webkit-gradient(linear, left top, left bottom, from(white), to(black));\nbackground: -webkit-linear-gradient(top, white, black);\nbackground: -o-linear-gradient(top, white, black);\nbackground: linear-gradient(to bottom, white, black);\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"gulp-watch\">gulp-watch</h3><p>Allows Gulp to listen for changes being made to source files, so that it may fire an event upon file change, such as:</p><h3 id=\"gulp-livereload\">gulp-livereload</h3><p>Compiles the changes made in directories being watched via gulp-watch automatically while you work.</p><h2 id=\"next-level-pro-shit\">Next Level Pro Shit</h2><p>While these plugins aren't 'essential', they are really cool and helpful.</p><h3 id=\"gulp-sourcemaps\">gulp-sourcemaps</h3><p>An obnoxious side effect of minifying and concating your files is when it comes time to debug errors on the frontend. Errors occurring at \"line 235\" are pretty useless considering your error codes are referring to the compiled files, without granting a hint as to where the problematic code may have come from in the first place. gulp-sourcemaps resolves this by adding commenting paths to which source files your code originated from.</p><h3 id=\"gulp-browser-sync\">gulp-browser-sync</h3><p>By leveraging <a href=\"https://browsersync.io/\">BrowserSync</a>, this plugin immediately refreshes an open browser which links to files just changed by gulp. This means you can code, compile, and see the results in real time. This takes a bit extra effort to set up, so be sure to check their <a href=\"https://browsersync.io/docs\">documentation</a>.</p><h3 id=\"gulp-load-plugins\">gulp-load-plugins</h3><p>Normally when creating our gulpfile, we need to start off by requiring our plugins via something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n    del = require('del'),\n    concat = require('gulp-concat'),\n    rename = require('gulp-rename'),\n    uglify = require('gulp-uglify'),\n    sass = require('gulp-sass'),\n    watch = require('gulp-watch'),\n    livereload = require('gulp-livereload'),\n    minifyCss = require('gulp-minify-css'),\n    autoprefixer = require('gulp-autoprefixer');\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>gulp-load-plugins</strong> instead checks your package.json for any Gulp plugins and immediately requires them, thus saving you a few precious minutes. The output instead looks like:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var $ = require('gulp-load-plugins')();\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"building-the-gulpfile\">Building The Gulpfile</h2><p>Now that we have all these dope plugins, we can finally build our gulpfile. Here's an example (without using <strong>gulp-load-plugins</strong> for now):</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var gulp = require('gulp'),\n    del = require('del'),\n    concat = require('gulp-concat'),\n    rename = require('gulp-rename'),\n    uglify = require('gulp-uglify'),\n    sass = require('gulp-sass'),\n    watch = require('gulp-watch'),\n    livereload = require('gulp-livereload'),\n    minifyCss = require('gulp-minify-css'),\n    autoprefixer = require('gulp-autoprefixer');\n\ngulp.task('styles', function() {\n    return gulp.src('src/sass/*.scss')\n        .pipe(sass({outputStyle: 'expanded', unix_newlines: true, linefeed: &quot;lf&quot;}).on('error', sass.logError))\n        .pipe(autoprefixer())\n        .pipe(minifyCss({\n            keepSpecialComments: 1\n        }))\n        .pipe(rename(&quot;theme.min.css&quot;))\n        .pipe(gulp.dest('./assets/css/'));\n});\n\n\ngulp.task('scripts', function() {\n    return gulp.src(['src/js/plugin/*.js', 'src/js/base.js'])\n        .pipe(uglify())\n        .pipe(concat('theme.min.js'))\n        .pipe(gulp.dest('assets/js'));\n});\n\n\ngulp.task('ghost_config', ['scripts'], function() {\n    return gulp.src(['src/js/config.js', 'assets/js/theme.min.js'])\n        .pipe(concat('theme.min.js'))\n        .pipe(gulp.dest('assets/js'));\n});\n\n\ngulp.task('default', function() {\n    gulp.start('styles', 'scripts', 'ghost_config', 'watch');\n});\n\n\ngulp.task('watch', function() {\n    gulp.watch('src/sass/*.scss', ['styles']);\n    gulp.watch('src/js/*.js', ['scripts', 'ghost_config']);\n    livereload.listen();\n    gulp.watch(['*']).on('change', livereload.changed);\n});\n\n</code></pre>\n<!--kg-card-end: markdown--><p>Just by looking at the file itself, you may be able to dissect what's happening. After we require our plugins, we define our <em>tasks,</em> which are essentially individual jobs consisting of one or more gulp plugins depending on how you've chained them.</p><p>Here's the general terminology to help clear things up:</p><ul><li><strong>gulp.task</strong>: Defines a task consisting of one of more plugin actions.</li><li><strong>gulp.src</strong>:  Specifies the folder containing source files.</li><li><strong>gulp.dest</strong>: Defines the folder to output compiled files to.</li><li><strong>pipe()</strong>: Allows multiple events to be chained together in a single task.</li></ul><h2 id=\"wrapping-up\">Wrapping up</h2><p>Once your file is ready to go, simply run the <strong>grunt</strong> command in your project directory. You should see Gulp output the status of each task you've set, as well as any errors which may have occurred.</p><p>In short, just use Gulp. The scientific community has come to a consensus that Gulp is objectively superior to its counterpart, Grunt. Ask Matt, he's a scientist.</p><p>Peace fam!</p>","url":"https://hackersandslackers.com/using-gulp-to-make-frontend-tolerable/","uuid":"f0ec0903-cd67-4663-b872-9ecdf9ffc557","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b0e1a41e88ecf2fbeb3f5fa"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673644","title":"Make Your First API Calls with JQuery AJAX","slug":"making-ajax-calls-with-jquery","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-2.jpg","excerpt":"Beginner's guide to consuming endpoints via Frontend Javascript.","custom_excerpt":"Beginner's guide to consuming endpoints via Frontend Javascript.","created_at_pretty":"24 April, 2018","published_at_pretty":"25 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-23T20:20:21.000-04:00","published_at":"2018-04-25T01:00:00.000-04:00","updated_at":"2019-03-28T09:44:00.000-04:00","meta_title":"Make Your First API Calls with JQuery AJAX | Hackers and Slackers","meta_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","og_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","og_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-2.jpg","og_title":"Make Your First API Calls with JQuery AJAX","twitter_description":"Beginner's guide to consuming endpoints via Frontend Javascript. Quick Introduction to REST APIs, as well as a hands-on tutorial.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jquery-2-1.jpg","twitter_title":"Make Your First API Calls with JQuery AJAX","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"The information age is over: we have all the information now. All of it. We're\nin a unique place in human history where we've somehow managed to mine more data\nthan we know what to do with... and a lot of that data is easily accessible via\nAPIs.\n\nWe're going to get our feet wet with REST APIs today, thus allowing us to\ninteract with meaningful information. Making Ajax GET calls with JQuery  is\nperhaps as basic as it gets: there's a good chance we already know all this\nstuff, but even I constantly forget the exact syntax of everyday functions.\nChances are I'm going to come back to this post at some point just to copy and\npaste the snippets below. \n\nIntroduction to REST APIs\nIf you're new to executing AJAX requests, chances are you may be new to REST\nAPIs in general. This crash course is going to be fast and rough around the\nedges, so strap in.\n\nIn the most simple sense, a REST API endpoint is a URL. It just so happens that\nthis URL probably expects more from you than simply visiting it, and as a\nresult, should output something useful for you. API Endpoints will almost always\noutput either JSON or XML; these responses will give you information varying\nfrom error codes to the actual data you seek.\n\nREST APIs expect requests to come in one of the following forms:\n\n * GET: A request looking for read-only data. Some GET requests simply need to\n   be copy and pasted into a browser window to receive results, but usually we\n   need to either authenticate or specify what we're looking for.\n * POST: A write  request to the target resource. Expects that new information\n   will come as a result of this request.\n * PUT: Updates pre-existing data somewhere, likely in some database.\n * PATCH: Somewhat similar to PUT, and in my experience rarely used at all.\n * DELETE: Expects that information will be deleted as a result of the request\n\nIf this all seems like new information, I'd highly recommend downloading Postman\n[https://www.getpostman.com/]  to become familiar with how API calls are\nstructured.\n\nFor now, we'll focus on working with a simple GET endpoint.\n\nLearning By Doing\nIf you've been checking out Snkia's roundup posts\n[https://hackersandslackers.com/tag/roundup/]  lately, you may have noticed\nnifty link previews being generated. To demonstrate how to make API calls via a\nfrontend client with JQuery, we'll be walking through how to create link\npreviews using the LinkPreview API [https://www.linkpreview.net/]. This service\nserves as a good tutorial because:\n\n * It's an example of a simple GET endpoint\n * There's a quick and immediately useful end result\n * It's free\n\nTell me That Ain't Insecurr\nI want to stress here that we're doing this for the sake of learning; while this\nis probably the fastest  way to start working with an API, it is most definitely\n not secure.\n\nMaking calls with private keys stored and passed via the client side exposes\nyour key publicly. In a production environment, this is like shoving your\npassword in people's faces. People will most definitely want to steal and\nexploit your private key: if what you were doing didn't have any value, it\nwouldn't require a key in the first place.\n\nHopefully this has scared you enough to consider passing credentials in the\nfuture. That said, there's another solid reason we selected LinkPreview as\ntoday's example. LinkPreview offers domain whitelisting for requests, so even if\nsomebody did steal your key, they'd only be able to use it from your domain ;).\n\nMake sure you whitelist the domain you'll be working from.Fetch Me Daddy\nGo get started with an API key over at LinkPreview  if you're following along.\nI'm going to assume you already have JQuery  readily available from here\nforward.\n\nTo get started, we'll wait for our document to load, and set two critical\nvariables: the API URL, and our API key.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n});\n\n\nIf you're following along what we've done with Lynx Roundups, our next step is\nto get all the relevant  <a>  tags on a page, loop through them, and replace\nthem with their respective link previews.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n  \n  $( \".content a\" ).each(function( index, element ) {\n      console.log($( this ).text());\n  }\n});\n\n\nThe JQuery  .each  method creates a loop which iterates over every element\nmatching the provided selector. In our example, we only want <a>  tags in the\ncontent of our page; otherwise we would get all  links, like navigation links\nand so forth.\n\nNow it's time to bring in that $.ajax()  thing we've been going off about.\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( \".content a\" ).each(function( index, element ) {\n\n    $.ajax({\n        url: api_url + \"?key=\" + key + \" &q=\" + $( this ).text(),\n        contentType: \"application/json\",\n        dataType: 'json',\n        success: function(result){\n            console.log(result);\n        }\n    })\n  });\n});\n\n\nThis is how Ajax request are structured: the contents of $.ajax()  is\nessentially an object taking values it will use to construct the request. The\nabove example is about as simple as it gets for making a barebones GET call.\nWe're looping through each <a>  tag and passing its contents (the url) to the\nAPI, and receiving an object in response.\n\nAjax requests can take way more parameters than the ones we just specified. I\nrecommend reading over the JQuery Ajax documentation\n[http://api.jquery.com/jquery.ajax/]  closely; not only for the sake of these\nrequests, but understanding the potential items we can specify will solidify an\nunderstanding for REST APIs in general.\n\nThe line contentType: \"application/json\"  specifies that the content coming back\nto us will be in JSON format - this is a very common header when dealing with\nREST APIs. \n\nWith any luck, your response should come back looking like:\n\n{\n    \"title\":\"Google\",\n    \"description\":\"Search webpages, images, videos and more.\",\n    \"image\":\"https//:www.google.com/images/logo.png\",\n    \"url\":\"https://www.google.com/\"\n}\n\n\nIf you'd like to use this in a meaningful way, feel free to do something like\nthe mess I've put together below:\n\n$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( \".content a\" ).each(function( index, element ) {\n    $.ajax({\n        url: api_url + \"?key=\" + key + \" &q=\" + $( this ).text(),\n        contentType: \"application/json\",\n        dataType: 'json',\n        success: function(result){\n            $( element ).after(\n            '<a href=\"' + result.url + '\"> \\n ' +\n              '<div class=\"link-preview\"> \\n ' +\n                '<div class=\"preview-image\" style=\"background-image:url(' + result.image + ');\"></div> \\n ' + \n                '<div style=\"width:70%;\" class=\"link-info\"> \\n ' +\n                  '<h4>' + result.title +'</h4> \\n ' +\n                  '<p>' + result.description +'</p> \\n ' +\n                '</div><br> \\n ' +\n                  '<a href=\"' + result.url + '\" class=\"url-info\"><i class=\"far fa-link\"></i>' + result.url + '</a> \\n ' +\n                '</div></a>');\n            $( element ).remove();\n        }\n    })\n  });\n});\n\n\nThat template should serve you well for most GET API calls you're going to make\nvia JQuery. Go wild and see what you can do to leverage APIs and expose some\npeople's personal data or whatever.\n\nSee how I just created HTML by stringing together a bunch of ugly strings in\nJavascript? Don't do that; there are countless better ways to handle this, they\njust so happen to be out of scope for this post.If we were to truly complete\nthis example, we'd want to refine our logic to ensure we're not receiving\nnonsense. There's no validation on what's coming back in these calls, so there's\nnothing in place to protect us in the case that a page doesn't comply with our\nformat.","html":"<p>The information age is over: we have all the information now. All of it. We're in a unique place in human history where we've somehow managed to mine more data than we know what to do with... and a lot of that data is easily accessible via APIs.</p><p>We're going to get our feet wet with REST APIs today, thus allowing us to interact with meaningful information. Making Ajax GET calls with <strong>JQuery</strong> is perhaps as basic as it gets: there's a good chance we already know all this stuff, but even I constantly forget the exact syntax of everyday functions. Chances are I'm going to come back to this post at some point just to copy and paste the snippets below. </p><h2 id=\"introduction-to-rest-apis\">Introduction to REST APIs</h2><p>If you're new to executing AJAX requests, chances are you may be new to REST APIs in general. This crash course is going to be fast and rough around the edges, so strap in.</p><p>In the most simple sense, a REST API endpoint is a URL. It just so happens that this URL probably expects more from you than simply visiting it, and as a result, should output something useful for you. API Endpoints will almost always output either JSON or XML; these responses will give you information varying from error codes to the actual data you seek.</p><p>REST APIs expect requests to come in one of the following forms:</p><ul><li><strong>GET</strong>: A request looking for read-only data. Some GET requests simply need to be copy and pasted into a browser window to receive results, but usually we need to either authenticate or specify what we're looking for.</li><li><strong>POST</strong>: A <em>write</em> request to the target resource. Expects that new information will come as a result of this request.</li><li><strong>PUT</strong>: Updates pre-existing data somewhere, likely in some database.</li><li><strong>PATCH</strong>: Somewhat similar to PUT, and in my experience rarely used at all.</li><li><strong>DELETE: </strong>Expects that information will be deleted as a result of the request</li></ul><p>If this all seems like new information, I'd highly recommend downloading <a href=\"https://www.getpostman.com/\">Postman</a> to become familiar with how API calls are structured.</p><p>For now, we'll focus on working with a simple GET endpoint.</p><h2 id=\"learning-by-doing\">Learning By Doing</h2><p>If you've been checking out <a href=\"https://hackersandslackers.com/tag/roundup/\">Snkia's roundup posts</a> lately, you may have noticed nifty link previews being generated. To demonstrate how to make API calls via a frontend client with <strong>JQuery</strong>, we'll be walking through how to create link previews using the <a href=\"https://www.linkpreview.net/\">LinkPreview API</a>. This service serves as a good tutorial because:</p><ul><li>It's an example of a simple GET endpoint</li><li>There's a quick and immediately useful end result</li><li>It's free</li></ul><h3 id=\"tell-me-that-ain-t-insecurr\">Tell me That Ain't Insecurr</h3><p>I want to stress here that we're doing this for the sake of learning; while this is probably the <em>fastest</em> way to start working with an API, it is most definitely <strong>not secure</strong>.</p><p>Making calls with private keys stored and passed via the client side exposes your key publicly. In a production environment, this is like shoving your password in people's faces. People will most definitely want to steal and exploit your private key: if what you were doing didn't have any value, it wouldn't require a key in the first place.</p><p>Hopefully this has scared you enough to consider passing credentials in the future. That said, there's another solid reason we selected LinkPreview as today's example. LinkPreview offers domain whitelisting for requests, so even if somebody did steal your key, they'd only be able to use it from your domain ;).</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2018-11-25-at-6.06.44-AM.png\" class=\"kg-image\"><figcaption>Make sure you whitelist the domain you'll be working from.</figcaption></figure><h2 id=\"fetch-me-daddy\">Fetch Me Daddy</h2><p>Go get started with an API key over at <strong>LinkPreview</strong> if you're following along. I'm going to assume you already have <strong>JQuery</strong> readily available from here forward.</p><p>To get started, we'll wait for our document to load, and set two critical variables: the API URL, and our API key.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n});\n</code></pre>\n<p>If you're following along what we've done with Lynx Roundups, our next step is to get all the <em>relevant</em> <code>&lt;a&gt;</code> tags on a page, loop through them, and replace them with their respective link previews.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n  \n  $( &quot;.content a&quot; ).each(function( index, element ) {\n      console.log($( this ).text());\n  }\n});\n</code></pre>\n<p>The <strong>JQuery</strong> <code>.each</code> method creates a loop which iterates over every element matching the provided selector. In our example, we only want <code>&lt;a&gt;</code> tags in the content of our page; otherwise we would get <em>all</em> links, like navigation links and so forth.</p><p>Now it's time to bring in that <code>$.ajax()</code> thing we've been going off about.</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( &quot;.content a&quot; ).each(function( index, element ) {\n\n    $.ajax({\n        url: api_url + &quot;?key=&quot; + key + &quot; &amp;q=&quot; + $( this ).text(),\n        contentType: &quot;application/json&quot;,\n        dataType: 'json',\n        success: function(result){\n            console.log(result);\n        }\n    })\n  });\n});\n</code></pre>\n<p>This is how Ajax request are structured: the contents of <code>$.ajax()</code> is essentially an object taking values it will use to construct the request. The above example is about as simple as it gets for making a barebones GET call. We're looping through each <code>&lt;a&gt;</code> tag and passing its contents (the url) to the API, and receiving an object in response.</p><p><strong>Ajax </strong>requests can take way more parameters than the ones we just specified. I recommend reading over the <a href=\"http://api.jquery.com/jquery.ajax/\">JQuery Ajax documentation</a> closely; not only for the sake of these requests, but understanding the potential items we can specify will solidify an understanding for REST APIs in general.</p><p>The line <code>contentType: \"application/json\"</code> specifies that the content coming back to us will be in JSON format - this is a very common header when dealing with REST APIs. </p><p>With any luck, your response should come back looking like:</p><pre><code class=\"language-json\">{\n    &quot;title&quot;:&quot;Google&quot;,\n    &quot;description&quot;:&quot;Search webpages, images, videos and more.&quot;,\n    &quot;image&quot;:&quot;https//:www.google.com/images/logo.png&quot;,\n    &quot;url&quot;:&quot;https://www.google.com/&quot;\n}\n</code></pre>\n<p>If you'd like to use this in a meaningful way, feel free to do something like the mess I've put together below:</p><pre><code class=\"language-javascript\">$( document ).ready(function() {\n  var api_url = 'https://api.linkpreview.net'\n  var key = '5b578yg9yvi8sogirbvegoiufg9v9g579gviuiub8' // not real\n\n  $( &quot;.content a&quot; ).each(function( index, element ) {\n    $.ajax({\n        url: api_url + &quot;?key=&quot; + key + &quot; &amp;q=&quot; + $( this ).text(),\n        contentType: &quot;application/json&quot;,\n        dataType: 'json',\n        success: function(result){\n            $( element ).after(\n            '&lt;a href=&quot;' + result.url + '&quot;&gt; \\n ' +\n              '&lt;div class=&quot;link-preview&quot;&gt; \\n ' +\n                '&lt;div class=&quot;preview-image&quot; style=&quot;background-image:url(' + result.image + ');&quot;&gt;&lt;/div&gt; \\n ' + \n                '&lt;div style=&quot;width:70%;&quot; class=&quot;link-info&quot;&gt; \\n ' +\n                  '&lt;h4&gt;' + result.title +'&lt;/h4&gt; \\n ' +\n                  '&lt;p&gt;' + result.description +'&lt;/p&gt; \\n ' +\n                '&lt;/div&gt;&lt;br&gt; \\n ' +\n                  '&lt;a href=&quot;' + result.url + '&quot; class=&quot;url-info&quot;&gt;&lt;i class=&quot;far fa-link&quot;&gt;&lt;/i&gt;' + result.url + '&lt;/a&gt; \\n ' +\n                '&lt;/div&gt;&lt;/a&gt;');\n            $( element ).remove();\n        }\n    })\n  });\n});\n</code></pre>\n<p>That template should serve you well for most GET API calls you're going to make via <strong>JQuery</strong>. Go wild and see what you can do to leverage APIs and expose some people's personal data or whatever.</p><div class=\"protip\">\n    See how I just created HTML by stringing together a bunch of ugly strings in Javascript? Don't do that; there are countless better ways to handle this, they just so happen to be out of scope for this post.\n</div><p>If we were to truly complete this example, we'd want to refine our logic to ensure we're not receiving nonsense. There's no validation on what's coming back in these calls, so there's nothing in place to protect us in the case that a page doesn't comply with our format.</p>","url":"https://hackersandslackers.com/making-ajax-calls-with-jquery/","uuid":"1fbf30e7-7ab7-48bb-8976-f100fdced4e0","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ade784572a629364c5364c7"}}]}},"pageContext":{"slug":"javascript","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}