{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c7","title":"Hacking Tableau to Handle ETL Workflows","slug":"turning-tableau-into-an-etl-tool-using-the-rest-api","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/fucktableau@2x.jpg","excerpt":"Weaponizing APIs against tyrannical software.","custom_excerpt":"Weaponizing APIs against tyrannical software.","created_at_pretty":"01 August, 2018","published_at_pretty":"03 August, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-08-01T08:49:36.000-04:00","published_at":"2018-08-03T08:57:00.000-04:00","updated_at":"2019-02-28T03:18:22.000-05:00","meta_title":"Hacking Tableau to Handle ETL Workflows | Hackers and Slackers","meta_description":"The presence of ETL as an acronym is a perfect reflection of where we are in data engineering's growth trajectory. ","og_description":"The presence of ETL as an acronym is a perfect reflection of where we are in data engineering's growth trajectory. The lack of effective Extract, Transform, and Load workflow products tell us a couple things: we have too many data sources (whether they be APIs or private data sets), and the raw data is virtually unusable until cleaned.","og_image":"https://hackersandslackers.com/content/images/2018/08/fucktableau@2x.jpg","og_title":"Hacking Tableau to Handle ETL Workflows","twitter_description":"The lack of effective Extract, Transform, and Load workflow products tell us a couple things: we have too many data sources, and the raw data is virtually unusable until cleaned.","twitter_image":"https://hackersandslackers.com/content/images/2018/08/fucktableau@2x.jpg","twitter_title":"Hacking Tableau to Handle ETL Workflows","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"}],"plaintext":"Before we get into the specifics of how to sadistically abuse Tableau, let's\nclear the air: there's something about inaccessible, expensive, proprietary\nenterprise software that tends to put me in a touchy mood. As we know, B2B\nsoftware pricing has nothing to do with code quality or even value-add, but\nrather the tendency of businesses to create time-based urgencies without\nwarning; the kinds of urgencies which may be solved by, say, a tool of sorts.\n\nMy first interaction with Tableau actually took place after I had committed\nmyself to the cult of Python's Pandas library and all that comes with it.\nTableau does little to hide the fact that it is a GUI for data manipulation and\nSQL queries; in most cases, the calculation syntax is exactly the same. From my\nperspective, Tableau could be a tool to save time: instead of rewriting\nvariations of the same scripts over and over, I could use Tableau to do these\ntasks visually for both speed and transparency's sake. It was a win-win for\ntrivial tasks, except for one: the ability to write back to a database. You'd\nthink I wouldn't think that far ahead before purchasing my own Tableau server\nand license, conveniently billed upfront annually.\n\nThe Rise of ETL\nThe presence of ETL as an acronym is a perfect reflection of where we are in\ndata engineering's growth trajectory. The lack of effective Extract, Transform,\nand Load  workflow products tell us a couple things: we have too many data\nsources (whether they be APIs or private data sets), and the raw data is\nvirtually unusable until cleaned. This process could be relatively trivial with\nthe right software. There are plenty of contenders to make this process simple,\nand I'd like to express in unadulterated astonishment that they are all  failing\nmiserably  at solving this task effectively, mostly thanks to poor decision\nmaking and human error alone.\n\nThe ETL Market\nAs it stands, Parabola.io  tops my list of ETL products. Parabola hits the nail\non the head when it comes to UI and ease of use. This begs the question: why,\nthen, are their latest releases focused on support for extraction to garbage\nproducts like Smartsheet? Currently the only extract location which is actually\na database  is MySQL. As much as I want Parabola to succeed, nothing has\nimproved if our workflow still involves manually setting up a third party DB\nwith a schema which perfectly matches our output.\n\nGoogle Cloud is doing its best to somehow tie separate products together such as\n Dataprep  and Bigquery. We'll see how that goes- there's no mention of data\nextraction from APIs in this flow just yet. We might be waiting for some time\nfor Google's perfect answer to mature.\n\nGithub Labs supposedly just announced recent efforts to tackle this space as\nwell with the upcoming Melatano\n[https://about.gitlab.com/2018/08/01/hey-data-teams-we-are-working-on-a-tool-just-for-you/]\n. Hopefully they have their heads on straight.\n\nAnyway, since the world has failed us, we'll just exploit a Tableau backdoor to\ndo this while humanity catches up.\n\nTableau's Rest API\nAs hard as Tableau tries to obfuscate anything and everything, their REST API\ngets us exactly what we want after a bit of red tape. We need to run 3 API\ncalls:\n\n * POST /api/[api-version]/auth/signin: Generate a token so we can actually use\n   the API\n * GET /api/3.0/sites/[site-id]/views:  List all view metadata in a Tableau\n   \"site.\"\n * GET /api/3.0/sites/[site-id]/views/[view-id]/data: Receive a comma\n   delimitated response of the content of your target view\n\nWhat R U Token about\nTo receive our token, we'll use basic auth to hit this simple endpoint via POST:\n \n\nPOST http://mywebsite/api/3.0/auth/signin\n\n\nThe response will come in the form of XML and give us two critical items: our \ntoken, and our site ID:\n\nClearly a user-friendly experience.List Views by Site\nNext up we're GETing the following endpoint:\n\nhttp://mywebsite/api/3.0/sites/543fc0-4123572-483276-9345d8c-96005d532b2fb33/views\n\n\nNote that Tableau asks for the site ID from the previous response to be part of\nthe URL string.\n\nWe'll also need to set headers, so do that.\n\nX-Tableau-Auth: SPMJsdfgHIDUFihdwPqf-5k8GCZJA|sXhFBHzzqksB6K567fsQvtCfTtakqrJLuQ6Cf\n\n\nThe motherload of views.Reap your Reward\nPick the notebook ID you're looking to extract data from. Chose wisely. Your\ntime now. Enter that view into the final endpoint URL:\n\nhttp://mysite/api/3.0/sites/983445c0-4172-4876-9d8c-96005db2gfdgdfb33/views/c0357db9-71b1-4besdfgd1-a14e-5a5f7a36d410/data\n\n\nWhoa mama.Well, Well, Well.\nSo now you know how to generate a Tableau REST API token at will. You also know\nall your view IDs, and how to extract the data from any of those views in a\nfriendly CSV format which happens to play nice with databases. There's a Pandas\nscript waiting to be written here somewhere.\n\nAt this point, you know have all the tools you need to automate the systematic\npillaging of your Tableau Server data. Take a brief moment to remember the days\nwhen Tableau would wave their flags through the countryside as a sign of\ntaunting warfare. They've collected your company's checks and gave you iFrames\nin return.\n\nGo onwards my brethren. For one chance, discover the lands of Plot.ly and D3 as\nFree Men. They may take our paychecks, but they will never take our data.","html":"<p>Before we get into the specifics of how to sadistically abuse Tableau, let's clear the air: there's something about inaccessible, expensive, proprietary enterprise software that tends to put me in a touchy mood. As we know, B2B software pricing has nothing to do with code quality or even value-add, but rather the tendency of businesses to create time-based urgencies without warning; the kinds of urgencies which may be solved by, say, a tool of sorts.</p><p>My first interaction with Tableau actually took place after I had committed myself to the cult of Python's Pandas library and all that comes with it. Tableau does little to hide the fact that it is a GUI for data manipulation and SQL queries; in most cases, the calculation syntax is exactly the same. From my perspective, Tableau could be a tool to save time: instead of rewriting variations of the same scripts over and over, I could use Tableau to do these tasks visually for both speed and transparency's sake. It was a win-win for trivial tasks, except for one: the ability to write back to a database. You'd think I wouldn't think that far ahead before purchasing my own Tableau server and license, conveniently billed upfront annually.</p><h2 id=\"the-rise-of-etl\">The Rise of ETL</h2><p>The presence of ETL as an acronym is a perfect reflection of where we are in data engineering's growth trajectory. The lack of effective <strong>Extract, Transform, and Load</strong> workflow products tell us a couple things: we have too many data sources (whether they be APIs or private data sets), and the raw data is virtually unusable until cleaned. This process could be relatively trivial with the right software. There are plenty of contenders to make this process simple, and I'd like to express in unadulterated astonishment that they are all<strong> failing miserably</strong> at solving this task effectively, mostly thanks to poor decision making and human error alone.</p><h3 id=\"the-etl-market\">The ETL Market</h3><p>As it stands, <a href=\"Parabola.io\">Parabola.io</a> tops my list of ETL products. Parabola hits the nail on the head when it comes to UI and ease of use. This begs the question: why, then, are their latest releases focused on support for extraction to garbage products like <strong>Smartsheet</strong>? Currently the only extract location which is <em>actually a database</em> is MySQL. As much as I want Parabola to succeed, nothing has improved if our workflow still involves manually setting up a third party DB with a schema which perfectly matches our output.</p><p>Google Cloud is doing its best to somehow tie separate products together such as <strong>Dataprep</strong> and <strong>Bigquery</strong>. We'll see how that goes- there's no mention of data extraction from APIs in this flow just yet. We might be waiting for some time for Google's perfect answer to mature.</p><p>Github Labs supposedly just announced recent efforts to tackle this space as well with the upcoming <a href=\"https://about.gitlab.com/2018/08/01/hey-data-teams-we-are-working-on-a-tool-just-for-you/\">Melatano</a>. Hopefully they have their heads on straight.</p><p>Anyway, since the world has failed us, we'll just exploit a Tableau backdoor to do this while humanity catches up.</p><h2 id=\"tableau-s-rest-api\">Tableau's Rest API</h2><p>As hard as Tableau tries to obfuscate anything and everything, their REST API gets us exactly what we want after a bit of red tape. We need to run 3 API calls:</p><ul><li><strong>POST /api/[<em>api-version]</em>/auth/signin</strong>: Generate a token so we can actually use the API</li><li><strong>GET /api/3.0/sites/[site-id]/views</strong>:<strong> </strong>List all view metadata in a Tableau \"site.\"</li><li><strong>GET /api/3.0/sites/[site-id]/views/[view-id]/data</strong>: Receive a comma delimitated response of the content of your target view</li></ul><h3 id=\"what-r-u-token-about\">What R U Token about</h3><p>To receive our token, we'll use basic auth to hit this simple endpoint via POST: </p><pre><code class=\"language-shell\">POST http://mywebsite/api/3.0/auth/signin\n</code></pre>\n<p>The response will come in the form of XML and give us two critical items: our <strong>token</strong>, and our <strong>site ID</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2018-08-03-at-7.50.59-AM.png\" class=\"kg-image\"><figcaption>Clearly a user-friendly experience.</figcaption></figure><h3 id=\"list-views-by-site\">List Views by Site</h3><p>Next up we're GETing the following endpoint:</p><pre><code class=\"language-shell\">http://mywebsite/api/3.0/sites/543fc0-4123572-483276-9345d8c-96005d532b2fb33/views\n</code></pre>\n<p>Note that Tableau asks for the <strong>site ID </strong>from the previous response to be part of the URL string.</p><p>We'll also need to set headers, so do that.</p><pre><code class=\"language-shell\">X-Tableau-Auth: SPMJsdfgHIDUFihdwPqf-5k8GCZJA|sXhFBHzzqksB6K567fsQvtCfTtakqrJLuQ6Cf\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2018-08-03-at-7.57.47-AM.png\" class=\"kg-image\"><figcaption>The motherload of views.</figcaption></figure><h3 id=\"reap-your-reward\">Reap your Reward</h3><p>Pick the notebook ID you're looking to extract data from. Chose wisely. Your time now. Enter that view into the final endpoint URL:</p><pre><code class=\"language-shell\">http://mysite/api/3.0/sites/983445c0-4172-4876-9d8c-96005db2gfdgdfb33/views/c0357db9-71b1-4besdfgd1-a14e-5a5f7a36d410/data\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2018-08-03-at-8.01.24-AM.png\" class=\"kg-image\"><figcaption>Whoa mama.</figcaption></figure><h2 id=\"well-well-well-\">Well, Well, Well.</h2><p>So now you know how to generate a Tableau REST API token at will. You also know all your view IDs, and how to extract the data from any of those views in a friendly CSV format which happens to play nice with databases. There's a Pandas script waiting to be written here somewhere.</p><p>At this point, you know have all the tools you need to automate the systematic pillaging of your Tableau Server data. Take a brief moment to remember the days when Tableau would wave their flags through the countryside as a sign of taunting warfare. They've collected your company's checks and gave you iFrames in return.</p><p>Go onwards my brethren. For one chance, discover the lands of Plot.ly and D3 as Free Men. They may take our paychecks, but they will <strong>never </strong>take our data.</p>","url":"https://hackersandslackers.com/turning-tableau-into-an-etl-tool-using-the-rest-api/","uuid":"7b86dd9c-7d93-4518-8f3a-b593a6cdb7f0","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b61ac60d2852c0dc51d9217"}},"pageContext":{"slug":"turning-tableau-into-an-etl-tool-using-the-rest-api"}}