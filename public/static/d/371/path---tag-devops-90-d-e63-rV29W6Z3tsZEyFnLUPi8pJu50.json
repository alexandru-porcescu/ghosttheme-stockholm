{"data":{"ghostTag":{"slug":"devops","name":"DevOps","visibility":"public","feature_image":null,"description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c85a8da181da30210ceca9d","title":"Serve Docker Containers With A Custom Domain and SSL","slug":"serve-docker-containers-with-custom-dns-and-ssl","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","excerpt":"Do even less work to deploy your Docker apps to production.","custom_excerpt":"Do even less work to deploy your Docker apps to production.","created_at_pretty":"11 March, 2019","published_at_pretty":"11 March, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-03-10T20:16:26.000-04:00","published_at":"2019-03-11T06:15:00.000-04:00","updated_at":"2019-04-09T15:05:49.000-04:00","meta_title":"Serve Docker Containers With A Domain and SSL | Hackers and Slackers","meta_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","og_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","og_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","og_title":"Serve Docker Containers With A Custom Domain and SSL","twitter_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","twitter_title":"Serve Docker Containers With A Custom Domain and SSL","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"The past few years of software development and architecture has witnessed\nmultiple revolutions. The rise of containers, unruly package management\necosystems, and one-click-deployments holds an unspoken narrative: most people\nprobably don’t care about how  things work beneath the top layer. Sure,\nadvancements in application infrastructure has undoubtedly made our lives\neasier. I suppose I find this lack of curiosity and unwillingness to dig deeper\ninto the innards, an unrelatable trait. Yet I digress.\n\nI’ve never found web server configurations to be particularly difficult, but\napparently most consider this enough of a nuisance to make something even easier\nto use. That’s where I came across Caddy [https://caddyserver.com/].\n\nCaddy  is a web server and free SSL service in which most of the actual work\nhappens via their download GUI [https://caddyserver.com/download]. It’s great.\nEven though I never expected us to reach a place where apt install nginx  and \napt install certbot  is considered too much of a burden, it only took a few\nminutes of wrestling with a Docker container running on a VPS that I realized\nthere was a better way.\n\nServe Anything With Caddy\nIn my particular example, the Docker container I was running produced an API\nendpoint. For some reason, this service forcefully insists that this endpoint is\nyour machine’s localhost, or it simply won’t work. While scoffing at vanity URLs\nfor APIs is fine, what isn’t  fine is you can’t assign an SSL certificate to an\nIP address. That means whichever app consuming your API will fail because your\napp surely has a cert of its own, and HTTPS > HTTP  API calls just aren’t gonna\nhappen.\n\nCaddy trivializes SSL certs to the point where you might not notice you’ve\nacquired one. Any host specified in a Caddyfile  immediately receives an SSL\ncert, but we'll get to that in a moment.\n\nCaddy’s download page is like a shopping cart for which things you might want\nyour web server to do. Proxies, CORS, you name it: just put it in the (free)\nshopping cart:\n\nSoon we won't even have to write code at all!Selecting your platform, plugins, and license  will provide you with a\nconvenient one-liner which downloads your exact package, unpacks, and installs\nit on your machine. For example, a Caddy installation for Linux with no bells or\nwhistles looks like this:\n\ncurl https://getcaddy.com | bash\n\n\nThis will install Caddy, which leaves only some trivial configuration before\nyou're up and running.\n\nConfiguring Your Caddyfile\nCaddy is configured via what is simply named Caddyfile, a file which can\nconveniently live in your project folder, as opposed to a distant land called \n/etc/nginx/sites-enabled. Go ahead and create your Caddy file.\n\nThe first line in our Caddyfile config is both simple and magic. It contains\nmerely the domain you’re intending to listen on, such something like \nhackersandslackers.com.  No matter what else happens in your config, the mere\nexistence of this line will generate an SSL cert for you when you run caddy.\n\nYou can serve content via any method that Nginx  or Apache  can, albeit much\neasier. A few examples:\n\n * root path/to/project  points your DNS to serve HTTP out a root folder.\n * websocket path/to/socket command  will serve an application via the specified\n   websocket.\n * rewrite [/original/folder/path] [/new/folder/path]  will reroute internal\n   requests made to origin A to origin B,\n\nThe point I’m trying to make here is that no matter what your desired\nconfiguration might be, it’s dead simple and likely won’t exceed more than 5\nlines.\n\nServing Our Docker Container via Proxy\nIf you’re using Node, chances are you’re going for a proxy configuration. In my\ncase I had no choice: I somehow needed to interact with an HTTP  url, while also\npassing the authentication headers necessary to make the app work. Luckily, this\nis trivial:\n\nexample.com\n\nproxy example.com proxy example.com localhost:4466/my_api/prod {\n transparent\n} \n\nerrors proxieserrors.log\n\nYes, really. Our proxy  block simply creates a proxy from  example.com, and\nserves localhost:4466/my_api/prod.\n\ntransparent  is a magic phrase which passes through all our headers to the\ntarget. It's shorthand for the following:\n\nheader_upstream Host {host}\nheader_upstream X-Real-IP {remote}\nheader_upstream X-Forwarded-For {remote}\nheader_upstream X-Forwarded-Port {server_port}\nheader_upstream X-Forwarded-Proto {scheme}\n\nDespite our Docker app requiring an authentication token to work hitting \nexample.com  will still result in a working endpoint thanks to the headers we're\npushing upstream.\n\nI even went the extra mile to include errors proxieserrors.log  as a way to log\nerrors. I didn't even need to. I only even got two errors total: Caddy works\nobnoxiously well.\n\nIn case you need anything more, I’d recommend reading the documentation\n[https://caddyserver.com/docs/proxy]. Even then, this basically summarizes the\nthings you can potentially configure:\n\nproxy from to... {\n\tpolicy name [value]\n\tfail_timeout duration\n\tmax_fails integer\n\tmax_conns in≈teger\n\ttry_duration duration\n\ttry_interval duration\n\thealth_check path\n\thealth_check_port port\n\thealth_check_interval interval_duration\n\thealth_check_timeout timeout_duration\n\tfallback_delay delay_duration\n\theader_upstream name value\n\theader_downstream name value\n\tkeepalive number\n\ttimeout duration\n\twithout prefix\n\texcept ignored_paths...\n\tupstream to\n\tca_certificates certs...\n\tinsecure_skip_verify\n\tpreset\n}\n\nRun Caddy And Never Worry About It Again\nSaving your Caddyfile  and running $ caddy  will issue your cert, and run Caddy\nas a process. This will result in a dialogue letting  you know that Caddy is\nlistening on both ports 80  and 443.\n\nCaddy won’t run as a background process by default. To do this, simply use the\ncommand $ nohup caddy &  and you're good to go.","html":"<p>The past few years of software development and architecture has witnessed multiple revolutions. The rise of containers, unruly package management ecosystems, and one-click-deployments holds an unspoken narrative: most people probably don’t care about <em>how</em> things work beneath the top layer. Sure, advancements in application infrastructure has undoubtedly made our lives easier. I suppose I find this lack of curiosity and unwillingness to dig deeper into the innards, an unrelatable trait. Yet I digress.</p><p>I’ve never found web server configurations to be particularly difficult, but apparently most consider this enough of a nuisance to make something even easier to use. That’s where I came across <a href=\"https://caddyserver.com/\" rel=\"noopener\">Caddy</a>.</p><p><strong>Caddy</strong> is a web server and free SSL service in which most of the actual work happens via their <a href=\"https://caddyserver.com/download\" rel=\"noopener\">download GUI</a>. It’s great. Even though I never expected us to reach a place where <code>apt install nginx</code> and <code>apt install certbot</code> is considered too much of a burden, it only took a few minutes of wrestling with a Docker container running on a VPS that I realized there was a better way.</p><h2 id=\"serve-anything-with-caddy\">Serve Anything With Caddy</h2><p>In my particular example, the Docker container I was running produced an API endpoint. For some reason, this service forcefully insists that this endpoint is your machine’s <em>localhost</em>, or it simply won’t work. While scoffing at vanity URLs for APIs is fine, what <em>isn’t</em> fine is <em>you can’t assign an SSL certificate to an IP address. </em>That means whichever app consuming your API will fail because your app <em>surely </em>has a cert of its own, and <strong>HTTPS &gt; HTTP</strong> API calls just aren’t gonna happen.</p><p>Caddy trivializes SSL certs to the point where you might not notice you’ve acquired one. Any host specified in a <code>Caddyfile</code> immediately receives an SSL cert, but we'll get to that in a moment.</p><p>Caddy’s download page is like a shopping cart for which things you might want your web server to do. Proxies, CORS, you name it: just put it in the (free) shopping cart:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/caddy-download.png\" class=\"kg-image\"><figcaption>Soon we won't even have to write code at all!</figcaption></figure><!--kg-card-end: image--><p>Selecting your <strong>platform</strong>, <strong>plugins</strong>, and <strong>license</strong> will provide you with a convenient one-liner which downloads your exact package, unpacks, and installs it on your machine. For example, a Caddy installation for Linux with no bells or whistles looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">curl https://getcaddy.com | bash\n</code></pre>\n<!--kg-card-end: markdown--><p>This will install Caddy, which leaves only some trivial configuration before you're up and running.</p><h2 id=\"configuring-your-caddyfile\">Configuring Your Caddyfile</h2><p>Caddy is configured via what is simply named <code>Caddyfile</code>, a file which can conveniently live in your project folder, as opposed to a distant land called <code>/etc/nginx/sites-enabled</code>. Go ahead and create your Caddy file.</p><p>The first line in our Caddyfile config is both simple and magic. It contains merely the domain you’re intending to listen on, such something like <em>hackersandslackers.com.</em> No matter what else happens in your config, the mere existence of this line will generate an SSL cert for you when you run caddy.</p><p>You can serve content via any method that <strong>Nginx</strong> or <strong>Apache</strong> can, albeit much easier. A few examples:</p><ul><li><code>root path/to/project</code> points your DNS to serve HTTP out a root folder.</li><li><code>websocket path/to/socket command</code> will serve an application via the specified websocket.</li><li><code>rewrite [/original/folder/path] [/new/folder/path]</code> will reroute internal requests made to origin A to origin B,</li></ul><p>The point I’m trying to make here is that no matter what your desired configuration might be, it’s dead simple and likely won’t exceed more than 5 lines.</p><h2 id=\"serving-our-docker-container-via-proxy\">Serving Our Docker Container via Proxy</h2><p>If you’re using Node, chances are you’re going for a proxy configuration. In my case I had no choice: I somehow needed to interact with an <strong>HTTP</strong> url, while also passing the authentication headers necessary to make the app work. Luckily, this is trivial:</p><!--kg-card-begin: code--><pre><code>example.com\n\nproxy example.com proxy example.com localhost:4466/my_api/prod {\n transparent\n} \n\nerrors proxieserrors.log</code></pre><!--kg-card-end: code--><p>Yes, really. Our <code>proxy</code> block simply creates a proxy <em>from</em> <code>example.com</code>, and serves <code>localhost:4466/my_api/prod</code>.</p><p><code>transparent</code> is a magic phrase which passes through all our headers to the target. It's shorthand for the following:</p><!--kg-card-begin: code--><pre><code>header_upstream Host {host}\nheader_upstream X-Real-IP {remote}\nheader_upstream X-Forwarded-For {remote}\nheader_upstream X-Forwarded-Port {server_port}\nheader_upstream X-Forwarded-Proto {scheme}</code></pre><!--kg-card-end: code--><p>Despite our Docker app requiring an authentication token to work hitting <code>example.com</code> will still result in a working endpoint thanks to the headers we're pushing upstream.</p><p>I even went the extra mile to include <code>errors proxieserrors.log</code> as a way to log errors. I didn't even need to. I only even got two errors total: Caddy works obnoxiously well.</p><p>In case you need anything more, I’d recommend reading <a href=\"https://caddyserver.com/docs/proxy\" rel=\"noopener\">the documentation</a>. Even then, this basically summarizes the things you can potentially configure:</p><!--kg-card-begin: code--><pre><code>proxy from to... {\n\tpolicy name [value]\n\tfail_timeout duration\n\tmax_fails integer\n\tmax_conns in≈teger\n\ttry_duration duration\n\ttry_interval duration\n\thealth_check path\n\thealth_check_port port\n\thealth_check_interval interval_duration\n\thealth_check_timeout timeout_duration\n\tfallback_delay delay_duration\n\theader_upstream name value\n\theader_downstream name value\n\tkeepalive number\n\ttimeout duration\n\twithout prefix\n\texcept ignored_paths...\n\tupstream to\n\tca_certificates certs...\n\tinsecure_skip_verify\n\tpreset\n}</code></pre><!--kg-card-end: code--><h2 id=\"run-caddy-and-never-worry-about-it-again\">Run Caddy And Never Worry About It Again</h2><p>Saving your <code>Caddyfile</code> and running <code>$ caddy</code> will issue your cert, and run Caddy as a process. This will result in a dialogue letting  you know that Caddy is listening on both ports <code>80</code> and <code>443</code>.</p><p>Caddy won’t run as a background process by default. To do this, simply use the command <code>$ nohup caddy &amp;</code> and you're good to go.</p>","url":"https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/","uuid":"5b609f96-d76d-4b8b-943e-d470ee414d97","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c85a8da181da30210ceca9d"}},{"node":{"id":"Ghost__Post__5c79b0070fa2b110f256e320","title":"Running Jupyter Notebooks on a Ubuntu Server","slug":"running-jupyter-notebooks-on-a-ubuntu-server","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","custom_excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","created_at_pretty":"01 March, 2019","published_at_pretty":"02 March, 2019","updated_at_pretty":"24 March, 2019","created_at":"2019-03-01T17:19:51.000-05:00","published_at":"2019-03-01T21:15:40.000-05:00","updated_at":"2019-03-23T22:30:42.000-04:00","meta_title":"Running Jupyter Notebooks on a Ubuntu Server | Hackers and Slackers","meta_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","og_title":"Running Jupyter Notebooks on a Ubuntu Server","twitter_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","twitter_title":"Running Jupyter Notebooks on a Ubuntu Server","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"It dawned on me the other day that for a publication which regularly uses and\ntalks about Jupyter notebooks [https://jupyter.org/], we’ve never actually taken\nthe time to explain what they are or how to start using them. No matter where\nyou may have been in your career, first exposure to Jupyter and the IPython\n[https://ipython.org/]  shell is often a confusingly magical experience. Writing\nprograms line-by-line and receiving feedback in real-time feels more like\npainting oil on canvas and programming. I suppose we can finally chalk up a win\nfor dynamically typed languages.\n\nThere are a couple of barriers for practical devs to overcome before using\nJupyter, the most obvious being hardware costs. If you’re utilizing a full \nAnaconda  installation, chances are you’re not the type of person to mess\naround. Real machine learning algorithms take real resources, and real resources\ntake real money. A few vendors have popped up here are offering managed\ncloud-hosted notebooks for this reason. For those of us who bothered to do the\nmath, it turns out most of these services are more expensive than spinning up a\ndedicated VPS.\n\nData scientists with impressive machines have no problem running notebooks\nlocally for most use cases. While that’s fine and good for scientists, this\nsetup is problematic for those of us with commitments to Python outside of\nnotebooks. Upon installation, Anaconda barges into your system’s ~/.bash_profile\n, shouts “I am the captain now,”  and crowns itself as your system’s default\nPython path. Conda and Pip have some trouble getting along, so for those of us\nwho build Python applications and use notebooks, it's best to keep these things\nisolated.\n\nSetting Up a VPS\nWe're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted\nfor DigitalOcean  in my case, both for simplicity and the fact that I'm\nincredibly broke. Depending on how broke you may or may not be, this is where\nyou'll have to make a judgment call for your system resources:\n\nMy kind sir, I would like to order the most exquisite almost-cheapest Droplet on\nthe menuSSH into that bad boy. You know what to do next:\n\n$ sudo apt update\n$ sudo apt upgrade\n\n\nWith that out of the way, next we'll grab the latest version of Python:\n\n$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n\n\nFinally, we'll open port 8888 for good measure, since this is the port Jupyter\nruns on:\n\n$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw status\n\n\nTo                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n\n\nCreate a New User\nAs always, we should create a Linux user besides root to do just about anything:\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n\n\nThen, add them to the sudoers  group:\n\n$ usermod -aG sudo myuser\n\n\nLog in as the user:\n\n$ su - myuser\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\n\nInstall The Latest Anaconda Distribution\nAnaconda comes with all the fantastic Data Science Python packages we'll need\nfor our notebook. To find the latest distribution, check here: \nhttps://www.anaconda.com/download/. We'll install this to a /tmp  folder:\n\ncd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n\n\nOnce downloaded, begin the installation:\n\n$ sh Anaconda3-2018.12-Linux-x86_64.sh\n\n\nComplete the resulting prompts:\n\nWelcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n>>>\n\n\nGet ready for the wall of text....\n\n===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n\n\nThis kicks off a rather lengthy install process. Afterward, you'll be prompted\nto add Conda to your startup script. Say yes:\n\ninstallation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n\n\nThe final part of the installation will ask if you'd like to install VS Code.\nDecline this offer because Microsoft sucks.\n\nFinally, reload your /.bashrc file to get apply Conda's changes:\n\n$ source ~/.bashrc\n\n\nSetting Up Conda Environments\nConda installations can be isolated to separate environments similarly to the\nway Pipenv might handle this. Create and activate a Conda env:\n\n$ conda create --name myenv python=3\n$ source activate myenv\n\n\nCongrats, you're now in an active Conda environment!\n\nStarting Up Jupyter\nMake sure you're in a directory you'd like to be running Jupyter in. Entering \njupyter notebook  in this directory should result in the following:\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n\n\nThis next part is tricky. To run our notebook, we need to reconnect to our VPS\nvia an SSH tunnel. Close the terminal and reconnect to your server with the\nfollowing format:\n\nssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nIndeed, localhost  is intended to stay the same, but your_server_ip  is to be\nreplaced with the address of your server.\n\nWith that done, let's try this one more time. Remember to reactivate your Conda\nenvironment first!\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n\n\nThis time around, the links which appear in the terminal should work!\n\nWE DID ITBONUS ROUND: Theme Your Notebooks\nIf ugly interfaces bother you as much as they bother me, I highly recommend\ntaking a look at the jupyter-themes package on Github\n[https://github.com/dunovank/jupyter-themes]. This package allows you to\ncustomize the look and feel of your notebook, either as simple as activating a\nstyle, or as complex as setting your margin width. I highly recommend checking\nout the available themes to spice up your notebook!","html":"<p>It dawned on me the other day that for a publication which regularly uses and talks about <a href=\"https://jupyter.org/\">Jupyter notebooks</a>, we’ve never actually taken the time to explain what they are or how to start using them. No matter where you may have been in your career, first exposure to Jupyter and the <a href=\"https://ipython.org/\">IPython</a> shell is often a confusingly magical experience. Writing programs line-by-line and receiving feedback in real-time feels more like painting oil on canvas and programming. I suppose we can finally chalk up a win for dynamically typed languages.</p><p>There are a couple of barriers for practical devs to overcome before using Jupyter, the most obvious being hardware costs. If you’re utilizing a full <strong>Anaconda</strong> installation, chances are you’re not the type of person to mess around. Real machine learning algorithms take real resources, and real resources take real money. A few vendors have popped up here are offering managed cloud-hosted notebooks for this reason. For those of us who bothered to do the math, it turns out most of these services are more expensive than spinning up a dedicated VPS.</p><p>Data scientists with impressive machines have no problem running notebooks locally for most use cases. While that’s fine and good for scientists, this setup is problematic for those of us with commitments to Python outside of notebooks. Upon installation, Anaconda barges into your system’s <code>~/.bash_profile</code>, shouts <strong><em>“I am the captain now,”</em></strong> and crowns itself as your system’s default Python path. Conda and Pip have some trouble getting along, so for those of us who build Python applications and use notebooks, it's best to keep these things isolated.</p><h2 id=\"setting-up-a-vps\">Setting Up a VPS</h2><p>We're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted for <strong>DigitalOcean</strong> in my case, both for simplicity and the fact that I'm incredibly broke. Depending on how broke you may or may not be, this is where you'll have to make a judgment call for your system resources:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/digitaloceanvps.png\" class=\"kg-image\"><figcaption>My kind sir, I would like to order the most exquisite almost-cheapest Droplet on the menu</figcaption></figure><!--kg-card-end: image--><p>SSH into that bad boy. You know what to do next:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt update\n$ sudo apt upgrade\n</code></pre>\n<!--kg-card-end: markdown--><p>With that out of the way, next we'll grab the latest version of Python:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we'll open port 8888 for good measure, since this is the port Jupyter runs on:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw status\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">To                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-new-user\">Create a New User</h3><p>As always, we should create a Linux user besides root to do just about anything:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Adding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n</code></pre>\n<!--kg-card-end: markdown--><p>Then, add them to the <strong>sudoers</strong> group:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ usermod -aG sudo myuser\n</code></pre>\n<!--kg-card-end: markdown--><p>Log in as the user:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ su - myuser\nTo run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.\nSee &quot;man sudo_root&quot; for details.\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"install-the-latest-anaconda-distribution\">Install The Latest Anaconda Distribution</h3><p>Anaconda comes with all the fantastic Data Science Python packages we'll need for our notebook. To find the latest distribution, check here: <a href=\"https://www.anaconda.com/download/\">https://www.anaconda.com/download/</a>. We'll install this to a <code>/tmp</code> folder:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">cd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Once downloaded, begin the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sh Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Complete the resulting prompts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Welcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n&gt;&gt;&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Get ready for the wall of text....</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>This kicks off a rather lengthy install process. Afterward, you'll be prompted to add Conda to your startup script. Say <strong>yes</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">installation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>The final part of the installation will ask if you'd like to install VS Code. Decline this offer because Microsoft sucks.</p><p>Finally, reload your <strong>/.bashrc </strong>file to get apply Conda's changes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ source ~/.bashrc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"setting-up-conda-environments\">Setting Up Conda Environments</h3><p>Conda installations can be isolated to separate environments similarly to the way Pipenv might handle this. Create and activate a Conda env:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda create --name myenv python=3\n$ source activate myenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Congrats, you're now in an active Conda environment!</p><h3 id=\"starting-up-jupyter\">Starting Up Jupyter</h3><p>Make sure you're in a directory you'd like to be running Jupyter in. Entering <code>jupyter notebook</code> in this directory should result in the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n</code></pre>\n<!--kg-card-end: markdown--><p>This next part is tricky. To run our notebook, we need to reconnect to our VPS via an SSH tunnel. Close the terminal and reconnect to your server with the following format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, <code>localhost</code> is intended to stay the same, but <code>your_server_ip</code> is to be replaced with the address of your server.</p><p>With that done, let's try this one more time. Remember to reactivate your Conda environment first!</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n</code></pre>\n<!--kg-card-end: markdown--><p>This time around, the links which appear in the terminal should work!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-01-at-7.01.42-PM.png\" class=\"kg-image\"><figcaption>WE DID IT</figcaption></figure><!--kg-card-end: image--><h2 id=\"bonus-round-theme-your-notebooks\">BONUS ROUND: Theme Your Notebooks</h2><p>If ugly interfaces bother you as much as they bother me, I highly recommend taking a look at the <a href=\"https://github.com/dunovank/jupyter-themes\">jupyter-themes package on Github</a>. This package allows you to customize the look and feel of your notebook, either as simple as activating a style, or as complex as setting your margin width. I highly recommend checking out the available themes to spice up your notebook!</p><!--kg-card-begin: gallery--><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/gruvbox-dark-python.png\" width=\"1013\" height=\"903\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/grade3_table.png\" width=\"1293\" height=\"809\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/jtplotDark_reach.png\" width=\"8400\" height=\"3600\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/oceans16_code_headers.png\" width=\"1293\" height=\"808\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/onedork_code_headers.png\" width=\"1293\" height=\"808\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/solarized-dark_iruby.png\" width=\"951\" height=\"498\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/chesterish_code_headers.png\" width=\"1293\" height=\"808\"></div></div></div></figure><!--kg-card-end: gallery-->","url":"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/","uuid":"0cfc9046-2e28-46a2-9f95-8851a9aea770","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c79b0070fa2b110f256e320"}},{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},{"node":{"id":"Ghost__Post__5c570ae30b20340296f57709","title":"Easily Build GraphQL APIs with Prisma","slug":"easily-build-graphql-apis-with-prisma","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","custom_excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","created_at_pretty":"03 February, 2019","published_at_pretty":"03 February, 2019","updated_at_pretty":"29 March, 2019","created_at":"2019-02-03T10:38:11.000-05:00","published_at":"2019-02-03T16:33:15.000-05:00","updated_at":"2019-03-29T14:47:01.000-04:00","meta_title":"Build GraphQL APIs with Prisma | Hackers and Slackers","meta_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","og_title":"Build GraphQL APIs with Prisma","twitter_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","twitter_title":"Build GraphQL APIs with Prisma","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"The technology sector is reeling after an official statement was released by the\nUN's International Council of Coolness last week. The statement clearly states\nwhat status-quo developers have feared for months: if you haven't shifted from\nREST to GraphQL by now, you are officially recognized by the international\ncommunity to hold \"uncool\" status. A humanitarian crisis is already unfolding as\nrefugees of coolness are threatening to overtake borders, sparking fears of an\ninflux of Thinkpad Laptops, IntelliJ, and other Class A  uncool narcotics.\n\nHold up: is GraphQL That Dramatic of an Improvement over REST?\nIn all honesty, I've found that the only way to properly answer this question is\nto first utter \"kinda,\" then mull back and forth for a little while, and then\nfinishing with a weak statement like \"so pretty much, yeah.\"\n\nLet’s put it this way. When you’re first familiarizing yourself with a set of\ndata, what do you do? Do you read extensive documentation about the SQL table\nyou’re about to check out? Do you read the entire spec for your version\nPostgreSQL to see if it contains the functionality that might be missing for\nsome reason? I’m going to guess you do neither of these- chances are you just\nlook at the data. \n\nUsing any REST API is inherently a context-switch. No matter how many APIs\nyou’ve worked with in the past, you’ll never be able to know a new API’s\nendpoints, quirks, or the awful manner in which the creator has abandoned any\ndistinction between GET, POST, or PUT methods altogether. GraphQL is not\nnecessarily more technologically impressive than REST, but it does  provide us a\nsyntax and workflow comparable to working directly with databases with which\nwe're already familiar.\n\nRemember when us young guys justified replacing older devs when we came out of\nthe gate with NodeJS, arguing that context-switching changes everything? GraphQL\nis just that: a \"better\" technology with less mental context-switching, which\nconveniently serves a double-purpose for enterprises looking to fire anybody\nthey perceive to be dead weight over the age of 30. Good luck finding a better\nsynopsis than that.\n\nWhat’s this Prisma Nonsense? \nPrisma [https://www.prisma.io/]  is a free (FREE!) service that provides with\nthe tools to create an API client, as well as an Admin panel to manage it.\nWithout any prior knowledge of GraphQL needed, Prisma provides us with:\n\n * A CLI which’s stand up a web server which will serve as our API: either cloud\n   or self-hosted.\n * Automatic integration with your database of choice (including cloud DBs, such\n   as RDS).\n * A clever obfuscation of data models via a simple config file. No classes, no\n   code, no bullshit.\n * A \"playground\" interface which allows us to mess around in GraphQL syntax\n   against our models without breaking everything.\n * A web GUI which displays the relationships between all of these things and\n   their usage.\n\nIn short, Prisma does our jobs for us. Now that tasks associated with building\nAPIs, creating ORMs, and managing databases have all been trivialized, we can\nfinally cut some more of that dead weight we mentioned earlier- specifically\nBob, the asshole coming up on his 35th birthday sitting on his high-horse just\nbecause he has an obligation to feed 3 beautiful children. Sorry Bob, it just\nwasn't working out.\n\nPrisma does  provide the option to set up a test environment on their cloud, but\nlet's do something useful with our lives for once and build something\nproduction-ready. In this case, that means standing up a 5-dollar Digital Ocean\nDroplet.\n\nCreate a Prisma Account\nGet over to the sexually appealing Prisma Cloud landing page\n[https://www.prisma.io/cloud]  and make yourself an account. When prompted, make\nsure you select Deploy a new Prisma Service.\n\nExample services are for sissys.You should then be prompted with the following\nscreen. It will instruct you to install an NPM package, but there are a few\nthings we need to do first.\n\nWhen she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my\nlove.\"Installing Prisma Dependencies on a Fresh VPS\nSSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this\ntutorial. If you happen to be using Ubuntu as well, feel free to copy + paste\nall the stuff I'm sure you've done a million times already. First, we need to\ninstall Node:\n\n$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n\n\nBefore you do anything crazy like copy & paste those two lines from Prisma,\nyou're going to need to set up Docker a few steps later, so you might as well do\nthat now: \n\n1. Install Docker Dependencies\n$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n\n2. Add Docker Key\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n\n\n3. Get Docker Repository\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\n\n4. Finally Install Docker\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGood job, you're doing great.\n\nInstall & Activate The Prisma CLI\nCool, now we can carry on with Prisma's demands. Install the Prisma CLI\nglobally, and then use said CLI to log in to Prisma.\n\n$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n\n\nWait a couple of seconds after entering the login prompt, and you'll notice your\nbrowser window will have changed to indicate that you're now logged in. \n\nThe next step will create the local files which serve as the heart and soul of\nour API. Make sure you init Prisma  in whichever directory you like to keep\nthings in:\n\n$ cd /my/desired/directory/\n$ prisma init my-prisma\n\n\nInitiating the project will kickstart a quick and painless interrogation\nprocess. Keep in mind that it's recommended to use Prisma with a fresh database\ninstance; in my case, I spun up a cloud PostgreSQL instance.\n\n? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n\n\nCompleting this will result in the following structure:\n\nmy-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n\n\nWe're almost there cowboy and/or cowgirl. \n\nSet Phasers to \"Deploy\"\nPrisma is going to stand itself up on port 4466, which is closed by default on\nmost servers. Make sure you have this port open:\n\n$ ufw allow 4466\n\n\nFinally, we need to set a secret  in order to connect to Prisma cloud. Open the \ndocker-compose.yml  file and uncomment the managementApiSecret  line. Replace\nthe value with some sort of deep dark personal secret of yours.\n\n$ vim docker-compose.yml\n\n\nversion: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - \"4466:4466\"\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n\n\nFor some reason, Prisma does not automatically specify your SSL preferences in\ndocker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your\ndatabase requires SSL, be sure to add ssl: true  to the end of your\ndocker-compose config. Otherwise, your deployment will fail.As per the comment\nin our yml  file, we need to export the secret we specify with \nmanagementApiSecret: my-secret  as an environment variable. Back in your Prisma\ndirectory, export your secret as such:\n\n$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n\n\nThis secret is used to generate a token to secure our endpoint. Skipping this\nstep would result in exposing your database to the world with full read/write\naccess to anybody.\n\nIt's Game Time\nIt's time to deploy, baby! Do it, push the button! DO IT NOW!\n\n$ docker-compose up -d\n$ prisma deploy\n\n\nDeploying for the first time does a few things. It'll stand up a 'playground'\ninterface on your local server (localhost:4466), as well as automatically get\nyou set up with Prisma Cloud, which is essentially an admin interface for your\ndeployment hosted on Prisma's site.\n\nCheck Out Your Workspace \nVisit [Your Server's IP]:4466 to see what you've done:\n\nA playground for children of all agesCheck it out! Along with documentation of\nthe generic data models Prisma shipped with, you can test queries or mutations\non the left side of the UI, and receive responses on the right. Sure beats\nPostman imho.\n\nDon't Look Down: You're in the Cloud\nYou can now add your server to Prisma Cloud to get the benefits of their admin\npanel. From here, you can modify information directly, review usage metrics, and\nmanage multiple instances:\n\nBreaking News: Prisma is Too Cool For School.Working With Prisma And GraphQL\nNow that we've spun up this shiny new toy, let's be sure we know how to drive\nit.\n\nOn your VPS, take a look at the datamodels.prisma  file:\n\n$ vim datamodels.prisma\n\n\nYou should see a data model called User (everybody has this model). To add or\nmodify data models, all we need to do is change the fields as we see fit, set\ntheir data type, and specify whether or not we'd like the field to be unique.\nBelow I've added a couple of new 'fields.'\n\ntype User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n\n\nDeploying Prisma again with these changes will modify our database's table\nstructure to match the new model:\n\n$ prisma deploy\n\n\nThere you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to\ncompletely falsify the existence of a GraphQL certification and throw that on\nthere, too. If you're the kind of person who enjoys reading technical posts like\nthis in your free time, chances are you're already qualified for the job. Unless\nyou're Bob.","html":"<p>The technology sector is reeling after an official statement was released by the UN's International Council of Coolness last week. The statement clearly states what status-quo developers have feared for months: if you haven't shifted from REST to GraphQL by now, you are officially recognized by the international community to hold \"uncool\" status. A humanitarian crisis is already unfolding as refugees of coolness are threatening to overtake borders, sparking fears of an influx of Thinkpad Laptops, IntelliJ, and other <em>Class A</em> uncool narcotics.</p><h3 id=\"hold-up-is-graphql-that-dramatic-of-an-improvement-over-rest\">Hold up: is GraphQL That Dramatic of an Improvement over REST?</h3><p>In all honesty, I've found that the only way to properly answer this question is to first utter \"kinda,\" then mull back and forth for a little while, and then finishing with a weak statement like \"so pretty much, yeah.\"</p><p>Let’s put it this way. When you’re first familiarizing yourself with a set of data, what do you do? Do you read extensive documentation about the SQL table you’re about to check out? Do you read the entire spec for your version PostgreSQL to see if it contains the functionality that might be missing for some reason? I’m going to guess you do neither of these- chances are you <em>just look at the data. </em></p><p>Using any REST API is inherently a context-switch. No matter how many APIs you’ve worked with in the past, you’ll never be able to know a new API’s endpoints, quirks, or the awful manner in which the creator has abandoned any distinction between GET, POST, or PUT methods altogether. GraphQL is not necessarily more technologically impressive than REST, but it <em>does</em> provide us a syntax and workflow comparable to working directly with databases with which we're already familiar.</p><p>Remember when us young guys justified replacing older devs when we came out of the gate with NodeJS, arguing that context-switching <em>changes everything</em>? GraphQL is just that: a \"better\" technology with less mental context-switching, which conveniently serves a double-purpose for enterprises looking to fire anybody they perceive to be dead weight over the age of 30. Good luck finding a better synopsis than that.</p><h2 id=\"what-s-this-prisma-nonsense\">What’s this Prisma Nonsense? </h2><p><a href=\"https://www.prisma.io/\">Prisma</a> is a free (FREE!) service that provides with the tools to create an API client, as well as an Admin panel to manage it. Without any prior knowledge of GraphQL needed, Prisma provides us with:</p><ul><li>A CLI which’s stand up a web server which will serve as our API: either cloud or self-hosted.</li><li>Automatic integration with your database of choice (including cloud DBs, such as RDS).</li><li>A clever obfuscation of data models via a simple config file. No classes, no code, no bullshit.</li><li>A \"playground\" interface which allows us to mess around in GraphQL syntax against our models without breaking everything.</li><li>A web GUI which displays the relationships between all of these things and their usage.</li></ul><p>In short, Prisma does our jobs for us. Now that tasks associated with building APIs, creating ORMs, and managing databases have all been trivialized, we can finally cut some more of that dead weight we mentioned earlier- specifically Bob, the asshole coming up on his 35th birthday sitting on his high-horse just because he has an obligation to feed 3 beautiful children. Sorry Bob, it just wasn't working out.</p><p>Prisma <em>does</em> provide the option to set up a test environment on their cloud, but let's do something useful with our lives for once and build something production-ready. In this case, that means standing up a 5-dollar Digital Ocean Droplet.</p><h3 id=\"create-a-prisma-account\">Create a Prisma Account</h3><p>Get over to the sexually appealing <a href=\"https://www.prisma.io/cloud\">Prisma Cloud landing page</a> and make yourself an account. When prompted, make sure you select <strong>Deploy a new Prisma Service</strong>.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step1.png\" class=\"kg-image\"><figcaption>Example services are for sissys.</figcaption></figure><!--kg-card-end: image--><p>You should then be prompted with the following screen. It will instruct you to install an NPM package, but there are a few things we need to do first.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step2.png\" class=\"kg-image\"><figcaption>When she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my love.\"</figcaption></figure><!--kg-card-end: image--><h2 id=\"installing-prisma-dependencies-on-a-fresh-vps\">Installing Prisma Dependencies on a Fresh VPS</h2><p>SSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this tutorial. If you happen to be using Ubuntu as well, feel free to copy + paste all the stuff I'm sure you've done a million times already. First, we need to install Node:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n</code></pre>\n<!--kg-card-end: markdown--><p>Before you do anything crazy like copy &amp; paste those two lines from Prisma, you're going to need to set up Docker a few steps later, so you might as well do that now: </p><h3 id=\"1-install-docker-dependencies\">1. Install Docker Dependencies</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"2-add-docker-key\">2. Add Docker Key</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"3-get-docker-repository\">3. Get Docker Repository</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo add-apt-repository \\\n   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable&quot;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"4-finally-install-docker\">4. Finally Install Docker</h3><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre>\n<!--kg-card-end: markdown--><p>Good job, you're doing great.</p><h2 id=\"install-activate-the-prisma-cli\">Install &amp; Activate The Prisma CLI</h2><p>Cool, now we can carry on with Prisma's demands. Install the Prisma CLI globally, and then use said CLI to log in to Prisma.  </p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n</code></pre>\n<!--kg-card-end: markdown--><p>Wait a couple of seconds after entering the login prompt, and you'll notice your browser window will have changed to indicate that you're now logged in. </p><p>The next step will create the local files which serve as the heart and soul of our API. Make sure you init <code>Prisma</code> in whichever directory you like to keep things in:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd /my/desired/directory/\n$ prisma init my-prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>Initiating the project will kickstart a quick and painless interrogation process. Keep in mind that it's recommended to use Prisma with a fresh database instance; in my case, I spun up a cloud PostgreSQL instance.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n</code></pre>\n<!--kg-card-end: markdown--><p>Completing this will result in the following structure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n</code></pre>\n<!--kg-card-end: markdown--><p>We're almost there cowboy and/or cowgirl. </p><h2 id=\"set-phasers-to-deploy\">Set Phasers to \"Deploy\"</h2><p>Prisma is going to stand itself up on port <strong>4466</strong>, which is closed by default on most servers. Make sure you have this port open:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ ufw allow 4466\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we need to set a <em>secret</em> in order to connect to Prisma cloud. Open the <code>docker-compose.yml</code> file and uncomment the <code>managementApiSecret</code> line. Replace the value with some sort of deep dark personal secret of yours.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim docker-compose.yml\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">version: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - &quot;4466:4466&quot;\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"proptip\">\nFor some reason, Prisma does not automatically specify your SSL preferences in docker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your database requires SSL, be sure to add <code>ssl: true</code> to the end of your docker-compose config. Otherwise, your deployment will fail. \n</div><!--kg-card-end: html--><p>As per the comment in our <code>yml</code> file, we need to export the secret we specify with <code>managementApiSecret: my-secret</code> as an environment variable. Back in your Prisma directory, export your secret as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n</code></pre>\n<!--kg-card-end: markdown--><p>This secret is used to generate a token to secure our endpoint. Skipping this step would result in exposing your database to the world with full read/write access to anybody.</p><h2 id=\"it-s-game-time\">It's Game Time</h2><p>It's time to deploy, baby! Do it, push the button! DO IT NOW!</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ docker-compose up -d\n$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying for the first time does a few things. It'll stand up a 'playground' interface on your local server (localhost:4466), as well as automatically get you set up with Prisma Cloud, which is essentially an admin interface for your deployment hosted on Prisma's site.</p><h3 id=\"check-out-your-workspace\">Check Out Your Workspace </h3><p>Visit [Your Server's IP]:4466 to see what you've done:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/playground.png\" class=\"kg-image\"><figcaption>A playground for children of all ages</figcaption></figure><!--kg-card-end: image--><p>Check it out! Along with documentation of the generic data models Prisma shipped with, you can test queries or mutations on the left side of the UI, and receive responses on the right. Sure beats Postman imho.</p><h3 id=\"don-t-look-down-you-re-in-the-cloud\">Don't Look Down: You're in the Cloud</h3><p>You can now add your server to Prisma Cloud to get the benefits of their admin panel. From here, you can modify information directly, review usage metrics, and manage multiple instances:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prismacloud.gif\" class=\"kg-image\"><figcaption>Breaking News: Prisma is Too Cool For School.</figcaption></figure><!--kg-card-end: image--><h2 id=\"working-with-prisma-and-graphql\">Working With Prisma And GraphQL</h2><p>Now that we've spun up this shiny new toy, let's be sure we know how to drive it.</p><p>On your VPS, take a look at the <code>datamodels.prisma</code> file:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim datamodels.prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>You should see a data model called User (everybody has this model). To add or modify data models, all we need to do is change the fields as we see fit, set their data type, and specify whether or not we'd like the field to be unique. Below I've added a couple of new 'fields.'</p><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">type User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying Prisma again with these changes will modify our database's table structure to match the new model:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>There you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to completely falsify the existence of a GraphQL certification and throw that on there, too. If you're the kind of person who enjoys reading technical posts like this in your free time, chances are you're already qualified for the job. Unless you're Bob.</p>","url":"https://hackersandslackers.com/easily-build-graphql-apis-with-prisma/","uuid":"86286c72-478c-4108-8bef-89ca01caf043","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c570ae30b20340296f57709"}},{"node":{"id":"Ghost__Post__5c34086694d3e847951adf3e","title":"Poetically Packaging Your Python Project","slug":"poetic-python-project-packaging","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","excerpt":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","custom_excerpt":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","created_at_pretty":"08 January, 2019","published_at_pretty":"08 January, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-01-07T21:18:14.000-05:00","published_at":"2019-01-08T10:16:00.000-05:00","updated_at":"2019-04-09T17:59:40.000-04:00","meta_title":"Poetically Packaging Your Python Project | Hackers and Slackers","meta_description":"Manage your projects with Poetry: a dependency manager and project packager all in one. Handle your environment and project data in a single file.","og_description":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","og_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","og_title":"Poetically Packaging Your Python Project","twitter_description":"Manage your projects with Poetry to handle dependencies, envs, packaging, etc.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/poetry4@2x.jpg","twitter_title":"Poetically Packaging Your Python Project","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn't long ago that we Hackers were singing the praises of Pipenv\n[https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/]\n: Python's seemingly superior dependency manager at the time. While we hold much\nlove in hearts, sometimes there is love to go around. We just so happen to be\nfair weather fans, which reminds me: what has Pipenv done for me lately?\n\nAs you've probably guessed (considering its a piece of software), nothing much.\nWell, there was that time when pip upgraded from v.18  to v.18.1, which broke\nPipenv entirely with almost minimal acknowledgment (for all I know this might\nstill be broken). As our lives seemed to fade, a miracle emerged from the ashes:\na young, smart, attractive alternative to Pipenv that's been whispering in my\near, and promising the world. Her name is Poetry [https://poetry.eustace.io/].\n\nWhat Light Through Yonder GitHub Breaks?\nPoetry stems from the genuine frustration that comes with not only managing\nenvironments and dependencies in Python, but the fact that even solving this\nproblem (albeit poorly) still doesn't solve the related tasks needing\nfulfillment when creating respectable Python projects. Consider Node's \npackage.json: a single file which contains a project's metadata, prod\ndependencies, dev dependencies, contact information, etc. Instead, Python\nprojects usually come with the following:\n\nSetup.py\nIf you've never bothered to publish a package to PyPI before, there's a decent\nchance you may not be very familiar with some of the nuances that come with \nsetup.py  or why you'd bother creating one. This is a losing mentality: we\nshould assume that most (or some) of the things we build might become useful\nenough to distribute some day.\n\nThus, we get this monstrosity:\n\nfrom setuptools import setup, find_packages, tests_require, packages, name\n\nwith open(\"README\", 'r') as f:\n    long_description = f.read()\n\nsetup = (\n    name='Fake Project',\n    version='1.0',\n    description='A fake project used for example purposes.',\n    long_description=long_description,\n    author='Todd Birchard',\n    author_email='todd@hackersandslackers.com',\n    maintainer='Some Loser',\n    maintainer_email='some.loser@example.com,\n    url=\"https://github.com/toddbirchard/fake-project\",\n    license='MIT',\n    include_package_data=True,\n    package_dir={'application'}\n    packages=['distutils', 'modules'],\n    tests_require=[\"pytest\"],\n    cmdclass={\"pytest\": PyTest},\n    classifiers=[\n          'Development Status :: 2 - Beta',\n          'Environment :: Console',\n          'Environment :: Web Environment',\n          'Intended Audience :: End Users/Desktop',\n          'Intended Audience :: Developers',\n          'Intended Audience :: System Administrators',\n          'License :: OSI Approved :: Python Software Foundation License',\n          'Operating System :: MacOS :: MacOS X',\n          'Operating System :: Microsoft :: Windows',\n          'Operating System :: POSIX',\n          'Programming Language :: Python',\n          'Topic :: Communications :: Email',\n          'Topic :: Office/Business',\n          'Topic :: Software Development :: Bug Tracking',\n          ],\n)\n\n\nMany of the metadata fields are rather self-explanatory. But what about the\nfields related to package dependencies, such as package_dir or packages? Wasn't\nthis already handled in our Pipfile? On top of that, we need to specify then the\ntest suite we're using via tests_require  and cmdclass? Short answer: pretty\nmuch.\n\nSetup.cfg\nThe real joke with setup.py  is that it needs its own configuration file: yes, a\nconfiguration file for your configuration file. setup.cfg, as the name\nsuggestions, sets even more granular configurations for the things mentioned in \nsetup.py, such as how pytest  should be handled, etc. Let's not get into it, but\nhere's an example:\n\n[coverage:run]\nomit = */test/*\n\n[flake8]\nexclude = *.egg*,.env,.git,.tox,_*,build*,dist*,venv*,python2/,python3/\nignore = E261,W503\nmax-line-length = 121\n\n[tool:pytest]\nminversion = 3.2\naddopts =\n  # --fulltrace\n  # -n auto\n  --cov-config=setup.cfg\n  --cov=httplib2\n  --noconftest\n  --showlocals\n  --strict\n  --tb=short\n  --timeout=17\n  --verbose\n  -ra\n\n\nPipfile and Pipfile.lock\nIf you have been using Pipenv, you'll recognize these files as being responsible\nfor setting your Python version and dependencies. But wait- didn't we also need\nto specify dependencies in setup.py?  Yes, we did. There is no God, but if there\nwere, he'd probably hate you. Here's all the work you'd need to do creating an\nacceptable Pipfile:\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nFlask-SQLAlchemy = \"*\"\npsycopg2 = \"*\"\npsycopg2-binary = \"*\"\nrequests = \"*\"\nconfigparser=\"*\"\nmapbox=\"*\"\nflask=\"*\"\npandas=\"*\"\nFlask-Assets=\"*\"\nlibsass=\"*\"\njsmin=\"*\"\ndash_core_components=\"*\"\ndash-table=\"*\"\ndash_html_components=\"*\"\ndash=\"*\"\nflask-session=\"*\"\nflask-redis=\"*\"\ngunicorn=\"*\"\npytest-flask=\"*\"\n\n\n[dev-packages]\n\n[requires]\npython_version = \"3.7.1\"\n\n\n\nBut wait, there's more!\n\nRequirements.txt\nBecause the Pipfile format has not been adopted as a standard for dependency\nmanagement, we still  need to create a requirements.txt file if we want to\ndeploy our application to respectable hosts such as Google App Engine  or\nwhat-have-you. So now we have this ugly son of a bitch from the stone age to\ndeal with as well:\n\natomicwrites==1.2.1\nattrs==18.2.0\nboto3==1.9.75\nbotocore==1.12.75\nCacheControl==0.12.5\ncertifi==2018.11.29\nchardet==3.0.4\nClick==7.0\nconfigparser==3.5.0\ndash==0.35.1\ndash-core-components==0.42.0\ndash-html-components==0.13.4\ndash-renderer==0.16.1\ndash-table==3.1.11\ndecorator==4.3.0\ndocutils==0.14\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Compress==1.4.0\nFlask-Redis==0.3.0\nFlask-Session==0.3.1\nFlask-SQLAlchemy==2.3.2\ngunicorn==19.9.0\nidna==2.8\nipython-genutils==0.2.0\niso3166==0.9\nitsdangerous==1.1.0\nJinja2==2.10\njmespath==0.9.3\njsmin==2.2.2\njsonschema==2.6.0\njupyter-core==4.4.0\nlibsass==0.17.0\nmapbox==0.17.2\nMarkupSafe==1.1.0\nmore-itertools==5.0.0\nmsgpack==0.6.0\nnbformat==4.4.0\nnumpy==1.15.4\npandas==0.23.4\nplotly==3.5.0\npluggy==0.8.0\npolyline==1.3.2\npsycopg2==2.7.6.1\npsycopg2-binary==2.7.6.1\npy==1.7.0\npytest==4.1.0\npytest-flask==0.14.0\npython-dateutil==2.7.5\npytz==2018.9\nredis==3.0.1\nrequests==2.21.0\nretrying==1.3.3\ns3transfer==0.1.13\nsix==1.12.0\nSQLAlchemy==1.2.15\ntraitlets==4.3.2\nuritemplate==3.0.0\nurllib3==1.24.1\nwebassets==0.12.1\nWerkzeug==0.14.1\n\n\nMANIFEST.in\nYES, THERE'S MORE. If you're not bothered by now, please leave this blog\nimmediately. The job market is ripe for neckbeards who take pleasure in\nunnecessary complexity. Until the robots take over, this blog is for humans.\n\nAnyway, there's an entire file dedicated to including files in your project\nwhich aren't code. We're entering comically ridiculous territory:\n\ninclude README.rst\ninclude docs/*.txt\ninclude funniest/data.json\n\n\nIt's a Bird! It's a Plane! Its... A Single, Sophisticated Config File?\nI hope you're thoroughly pissed off after looking back at all the things we've\nlet slide by year after year, telling ourselves that this patchwork of standards\nis just fine. Cue our hero: the creator of Poetry:\n\n> Packaging systems and dependency management in Python are rather convoluted and\nhard to understand for newcomers. Even for seasoned developers it might be\ncumbersome at times to create all files needed in a Python project: setup.py,\nrequirements.txt, setup.cfg, MANIFEST.in  and the newly added Pipfile. So I\nwanted a tool that would limit everything to a single configuration file to do:\ndependency management, packaging and publishing.\nOh God yes, but HOW?!?!\n\nIntroducing pyproject.toml\nPoetry is built around a single configuration dubbed pyproject.toml  which has\nbecome an accepted standard in the Python community\n[https://www.python.org/dev/peps/pep-0518/]  by way of PEP 518.  With the weight\nof the Python development community itself, it's safe to say this isn't another\nfad and is worth using.\n\nHere's an example .toml file from the Poetry Github repository\n[https://github.com/sdispater/poetry]:\n\n[tool.poetry]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"The description of the package\"\n\nlicense = \"MIT\"\n\nauthors = [\n    \"Sébastien Eustace <sebastien@eustace.io>\"\n]\n\nreadme = 'README.md'  # Markdown files are supported\n\nrepository = \"https://github.com/sdispater/poetry\"\nhomepage = \"https://github.com/sdispater/poetry\"\n\nkeywords = ['packaging', 'poetry']\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.2\"  # Compatible python versions must be declared here\ntoml = \"^0.9\"\n# Dependencies with extras\nrequests = { version = \"^2.13\", extras = [ \"security\" ] }\n# Python specific dependencies with prereleases allowed\npathlib2 = { version = \"^2.2\", python = \"~2.7\", allows-prereleases = true }\n# Git dependencies\ncleo = { git = \"https://github.com/sdispater/cleo.git\", branch = \"master\" }\n\n# Optional dependencies (extras)\npendulum = { version = \"^1.4\", optional = true }\n\n[tool.poetry.dev-dependencies]\npytest = \"^3.0\"\npytest-cov = \"^2.4\"\n\n[tool.poetry.scripts]\nmy-script = 'my_package:main'\n\n\nIn addition to covering the scope of all previously mentioned files, using \npyproject.toml  with Poetry also covers:\n\n * Auto-populating the exclude  section from values found in .gitignore\n * The addition of a keywords  section to be included with the resulting PyPi\n   package\n * Support for version numbers using any syntax, such as wildcard (*)  or carrot\n   (^1.0.0)  syntax\n * Auto-detection for virtual environments, thus a global install that can be\n   used within envs\n\nCreating Poetic Art\nAre we all fired up yet? Right: let's change our workflow forever.\n\nInstallation\n  To install Poetry on OSX, use the following:\n\n$ curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python\n\n\nThis will create an addition to your ~/.bash_profile. Restart your terminal and\nverify the installation:\n\n$ poetry --version\nPoetry 0.12.10\n\n\nCreating a New Python Project\nNavigate to whichever file path you'd like your new project to call home. To get\nstarted, all we need next is the following command:\n\npoetry new my-package\n\n\nReady for a breath of fresh air? This command generates a basic project\nstructure for you- something that's been missing from Python for a long time\nwhen compared to similar generators for Node or otherwise. The resulting project\nstructure looks as such:\n\nmy-package\n├── pyproject.toml\n├── README.rst\n├── my_package\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_my_package\n\n\nOf the beautiful things happening here, the only one we haven't touched on yet\nis Poetry's built-in integration with pytest. Oh, happy day!\n\nAlternative Interactive Installation Method\nIf you'd prefer a bit more handholding, feel free to use poetry init  in an\nempty directory (or a directory without the existing .toml  file) to be walked\nthrough the creation process:\n\n$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [my-package]: Great Package\nVersion [0.1.0]:\nDescription []: Great package for great people.\nAuthor [Todd Birchard <todd@hackersandslackers.com>, n to skip]:\nLicense []: MIT\nCompatible Python versions [^2.7]: ^3.7\n\nWould you like to define your dependencies (require) interactively? (yes/no) [yes] no\n\n\n\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\n\nGenerated file\n\n[tool.poetry]\nname = \"Great Package\"\nversion = \"0.1.0\"\ndescription = \"Great package for great people.\"\nauthors = [\"Todd Birchard <todd@hackersandslackers.com>\"]\nlicense = \"MIT\"\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n\n\nDo you confirm generation? (yes/no) [yes] yes\n\n\nManaging Dependencies in pyproject.toml\nIf you're familiar with Pipfiles, pyproject.toml handles dependencies the same\nway. Just remember that poetry install  installs your listed dependencies, and \npoetry update  will update dependencies in poetry.lock to their latest versions.\n\nCarry on my Wayward Son\nI could spend all day copy-pasting general usage from the Poetry Github page,\nbut I think my work here is done. Do yourself a favor and  take a look at the\nGithub repo [https://github.com/sdispater/poetry]  to make your life easier\nforever. Or at least until the next replacement solution comes along.","html":"<p>It wasn't long ago that we Hackers were <a href=\"https://hackersandslackers.com/pipenv-etc-tracking-your-projects-dependancies/\">singing the praises of Pipenv</a>: Python's seemingly superior dependency manager at the time. While we hold much love in hearts, sometimes there is love to go around. We just so happen to be fair weather fans, which reminds me: what has Pipenv done for me <em>lately</em>?</p><p>As you've probably guessed (considering its a piece of software), nothing much. Well, there was that time when pip upgraded from <code>v.18</code> to <code>v.18.1</code>, which broke Pipenv entirely with almost minimal acknowledgment (for all I know this might still be broken). As our lives seemed to fade, a miracle emerged from the ashes: a young, smart, attractive alternative to Pipenv that's been whispering in my ear, and promising the world. Her name is <a href=\"https://poetry.eustace.io/\"><strong>Poetry</strong></a>.</p><h2 id=\"what-light-through-yonder-github-breaks\">What Light Through Yonder GitHub Breaks?</h2><p>Poetry stems from the genuine frustration that comes with not only managing environments and dependencies in Python, but the fact that even solving this problem (albeit poorly) still doesn't solve the related tasks needing fulfillment when creating respectable Python projects. Consider Node's <code>package.json</code>: a single file which contains a project's metadata, prod dependencies, dev dependencies, contact information, etc. Instead, Python projects usually come with the following:</p><h3 id=\"setup-py\">Setup.py</h3><p>If you've never bothered to publish a package to PyPI before, there's a decent chance you may not be very familiar with some of the nuances that come with <code>setup.py</code> or why you'd bother creating one. This is a losing mentality: we should assume that most (or some) of the things we build might become useful enough to distribute some day.</p><p>Thus, we get this monstrosity:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from setuptools import setup, find_packages, tests_require, packages, name\n\nwith open(&quot;README&quot;, 'r') as f:\n    long_description = f.read()\n\nsetup = (\n    name='Fake Project',\n    version='1.0',\n    description='A fake project used for example purposes.',\n    long_description=long_description,\n    author='Todd Birchard',\n    author_email='todd@hackersandslackers.com',\n    maintainer='Some Loser',\n    maintainer_email='some.loser@example.com,\n    url=&quot;https://github.com/toddbirchard/fake-project&quot;,\n    license='MIT',\n    include_package_data=True,\n    package_dir={'application'}\n    packages=['distutils', 'modules'],\n    tests_require=[&quot;pytest&quot;],\n    cmdclass={&quot;pytest&quot;: PyTest},\n    classifiers=[\n          'Development Status :: 2 - Beta',\n          'Environment :: Console',\n          'Environment :: Web Environment',\n          'Intended Audience :: End Users/Desktop',\n          'Intended Audience :: Developers',\n          'Intended Audience :: System Administrators',\n          'License :: OSI Approved :: Python Software Foundation License',\n          'Operating System :: MacOS :: MacOS X',\n          'Operating System :: Microsoft :: Windows',\n          'Operating System :: POSIX',\n          'Programming Language :: Python',\n          'Topic :: Communications :: Email',\n          'Topic :: Office/Business',\n          'Topic :: Software Development :: Bug Tracking',\n          ],\n)\n</code></pre>\n<!--kg-card-end: markdown--><p>Many of the metadata fields are rather self-explanatory. But what about the fields related to package dependencies, such as package_dir or packages? Wasn't this already handled in our Pipfile? On top of that, we need to specify then the test suite we're using via <strong>tests_require</strong> and <strong>cmdclass</strong>? Short answer: pretty much.</p><h3 id=\"setup-cfg\">Setup.cfg</h3><p>The real joke with <code>setup.py</code> is that it needs its own configuration file: yes, a configuration file for your configuration file. <code>setup.cfg</code>, as the name suggestions, sets even more granular configurations for the things mentioned in <code>setup.py</code>, such as how <strong>pytest</strong> should be handled, etc. Let's not get into it, but here's an example:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">[coverage:run]\nomit = */test/*\n\n[flake8]\nexclude = *.egg*,.env,.git,.tox,_*,build*,dist*,venv*,python2/,python3/\nignore = E261,W503\nmax-line-length = 121\n\n[tool:pytest]\nminversion = 3.2\naddopts =\n  # --fulltrace\n  # -n auto\n  --cov-config=setup.cfg\n  --cov=httplib2\n  --noconftest\n  --showlocals\n  --strict\n  --tb=short\n  --timeout=17\n  --verbose\n  -ra\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"pipfile-and-pipfile-lock\">Pipfile and Pipfile.lock</h3><p>If you have been using Pipenv, you'll recognize these files as being responsible for setting your Python version and dependencies. <em>But wait- didn't we also need to specify dependencies in setup.py?</em> Yes, we did. There is no God, but if there were, he'd probably hate you. Here's all the work you'd need to do creating an acceptable Pipfile:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">[[source]]\nurl = &quot;https://pypi.org/simple&quot;\nverify_ssl = true\nname = &quot;pypi&quot;\n\n[packages]\nFlask-SQLAlchemy = &quot;*&quot;\npsycopg2 = &quot;*&quot;\npsycopg2-binary = &quot;*&quot;\nrequests = &quot;*&quot;\nconfigparser=&quot;*&quot;\nmapbox=&quot;*&quot;\nflask=&quot;*&quot;\npandas=&quot;*&quot;\nFlask-Assets=&quot;*&quot;\nlibsass=&quot;*&quot;\njsmin=&quot;*&quot;\ndash_core_components=&quot;*&quot;\ndash-table=&quot;*&quot;\ndash_html_components=&quot;*&quot;\ndash=&quot;*&quot;\nflask-session=&quot;*&quot;\nflask-redis=&quot;*&quot;\ngunicorn=&quot;*&quot;\npytest-flask=&quot;*&quot;\n\n\n[dev-packages]\n\n[requires]\npython_version = &quot;3.7.1&quot;\n\n</code></pre>\n<!--kg-card-end: markdown--><p>But wait, there's more!</p><h3 id=\"requirements-txt\">Requirements.txt</h3><p>Because the Pipfile format has not been adopted as a standard for dependency management, we <em>still</em> need to create a requirements.txt file if we want to deploy our application to respectable hosts such as <strong>Google App Engine</strong> or what-have-you. So now we have this ugly son of a bitch from the stone age to deal with as well:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">atomicwrites==1.2.1\nattrs==18.2.0\nboto3==1.9.75\nbotocore==1.12.75\nCacheControl==0.12.5\ncertifi==2018.11.29\nchardet==3.0.4\nClick==7.0\nconfigparser==3.5.0\ndash==0.35.1\ndash-core-components==0.42.0\ndash-html-components==0.13.4\ndash-renderer==0.16.1\ndash-table==3.1.11\ndecorator==4.3.0\ndocutils==0.14\nFlask==1.0.2\nFlask-Assets==0.12\nFlask-Compress==1.4.0\nFlask-Redis==0.3.0\nFlask-Session==0.3.1\nFlask-SQLAlchemy==2.3.2\ngunicorn==19.9.0\nidna==2.8\nipython-genutils==0.2.0\niso3166==0.9\nitsdangerous==1.1.0\nJinja2==2.10\njmespath==0.9.3\njsmin==2.2.2\njsonschema==2.6.0\njupyter-core==4.4.0\nlibsass==0.17.0\nmapbox==0.17.2\nMarkupSafe==1.1.0\nmore-itertools==5.0.0\nmsgpack==0.6.0\nnbformat==4.4.0\nnumpy==1.15.4\npandas==0.23.4\nplotly==3.5.0\npluggy==0.8.0\npolyline==1.3.2\npsycopg2==2.7.6.1\npsycopg2-binary==2.7.6.1\npy==1.7.0\npytest==4.1.0\npytest-flask==0.14.0\npython-dateutil==2.7.5\npytz==2018.9\nredis==3.0.1\nrequests==2.21.0\nretrying==1.3.3\ns3transfer==0.1.13\nsix==1.12.0\nSQLAlchemy==1.2.15\ntraitlets==4.3.2\nuritemplate==3.0.0\nurllib3==1.24.1\nwebassets==0.12.1\nWerkzeug==0.14.1\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"manifest-in\">MANIFEST.in</h3><p>YES, THERE'S MORE. If you're not bothered by now, please leave this blog immediately. The job market is ripe for neckbeards who take pleasure in unnecessary complexity. Until the robots take over, this blog is for humans.</p><p>Anyway, there's an entire file dedicated to including files in your project which aren't code. We're entering comically ridiculous territory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-ini\">include README.rst\ninclude docs/*.txt\ninclude funniest/data.json\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"it-s-a-bird-it-s-a-plane-its-a-single-sophisticated-config-file\">It's a Bird! It's a Plane! Its... A Single, Sophisticated Config File?</h2><p>I hope you're thoroughly pissed off after looking back at all the things we've let slide by year after year, telling ourselves that this patchwork of standards is just fine. Cue our hero: the creator of Poetry:</p><blockquote>Packaging systems and dependency management in Python are rather convoluted and hard to understand for newcomers. Even for seasoned developers it might be cumbersome at times to create all files needed in a Python project: <code>setup.py</code>,<code>requirements.txt</code>, <code>setup.cfg</code>, <code>MANIFEST.in</code> and the newly added <code>Pipfile</code>. So I wanted a tool that would limit everything to a single configuration file to do: dependency management, packaging and publishing.</blockquote><p>Oh God yes, but HOW?!?!</p><h3 id=\"introducing-pyproject-toml\">Introducing pyproject.toml</h3><p>Poetry is built around a single configuration dubbed <code>pyproject.toml</code> which has become an <a href=\"https://www.python.org/dev/peps/pep-0518/\">accepted standard in the Python community</a> by way of <strong>PEP 518.</strong> With the weight of the Python development community itself, it's safe to say this isn't another fad and is worth using.</p><p>Here's an example <strong>.toml </strong>file from the <a href=\"https://github.com/sdispater/poetry\">Poetry Github repository</a>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-toml\">[tool.poetry]\nname = &quot;my-package&quot;\nversion = &quot;0.1.0&quot;\ndescription = &quot;The description of the package&quot;\n\nlicense = &quot;MIT&quot;\n\nauthors = [\n    &quot;Sébastien Eustace &lt;sebastien@eustace.io&gt;&quot;\n]\n\nreadme = 'README.md'  # Markdown files are supported\n\nrepository = &quot;https://github.com/sdispater/poetry&quot;\nhomepage = &quot;https://github.com/sdispater/poetry&quot;\n\nkeywords = ['packaging', 'poetry']\n\n[tool.poetry.dependencies]\npython = &quot;~2.7 || ^3.2&quot;  # Compatible python versions must be declared here\ntoml = &quot;^0.9&quot;\n# Dependencies with extras\nrequests = { version = &quot;^2.13&quot;, extras = [ &quot;security&quot; ] }\n# Python specific dependencies with prereleases allowed\npathlib2 = { version = &quot;^2.2&quot;, python = &quot;~2.7&quot;, allows-prereleases = true }\n# Git dependencies\ncleo = { git = &quot;https://github.com/sdispater/cleo.git&quot;, branch = &quot;master&quot; }\n\n# Optional dependencies (extras)\npendulum = { version = &quot;^1.4&quot;, optional = true }\n\n[tool.poetry.dev-dependencies]\npytest = &quot;^3.0&quot;\npytest-cov = &quot;^2.4&quot;\n\n[tool.poetry.scripts]\nmy-script = 'my_package:main'\n</code></pre>\n<!--kg-card-end: markdown--><p>In addition to covering the scope of all previously mentioned files, using <strong>pyproject.toml</strong> with Poetry also covers:</p><ul><li>Auto-populating the <strong>exclude</strong> section from values found in <code>.gitignore</code></li><li>The addition of a <strong>keywords</strong> section to be included with the resulting PyPi package</li><li>Support for version numbers using any syntax, such as <strong>wildcard (*)</strong> or <strong>carrot (^1.0.0)</strong> syntax</li><li>Auto-detection for virtual environments, thus a global install that can be used within envs</li></ul><h2 id=\"creating-poetic-art\">Creating Poetic Art</h2><p>Are we all fired up yet? Right: let's change our workflow forever.</p><h3 id=\"installation\">Installation</h3><p> To install Poetry on OSX, use the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python\n</code></pre>\n<!--kg-card-end: markdown--><p>This will create an addition to your <code>~/.bash_profile</code>. Restart your terminal and verify the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ poetry --version\nPoetry 0.12.10\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"creating-a-new-python-project\">Creating a New Python Project</h3><p>Navigate to whichever file path you'd like your new project to call home. To get started, all we need next is the following command:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">poetry new my-package\n</code></pre>\n<!--kg-card-end: markdown--><p>Ready for a breath of fresh air? This command generates a basic project structure for you- something that's been missing from Python for a long time when compared to similar generators for Node or otherwise. The resulting project structure looks as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">my-package\n├── pyproject.toml\n├── README.rst\n├── my_package\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_my_package\n</code></pre>\n<!--kg-card-end: markdown--><p>Of the beautiful things happening here, the only one we haven't touched on yet is Poetry's built-in integration with <strong>pytest</strong>. Oh, happy day!</p><h4 id=\"alternative-interactive-installation-method\">Alternative Interactive Installation Method</h4><p>If you'd prefer a bit more handholding, feel free to use <code>poetry init</code> in an empty directory (or a directory without the existing <strong>.toml</strong> file) to be walked through the creation process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [my-package]: Great Package\nVersion [0.1.0]:\nDescription []: Great package for great people.\nAuthor [Todd Birchard &lt;todd@hackersandslackers.com&gt;, n to skip]:\nLicense []: MIT\nCompatible Python versions [^2.7]: ^3.7\n\nWould you like to define your dependencies (require) interactively? (yes/no) [yes] no\n\n\n\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\n\nGenerated file\n\n[tool.poetry]\nname = &quot;Great Package&quot;\nversion = &quot;0.1.0&quot;\ndescription = &quot;Great package for great people.&quot;\nauthors = [&quot;Todd Birchard &lt;todd@hackersandslackers.com&gt;&quot;]\nlicense = &quot;MIT&quot;\n\n[tool.poetry.dependencies]\npython = &quot;^3.7&quot;\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [&quot;poetry&gt;=0.12&quot;]\nbuild-backend = &quot;poetry.masonry.api&quot;\n\n\nDo you confirm generation? (yes/no) [yes] yes\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"managing-dependencies-in-pyproject-toml\">Managing Dependencies in pyproject.toml</h2><p>If you're familiar with Pipfiles, pyproject.toml handles dependencies the same way. Just remember that <code>poetry install</code> installs your listed dependencies, and <code>poetry update</code> will update dependencies in <em>poetry.lock </em>to their latest versions.</p><h3 id=\"carry-on-my-wayward-son\">Carry on my Wayward Son</h3><p>I could spend all day copy-pasting general usage from the Poetry Github page, but I think my work here is done. Do yourself a favor and<a href=\"https://github.com/sdispater/poetry\"> take a look at the Github repo</a> to make your life easier forever. Or at least until the next replacement solution comes along.</p>","url":"https://hackersandslackers.com/poetic-python-project-packaging/","uuid":"10ddf06a-b12f-40b4-9849-2b057d3fe2f4","page":false,"codeinjection_foot":null,"codeinjection_head":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@glorious/demo/dist/gdemo.min.css\">\n<script src=\"https://cdn.jsdelivr.net/npm/@glorious/demo/dist/gdemo.min.js\"></script>\n","comment_id":"5c34086694d3e847951adf3e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673665","title":"Stitch's “Query Anywhere”: Executing Business Logic via Frontend","slug":"mongodb-stitch-query-anywhere","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/stitch5@2x.jpg","excerpt":"MongoDB Stitch vs the impossible: secure database queries via frontend JS.","custom_excerpt":"MongoDB Stitch vs the impossible: secure database queries via frontend JS.","created_at_pretty":"02 June, 2018","published_at_pretty":"23 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-06-02T12:07:57.000-04:00","published_at":"2018-11-23T07:00:00.000-05:00","updated_at":"2019-01-04T21:09:07.000-05:00","meta_title":"MongoDB Stitch \"Query Anywhere\" | Hackers and Slackers","meta_description":"Use MongoDB Stitch to query databases via Frontend code.","og_description":"Use MongoDB Stitch to query databases via Frontend code.","og_image":"https://hackersandslackers.com/content/images/2018/06/stitch5@2x.jpg","og_title":"MongoDB Stitch \"Query Anywhere\"","twitter_description":"Use MongoDB Stitch to query databases via Frontend code.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/stitch5@2x.jpg","twitter_title":"MongoDB Stitch \"Query Anywhere\"","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Frontend","slug":"frontend","description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","feature_image":null,"meta_description":"Frontend Javascript Logic typically assisted by Babel and Webpack. Primarily focused on fundamentals, as opposed to bandwagon frameworks.","meta_title":"Frontend Development | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Some tools are simply the right tool for the job. I imagine this must have been\nthe thinking behind the wave of JSON-like NoSQL databases at their peak, and\neven so today. If we figure we’ll be passing information as JSON to an endpoint,\nto then have it structured into a schema, only to be promptly broken down again\nfor our request seconds later, if you will, it’s fair to question the\ncost-benefit of schemas in some cases. A lot of those cases cover the apps we\nbuild for ourselves: ones that let us do stupid things like spamming selfies or\nfilling the internet with vast mindless thoughts.\n\nMongoDB Atlas  is a hell of product in its own right, being a cloud NoSQL\ndatabase with the ability to execute queries similar to SQL JOINs, countless\naggregations, and more possibilities to work into a pipeline than I’ve even had\ntime to explore (we’ll get there). If you’ve ever been tasked to build endpoints\nfor yourself, chances are you already appreciate side-stepping the manual\none-to-one key association that comes with passing JSON to Lambda Functions or\nwhat-have-you.\n\nTake our situation at Hackers And Slackers, for instance. We’re running a Ghost\nblog, which is a young piece of software built by a non-profit organization:\nthis software is constantly being updated and improved, which means if we want\nto modify the logic of our Node app at all, our choices are:\n\n 1. Modify the Ghost source and refuse future updates\n 2. Merge our custom backend with Ghost changes in the event of an update\n 3. Build a third-party API using a platform such as AWS\n\nMongoDB Stitch  gives us a new fourth option: extend our app without all the\nrepetitive boilerplate.  I say extend  because it empowers us to build on top of\nthings which were previously black-boxed to us, such developing a theme atop a\nblogging system.\n\nCarrying on the Legacy\nMongoDB Stitch extends the philosophy of avoiding repetition. In a similar way\nto how NoSQL removed a pain point for many developers, Stitch wants you to keep\ndoing what you do best, which is probably writing NodeJS apps. Forever.\n\nIf I worked for Mongo, I’d sell the product like this:\n\nMongoDB Stitch empowers you to build powerful features without ever switching\ngears to the menial aspects of development.What I’m really saying is that MongoDB Stitch  is Google Firebase. Both products\ntarget the frontend  and mobile  developer markets, and both are very young and\nearly in fully achieving this goal. I’m watching the MongoDB product video for\nthe first time, and it feels like what I’ve assumed from using the product\naligns with their sell (good job on their part, I suppose):\n\nAs warm and uppity as that video is, Mongo has been rather bashful about their\nCloud. I'm guessing that has something to do with an IPO.\n\nOn the other hand, Google Firebase  has been tooting its own horn loudly for a\nyoung product, with a level of growth which feels almost reckless at times (I\nwon't get into it):\n\nAnyway, we're not here to compare. We're here to judge.\n\nGetting Started with a New Database\nFeel free to follow along by setting up a free tier cluster\n[https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/]. \n\nProper Mongo accounts are managed at https://cloud.mongodb.com  once created.\nThis landing dash has plenty of useful info and stats regarding the cluster\nitself. We'll also need to be sure that a database exists before we crate any\napps, otherwise we'll just be interacting with nothing.\n\nI highly  suggest using the MongoDB Compass  desktop app to connect to your your\ncluster. It's easy to download, and even saves you the time of entering\ncredentials by connecting with a copy+pasted URI:\n\nConnect with your database; emotionally.Within Compass, simply create a database\nand collection:\n\nIn MongoWorld, \"collections\" are the equivalent of \"tables\".Let's Get Stitched\nWith all that out of the way, head back to your account on the Mongo Cloud. Now\nour interest is entirely in the Stitch Apps  link on the left side nav:\n\nThere’s so much to explore!Create and name a new Stitch application, and we'll\nland on the \"getting started\" page. \n\nEnable anonymous auth & point to your collectionOnce we create our app, Stitch\nimmediately throws us in to a quick 101 of how to interact with our database.\nWe're going to use the exact example that Stitch gives us; it's important to\nhave the \"A-ha\" moment where everything comes together. \n\nBefore getting to any code, the only two things we need to do are:\n\n 1. Enable Anonymous Authentication: This is fancy language for creating a user\n    type where anybody who accesses our app can make queries\n 2. Pointing to our Mongo Collection: We need somewhere to store the data we'll\n    be messing with.\n\nConnecting Your App\nWe're going to copy and paste this code on to a page of our app. Once this is\nlive, visit the page and keep an eye on the console:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n\n<script>\n  const clientPromise = stitch.StitchClientFactory.create('hackerjira-bzmfe');\n  clientPromise.then(client => {\n    const db = client.service('mongodb', 'mongodb-atlas').db('HackersBlog');\n    client.login().then(() =>\n      db.collection('jira').updateOne({owner_id: client.authedId()}, {$set:{number:42}}, {upsert:true})\n    ).then(()=>\n      db.collection('jira').find({owner_id: client.authedId()}).limit(100).execute()\n    ).then(docs => {\n      console.log(\"Found docs\", docs)\n      console.log(\"[MongoDB Stitch] Connected to Stitch\")\n    }).catch(err => {\n      console.error(err)\n    });\n  });\n</script>\n\n\nChecking this on the live sites looks like this:\n\nNote the \"docs\" found in the console on the right.It worked, but what exactly?\nThe first thing the snippet tells the database to do is to upsert a row where\n\"number\" is equal to 42:\n\ndb.collection('jira').updateOne({owner_id: client.authedId()}, {$set:{number:42}}, {upsert:true})\n\n\nFor sanity, let's check the database to see what's up:\n\nIs that… a new record?!?!Sure enough, a new entry has been added to our database\nin the collection we specified. That's fun and all, but what about our actual\ndata? Isn't that what we came here for? Consider the next line:\n\ndb.collection('jira').find({owner_id: client.authedId()}).limit(100).execute()\n\n\nAhhh, we’re querying based on entries only  created from the current user!\nBecause the sample code we pasted creates a record, we can then query the\ndatabase for records created by that user. Let’s not ignore how cool that is\nhaving not actually done any work: we already have logic in place to allow\nanonymous users to create records and recognize them based on their session.\n\n.find()  is our bread and butter for retrieving records, much like SQL SELECT.\nSo in theory, to show all issues from this collection we'd just need to run the\nfollowing, right?\n\ndb.collection('jira').find({}).execute()\n\n\nSlow down there, buddy- but yes, pretty much. We just need to make read\npermissions public on the MongoDB Stitch side first. Back in the Stitch UI,\nselect \"Rules\" from the sidebar. Here, we can modify the rules for who can\nread/write which records from which DB:\n\nIt's less complicated than it looks.We can create rules as advanced as we'd\nlike, but the rules we need right now are simple enough to handle purely via the\nUI.\n\nGet All The Records\nGo ahead and add a bunch of records to your database collection. Experiment with\nimporting data via JSON or CSV, or just add some records one-by-one.\n\nWhen that's done, go back to your app and see what .find({})  comes back with:\n\nNow that's a collection.There they are: every record from a database collection,\ngrabbed with a single line of code on our frontend. Feel free to take a moment\nto reflect on this: we didn’t need to create an API, write logic, or log in to\nany shitty IAM policy management UIs. We didn’t even need to write a query; the\n‘query’ in this case is just a JSON object.\n\nStitching it All Together\nWhen I first reached this point, I experienced a rush of emotions: can creating\nnew features truly be this simple? If so, what have we been doing with our lives\nuntil this moment- repeating the same boilerplate and relearning the same\nconcepts as millions before us? Is this knowledge all worthless now? Does the\nexistence of Stitch reduce our lives’ greatest accomplishments to something that\ncan now be reproduced in minutes?\n\nWhile there are a great number of things that come easily with Stitch, there are\na fair share of headaches that come along with them. Many intricacies of complex\nflows and user management lack documentation or examples altogether. Creating a\ncloud based on ease-of-use even more frustrating: there’s not much that sucks\nmore than knowing something should be simple, but lacking the few lines of code\nto do it.\n\nThat’s where we’ll be filling in the blanks. Next time, we’ll take a look into\nStitch’s Serverless functions.","html":"<p>Some tools are simply the right tool for the job. I imagine this must have been the thinking behind the wave of JSON-like NoSQL databases at their peak, and even so today. If we figure we’ll be passing information as JSON to an endpoint, to then have it structured into a schema, only to be promptly broken down again for our request seconds later, if you will, it’s fair to question the cost-benefit of schemas in some cases. A lot of those cases cover the apps we build for ourselves: ones that let us do stupid things like spamming selfies or filling the internet with vast mindless thoughts.</p><p><strong><strong>MongoDB Atlas</strong></strong> is a hell of product in its own right, being a cloud NoSQL database with the ability to execute queries similar to SQL JOINs, countless aggregations, and more possibilities to work into a pipeline than I’ve even had time to explore (we’ll get there). If you’ve ever been tasked to build endpoints for yourself, chances are you already appreciate side-stepping the manual one-to-one key association that comes with passing JSON to Lambda Functions or what-have-you.</p><p>Take our situation at Hackers And Slackers, for instance. We’re running a Ghost blog, which is a young piece of software built by a non-profit organization: this software is constantly being updated and improved, which means if we want to modify the logic of our Node app at all, our choices are:</p><ol><li>Modify the Ghost source and refuse future updates</li><li>Merge our custom backend with Ghost changes in the event of an update</li><li>Build a third-party API using a platform such as AWS</li></ol><p><strong><strong>MongoDB Stitch</strong></strong> gives us a new fourth option: <em>extend our app without all the repetitive boilerplate.</em> I say <em>extend</em> because it empowers us to build on top of things which were previously black-boxed to us, such developing a theme atop a blogging system.</p><h2 id=\"carrying-on-the-legacy\">Carrying on the Legacy</h2><p><strong><strong>MongoDB Stitch </strong></strong>extends the philosophy of avoiding repetition. In a similar way to how NoSQL removed a pain point for many developers, Stitch wants you to keep doing what you do best, which is probably writing NodeJS apps. Forever.</p><p>If I worked for Mongo, I’d sell the product like this:</p><blockquote><em><em>MongoDB Stitch empowers you to build powerful features without ever switching gears to the menial aspects of development.</em></em></blockquote><p>What I’m really saying is that <strong><strong>MongoDB Stitch</strong></strong> is <strong><strong>Google Firebase</strong></strong>. Both products target the <em>frontend</em> and <em>mobile</em> developer markets, and both are very young and early in fully achieving this goal. I’m watching the MongoDB product video for the first time, and it feels like what I’ve assumed from using the product aligns with their sell (good job on their part, I suppose):</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/H3P0lW94L2Q?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><p>As warm and uppity as that video is, Mongo has been rather bashful about their Cloud. I'm guessing that has something to do with an IPO.</p><p>On the other hand, <strong>Google Firebase</strong> has been tooting its own horn loudly for a young product, with a level of growth which feels almost reckless at times (I won't get into it):</p><figure class=\"kg-card kg-embed-card\"><iframe width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/iosNuIdQoy8?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></figure><p>Anyway, we're not here to compare. We're here to judge.</p><h2 id=\"getting-started-with-a-new-database\">Getting Started with a New Database</h2><p>Feel free to follow along by <a href=\"https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/\">setting up a free tier cluster</a>. </p><p>Proper Mongo accounts are managed at <a href=\"https://cloud.mongodb.com\">https://cloud.mongodb.com</a> once created. This landing dash has plenty of useful info and stats regarding the cluster itself. We'll also need to be sure that a database exists before we crate any apps, otherwise we'll just be interacting with nothing.</p><p>I <em>highly</em> suggest using the <strong>MongoDB Compass</strong> desktop app to connect to your your cluster. It's easy to download, and even saves you the time of entering credentials by connecting with a copy+pasted URI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/connectcompass.gif\" class=\"kg-image\"><figcaption>Connect with your database; emotionally.</figcaption></figure><p>Within Compass, simply create a database and collection:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/createdatabase_o.jpg\" class=\"kg-image\"><figcaption>In MongoWorld, \"collections\" are the equivalent of \"tables\".</figcaption></figure><h2 id=\"let-s-get-stitched\">Let's Get Stitched</h2><p>With all that out of the way, head back to your account on the Mongo Cloud. Now our interest is entirely in the <strong>Stitch Apps</strong> link on the left side nav:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/selectstitch.png\" class=\"kg-image\"><figcaption>There’s so much to explore!</figcaption></figure><p>Create and name a new Stitch application, and we'll land on the \"getting started\" page. </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/cf61a09bd893c453854634988b71d500.gif\" class=\"kg-image\"><figcaption><em>Enable anonymous auth &amp; point to your collection</em></figcaption></figure><p>Once we create our app, Stitch immediately throws us in to a quick 101 of how to interact with our database. We're going to use the exact example that Stitch gives us; it's important to have the \"A-ha\" moment where everything comes together. </p><p>Before getting to any code, the only two things we need to do are:</p><ol><li><strong>Enable Anonymous Authentication</strong>: This is fancy language for creating a user type where anybody who accesses our app can make queries</li><li><strong>Pointing to our Mongo Collection</strong>: We need somewhere to store the data we'll be messing with.</li></ol><h3 id=\"connecting-your-app\">Connecting Your App</h3><p>We're going to copy and paste this code on to a page of our app. Once this is live, visit the page and keep an eye on the console:</p><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n\n&lt;script&gt;\n  const clientPromise = stitch.StitchClientFactory.create('hackerjira-bzmfe');\n  clientPromise.then(client =&gt; {\n    const db = client.service('mongodb', 'mongodb-atlas').db('HackersBlog');\n    client.login().then(() =&gt;\n      db.collection('jira').updateOne({owner_id: client.authedId()}, {$set:{number:42}}, {upsert:true})\n    ).then(()=&gt;\n      db.collection('jira').find({owner_id: client.authedId()}).limit(100).execute()\n    ).then(docs =&gt; {\n      console.log(&quot;Found docs&quot;, docs)\n      console.log(&quot;[MongoDB Stitch] Connected to Stitch&quot;)\n    }).catch(err =&gt; {\n      console.error(err)\n    });\n  });\n&lt;/script&gt;\n</code></pre>\n<p>Checking this on the live sites looks like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screenshot-2018-06-02-16.40.43.png\" class=\"kg-image\"><figcaption>Note the \"docs\" found in the console on the right.</figcaption></figure><p>It worked, but what exactly? The first thing the snippet tells the database to do is to upsert a row where \"number\" is equal to 42:</p><pre><code class=\"language-javascript\">db.collection('jira').updateOne({owner_id: client.authedId()}, {$set:{number:42}}, {upsert:true})\n</code></pre>\n<p>For sanity, let's check the database to see what's up:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/createdatabase_o.jpg\" class=\"kg-image\"><figcaption>Is that… a new record?!?!</figcaption></figure><p>Sure enough, a new entry has been added to our database in the collection we specified. That's fun and all, but what about our actual data? Isn't that what we came here for? Consider the next line:</p><pre><code class=\"language-javascript\">db.collection('jira').find({owner_id: client.authedId()}).limit(100).execute()\n</code></pre>\n<p>Ahhh, we’re querying based on entries <em>only</em> created from the current user! Because the sample code we pasted creates a record, we can then query the database for records created by that user. Let’s not ignore how cool that is having not actually done any work: we already have logic in place to allow anonymous users to create records and recognize them based on their session.</p><p><code>.find()</code> is our bread and butter for retrieving records, much like SQL <code>SELECT</code>. So in theory, to show all issues from this collection we'd just need to run the following, right?</p><pre><code class=\"language-javascript\">db.collection('jira').find({}).execute()\n</code></pre>\n<p>Slow down there, buddy- but yes, pretty much. We just need to make read permissions public on the MongoDB Stitch side first. Back in the Stitch UI, select \"Rules\" from the sidebar. Here, we can modify the rules for who can read/write which records from which DB:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screenshot-2018-06-02-16.43.31.png\" class=\"kg-image\"><figcaption>It's less complicated than it looks.</figcaption></figure><p>We can create rules as advanced as we'd like, but the rules we need right now are simple enough to handle purely via the UI.</p><h2 id=\"get-all-the-records\">Get All The Records</h2><p>Go ahead and add a bunch of records to your database collection. Experiment with importing data via JSON or CSV, or just add some records one-by-one.</p><p>When that's done, go back to your app and see what <code>.find({})</code> comes back with:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-11-24-at-4.45.50-PM_o.png\" class=\"kg-image\"><figcaption>Now that's a collection.</figcaption></figure><p>There they are: every record from a database collection, grabbed with a single line of code on our frontend. Feel free to take a moment to reflect on this: we didn’t need to create an API, write logic, or log in to any shitty IAM policy management UIs. We didn’t even need to write a query; the ‘query’ in this case is just a JSON object.</p><h3 id=\"stitching-it-all-together\">Stitching it All Together</h3><p>When I first reached this point, I experienced a rush of emotions: can creating new features truly be this simple? If so, what have we been doing with our lives until this moment- repeating the same boilerplate and relearning the same concepts as millions before us? Is this knowledge all worthless now? Does the existence of Stitch reduce our lives’ greatest accomplishments to something that can now be reproduced in minutes?</p><p>While there are a great number of things that come easily with Stitch, there are a fair share of headaches that come along with them. Many intricacies of complex flows and user management lack documentation or examples altogether. Creating a cloud based on ease-of-use even more frustrating: there’s not much that sucks more than knowing something should be simple, but lacking the few lines of code to do it.</p><p>That’s where we’ll be filling in the blanks. Next time, we’ll take a look into Stitch’s Serverless functions.</p>","url":"https://hackersandslackers.com/mongodb-stitch-query-anywhere/","uuid":"76a0bed5-d98a-47a1-a00a-64cff37d16a8","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b12c0ddb5ac11477416d88d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373d","title":"MongoDB Cloud: \"Backend as a Service\" with Atlas & Stitch","slug":"mongodb-cloud-backend-as-a-service-with-atlas-and-stitch","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","custom_excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","created_at_pretty":"13 November, 2018","published_at_pretty":"15 November, 2018","updated_at_pretty":"15 February, 2019","created_at":"2018-11-13T16:05:20.000-05:00","published_at":"2018-11-15T08:00:00.000-05:00","updated_at":"2019-02-15T12:49:05.000-05:00","meta_title":"MongoDB Cloud: \"Backend as a Service\" | Hackers and Slackers","meta_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","og_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","twitter_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","twitter_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Unless you've been living under a rock (or only visit this site via work-related\nGoogle Searches, like most people) you've probably heard me drone on here and\nthere about MongoDB Atlas  and MongoDB Stitch. I even went so far as to hack\ntogether an awful workflow that somehow utilized Tableau as an ETL tool to feed\nJIRA information into Mongo. I'd like to formally apologize for that entire\nseries: I can't imagine there's a single soul on this planet interested in\nlearning about all of those things simultaneously. Such hobbies reserved for\nmasochists with blogging addictions. I apologize. Let's start over.\n\nFirst off, this is not a tutorial on how to use MongoDB: the database. I have\nzero interest cluttering the internet by reiterating what a MEAN stack is for\nthe ten thousandth time, nor will I bore you with core NoSQL concepts you\nalready understand. I'm here to talk about the giant on the horizon we didn't\nsee coming, where MongoDB the database decided to become MongoDB Inc\n[https://en.wikipedia.org/wiki/MongoDB_Inc.]:  the enterprise cloud provider.\nThe same MongoDB that recently purchased mLab\n[https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab]\n, the other  cloud-hosted solution for Mongo databases. MongoDB the company is\nbold enough to place its bets on building a cloud far  simpler and restricted\nthan either AWS or GCloud. The core of that bet implies that most of us aren't\nexactly building unicorn products as much as we're reinventing the wheel: and\nthey're probably right.\n\nWelcome to our series on MongoDB cloud, where we break down every service\nMongoDB has to offer; one by one.\n\nWhat is MongoDB Cloud, and Does it Exist?\nWhat I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual\nname of the suite MongoDB offers) is actually two products:\n\n * MongoDB Atlas: A cloud-hosted MongoDB cluster with a beefy set of features.\n   Real-time dashboards, high-availability, security features,  an awesome\n   desktop client, and a CLI to top it all off.\n * MongoDB Stitch: A group of services designed to interact with Atlas in every\n   conceivable way, including creating endpoints, triggers, user authentication\n   flows, serverless functions, and a UI to handle all of this.\n\nI'm spying on you and every query you make.Atlas as a Standalone Database\nThere are plenty of people who simply want an instance of MongoDB hosted in the\ncloud as-is: just ask the guys at mLab. This was in fact how I got pulled into\nMongo's cloud myself.\n\nMongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo,\nwhich Mongo itself is confident in by offering a free tier\n[https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/]  of Atlas to\nprospective buyers. If you're a company or enterprise, the phrases High\nAvailability, Horizontal Scalability, relatively Higher Performance  will\nprobably be enough for you. But for us hobbyists, why pay for a Mongo cloud\ninstance?\n\nMongo themselves gives this comparison:\n\nOverview\n MongoDB Atlas\n Compose\n ObjectRocket\n Free Tier\n Yes\nStorage: 512 MB\nRAM: Variable\n No\n30-day free trial\n No\n30-day free trial\n Live migration\n Yes\nNo\nNo\nChoice of cloud providers\n AWS, Azure & GCP\n AWS, Softlayer & GCP\nAvailable in 2 regions for each provider\n Rackspace\n Choice of instance configuration\n Yes\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n Availability of latest MongoDB version\n Yes\nNew versions of the database are available on MongoDB Atlas as soon as they are\nreleased\n No\nNew versions typically available 1-2 quarters following database release\nNo\nNew versions typically available 1-2 quarters following database release\nReplica Set Configuration\n Up to 7 replicas\nAll replicas configured as data-bearing nodes\n 3 data-bearing nodes\nOne of the data-bearing nodes is hidden and used for backups only\n 3 data-bearing nodes\nAutomatic Sharding Support\n Yes\nNo\nYes\nData explorer\n Yes\nYes\nNo\nSQL-based BI Connectivity\n Yes\nNo\nNo\nPause and resume clusters\n Yes\nNo\nNo\nDatabase supported in on-premise deployments\n Yes\nMongoDB Enterprise Advanced [/products/mongodb-enterprise-advanced]\n No\nNo\nGlobal writes Low-latency writes from anywhere in the world Yes\n No\n No\n Cross-region replication Distribute data around the world for multi-region\nfault tolerance and local reads Yes\n No\n No\n Monitoring of database health with automated alerting\n Yes\nMongoDB Atlas UI & support for APM platforms (New Relic)\n Yes\nNew Relic\n Yes\nNew Relic\n Continuous backup\n Yes\nBackups maintained\nseconds behind production cluster\n No\nBackups taken with mongodump against hidden replica set member\n No\nBackups taken with mongodump\n Queryable backups\n Yes\nNo\nNo\nAutomated & consistent snapshots of sharded clusters\n Yes\nNot Applicable\nNo support for auto-sharding\n No\nRequires manually coordinating the recovery of mongodumps across shards\n Access control & IP whitelisting\n Yes\nYes\nYes\nAWS VPC Peering\n Yes\nBeta Release\nYes\nAdditional Charge\n Encryption of data in-flight\n Yes\nTLS/SSL as standard\n Yes\nYes\nEncryption of data at-rest\n Yes\nAvailable for AWS deployments; always on with Azure and GCP\n No\nYes\nAvailable only with specific pricing plans and data centers\n LDAP Integration\n Yes\n No\nNo\n Database-level auditing\nTrack DDL, DML, DCL operations\n Yes\n No\nNo\n Bring your own KMS\n Yes\n No\nNo\n Realistically there are probably only a number of items that stand out on the\ncomparison list when we go strictly database-to-database. Freedom over instance\nconfiguration sounds great, but in practice is more similar to putting a cap on\nhow much MongoDB decides to charge you that month (by the way, it's usually a\nlot; keep this mind). Having the Latest Version  seems great, but this can just\nas easily mean breaking production unannounced as much as it means new features.\n\nMongoDB clearly wins over the enterprise space with Continuous & queryable\nbackups, integration with LDAP, and automatic sharding support. Truthfully if\nthis were merely a database-level feature and cost comparison, the decision to\ngo with  MongoDB Atlas  would come down to how much you like their pretty\ndesktop interface:\n\nA perfectly legitimate reason to pay up, imho.So let's say MongoDB Atlas is\nmarginally better than a competitor in the confined realm of \"being a database.\"\nAre Stitch microservices enough to justify keeping your instance with the\nMongoDB team?\n\nService-by-Service Breakdown of Stitch\nStitch is kind of like if AWS exited in an alternative universe, where JSON and\nJavaScript were earth's only technologies. Thinking back to how we create APIs\nin AWS, the status quo almost always involves spinning up a Dynamo  (NoSQL)\ndatabase to put behind Lambda functions, accessible by API Gateway endpoints.\nStitch's core use case revolves around this use-case of end-user-accessing-data,\nwith a number of services dedicated specifically to supporting or improving this\nflow. The closest comparison to Stitch would be GCloud's Firebase. \n\nSo what makes Stitch so special?\n\nService 1: Querying Atlas Securely via Frontend Code\nSomething that cannot be understated is the ability to query Atlas via frontend\nJavascript. We're not passing API keys, Secrets, or any sort of nonsense;\nbecause you're configured things correctly, whitelisted domains can run queries\nof any complexity without ever interacting with an app's backend.  This is not a\ncrazy use case: consider this blog for example, or more so lately, mobile\napplications:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n<script>\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('<DATABASE>');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => \n    db.collection('<COLLECTION>').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() => \n    db.collection('<COLLECTION>').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs => {\n      console.log(\"Found docs\", docs)\n      console.log(\"[MongoDB Stitch] Connected to Stitch\")\n  }).catch(err => {\n    console.error(err)\n  });\n</script>\n\n\nThis isn't to say we're allowing any user to query any data all willy-nilly just\nbecause they're on our whitelisted IP: all data stored in Atlas is restricted to\nspecified Users  by defining User Roles. Joe Schmoe can't just inject a query\ninto any presumed database and wreak havoc, because Joe Schmoe can only access\ndata we've permitted his user account to view or write to. What is this \"user\naccount\" you ask? This brings us to the next big feature...\n\nService 2: End-User Account Creation & Management\nStitch will handle user account creation for you without the boilerplate.\nCreating an app with user accounts is a huge pain in the ass. Cheeky phrases\nlike 'Do the OAuth Dance'  can't ever hope to minimize the agonizing repetitive\npain of creating user accounts or managing relationships between users and data\n(can user X  see a comment from user Y?). Stitch allows most of the intolerably\nbenign logic behind these features to be handled via a UI.\n\nIt would be a far cry to say these processes have been \"trivialized\", but the\ntime saved is perhaps just enough to keep a coding hobbyist interested in their\nside projects as opposed to giving up and playing Rocket League.\n\nAs far as the permissions to read comments go... well, here's a self-explanatory\nscreenshot of how Stitch handles read/write document permission in its simplest\nform:\n\nOwners of comments can write their comments. Everybody else reads. Seems simple.\nService 3: Serverless Functions\nStitch functions are akin to AWS Lambda functions, but much easier to configure\nfor cross-service integration (and also limited to JavaScript ECMA 2015 or\nsomething). Functions benefit from the previous two features, in that they too\ncan be triggered from a whitelisted app's frontend, and are governed by a simple\n\"rules\" system, eliminating the need for security group configurations etc.\n\nThis is what calling a function from an app's frontend looks like:\n\n<script>\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n     client.callFunction(\"numCards\", [\"In Progress\"]).then(results => {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n</script>\n\n\nFunctions can run any query against Atlas, retrieve values  (such as environment\nvariables), and even call other functions. Functions can also be fired by\ndatabase triggers,  where a change to a collection will prompt an action such as\nan alert.\n\nService 4: HTTP Webhooks\nWebhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to\none another in that they are one-off URLs to perform single tasks. We could\nnever build a well-designed API using Stitch Webhooks, as we could with API\nGateway; this simply isn't the niche MongoDB is trying to hit (the opposite, in\nfact). \n\nConfiguration for a single Webhook.This form with a mere 6 fields clearly\nillustrates what Stitch intends to do: trivializing the creation of\ntraditionally non-trivial features.\n\nService 5: Storing 'Values' in Stitch\nA \"value\" is equivalent to an environment variable. These can be used to store\nAPI keys, secrets, or whatever. Of course, values are retrieved via functions.\n\nShhh, it's a secret ;)Service 6+: A Bunch of Mostly Bloated Extras\nFinally, Stitch has thrown in a few third-party integrations for good measure.\nSome integrations like S3 Integration could definitely come in handy, but it's\nworth asking why Mongo constantly over advertises their integrations with Github \n and Twilio. We've already established that we can create endpoints which accept\ninformation, and we can make functions which GET  information... so isn't\nanything with an API pretty easy to 'integrate' with?\n\nThis isn't to say the extra services aren't useful, they just seem a bit... odd.\nIt feels a lot like bloating the catalog, but the catalog isn't nearly bloated\nenough where it feels normal (like Heroku add-ons, for example). The choice to\nlaunch Stitch with a handful of barely-useful integrations only comes off as\nmore and more aimless as time passes; as months turn to years and no additions\nor updates are made to service offerings, it's worth questioning what the vision\nhad been for the product in the first place. In my experience, feature sets like\nthese happen when Product Managers are more powerful than they are useful.\n\nThe Breathtaking Climax: Is Stitch Worth It?\nI've been utilizing Stitch to fill in the blanks in development for months now,\nperhaps nearly a year. Each time I find myself working with Stitch or looking at\nthe bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive\ntoy with an infuriating lack of accurate documentation.\n\n  Stitch is very much a copy-and-paste-cookie-cutter-code  type of product,\nwhich begs the question of why their tutorials are recklessly outdated;\nsometimes to the point where MongoDB's own tutorial source code doesn't work. \nThere are so many use cases and potential benefits to Stitch, so why is the \nGithub repo [https://github.com/mongodb/stitch-examples]  containing example\ncode snippets so unmaintained, and painfully irrelevant? Lastly, why am I\nselling this product harder than their own internal team?\n\nStitch is a good product with a lot of unfortunate oversight. That said, Google\nFirebase still doesn't even have an \"import data\" feature, so I suppose it's\ntime to dig deep into this vendor lock and write a 5-post series about it before\nSilicon Valley's best and brightest get their shit together enough to actually\ncreate something useful and intuitive for other human beings to use. In the\nmeantime, feel free to steal source from tutorials I'll be posting, because\nthey'll be sure to, you know, actually work.","html":"<p>Unless you've been living under a rock (or only visit this site via work-related Google Searches, like most people) you've probably heard me drone on here and there about <strong>MongoDB Atlas</strong> and <strong>MongoDB Stitch</strong>. I even went so far as to hack together an awful workflow that somehow utilized Tableau as an ETL tool to feed JIRA information into Mongo. I'd like to formally apologize for that entire series: I can't imagine there's a single soul on this planet interested in learning about all of those things simultaneously. Such hobbies reserved for masochists with blogging addictions. I apologize. Let's start over.</p><p>First off, this is not a tutorial on how to use <em>MongoDB: the database</em>. I have zero interest cluttering the internet by reiterating what a MEAN stack is for the ten thousandth time, nor will I bore you with core NoSQL concepts you already understand. I'm here to talk about the giant on the horizon we didn't see coming, where MongoDB the database decided to become <a href=\"https://en.wikipedia.org/wiki/MongoDB_Inc.\"><strong>MongoDB Inc</strong></a><strong>:</strong> the enterprise cloud provider. The same MongoDB that recently purchased <a href=\"https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab\">mLab</a>, the <em>other</em> cloud-hosted solution for Mongo databases. MongoDB the company is bold enough to place its bets on building a cloud <em>far</em> simpler and restricted than either AWS or GCloud. The core of that bet implies that most of us aren't exactly building unicorn products as much as we're reinventing the wheel: and they're probably right.</p><p>Welcome to our series on MongoDB cloud, where we break down every service MongoDB has to offer; one by one.</p><h2 id=\"what-is-mongodb-cloud-and-does-it-exist\">What is MongoDB Cloud, and Does it Exist?</h2><p>What I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual name of the suite MongoDB offers) is actually two products:</p><ul><li><strong>MongoDB Atlas</strong>: A cloud-hosted MongoDB cluster with a beefy set of features. Real-time dashboards, high-availability, security features,  an awesome desktop client, and a CLI to top it all off.</li><li><strong>MongoDB Stitch: </strong>A group of services designed to interact with Atlas in every conceivable way, including creating endpoints, triggers, user authentication flows, serverless functions, and a UI to handle all of this.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/metrics.gif\" class=\"kg-image\"><figcaption>I'm spying on you and every query you make.</figcaption></figure><h3 id=\"atlas-as-a-standalone-database\">Atlas as a Standalone Database</h3><p>There are plenty of people who simply want an instance of MongoDB hosted in the cloud as-is: just ask the guys at mLab. This was in fact how I got pulled into Mongo's cloud myself.</p><p>MongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo, which Mongo itself is confident in by offering a <a href=\"https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/\">free tier</a> of Atlas to prospective buyers. If you're a company or enterprise, the phrases <strong>High Availability</strong>, <strong>Horizontal Scalability, </strong>relatively <strong>Higher Performance</strong> will probably be enough for you. But for us hobbyists, why pay for a Mongo cloud instance?</p><p>Mongo themselves gives this comparison:</p><div class=\"tableContainer\">\n<table class=\"table left\">\n  <thead>\n    <tr>\n      <th>\n        <strong>Overview</strong>\n      </th>\n      <th>\n        <strong>MongoDB Atlas</strong>\n      </th>\n      <th>\n        <strong>Compose</strong>\n      </th>\n      <th>\n        <strong>ObjectRocket</strong>\n      </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\n        Free Tier\n      </td>\n      <td>\n        Yes<br><small>Storage: 512 MB<br>RAM: Variable</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Live migration\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of cloud providers\n      </td>\n      <td>\n        AWS, Azure &amp; GCP\n      </td>\n      <td>\n        AWS, Softlayer &amp; GCP<br><small>Available in 2 regions for each provider</small>\n      </td>\n      <td>\n        Rackspace\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of instance configuration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Availability of latest MongoDB version\n      </td>\n      <td>\n        Yes<br><small>New versions of the database are available on MongoDB Atlas as soon as they are released</small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Replica Set Configuration\n      </td>\n      <td>\n        Up to 7 replicas<br><small>All replicas configured as data-bearing nodes</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small>One of the data-bearing nodes is hidden and used for backups only</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automatic Sharding Support\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Data explorer\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        SQL-based BI Connectivity\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Pause and resume clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database supported in on-premise deployments\n      </td>\n      <td>\n        Yes<br><small><a href=\"/products/mongodb-enterprise-advanced\">MongoDB Enterprise Advanced</a></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Global writes <small>Low-latency writes from anywhere in the world </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Cross-region replication <small>Distribute data around the world for multi-region fault tolerance and local reads </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr><tr>\n      <td>\n        Monitoring of database health with automated alerting\n      </td>\n      <td>\n        Yes<br><small>MongoDB Atlas UI &amp; support for APM platforms (New Relic)</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Continuous backup\n      </td>\n      <td>\n        Yes<br><small>Backups maintained<br>seconds behind production cluster</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump against hidden replica set member</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Queryable backups\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automated &amp; consistent snapshots of sharded clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Not Applicable<br><small>No support for auto-sharding</small>\n      </td>\n      <td>\n        No<br><small>Requires manually coordinating the recovery of mongodumps across shards</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Access control &amp; IP whitelisting\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        AWS VPC Peering\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Beta Release<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Additional Charge</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data in-flight\n      </td>\n      <td>\n        Yes<br><small>TLS/SSL as standard</small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data at-rest\n      </td>\n      <td>\n        Yes<br><small>Available for AWS deployments; always on with Azure and GCP</small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Available only with specific pricing plans and data centers</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        LDAP Integration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database-level auditing<br><small>Track DDL, DML, DCL operations</small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Bring your own KMS\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Realistically there are probably only a number of items that stand out on the comparison list when we go strictly database-to-database. Freedom over <strong>instance configuration </strong>sounds great, but in practice is more similar to putting a cap on how much MongoDB decides to charge you that month (by the way, it's usually a lot; keep this mind). Having the <strong>Latest Version</strong> seems great, but this can just as easily mean breaking production unannounced as much as it means new features.</p><p>MongoDB clearly wins over the enterprise space with <strong>Continuous &amp; queryable backups</strong>, integration with <strong>LDAP, </strong>and <strong>automatic sharding support. </strong>Truthfully if this were merely a database-level feature and cost comparison, the decision to go with<strong> MongoDB Atlas</strong> would come down to how much you like their pretty desktop interface:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/compass.gif\" class=\"kg-image\"><figcaption>A perfectly legitimate reason to pay up, imho.</figcaption></figure><p>So let's say MongoDB Atlas is marginally better than a competitor in the confined realm of \"being a database.\" Are Stitch microservices enough to justify keeping your instance with the MongoDB team?</p><h2 id=\"service-by-service-breakdown-of-stitch\">Service-by-Service Breakdown of Stitch</h2><p>Stitch is kind of like if AWS exited in an alternative universe, where JSON and JavaScript were earth's only technologies. Thinking back to how we create APIs in AWS, the status quo almost always involves spinning up a <strong>Dynamo</strong> (NoSQL) database to put behind <strong>Lambda </strong>functions, accessible by <strong>API Gateway </strong>endpoints. Stitch's core use case revolves around this use-case of <em>end-user-accessing-data</em>, with a number of services dedicated specifically to supporting or improving this flow. The closest comparison to Stitch would be GCloud's <strong>Firebase</strong>. </p><p>So what makes Stitch so special?</p><h3 id=\"service-1-querying-atlas-securely-via-frontend-code\">Service 1: Querying Atlas Securely via Frontend Code</h3><p>Something that cannot be understated is the ability to query Atlas via frontend Javascript. We're not passing API keys, Secrets, or any sort of nonsense; because you're configured things correctly, whitelisted domains can run queries of any complexity <em>without ever interacting with an app's backend.</em> This is not a crazy use case: consider this blog for example, or more so lately, mobile applications:</p><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('&lt;DATABASE&gt;');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; \n    db.collection('&lt;COLLECTION&gt;').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() =&gt; \n    db.collection('&lt;COLLECTION&gt;').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs =&gt; {\n      console.log(&quot;Found docs&quot;, docs)\n      console.log(&quot;[MongoDB Stitch] Connected to Stitch&quot;)\n  }).catch(err =&gt; {\n    console.error(err)\n  });\n&lt;/script&gt;\n</code></pre>\n<p>This isn't to say we're allowing any user to query any data all willy-nilly just because they're on our whitelisted IP: all data stored in Atlas is restricted to specified <strong>Users</strong> by defining <strong>User Roles. </strong>Joe Schmoe can't just inject a query into any presumed database and wreak havoc, because Joe Schmoe can only access data we've permitted his user account to view or write to. What is this \"user account\" you ask? This brings us to the next big feature...</p><h3 id=\"service-2-end-user-account-creation-management\">Service 2: End-User Account Creation &amp; Management</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-5.59.03-PM.png\" class=\"kg-image\"><figcaption>Stitch will handle user account creation for you without the boilerplate.</figcaption></figure><p>Creating an app with user accounts is a huge pain in the ass. Cheeky phrases like '<strong>Do the OAuth Dance'</strong> can't ever hope to minimize the agonizing repetitive pain of creating user accounts or managing relationships between users and data (can <em>user X</em> see a comment from <em>user Y</em>?). Stitch allows most of the intolerably benign logic behind these features to be handled via a UI.</p><p>It would be a far cry to say these processes have been \"trivialized\", but the time saved is perhaps just enough to keep a coding hobbyist interested in their side projects as opposed to giving up and playing Rocket League.</p><p>As far as the permissions to read comments go... well, here's a self-explanatory screenshot of how Stitch handles read/write document permission in its simplest form:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-6.09.25-PM.png\" class=\"kg-image\"><figcaption>Owners of comments can write their comments. Everybody else reads. Seems simple.</figcaption></figure><h3 id=\"service-3-serverless-functions\">Service 3: Serverless Functions</h3><p>Stitch functions are akin to AWS Lambda functions, but much easier to configure for cross-service integration (and also limited to JavaScript ECMA 2015 or something). Functions benefit from the previous two features, in that they too can be triggered from a whitelisted app's frontend, and are governed by a simple \"rules\" system, eliminating the need for security group configurations etc.</p><p>This is what calling a function from an app's frontend looks like:</p><pre><code class=\"language-javascript\">&lt;script&gt;\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n     client.callFunction(&quot;numCards&quot;, [&quot;In Progress&quot;]).then(results =&gt; {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n&lt;/script&gt;\n</code></pre>\n<p>Functions can run any query against Atlas, retrieve <em>values</em> (such as environment variables), and even call other functions. Functions can also be fired by database <strong>triggers,</strong> where a change to a collection will prompt an action such as an alert.</p><h3 id=\"service-4-http-webhooks\">Service 4: HTTP Webhooks</h3><p>Webhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to one another in that they are one-off URLs to perform single tasks. We could never build a well-designed API using Stitch Webhooks, as we could with <strong>API Gateway</strong>; this simply isn't the niche MongoDB is trying to hit (the opposite, in fact). </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-4.33.27-PM.png\" class=\"kg-image\"><figcaption>Configuration for a single Webhook.</figcaption></figure><p>This form with a mere 6 fields clearly illustrates what Stitch intends to do: trivializing the creation of traditionally non-trivial features.</p><h3 id=\"service-5-storing-values-in-stitch\">Service 5: Storing 'Values' in Stitch</h3><p>A \"value\" is equivalent to an environment variable. These can be used to store API keys, secrets, or whatever. Of course, values are retrieved via functions.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-7.45.28-PM.png\" class=\"kg-image\"><figcaption>Shhh, it's a secret ;)</figcaption></figure><h3 id=\"service-6-a-bunch-of-mostly-bloated-extras\">Service 6+: A Bunch of Mostly Bloated Extras</h3><p>Finally, Stitch has thrown in a few third-party integrations for good measure. Some integrations like <strong>S3 Integration </strong>could definitely come in handy, but it's worth asking why Mongo constantly over advertises their integrations with <strong>Github</strong> and <strong>Twilio</strong>. We've already established that we can create endpoints which accept information, and we can make functions which <em>GET</em> information... so isn't anything with an API pretty easy to 'integrate' with?</p><p>This isn't to say the extra services aren't useful, they just seem a bit... odd. It feels a lot like bloating the catalog, but the catalog isn't nearly bloated enough where it feels normal (like Heroku add-ons, for example). The choice to launch Stitch with a handful of barely-useful integrations only comes off as more and more aimless as time passes; as months turn to years and no additions or updates are made to service offerings, it's worth questioning what the vision had been for the product in the first place. In my experience, feature sets like these happen when Product Managers are more powerful than they are useful.</p><h2 id=\"the-breathtaking-climax-is-stitch-worth-it\">The Breathtaking Climax: Is Stitch Worth It?</h2><p>I've been utilizing Stitch to fill in the blanks in development for months now, perhaps nearly a year. Each time I find myself working with Stitch or looking at the bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive toy with an infuriating lack of accurate documentation.</p><p> Stitch is very much a <em>copy-and-paste-cookie-cutter-code</em> type of product, which begs the question of why their tutorials are recklessly outdated; sometimes to the point where MongoDB's own tutorial source code <em>doesn't work. </em>There are so many use cases and potential benefits to Stitch, so why is the <a href=\"https://github.com/mongodb/stitch-examples\">Github repo</a> containing example code snippets so unmaintained, and painfully irrelevant? Lastly, why am I selling this product harder than their own internal team?</p><p>Stitch is a good product with a lot of unfortunate oversight. That said, Google Firebase still doesn't even have an \"import data\" feature, so I suppose it's time to dig deep into this vendor lock and write a 5-post series about it before Silicon Valley's best and brightest get their shit together enough to actually create something useful and intuitive for other human beings to use. In the meantime, feel free to steal source from tutorials I'll be posting, because they'll be sure to, you know, actually work.</p>","url":"https://hackersandslackers.com/mongodb-cloud-backend-as-a-service-with-atlas-and-stitch/","uuid":"5555fa6e-07f0-4f9a-8069-e1e68868e608","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5beb3c900dbec217f3ce801b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673735","title":"Reselling AWS Load Balancing","slug":"reselling-aws-load-balancer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","custom_excerpt":"Providing Cloud Load Balancing for your customers; My ultimatum.","created_at_pretty":"12 November, 2018","published_at_pretty":"12 November, 2018","updated_at_pretty":"13 November, 2018","created_at":"2018-11-11T19:49:57.000-05:00","published_at":"2018-11-11T20:16:49.000-05:00","updated_at":"2018-11-12T23:05:57.000-05:00","meta_title":"Reselling AWS Load Balancing | Hackers and Slackers","meta_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","og_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","og_title":"Reselling AWS Load Balancing | Hackers and Slackers","twitter_description":"Providing Cloud Load Balancing for your customers by leveraging AWS.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/loadbalancer@2x.jpg","twitter_title":"Reselling AWS Load Balancing | Hackers and Slackers","authors":[{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"}],"primary_author":{"name":"Ryan Rosado","slug":"xodz","bio":"World renowned DJ who got his start from being famous on the internet. Averages 3 headshots per second in daily life and pays for all and essentials in bitcoin.","profile_image":"https://hackersandslackers.com/content/images/2019/03/ryan2.jpg","twitter":"@Zawdz","facebook":null,"website":"http://twitch.tv/xodz/videos/all"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Let's say we have a hosting service for users who bring their own domain name.\nIn this scenario we'd like to be able to service customers no matter who manages\ntheir DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore\nplace, etc.\n\nAt the same time, we'd also like to provide Load balancing so no one-user can\noverload any of our systems. This means, instead of having a customer's domain\nname point directly to the system where their webserver or app resides, it will\npoint the HTTP connection to a Load Balancer which is prepared to handle serious\nconnection load before divvying it out to whichever cluster of systems is ready\nto deliver the user's content. \n\nIn an ideal world, we would have the user point their domain name to the Load\nBalancer's IP address. Very simple DNS A-Record adjustment. \n\nIn the real world, these type of cloud load balancers run over several ip\naddresses that rotate over time. So, if we were to place one of these IP\naddresses in a domain name's A-Record, it would soon be useless as it rotates\nout. Instead, the cloud load balancer offers us an end point (also an A-Record)\nsuch as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static\nwhile they dynamically rotate the IP addresses the entrypoint leads to. \n\nThe catch? You can't place an A-Record in another DNS A-Record, you can only\nplace an IP address in an A-Record. the DNS A-Record is simply a key-value pair\nwhere the key  is the Domain name (yoursiteEndpoint.com) and the value  is an IP\naddress (and nothing else). \n\nThen how do we leverage a cloud load balancer for our customers?\n\n{workaround}  Each customer with their own domain name must make the following\nchanges in their DNS provider records. \n\n * Make a CNAME Record called \"www\" which leads to the AWS Load Balancer\n   A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)\n * Setup DNS forwarding so customersite.com forwards to www.customersite.com\n\nThe Problem:  The customer will literally be entering evident AWS data into\ntheir config, and it's much more information to update than just an IP address\nin an A-Record. \n\nMore Options: \n\n{Route 53 Nameservers}  You have to automate Route 53, adding a new Hosted Zone\nbased on the customer's domain name, retrieve and deliver the Route 53 Hosted\nZone nameservers to the customer so the customer can update their DNS records at\ntheir service of choice.\n\nThe Problem:  Lots more automation and costs, AWS 500 Hosted Zone limit, more\ncustomer sync interaction\n\nMy Ultimatum:\nMake my own Load Balancer out of a network-enhanced AWS EC2 instance. I will\ngive two options for the customers - the simple A-record update to EC2-instance\nstatic IP. If they want DDoS protection and load balancing, they can do the \n{workaround}  step above additionally. If they decide not to do {workaround} \nstep above, the customer understands that we are leaving leaving the uptime\ncompletely up to the EC2 instance IP address.  Also, forget that Route 53\nnameservers update BS, as that is way too much additional business logic\nautomation and costs for reselling standpoint.","html":"<p>Let's say we have a hosting service for users who bring their own domain name. In this scenario we'd like to be able to service customers no matter who manages their DNS records. Be it GoDaddy, Namecheap, Google, Hostgator, some offshore place, etc.</p><p>At the same time, we'd also like to provide Load balancing so no one-user can overload any of our systems. This means, instead of having a customer's domain name point directly to the system where their webserver or app resides, it will point the HTTP connection to a Load Balancer which is prepared to handle serious connection load before divvying it out to whichever cluster of systems is ready to deliver the user's content. </p><p>In an ideal world, we would have the user point their domain name to the Load Balancer's IP address. Very simple DNS A-Record adjustment. </p><p>In the real world, these type of cloud load balancers run over several ip addresses that rotate over time. So, if we were to place one of these IP addresses in a domain name's A-Record, it would soon be useless as it rotates out. Instead, the cloud load balancer offers us an end point (also an A-Record) such as 'entrypoint-797000074.us-east-1.elb.amazonaws.com', which is static while they dynamically rotate the IP addresses the entrypoint leads to. </p><p>The catch? You can't place an A-Record in another DNS A-Record, you can only place an IP address in an A-Record. the DNS A-Record is simply a key-value pair where the <em>key</em> is the Domain name (yoursiteEndpoint.com) and the <em>value</em> is an IP address (and nothing else). </p><p>Then how do we leverage a cloud load balancer for our customers?</p><p><em>{workaround}</em> Each customer with their own domain name must make the following changes in their DNS provider records. </p><ul><li>Make a CNAME Record called \"www\" which leads to the AWS Load Balancer A-Record ('entrypoint-797000074.us-east-1.elb.amazonaws.com)</li><li>Setup DNS forwarding so customersite.com forwards to www.customersite.com</li></ul><p><em>The Problem:</em><strong> </strong>The customer will literally be entering evident AWS data into their config, and it's much more information to update than just an IP address in an A-Record. </p><p>More Options: </p><p><em>{Route 53 Nameservers}</em> You have to automate Route 53, adding a new Hosted Zone based on the customer's domain name, retrieve and deliver the Route 53 Hosted Zone nameservers to the customer so the customer can update their DNS records at their service of choice.</p><p><em>The Problem:</em> Lots more automation and costs, AWS 500 Hosted Zone limit, more customer sync interaction</p><h2 id=\"my-ultimatum-\">My Ultimatum:</h2><p>Make my own Load Balancer out of a network-enhanced AWS EC2 instance. I will give two options for the customers - the simple A-record update to EC2-instance static IP. If they want DDoS protection and load balancing, they can do the <em>{workaround}</em> step above additionally. If they decide not to do <em>{workaround}</em> step above, the customer understands that we are leaving leaving the uptime completely up to the EC2 instance IP address.  Also, forget that Route 53 nameservers update BS, as that is way too much additional business logic automation and costs for reselling standpoint. </p>","url":"https://hackersandslackers.com/reselling-aws-load-balancer/","uuid":"5dd15e2a-c368-4717-900c-e85095331c4b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be8ce3574f90031d0a61650"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673651","title":"Building an API with Amazon's API Gateway","slug":"creating-apis-with-api-gateway","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","custom_excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","created_at_pretty":"13 May, 2018","published_at_pretty":"29 October, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-05-13T17:29:07.000-04:00","published_at":"2018-10-29T19:41:00.000-04:00","updated_at":"2019-01-05T13:28:10.000-05:00","meta_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","meta_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","og_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","twitter_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","twitter_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our last adventure, we ventured off to create our very own cloud database\n[https://hackersandslackers.com/setting-up-mysql-on-aws/]  by using Amazon's RDS \n service. We've also briefly covered\n[https://hackersandslackers.com/building-an-api-using-aws/]  the general concept\nbehind what Lambda functions. In case you've already forgotten, Lambdas are\nbasically just chunks of code in the cloud; think of them as tiny virtual\nservers, which have already been configured (and locked down) to serve one\nspecific purpose. Because that's literally what it is.\n\nThe data being stored in RDS is ultimately what we're targeting, and Lambdas \nserve as the in-between logic to serve up, modify, or add to the proper data.\nThe only piece missing from the picture is API Gateway. \n\nAs the name suggests, API Gateway  is the, uh, gateway  that users or systems\ninteract with to obtain what they're seeking. It is (hopefully) the only part of\nthis VPC structure an external user can interact with:\n\nSimple API to interact with RDS.Serving as a \"gateway\" is obviously what all\nAPIs so, but the term is also true in the sense that API Gateway  is completely\nconfigured via UI, thus engineers of any programming background can safely\nmodify endpoints, methods,  CORs  configurations, or any of the high-level API\nstructure without being locked into a programming language. API Gateway  is\ntherefore very much an enterprise-geared product: it lends itself to large teams\nand scaling. That said, if it were to be compared to building an API via a\nframework designed to do such things (such as Express or Flask) the experience\nis undoubtedly more clunky. The trade-off being made for speed is immediate\nvisibility, assurance, and a higher chance for collaboration.\n\nThe Challenge of Building a \"Well-Designed\" API\nGood APIs are premeditated. A complex API might accept multiple methods per\nendpoint, allow advanced filtering of results, or handle advanced\nAuthentication. Neither of us have the time to attempt covering all of those\nthings in detail, but I will  leave you with the knowledge that all these\nfeatures are very much possible.\n\nThe API Gateway  interface is where you'd get started. Let's blow through the\nworld's most inappropriately fast explanation of building APIs ever ,and check\nout the UI:\n\nIt ain't pretty, but it works. * Your APIs are listed on the left. You can create more than one, if you're\n   some sort of sadist.\n * The Resources pane is the full structure of your API. At the highest level,\n   'resources' refers to Endpoints,  which are the URLs your API will ultimately\n   expose.\n * Every Endpoint  can contain whichever Methods  you choose to associate with\n   them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST\n   method could contain entirely unrelated logic from a PUT method: its your\n   responsibility to make sure your API design makes sense.\n * Finally, each Method has their expected Request and Response  structures\n   defined individually, which what the horribly designed box diagram is\n   attempting to explain on the right. The box on the left labeled CLIENT refers\n   to the requester, where the box on the right represents the triggered action.\n\nThis UI is your bread and butter. I hope you're strapped in, because walking\nthrough this interface is going to be hella boring for all of us.\n\nCreating a Method Request\nThe first step to creating an endpoint (let's say a GET endpoint) is to set the\nexpectation for what the user will send to us:\n\nAwe yea, authorization. 1. Authorization  allows you to restrict users from using your API unless they\n    follow your IAM policy.\n 2. Request Validator  lets you chose if you'd like this validation to happen\n    via the body, query string parameters, headers, or all of the above.\n 3. API Keys  are useful if you're creating an API to sell commercially or\n    enforce limited access. If your business model revolves around selling an\n    API, you can realistically do this.\n 4. Query String Parameters  are... actually forget it, you know this by now.\n 5. See above.\n 6. If preferred, the Request Body can be assigned a model,  which is\n    essentially a JSON schema. If a request is made to your endpoint which does\n    not match the request body model, it is a malformed request. We'll cover \n    models  in the advanced course, once somebody actually starts paying me to\n    write this stuff.\n\nMethod Execution: AKA \"What do we do with this?\"\nSet the game plan. 1. Integration Type  specifies which AWS service will be accepting or affected\n    by this request. The vast majority of the time, this will be Lambda. If\n    you're wondering why other AWS Services aren't present, this has been made\n    intentional over time as just about any AWS service you can interact with\n    will still need logic to do anything useful: you can't just shove a JSON\n    object in a database's face and expect to get results. Unless you're using\n    MongoDB or something.\n 2. Lambda Proxies  are generally a bad idea. They auto-format your Lambda's \n    request  and response  body to follow a very  specific structure, which is\n    presumably intended to help speed up or standardize development. The\n    downside is these structures are bloated and most likely contain useless\n    information. To get an idea of what these structures look like, check them\n    out here.\n 3. The Region  your Lambda hosted lives in.\n 4. Name of the Lamba Function  your request will be directed to.\n 5. Execution role  refers to the IAM role your Lambda policy will be a part of.\n    This is kind of an obnoxious concept, but your function has permissions as\n    though it were a user. This is presumably Amazon's way of thinking ahead to\n    extending human rights to robots.\n 6. Caller Credentials  refers to API keys, assuming you chose to use them. If\n    this is checked, the API will not be usable without an API key, thus making\n    it difficult to test\n 7. Credentials Cache  probably refers to expiring credentials or something, I'm\n    sure you'll figure it out.\n 8. Timeout  can be increased if you're dealing with an API call that takes a\n    lot of time to respond, such as occasions with heavy data sets.\n 9. URL Paths probably do something, I don't know. Who really cares?\n\nINTERMISSION: The Part Where Things Happen\nThe next step in the flow would be where the AWS service we selected to handle\nthe request would do its thing. We'll get into that next time.\n\nResponse Codes and Headers\nKeep it 200 baby. 1. While AWS provides users with standard error codes  and generic errors, you\n    can add your own specific error/success messages. Props to whoever puts in\n    the effort.\n 2. Header Mappings  are the headings returned with the response. For example,\n    this is where you might solve cross-domain issues via the \n    Access-Control-Allow-Origin  header.\n\n3. Mapping Templates  are the Content-Type  of the response returned, most\ncommonly application/json.\n\nMethod Response\nI almost never spend time hereThis step is a continuation of the previous step.\nI'm not entirely sure what the point in splitting this into two screens is, but\nI'm guessing its not important.\n\nReap Your Rewards\nAt long last, this brings us to the end of our journey. This is presumably where\n you've executed a successful AWS test or something. However, there's a final\nstep before you go live; deploying your API:\n\nDeploy your API to a live \"stage\" and retire.Next time we'll cover the logical,\nless boring part of writing actual code behind these endpoints.","html":"<p>In our last adventure, we ventured off to create our very own cloud <a href=\"https://hackersandslackers.com/setting-up-mysql-on-aws/\">database</a> by using Amazon's <strong>RDS</strong> service. We've also <a href=\"https://hackersandslackers.com/building-an-api-using-aws/\">briefly covered</a> the general concept behind what <strong>Lambda functions</strong>. In case you've already forgotten, Lambdas are basically just chunks of code in the cloud; think of them as tiny virtual servers, which have already been configured (and locked down) to serve one specific purpose. Because that's literally what it is.</p><p>The data being stored in <strong>RDS </strong>is ultimately what we're targeting, and <strong>Lambdas</strong> serve as the in-between logic to serve up, modify, or add to the proper data. The only piece missing from the picture is <strong>API Gateway</strong>. </p><p>As the name suggests, <strong>API Gateway</strong> is the, uh, <em>gateway</em> that users or systems interact with to obtain what they're seeking. It is (hopefully) the only part of this VPC structure an external user can interact with:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_o-1.jpg\" class=\"kg-image\"><figcaption>Simple API to interact with RDS.</figcaption></figure><p>Serving as a \"gateway\" is obviously what all APIs so, but the term is also true in the sense that <strong>API Gateway</strong> is completely configured via UI, thus engineers of any programming background can safely modify <em>endpoints</em>, <em>methods,</em> <em>CORs</em> configurations, or any of the high-level API structure without being locked into a programming language. <strong>API Gateway</strong> is therefore very much an enterprise-geared product: it lends itself to large teams and scaling. That said, if it were to be compared to building an API via a framework designed to do such things (such as Express or Flask) the experience is undoubtedly more clunky. The trade-off being made for speed is immediate visibility, assurance, and a higher chance for collaboration.</p><h2 id=\"the-challenge-of-building-a-well-designed-api\">The Challenge of Building a \"Well-Designed\" API</h2><p>Good APIs are premeditated. A complex API might accept multiple methods per endpoint, allow advanced filtering of results, or handle advanced Authentication. Neither of us have the time to attempt covering all of those things in detail, but I <em>will</em> leave you with the knowledge that all these features are very much possible.  </p><p>The <strong>API Gateway</strong> interface is where you'd get started. Let's blow through the world's most inappropriately fast explanation of building APIs ever ,and check out the UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_overview3.png\" class=\"kg-image\"><figcaption>It ain't pretty, but it works.</figcaption></figure><ul><li>Your <strong>APIs </strong>are listed on the left. You can create more than one, if you're some sort of sadist.</li><li>The <strong>Resources </strong>pane is the full structure of your API. At the highest level, 'resources' refers to <strong>Endpoints,</strong> which are the URLs your API will ultimately expose.</li><li>Every <strong>Endpoint</strong> can contain whichever <strong>Methods</strong> you choose to associate with them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST method could contain entirely unrelated logic from a PUT method: its your responsibility to make sure your API design makes sense.</li><li>Finally, each <strong>Method </strong>has their expected <strong>Request </strong>and <strong>Response</strong> structures defined individually, which what the horribly designed box diagram is attempting to explain on the right. The box on the left labeled CLIENT refers to the requester, where the box on the right represents the triggered action.</li></ul><p>This UI is your bread and butter. I hope you're strapped in, because walking through this interface is going to be hella boring for all of us.</p><h3 id=\"creating-a-method-request\">Creating a Method Request</h3><p>The first step to creating an endpoint (let's say a GET endpoint) is to set the expectation for what the user will send to us:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodrequest_o.jpg\" class=\"kg-image\"><figcaption>Awe yea, authorization.</figcaption></figure><ol><li><strong>Authorization</strong> allows you to restrict users from using your API unless they follow your IAM policy.</li><li><strong>Request Validator</strong> lets you chose if you'd like this validation to happen via the body, query string parameters, headers, or all of the above.</li><li><strong>API Keys</strong> are useful if you're creating an API to sell commercially or enforce limited access. If your business model revolves around selling an API, you can realistically do this.</li><li><strong>Query String Parameters</strong> are... actually forget it, you know this by now.</li><li>See above.</li><li>If preferred, the <strong>Request Body </strong>can be assigned a <strong>model,</strong> which is essentially a JSON schema. If a request is made to your endpoint which does not match the request body model, it is a malformed request. We'll cover <strong>models</strong> in the advanced course, once somebody actually starts paying me to write this stuff.</li></ol><h3 id=\"method-execution-aka-what-do-we-do-with-this\">Method Execution: AKA \"What do we do with this?\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodexecution_o.jpg\" class=\"kg-image\"><figcaption>Set the game plan.</figcaption></figure><ol><li><strong>Integration Type</strong> specifies which AWS service will be accepting or affected by this request. The vast majority of the time, this will be Lambda. If you're wondering why other AWS Services aren't present, this has been made intentional over time as just about any AWS service you can interact with will still need logic to do anything useful: you can't just shove a JSON object in a database's face and expect to get results. Unless you're using MongoDB or something.</li><li><strong>Lambda Proxies</strong> are generally a bad idea. They auto-format your Lambda's <em>request</em> and <em>response</em> body to follow a very  specific structure, which is presumably intended to help speed up or standardize development. The downside is these structures are bloated and most likely contain useless information. To get an idea of what these structures look like, check them out <a href=\"https://github.com/bbilger/jrestless/tree/master/aws/gateway/jrestless-aws-gateway-handler#response-schema\">here</a>.</li><li>The <strong>Region</strong> your Lambda hosted lives in.</li><li>Name of the <strong>Lamba Function</strong> your request will be directed to.</li><li><strong>Execution role</strong> refers to the IAM role your Lambda policy will be a part of. This is kind of an obnoxious concept, but your function has permissions as though it were a user. This is presumably Amazon's way of thinking ahead to extending human rights to robots.</li><li><strong>Caller Credentials</strong> refers to API keys, assuming you chose to use them. If this is checked, the API will not be usable without an API key, thus making it difficult to test</li><li><strong>Credentials Cache</strong> probably refers to expiring credentials or something, I'm sure you'll figure it out.</li><li><strong>Timeout</strong> can be increased if you're dealing with an API call that takes a lot of time to respond, such as occasions with heavy data sets.</li><li><strong>URL Paths </strong>probably do something, I don't know. Who really cares?</li></ol><h3 id=\"intermission-the-part-where-things-happen\">INTERMISSION: The Part Where Things Happen</h3><p>The next step in the flow would be where the AWS service we selected to handle the request would do its thing. We'll get into that next time.</p><h3 id=\"response-codes-and-headers\">Response Codes and Headers</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/response_o.jpg\" class=\"kg-image\"><figcaption>Keep it 200 baby.</figcaption></figure><ol><li>While AWS provides users with standard <strong>error codes</strong> and generic errors, you can add your own specific error/success messages. Props to whoever puts in the effort.</li><li><strong>Header Mappings</strong> are the headings returned with the response. For example, this is where you might solve cross-domain issues via the <em>Access-Control-Allow-Origin</em> header.</li></ol><p>3. <strong>Mapping Templates</strong> are the <em>Content-Type</em> of the response returned, most commonly <em>application/json.</em></p><h3 id=\"method-response\">Method Response</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-8.54.51-PM_o.png\" class=\"kg-image\"><figcaption>I almost never spend time here</figcaption></figure><p>This step is a continuation of the previous step. I'm not entirely sure what the point in splitting this into two screens is, but I'm guessing its not important.</p><h2 id=\"reap-your-rewards\">Reap Your Rewards</h2><p>At long last, this brings us to the end of our journey. This is presumably where  you've executed a successful AWS test or something. However, there's a final step before you go live; deploying your API:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-9.02.52-PM_o.png\" class=\"kg-image\"><figcaption>Deploy your API to a live \"stage\" and retire.</figcaption></figure><p>Next time we'll cover the logical, less boring part of writing actual code behind these endpoints.</p>","url":"https://hackersandslackers.com/creating-apis-with-api-gateway/","uuid":"ce9c1023-431b-4580-b3ca-1a3e2074f9c5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5af8ae23092feb404eb9981e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372f","title":"Deploy Isolated Applications with Google App Engine","slug":"deploy-app-containters-with-gcp-app-engine","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","excerpt":"Doing everything to avoid server configuration or any mild discomfort.","custom_excerpt":"Doing everything to avoid server configuration or any mild discomfort.","created_at_pretty":"25 October, 2018","published_at_pretty":"25 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-24T20:30:22.000-04:00","published_at":"2018-10-25T07:30:00.000-04:00","updated_at":"2019-01-06T11:26:06.000-05:00","meta_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","meta_description":"Doing everything to avoid server configuration or any mild discomfort.","og_description":"Doing everything to avoid server configuration or any mild discomfort.","og_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","og_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","twitter_description":"Doing everything to avoid server configuration or any mild discomfort.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","twitter_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"We've been on a bit of a tear lately on Google Cloud lately (or at least I\nhave), and I have no desire to stop any time soon. I probably should though...\n our analytics show that half our viewers are just people struggling to us AWS.\nSpeaking of capitalizing on shitty UI, stay tuned in the future where we'll\noffer grossly overpriced unauthorized AWS certification programs.\n\nAWS aside, I'm here to talk about the other  Cloud in town - in particular,\nGoogle's solution to make sure you never configure a webserver again. This is a\ntrend that's been grinding my gears a bit: as much as I appreciate the reduction\nin effort, the costs of choosing proprietary (paid) services to avoid opening a\nLinux shell seems like a dangerous prospect over time as every day developers\nbecome more and more reliant on convenience. Then again, I'm probably just angry\nthat nobody will have to endure the pain of Python development circa 2012. \n\nRandom Old-Man Tangent About Configuring Webservers\nRemember when we all ran Apache servers, and the world decided that mod_python \nshould stop existing for no reason? The replacement was, of course, mod_wsgi:  \nan entirely undocumented way of running Python on an Apache server created by a\nsingle guy from Google (who has apparently opted to spend the entirety of his\nlife attempting to explain mod_wsgi on StackOverflow\n[https://stackoverflow.com/users/128141/graham-dumpleton]). \n\nBesides mod_wsgi, the Nginx alternatives (Gunicorn  and uWSGI) are almost\nequally insufferable to implement. Much of this can be attributed to tutorials\n(such as those posted by DigitalOcean) which dominate SEO, merely because those\ntutorials include glaring inexcusable typos  in their configuration files. This\nresults in an infinite Google search feedback loop, where you find what seems to\nbe a perfectly fine tutorial... plus 10 pages of frustrated developers\nbacklinking to said tutorial, trying to figure out where the hell the missing\ncolon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm\npretty sure at this point nobody cares to put up with monetized troubleshooting\nbullshit schemes (calling it now: the slightly-false-tutorial is an elaborate\nSEO scam).  So yeah, app containers it is then.\n\nThe Benefits of App Engine\nBesides not needing to know anything about Linux, hosting on App Engine provides\na few other benefits. Considering all microservices are obfuscated in the cloud,\nwe can easily hook into other services such as setting up CRON jobs, Tasks, and\nDNS, for instance. GCP's catalogue of offerings is destined to grow, whether\nthose offerings are ferociously released from Google's ambitious backlog, or the\nresult of a partnership utilizing Google Cloud for architecture, such as MongoDB\ncloud and others. Prepare to witness fierce and unapologetic growth from GCP by\nevery metric, year-over-year. \n\nApp Engine  is also intimately close with your source code. Despite the\ndynamically typed nature of Python and Javascript, App Engine will catch fatal\nerrors when attempting to deploy your app which would not happen otherwise.\nAdding this type of interpreter adds a convenient level of 'easy mode,' where\npotentially fatal production errors are caught before deployment is even\npermitted. I even tried deploying some personal projects to App Engine which had\nbeen running live elsewhere, and App Engine was able to catch errors existing in\nmy code which had been shamelessly running in production. Oops.\n\nEven while the app is live, all errors are conveniently detected and reported\nfront and center in the app engine dashboard:\n\n\"No module named App\" seems like a pretty bad error.So yes, there are plenty of\nbenefits and reasons to use App Engine over a VPS: removal of webserver\nconfiguration, build errors caught at runtime, and easy command-line deployments\nname a few of such benefits. The question of whether or not these perks are\nworth the price tag and vendor-lock are a personal decision.\n\nCreating your First App... Engine\nGoogle provides the luxury of creating apps in a variety of languages, including\nhot new -comer to the game, PHP. Lucky us!\n\nHah, .NET is still a thing too.Google will forcefully insist you complete their\nown step-by-step tutorial, which essentially teaches you how to use git clone \nand explains the contents of their YAML file. You can follow this if you want. \n\nMore interestingly is what you'll find when you open the GCP browser shell.\nWhile working through this tutorial, it's impossible to ignore that Google Cloud\nis essentially just a giant VPS across all your projects:\n\nAll of these directories were created in different projects.Just when we were\ndone ranting, it turns out every service we pay for is just a thinly veiled\nobfuscation of something we could probably do on a 10 dollar Droplet. Fuck,\nlet's just move on.\n\nSimple Project Configuration\nPerhaps majority of what one needs to learn to deploy apps is contained within a\nsingle YAML file. Add a YAML file in your directory:\n\nruntime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n\n\nSet your Static directory if you're working in Python. Use Python 3.7.\n\nGoogle also invented its own version of .gitignore  for App Engine called \n.gcloudignore, so be aware of that.\n\nHaving worked with Flask in the past, you should presumably be familiar with a\nstartup script such as the following:\n\nfrom framewrk import create_app\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0')\n\n\nThat's pretty much it man. Just remember that Google prefers requirements.txt \nover other forms of package management (Google will actually bar Pipfile from\nbeing committed, interestingly enough). \n\nIf you're working locally, gcloud app deploy  is all you need to push to\nproduction (doesn't require a git commit, interestingly enough. gcloud app\nbrowse  will open you newly deployed app, and gcloud app logs tail -s default \nwill display your logs when something goes horribly wrong.\n\nAnd there you have it: the practical and completely cynical guide to embracing\nmodern architecture. Join us next time when we pay 50 dollars to deploy a\nsingle-click app simply because we're too lazy or unmotivated to set anything up\nourselves.","html":"<p>We've been on a bit of a tear lately on Google Cloud lately (or at least I have), and I have no desire to stop any time soon. I probably should though...  our analytics show that half our viewers are just people struggling to us AWS. Speaking of capitalizing on shitty UI, stay tuned in the future where we'll offer grossly overpriced unauthorized AWS certification programs.</p><p>AWS aside, I'm here to talk about the <em>other</em> Cloud in town - in particular, Google's solution to make sure you never configure a webserver again. This is a trend that's been grinding my gears a bit: as much as I appreciate the reduction in effort, the costs of choosing proprietary (paid) services to avoid opening a Linux shell seems like a dangerous prospect over time as every day developers become more and more reliant on convenience. Then again, I'm probably just angry that nobody will have to endure the pain of Python development circa 2012. </p><h3 id=\"random-old-man-tangent-about-configuring-webservers\">Random Old-Man Tangent About Configuring Webservers</h3><p>Remember when we all ran Apache servers, and the world decided that <strong>mod_python</strong> should stop existing for no reason? The replacement was, of course, <strong>mod_wsgi</strong>:<strong> </strong>an entirely undocumented way of running Python on an Apache server created by a single guy from Google (who has apparently opted to spend the entirety of his life attempting to <a href=\"https://stackoverflow.com/users/128141/graham-dumpleton\">explain <strong>mod_wsgi</strong> on StackOverflow</a>). </p><p>Besides <strong>mod_wsgi</strong>, the Nginx alternatives (<strong>Gunicorn</strong> and <strong>uWSGI</strong>) are almost equally insufferable to implement. Much of this can be attributed to tutorials (such as those posted by DigitalOcean) which dominate SEO, merely because those tutorials include <em>glaring inexcusable typos</em> in their configuration files. This results in an infinite Google search feedback loop, where you find what seems to be a perfectly fine tutorial... plus 10 pages of frustrated developers backlinking to said tutorial, trying to figure out where the hell the missing colon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm pretty sure at this point nobody cares to put up with monetized troubleshooting bullshit schemes (calling it now: the slightly-false-tutorial is an elaborate SEO scam).  So yeah, app containers it is then.</p><h2 id=\"the-benefits-of-app-engine\">The Benefits of App Engine</h2><p>Besides not needing to know anything about Linux, hosting on App Engine provides a few other benefits. Considering all microservices are obfuscated in the cloud, we can easily hook into other services such as setting up CRON jobs, Tasks, and DNS, for instance. GCP's catalogue of offerings is destined to grow, whether those offerings are ferociously released from Google's ambitious backlog, or the result of a partnership utilizing Google Cloud for architecture, such as MongoDB cloud and others. Prepare to witness fierce and unapologetic growth from GCP by every metric, year-over-year. </p><p><strong>App Engine</strong> is also intimately close with your source code. Despite the dynamically typed nature of Python and Javascript, App Engine will catch fatal errors when attempting to deploy your app which would not happen otherwise. Adding this type of interpreter adds a convenient level of 'easy mode,' where potentially fatal production errors are caught before deployment is even permitted. I even tried deploying some personal projects to App Engine which had been running live elsewhere, and App Engine was able to catch errors existing in my code which had been shamelessly running in production. Oops.</p><p>Even while the app is live, all errors are conveniently detected and reported front and center in the app engine dashboard:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.12.19-PM.png\" class=\"kg-image\"><figcaption>\"No module named App\" seems like a pretty bad error.</figcaption></figure><p>So yes, there are plenty of benefits and reasons to use App Engine over a VPS: removal of webserver configuration, build errors caught at runtime, and easy command-line deployments name a few of such benefits. The question of whether or not these perks are worth the price tag and vendor-lock are a personal decision.</p><h2 id=\"creating-your-first-app-engine\">Creating your First App... Engine</h2><p>Google provides the luxury of creating apps in a variety of languages, including hot new -comer to the game, PHP. Lucky us!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.17.59-PM.png\" class=\"kg-image\"><figcaption>Hah, .NET is still a thing too.</figcaption></figure><p>Google will forcefully insist you complete their own step-by-step tutorial, which essentially teaches you how to use <strong>git clone</strong> and explains the contents of their YAML file. You can follow this if you want. </p><p>More interestingly is what you'll find when you open the GCP browser shell. While working through this tutorial, it's impossible to ignore that Google Cloud is essentially just a giant VPS <em>across all your projects:</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.26.44-PM.png\" class=\"kg-image\"><figcaption>All of these directories were created in different projects.</figcaption></figure><p>Just when we were done ranting, it turns out every service we pay for is just a thinly veiled obfuscation of something we could probably do on a 10 dollar Droplet. Fuck, let's just move on.</p><h3 id=\"simple-project-configuration\">Simple Project Configuration</h3><p>Perhaps majority of what one needs to learn to deploy apps is contained within a single YAML file. Add a YAML file in your directory:</p><pre><code class=\"language-yaml\">runtime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n</code></pre>\n<p>Set your Static directory if you're working in Python. Use Python 3.7.</p><p>Google also invented its own version of <code>.gitignore</code> for App Engine called <code>.gcloudignore</code>, so be aware of that.</p><p>Having worked with Flask in the past, you should presumably be familiar with a startup script such as the following:</p><pre><code class=\"language-python\">from framewrk import create_app\n\napp = create_app()\n\nif __name__ == &quot;__main__&quot;:\n    app.run(host='0.0.0.0')\n</code></pre>\n<p>That's pretty much it man. Just remember that Google prefers <strong>requirements.txt </strong>over other forms of package management (Google will actually bar Pipfile from being committed, interestingly enough). </p><p>If you're working locally, <code>gcloud app deploy</code> is all you need to push to production (doesn't require a git commit, interestingly enough. <code>gcloud app browse</code> will open you newly deployed app, and <code>gcloud app logs tail -s default</code> will display your logs when something goes horribly wrong.</p><p>And there you have it: the practical and completely cynical guide to embracing modern architecture. Join us next time when we pay 50 dollars to deploy a single-click app simply because we're too lazy or unmotivated to set anything up ourselves.</p>","url":"https://hackersandslackers.com/deploy-app-containters-with-gcp-app-engine/","uuid":"70f7a025-6774-4555-9eff-e63eb40c4fdf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd10e9e4ba34679679904f2"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372e","title":"MySQL, Google Cloud, and a REST API that Generates Itself","slug":"mysql-google-cloud-and-a-rest-api-that-autogenerates","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","custom_excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","created_at_pretty":"23 October, 2018","published_at_pretty":"23 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-23T14:57:12.000-04:00","published_at":"2018-10-23T18:47:28.000-04:00","updated_at":"2019-02-02T05:26:16.000-05:00","meta_title":"MySQL, Google Cloud, and a REST API | Hackers and Slackers","meta_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","og_title":"MySQL, Google Cloud, and a REST API that Generates Itself","twitter_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","twitter_title":"MySQL, Google Cloud, and a REST API that Generates Itself","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn’t too long ago that I haphazardly forced us down a journey of exploring\nGoogle Cloud’s cloud SQL service. The focus of this exploration was Google’s\naccompanying REST API for all of its cloud SQL instances. That API turned out to\nbe a relatively disappointing administrative API which did little to extend the\nfeatures you’d expect from the CLI or console.\n\nYou see, I’ve had a dream stuck in my head for a while now. Like most of my\nutopian dreams, this dream is related to data, or more specifically simplifying\nthe manner in which we interact with it. For industry synonymous with AI and\nautomation, many of our very own tools (including ETL tools) involve way too\nmuch manual effort in my opinion. That’s right: I’m talking about the aspiration\nto Slack while we Hack.\n\nThe pitch is this: why do we keep setting up databases, endpoints, and the logic\nto connect them when, 90% of the time, we’re building the same thing over and\nover? Let me guess: there’s a GET endpoint to get records from table X, or a\nPOST endpoint to create users. I know you’ve built this because we all have, but\nwhy do we keep building the same things over and over in isolation? It looks\nlike we might not have to anymore, but first let’s create our database.\n\nCreating a MySQL Instance in GCP \nFull disclosure here: the magical REST API thing is actually independent from\nGoogle Cloud; the service we’ll be using can integrate with any flavor of MySQL\nyou prefer, so go ahead and grab that RDS instance you live so much if you\nreally have to.\n\nFor the rest of us, hit up your GCP console and head into making a new SQL\ninstance. MySQL and Postgres are our only choices here; stick with MySQL.\n\nThere isn’t much to spinning up your instance. Just be sure to create a user and\ndatabase to work from.\n\nOh yeah, and remember to name your instance.Your SQL Firewall and Permissions\nYour instance is set to “public” by default. Oddly, “public” in this case means\n“accessible to everybody on your IP whitelist, which is empty by default,” so\nreally kind of the opposite of public really.\n\nIn fact, if you hypothetically did want to open your instance publicly, Google\nCloud will not allow it. This is good on them, and is actually fairly impressive\nthe depths they go to avoid the IP 0.0.0.0  from ever appearing anywhere in the\ninstance. Go ahead, open the shell and try to add bind address=0.0.0.0 \nyourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s\nversion of MySQL is actually a MariaDB instance)?\n\nThe point is, whitelist your IP address. Simply \"Edit\" your instance and add\nyour address to the authorized networks.\n\nAuthorize that bad boy.The Magic API \nNow, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so\nthis next part is going to feel a bit a bit weird. I’m not sure why, as the\nservice is apparently free, thus I’m clearly not getting paid for any of this.\n\nAnyway, the service is called Apisentris [https://apisentris.com/], and the idea\nis that it will build whatever time-consuming monstrosity of a REST API you were\nplanning to build to access your data for you. Via their own words:\n\nSee, I told you.What does this actually mean? It means if you create a table\ncalled articles  in your database, you will immediately have an endpoint to\nfetch said articles, and it would look like \nhttps://apisentris.com/api/v1/articles. Your client ID and credentials would\nobviously need to be provided to indicate that you're, well, you.\n\nGrabbing entire tables at once would be silly, which is why they also\nautogenerate filters based on the contents of your table:\n\nEndpoints accept query parameters to essentially create a query.Oh yeah, and you\ncan also handle user management via this API as well, if you're building an\nactual app:\n\nPretty easy to hook up into a form or whatever.I'll assume you're sold on the\nidea by now. If a free service that handles the hard parts of backend logic for\nfree isn't your cup of tea, clearly you aren't Slacker material.\n\nSetting it all up\nAs we did before with our own IP, we'll need to whitelist Apisentris' IP the\nsame way in GCP console. Their IP is 104.199.181.125.\n\nCreate a table in your database with some data just to test things out. When\nyou're logged in, you'll be able to see all the endpoints available to you and\nthe associated attributes they have:\n\nNot bad.Any way you slice it, the concept of a self-generating API is very cool\nand yet somehow still not the norm. I'm actually shocked that there are so few\npeople in the Data industry who know \"there must be a better way,\" but then\nagain, data science and software engineering are two very different things. For\nmy fellow Data Engineers out there, take this as a gift and a curse: you have\nthe gift of knowing better from your software background, but are cursed with\nwatching the world not quite realize how pointless half the things they do truly\nare.\n\nOh well. We'll be the ones building the robots anyway.","html":"<p>It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively disappointing administrative API which did little to extend the features you’d expect from the CLI or console.</p><p>You see, I’ve had a dream stuck in my head for a while now. Like most of my utopian dreams, this dream is related to data, or more specifically simplifying the manner in which we interact with it. For industry synonymous with AI and automation, many of our very own tools (including ETL tools) involve way too much manual effort in my opinion. That’s right: I’m talking about the aspiration to Slack while we Hack.</p><p>The pitch is this: why do we keep setting up databases, endpoints, and the logic to connect them when, 90% of the time, we’re building the same thing over and over? Let me guess: there’s a GET endpoint to get records from table X, or a POST endpoint to create users. I know you’ve built this because we all have, but why do we keep building the same things over and over in isolation? It looks like we might not have to anymore, but first let’s create our database.</p><h2 id=\"creating-a-mysql-instance-in-gcp\">Creating a MySQL Instance in GCP </h2><p>Full disclosure here: the magical REST API thing is actually independent from Google Cloud; the service we’ll be using can integrate with any flavor of MySQL you prefer, so go ahead and grab that RDS instance you live so much if you really have to.</p><p>For the rest of us, hit up your GCP console and head into making a new SQL instance. MySQL and Postgres are our only choices here; stick with MySQL.</p><p>There isn’t much to spinning up your instance. Just be sure to create a user and database to work from.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.15.18-PM.png\" class=\"kg-image\"><figcaption>Oh yeah, and remember to name your instance.</figcaption></figure><h3 id=\"your-sql-firewall-and-permissions\">Your SQL Firewall and Permissions</h3><p>Your instance is set to “public” by default. Oddly, “public” in this case means “accessible to everybody on your IP whitelist, which is empty by default,” so really kind of the opposite of public really.</p><p>In fact, if you hypothetically did want to open your instance publicly, Google Cloud will not allow it. This is good on them, and is actually fairly impressive the depths they go to avoid the IP <strong>0.0.0.0</strong> from ever appearing anywhere in the instance. Go ahead, open the shell and try to add <code>bind address=0.0.0.0</code> yourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s version of MySQL is actually a MariaDB instance)?</p><p>The point is, whitelist your IP address. Simply \"Edit\" your instance and add your address to the authorized networks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.12.23-PM.png\" class=\"kg-image\"><figcaption>Authorize that bad boy.</figcaption></figure><h2 id=\"the-magic-api\">The Magic API </h2><p>Now, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so this next part is going to feel a bit a bit weird. I’m not sure why, as the service is apparently free, thus I’m clearly not getting paid for any of this.</p><p>Anyway, the service is called <strong><a href=\"https://apisentris.com/\">Apisentris</a>, </strong>and the idea is that it will build whatever time-consuming monstrosity of a REST API you were planning to build to access your data for you. Via their own words:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.19.16-PM.png\" class=\"kg-image\"><figcaption>See, I told you.</figcaption></figure><p>What does this actually mean? It means if you create a table called <em>articles</em> in your database, you will immediately have an endpoint to fetch said articles, and it would look like <strong>https://apisentris.com/api/v1/articles. </strong>Your client ID and credentials would obviously need to be provided to indicate that you're, well, you.</p><p>Grabbing entire tables at once would be silly, which is why they also autogenerate filters based on the contents of your table:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.25.13-PM.png\" class=\"kg-image\"><figcaption>Endpoints accept query parameters to essentially create a query.</figcaption></figure><p>Oh yeah, and you can also handle user management via this API as well, if you're building an actual app:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.27.43-PM.png\" class=\"kg-image\"><figcaption>Pretty easy to hook up into a form or whatever.</figcaption></figure><p>I'll assume you're sold on the idea by now. If a free service that handles the hard parts of backend logic for free isn't your cup of tea, clearly you aren't Slacker material.</p><h2 id=\"setting-it-all-up\">Setting it all up</h2><p>As we did before with our own IP, we'll need to whitelist Apisentris' IP the same way in GCP console. Their IP is <code>104.199.181.125</code>.</p><p>Create a table in your database with some data just to test things out. When you're logged in, you'll be able to see all the endpoints available to you and the associated attributes they have:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/schema.gif\" class=\"kg-image\"><figcaption>Not bad.</figcaption></figure><p>Any way you slice it, the concept of a self-generating API is very cool and yet somehow still not the norm. I'm actually shocked that there are so few people in the Data industry who know \"there must be a better way,\" but then again, data science and software engineering are two very different things. For my fellow Data Engineers out there, take this as a gift and a curse: you have the gift of knowing better from your software background, but are cursed with watching the world not quite realize how pointless half the things they do truly are.</p><p>Oh well. We'll be the ones building the robots anyway.</p>","url":"https://hackersandslackers.com/mysql-google-cloud-and-a-rest-api-that-autogenerates/","uuid":"c45478bb-54da-4563-89bd-ddd356a234d4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bcf6f08d7ab443ba8b7a5ab"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673719","title":"Creating an AMI with HashiCorp Packer","slug":"hashicorp-packer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","excerpt":"HashiCorp's version control for infrastructure .","custom_excerpt":"HashiCorp's version control for infrastructure .","created_at_pretty":"02 October, 2018","published_at_pretty":"03 October, 2018","updated_at_pretty":"30 December, 2018","created_at":"2018-10-02T16:05:02.000-04:00","published_at":"2018-10-03T07:00:00.000-04:00","updated_at":"2018-12-30T07:00:11.000-05:00","meta_title":"Creating an AMI with HashiCorp Packer | Hackers and Slackers","meta_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","og_title":"Creating an AMI with HashiCorp Packer","twitter_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","twitter_title":"Creating an AMI with HashiCorp Packer","authors":[{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null}],"primary_author":{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Hashicorp","slug":"hashicorp","description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","feature_image":null,"meta_description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","meta_title":"Hashicorp Suite | Hackers and Slackers","visibility":"public"}],"plaintext":"Why use Packer [https://www.packer.io/]? Infrastructure as code has become part\nof the buzzword bingo surrounding operational teams and their desired optimal\nworkflows.\n\nOne could theoretically just start with a base AMI and manually update it and\nthen re-save it as a new AMI, but this process is not repeatable.  We can check\nin our desired infrastructure states as code to version control.  This is good\npractice for change control management.  We can readily see what worked before\nand what was changed in the latest update.  If something catastrophic happens or\nwe encounter unforeseen issues, we can always rollback to a previous state.\n\nI'm the first new guy on our ops team in a few years.  We work with a base image\nto create our EC2 instances and that image does not have my ssh keys.  In our\ncurrent workflow, when I spin up a new instance using our latest base AMI, I\ncan't ssh to the box because my key isn't on there.  Amazon has also released\nAmazon Linux 2, so we had a card to update the base AMI in the backlog.  I\npicked up this task and found the HashiCorp tool to be very powerful and useful.\n\n{\n  \"description\": \"Builds a Base Image for EC2 AWS provisioner\",\n  \"variables\":{\n    \"hostname\": \"cne-aws-trusty64\",\n    \"config_dir\": \".\"\n  },\n\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-04681a1dbd79675a5\",\n      \"instance_type\": \"m5.xlarge\",\n      \"ssh_username\": \"ec2-user\",\n      \"ami_name\": \"snapdragon-v3.6.9\",\n      \"subnet_id\": \"subnet-0000000000\",\n      \"tags\": {\n        \"OS_Version\": \"Amazon Linux 2\",\n        \"Release\": \"2017-12\",\n        \"Builder\": \"packer\"\n      },\n      \"ssh_timeout\": \"60m\"\n    }\n  ],\n\n  \"provisioners\": [\n    {\n      \"type\": \"shell\",\n        \"scripts\": [\n          \"scripts/setup-example.sh\"\n        ]\n    } \n  ]\n}\n\n\nIn our setup script, we install dependencies and a configuration management tool\nadds users, and updates permissions as needed for all of our applications.  It's\nbasically the equivalent of whatever you would do manually to achieve a desired\nstate.","html":"<p><strong>Why use <a href=\"https://www.packer.io/\">Packer</a>? </strong>Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.</p><p>One could theoretically just start with a base AMI and manually update it and then re-save it as a new AMI, but this process is not repeatable.  We can check in our desired infrastructure states as code to version control.  This is good practice for change control management.  We can readily see what worked before and what was changed in the latest update.  If something catastrophic happens or we encounter unforeseen issues, we can always rollback to a previous state.</p><p>I'm the first new guy on our ops team in a few years.  We work with a base image to create our EC2 instances and that image does not have my ssh keys.  In our current workflow, when I spin up a new instance using our latest base AMI, I can't ssh to the box because my key isn't on there.  Amazon has also released Amazon Linux 2, so we had a card to update the base AMI in the backlog.  I picked up this task and found the HashiCorp tool to be very powerful and useful.</p><pre><code>{\n  &quot;description&quot;: &quot;Builds a Base Image for EC2 AWS provisioner&quot;,\n  &quot;variables&quot;:{\n    &quot;hostname&quot;: &quot;cne-aws-trusty64&quot;,\n    &quot;config_dir&quot;: &quot;.&quot;\n  },\n\n  &quot;builders&quot;: [\n    {\n      &quot;type&quot;: &quot;amazon-ebs&quot;,\n      &quot;region&quot;: &quot;us-east-1&quot;,\n      &quot;source_ami&quot;: &quot;ami-04681a1dbd79675a5&quot;,\n      &quot;instance_type&quot;: &quot;m5.xlarge&quot;,\n      &quot;ssh_username&quot;: &quot;ec2-user&quot;,\n      &quot;ami_name&quot;: &quot;snapdragon-v3.6.9&quot;,\n      &quot;subnet_id&quot;: &quot;subnet-0000000000&quot;,\n      &quot;tags&quot;: {\n        &quot;OS_Version&quot;: &quot;Amazon Linux 2&quot;,\n        &quot;Release&quot;: &quot;2017-12&quot;,\n        &quot;Builder&quot;: &quot;packer&quot;\n      },\n      &quot;ssh_timeout&quot;: &quot;60m&quot;\n    }\n  ],\n\n  &quot;provisioners&quot;: [\n    {\n      &quot;type&quot;: &quot;shell&quot;,\n        &quot;scripts&quot;: [\n          &quot;scripts/setup-example.sh&quot;\n        ]\n    } \n  ]\n}\n</code></pre>\n<p>In our setup script, we install dependencies and a configuration management tool adds users, and updates permissions as needed for all of our applications.  It's basically the equivalent of whatever you would do manually to achieve a desired state.</p>","url":"https://hackersandslackers.com/hashicorp-packer/","uuid":"0c50713b-1c8c-4071-b65d-e57fd87f3536","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bb3cf6e7ae39d0d60547523"}}]}},"pageContext":{"slug":"devops","limit":12,"skip":0,"numberOfPages":3,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/devops/page/2/"}}