{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673700","title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","slug":"random-forests-hyperparameters-min_samples_leaf","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","custom_excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","created_at_pretty":"17 September, 2018","published_at_pretty":"17 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-16T21:33:48.000-04:00","published_at":"2018-09-17T07:30:00.000-04:00","updated_at":"2019-02-19T03:44:33.000-05:00","meta_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf | Hackers and Slackers","meta_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","og_description":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","og_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","twitter_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","twitter_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Part 1 (n_estimators) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\nPart 2 (max_depth) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/]\nNotebook here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb]\n\n\n--------------------------------------------------------------------------------\n\nAnother parameter, another set of quirks!\n\nmin_samples_leaf  is sort of similar to max_depth.  It helps us avoid\noverfitting.  It's also non-obvious what you should use as your upper and lower\nlimits to search between.  Let's do what we did last week - build a forest with\nno parameters, see what it does, and use the upper and lower limits!\n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\n\nLet's use the handy function from here\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html]  to\ncrawl the number of samples in a tree's leaf nodes: \n\ndef leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n\n\nLast week we made a function to grab them for a whole forest - since this is the\nsecond time we're doing this, and we may do it again, let's make a modular\nlittle function that takes a crawler function as an argument!\n\ndef getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {\"min\": params.min(),\n           \"max\": params.max()}\n\n\nLet's see it in action!\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#> {'max': 199, 'min': 1}\n\n\nAlmost ready to start optimizing!  Since part of what we get out of optimizing \nmin_samples_leaf  is regularization (and because it's just good practice!),\nlet's make a metric with some cross-validation.  Luckily, Scikit  has a builtin \ncross_val_score  function.  We'll just need to do a teensy bit of tweaking to\nmake it use the area under a precision_recall_curve.\n\nfrom sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n\n\nAwesome, now we have a metric that can be fed into our binary search.\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.402102\n 199\n 0.506455\n 1.416349\n 100\n 0.506455\n 1.401090\n 51\n 0.506455\n 1.394548\n 26\n 0.975894\n 1.396503\n 14\n 0.982954\n 1.398522\n 7\n 0.979888\n 1.398929\n 10\n 0.984789\n 1.404815\n 12\n 0.986302\n 1.391171\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.473848\n 0.082938\n 199\n 0.002084\n 1.039718\n 0.000000\n 100\n 0.002084\n 0.433676\n 0.000111\n 51\n 0.002084\n 0.173824\n 0.000396\n 26\n 0.980393\n 0.251484\n 0.154448\n 14\n 0.995105\n 0.331692\n 0.118839\n 7\n 0.988716\n 0.347858\n 0.112585\n 10\n 0.998930\n 0.581632\n 0.067998\n 12\n 1.002084\n 0.039718\n 1.000000\n Looks like the action's between 1 and 51.  More than that, and the score goes\nwhile simultaneously increasing the runtime - the opposite of what we want!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.389387\n 51\n 0.506455\n 1.403807\n 26\n 0.975894\n 1.404517\n 14\n 0.982954\n 1.385420\n 7\n 0.979888\n 1.398840\n 10\n 0.984789\n 1.393863\n 12\n 0.986302\n 1.411774\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n Big drop-off after 26, it seems!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.407957\n 26\n 0.975894\n 1.398042\n 14\n 0.982954\n 1.396782\n 7\n 0.979888\n 1.396096\n 10\n 0.984789\n 1.402322\n 12\n 0.986302\n 1.401080\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.650270\n 1.084306\n 0.040144\n 26\n 0.096077\n 0.248406\n 0.000000\n 14\n 0.774346\n 0.142157\n 0.954016\n 7\n 0.479788\n 0.084306\n 1.000000\n 10\n 0.950677\n 0.609184\n 0.221294\n 12\n 1.096077\n 0.504512\n 0.336668\n One more with 14 as our upper limit!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.401341\n 14\n 0.982954\n 1.400361\n 7\n 0.979888\n 1.402408\n 4\n 0.981121\n 1.401396\n 3\n 0.983580\n 1.401332\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n 3 it is!I suppose when it gets this small we could use a regular Grid Search,\nbut...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs \nGridSearchCV  and/or RandomizedSearchCV.  Who knows what the future holds?","html":"<p>Part 1 (n_estimators) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a><br>Part 2 (max_depth) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/\">here</a><br>Notebook <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb\">here</a></p><hr><p>Another parameter, another set of quirks!</p><p><code>min_samples_leaf</code> is sort of similar to <code>max_depth</code>.  It helps us avoid overfitting.  It's also non-obvious what you should use as your upper and lower limits to search between.  Let's do what we did last week - build a forest with no parameters, see what it does, and use the upper and lower limits!</p><pre><code class=\"language-python\">import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n</code></pre>\n<p>Let's use the handy function from <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">here</a> to crawl the number of samples in a tree's leaf nodes: </p><pre><code class=\"language-python\">def leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n</code></pre>\n<p>Last week we made a function to grab them for a whole forest - since this is the second time we're doing this, and we may do it again, let's make a modular little function that takes a crawler function as an argument!</p><pre><code class=\"language-python\">def getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {&quot;min&quot;: params.min(),\n           &quot;max&quot;: params.max()}\n</code></pre>\n<p>Let's see it in action!</p><pre><code class=\"language-python\">data = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#&gt; {'max': 199, 'min': 1}\n</code></pre>\n<p>Almost ready to start optimizing!  Since part of what we get out of optimizing <code>min_samples_leaf</code> is regularization (and because it's just good practice!), let's make a metric with some cross-validation.  Luckily, <strong>Scikit</strong> has a builtin <code>cross_val_score</code> function.  We'll just need to do a teensy bit of tweaking to make it use the area under a <code>precision_recall_curve</code>.</p><pre><code class=\"language-python\">from sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n</code></pre>\n<p>Awesome, now we have a metric that can be fed into our binary search.</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.402102</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.506455</td>\n      <td>1.416349</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.506455</td>\n      <td>1.401090</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.394548</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.396503</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.398522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398929</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.404815</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.391171</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.473848</td>\n      <td>0.082938</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.002084</td>\n      <td>1.039718</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.002084</td>\n      <td>0.433676</td>\n      <td>0.000111</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.173824</td>\n      <td>0.000396</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.251484</td>\n      <td>0.154448</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.331692</td>\n      <td>0.118839</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.347858</td>\n      <td>0.112585</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.581632</td>\n      <td>0.067998</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>0.039718</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf1.png\" class=\"kg-image\"></figure><p>Looks like the action's between 1 and 51.  More than that, and the score goes while simultaneously increasing the runtime - the opposite of what we want!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.389387</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.403807</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.404517</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.385420</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398840</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.393863</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.411774</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf2.png\" class=\"kg-image\"></figure><p>Big drop-off after 26, it seems!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.407957</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.398042</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.396782</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.396096</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.402322</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.401080</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.650270</td>\n      <td>1.084306</td>\n      <td>0.040144</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.096077</td>\n      <td>0.248406</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.774346</td>\n      <td>0.142157</td>\n      <td>0.954016</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.479788</td>\n      <td>0.084306</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.950677</td>\n      <td>0.609184</td>\n      <td>0.221294</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.096077</td>\n      <td>0.504512</td>\n      <td>0.336668</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf3.png\" class=\"kg-image\"></figure><p>One more with 14 as our upper limit!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.401341</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.400361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.402408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.981121</td>\n      <td>1.401396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.983580</td>\n      <td>1.401332</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf4.png\" class=\"kg-image\"><figcaption>3 it is!</figcaption></figure><p>I suppose when it gets this small we could use a regular Grid Search, but...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs <code>GridSearchCV</code> and/or <code>RandomizedSearchCV</code>.  Who knows what the future holds?</p>","url":"https://hackersandslackers.com/random-forests-hyperparameters-min_samples_leaf/","uuid":"766a3eb8-aacc-47c6-91a9-744b84613626","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b9f047cab64c97c60f7be90"}},"pageContext":{"slug":"random-forests-hyperparameters-min_samples_leaf"}}