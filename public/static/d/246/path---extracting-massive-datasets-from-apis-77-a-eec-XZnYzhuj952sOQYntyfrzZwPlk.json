{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369d","title":"Extract Massive Amounts of Data from APIs in Python","slug":"extracting-massive-datasets-from-apis","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","excerpt":"Abusing REST APIs for all they’re worth.","custom_excerpt":"Abusing REST APIs for all they’re worth.","created_at_pretty":"04 July, 2018","published_at_pretty":"04 July, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-07-04T15:24:18.000-04:00","published_at":"2018-07-04T17:26:00.000-04:00","updated_at":"2019-03-28T07:55:05.000-04:00","meta_title":"Extracting Massive Datasets in Python | Hackers and Slackers","meta_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","og_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","og_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","og_title":"Extracting Massive Datasets in Python","twitter_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","twitter_title":"Extracting Massive Datasets in Python","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"}],"plaintext":"Taxation without representation. Colonialism. Not letting people eat cake. Human\nbeings rightfully meet atrocities with action in an effort to change the worked\nfor the better. Cruelty by mankind justifies revolution, and it is this writer’s\nopinion that API limitations are one such cruelty.\n\nThe data we need and crave is stashed in readily available APIs all around us.\nIt’s as though we have the keys to the world, but that power often comes with a\nfew caveats:\n\n * Your “key” only lasts a couple of hours, and if you want another one, you’ll\n   have to use some other keys to get another key.\n * You can have the ten thousand records you’re looking for, but you can only\n   pull 50 at a time.\n * You won’t know the exact structure of the data you’re getting, but it’ll\n   probably be a JSON hierarchy designed by an 8-year-old.\n\nAll men may be created equal, but APIs are not. In the spirit of this 4th of\nJuly, let us declare independence from repetitive tasks: One Script, under\nPython, for Liberty and Justice for all.\n\nProject Setup\nWe'll split our project up by separation of concern into just a few files:\n\nmyProject\n├── main.py\n├── config.py\n└── token.py\n\n\nMain.py will unsurprisingly hold the core logic of our script.\n\nConfig.py contains variables such as client secrets and endpoints which we can\neasily swap when applying this script to different APIs. For now we'll just keep\nvariables client_id  and client_secret  in there for now.\n\nToken.py  serves the purpose of Token Generation. Let's start there.\n\nThat's the Token \nSince we're assuming worst case scenarios let's focus on atrocity number one:\nAPIs which require expiring tokens. There are some tyrants in this world who\nbelieve that in order to use their API, it is necessary to to first use a client\nID and client secret to generate a Token which quickly becomes useless hours\nlater. In other words, you need to use an API every time you want to use the\nactual API. Fuck that.\n\nimport requests\nfrom config import client_id, client_secret\n\ntoken_url = 'https://api.fakeapi.com/auth/oauth2/v2/token'\n\ndef generateToken():\n    r = requests.post(token_url, auth=(client_id, client_secret), json={\"grant_type\": \"client_credentials\"})\n    bearer_token = r.json()['access_token']\n    print('new token = ', bearer_token)\n    return bearer_token\n\ntoken = generateToken()\n\n\nWe import client_id  and client_secret  from our config file off the bat: most\nservices will grant these things simply by signing up for their API.\n\nMany APIs have an endpoint which specifically serves the purpose of accepting\nthese variables and spitting out a generated token. token_url  is the variable\nwe use to store this endpoint.\n\nOur token  variable invokes our generateToken()  function which stores the\nresulting Token. With this out of the way, we can now call this function every\ntime we use the API, so we never have to worry about expiring tokens.\n\nPandas to the Rescue\nWe've established that we're looking to pull a large set of data, probably\nsomewhere in the range of thousands of records. While JSON is all fine and\ndandy, it probably isn't very useful for human beings to consume a JSON file\nwith thousands of records. \n\nAgain, we have no idea what the nature of the data coming through will look\nlike. I don't really care to manually map values to fields, and I'm guessing you\ndon't either. Pandas can help us out here: by passing the first page of records\nto Pandas, we can generate the resulting keys into columns in a Dataframe. It's\nalmost like having a database-type schema created for you simply by looking at\nthe data coming through:\n\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nfrom token import token\n\ndef setKeys():\n    headers = {\"Authorization\":\"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    dataframe = pd.DataFrame(columns=r.json()['data'][0].keys())\n    return dataframe\n\nrecords_df = setKeys()\n\nWe can now store all data into records_df  moving forward, allowing us to build\na table of results.\n\nNo Nation for Pagination\nAnd here we are, one of the most obnoxious parts of programming: paginated\nresults. We want thousands of records, but we're only allowed 50 at a time. Joy.\n\nWe've already set records_df  earlier as a global variable, so we're going to\nappend every page of results we get to that Dataframe, starting at page #1. The\nfunction getRecords  is going to pull that first page for us.\n\nbase_url = 'https://api.fakeapi.com/api/1/'\n\ndef getRecords():\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records_df = pd.DataFrame(columns=r.json()['data'][0].keys())\n    if nextpage:\n        getNextPage(nextpage)\n\ngetRecords()\n\n\nLuckily APIs if there are  additional pages of results to a request, most APIs\nwill provide a URL to said page, usually stored in the response as a value. In\nour case, you can see we find this value after making the request: nextpage =\nr.json()['pagination']['next_link']. If this value exists, we make a call to get\nthe next page of results.\n\npage = 1\n\ndef getNextPage(nextpage):\n    global page\n    page = page + 1\n    print('PAGE ', page)\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(nextpage, headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records = r.json()['data']\n    for user in records:\n        s  = pd.Series(user,index=user.keys())\n        global records_df\n        records_df.loc[len(records_df)] = s\n    records_df.to_csv('records.csv')\n    if nextpage:\n        getNextPage(nextpage)\n\nOur function getNextPage  hits that next page of results, and appends them to\nthe pandas DataFrame we created earlier. If another page exists after that, the\nfunction runs again, and our page increments by 1. As long as more pages exist,\nthis function will fire again and again until all innocent records are driven\nout of their comfortable native resting place and forced into our contained\ndataset. There's not much more American than that.\n\nThere's More We Can Do\nThis script is fine, but it can optimized to be even more modular to truly be\none-size-fits-all. For instance, some APIs don't tell you the number of pages \nyou should except, but rather the number of records.  In those cases, we'd have\nto divide total number of records by records per page to know how many pages to\nexpect. As much as I want to go into detail about writing loops on the 4th of\nJuly, I don't. At all.\n\nThere are plenty more examples, but this should be enough to get us thinking how\nwe can replace tedious work with machines. That sounds like a flavor that pairs\nperfectly with Bud Light and hotdogs if you ask me.","html":"<p>Taxation without representation. Colonialism. Not letting people eat cake. Human beings rightfully meet atrocities with action in an effort to change the worked for the better. Cruelty by mankind justifies revolution, and it is this writer’s opinion that API limitations are one such cruelty.</p><p>The data we need and crave is stashed in readily available APIs all around us. It’s as though we have the keys to the world, but that power often comes with a few caveats:</p><ul><li>Your “key” only lasts a couple of hours, and if you want another one, you’ll have to use some other keys to get another key.</li><li>You can have the ten thousand records you’re looking for, but you can only pull 50 at a time.</li><li>You won’t know the exact structure of the data you’re getting, but it’ll probably be a JSON hierarchy designed by an 8-year-old.</li></ul><p>All men may be created equal, but APIs are not. In the spirit of this 4th of July, let us declare independence from repetitive tasks: One Script, under Python, for Liberty and Justice for all.</p><h2 id=\"project-setup\">Project Setup</h2><p>We'll split our project up by separation of concern into just a few files:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">myProject\n├── main.py\n├── config.py\n└── token.py\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Main.py </strong>will unsurprisingly hold the core logic of our script.</p><p><strong>Config.py </strong>contains variables such as client secrets and endpoints which we can easily swap when applying this script to different APIs. For now we'll just keep variables <code>client_id</code> and <code>client_secret</code> in there for now.</p><p><strong>Token.py</strong> serves the purpose of Token Generation. Let's start there.</p><h2 id=\"that-s-the-token\">That's the Token </h2><p>Since we're assuming worst case scenarios let's focus on atrocity number one: APIs which require expiring tokens. There are some tyrants in this world who believe that in order to use their API, it is necessary to to first use a client ID and client secret to generate a Token which quickly becomes useless hours later. In other words, you need to use an API every time you want to use the actual API. Fuck that.</p><!--kg-card-begin: code--><pre><code>import requests\nfrom config import client_id, client_secret\n\ntoken_url = 'https://api.fakeapi.com/auth/oauth2/v2/token'\n\ndef generateToken():\n    r = requests.post(token_url, auth=(client_id, client_secret), json={\"grant_type\": \"client_credentials\"})\n    bearer_token = r.json()['access_token']\n    print('new token = ', bearer_token)\n    return bearer_token\n\ntoken = generateToken()\n</code></pre><!--kg-card-end: code--><p>We import <code>client_id</code> and <code>client_secret</code> from our config file off the bat: most services will grant these things simply by signing up for their API.  </p><p>Many APIs have an endpoint which specifically serves the purpose of accepting these variables and spitting out a generated token. <code>token_url</code> is the variable we use to store this endpoint.</p><p>Our <code>token</code> variable invokes our <code>generateToken()</code> function which stores the resulting Token. With this out of the way, we can now call this function every time we use the API, so we never have to worry about expiring tokens.</p><h2 id=\"pandas-to-the-rescue\">Pandas to the Rescue</h2><p>We've established that we're looking to pull a large set of data, probably somewhere in the range of thousands of records. While JSON is all fine and dandy, it probably isn't very useful for human beings to consume a JSON file with thousands of records. </p><p>Again, we have no idea what the nature of the data coming through will look like. I don't really care to manually map values to fields, and I'm guessing you don't either. Pandas can help us out here: by passing the first page of records to Pandas, we can generate the resulting keys into columns in a Dataframe. It's almost like having a database-type schema created for you simply by looking at the data coming through:</p><!--kg-card-begin: code--><pre><code>import requests\nimport pandas as pd\nimport numpy as np\nimport json\nfrom token import token\n\ndef setKeys():\n    headers = {\"Authorization\":\"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    dataframe = pd.DataFrame(columns=r.json()['data'][0].keys())\n    return dataframe\n\nrecords_df = setKeys()</code></pre><!--kg-card-end: code--><p>We can now store all data into <code>records_df</code> moving forward, allowing us to build a table of results.</p><h2 id=\"no-nation-for-pagination\">No Nation for Pagination</h2><p>And here we are, one of the most obnoxious parts of programming: paginated results. We want thousands of records, but we're only allowed 50 at a time. Joy.</p><p>We've already set <code>records_df</code> earlier as a global variable, so we're going to append every page of results we get to that Dataframe, starting at page #1. The function <code>getRecords</code> is going to pull that first page for us.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">base_url = 'https://api.fakeapi.com/api/1/'\n\ndef getRecords():\n    headers = {&quot;Authorization&quot;: &quot;Bearer &quot; + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records_df = pd.DataFrame(columns=r.json()['data'][0].keys())\n    if nextpage:\n        getNextPage(nextpage)\n\ngetRecords()\n</code></pre>\n<!--kg-card-end: markdown--><p>Luckily APIs if there <em>are</em> additional pages of results to a request, most APIs will provide a URL to said page, usually stored in the response as a value. In our case, you can see we find this value after making the request: <code>nextpage = r.json()['pagination']['next_link']</code>. If this value exists, we make a call to get the next page of results.</p><!--kg-card-begin: code--><pre><code>page = 1\n\ndef getNextPage(nextpage):\n    global page\n    page = page + 1\n    print('PAGE ', page)\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(nextpage, headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records = r.json()['data']\n    for user in records:\n        s  = pd.Series(user,index=user.keys())\n        global records_df\n        records_df.loc[len(records_df)] = s\n    records_df.to_csv('records.csv')\n    if nextpage:\n        getNextPage(nextpage)</code></pre><!--kg-card-end: code--><p>Our function <code>getNextPage</code> hits that next page of results, and <em>appends them to the pandas DataFrame </em>we created earlier. If another page exists after that, the function runs again, and our page increments by 1. As long as more pages exist, this function will fire again and again until all innocent records are driven out of their comfortable native resting place and forced into our contained dataset. There's not much more American than that.</p><h2 id=\"there-s-more-we-can-do\">There's More We Can Do</h2><p>This script is fine, but it can optimized to be even more modular to truly be one-size-fits-all. For instance, some APIs don't tell you the number of <em>pages</em> you should except, but rather the number of <em>records.</em> In those cases, we'd have to divide total number of records by records per page to know how many pages to expect. As much as I want to go into detail about writing loops on the 4th of July, I don't. At all.</p><p>There are plenty more examples, but this should be enough to get us thinking how we can replace tedious work with machines. That sounds like a flavor that pairs perfectly with Bud Light and hotdogs if you ask me.</p>","url":"https://hackersandslackers.com/extracting-massive-datasets-from-apis/","uuid":"39a94407-5d5a-4038-a6b6-04fa228ad0f0","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3d1ee2d0ac8a143588f36e"}},"pageContext":{"slug":"extracting-massive-datasets-from-apis"}}