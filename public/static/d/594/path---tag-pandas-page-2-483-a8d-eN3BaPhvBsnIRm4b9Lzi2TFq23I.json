{"data":{"ghostTag":{"slug":"pandas","name":"Pandas","visibility":"public","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673682","title":"Using Pandas with AWS Lambda Functions","slug":"using-pandas-with-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","custom_excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","created_at_pretty":"20 June, 2018","published_at_pretty":"21 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-20T18:21:31.000-04:00","published_at":"2018-06-21T07:30:00.000-04:00","updated_at":"2019-03-28T08:49:25.000-04:00","meta_title":"Using Pandas with AWS Lambda Functions | Hackers and Slackers","meta_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","og_title":"Using Pandas with AWS Lambda","twitter_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-2.jpg","twitter_title":"Using Pandas with AWS Lambda","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In one corner we have Pandas: Python's beloved data analysis library. In the\nother, AWS: the unstoppable cloud provider we're obligated to use for all\neternity. We should have known this day would come.\n\nWhile not the prettiest workflow, uploaded Python package dependencies for usage\nin AWS Lambda is typically straightforward. We install the packages locally to a\nvirtual env, package them with our app logic, and upload a neat CSV to Lambda.\nIn some cases this doesn't always work: some packages result in a cryptic error\nmessage with absolutely no helpful instruction. Pandas is one of those packages.\n\nWhy is this? I can't exactly speak to that, but I can speak to how to fix it.\n\nSpin up an EC2 Instance\nCertain Python packages need to be installed and compiled on an EC2 instance in\norder to work properly with AWS microservices. I wish I could say that this fun\nlittle fact is well-documented somewhere in AWS with a perfectly good\nexplanation. It's not, and it doesn't.  It's probably best not to ask questions.\n\nSpin up a free tier EC2 instance, update your system packages, and make sure\nPython3 is installed. Some people theorize that the Python dependency package\nerrors happen when said dependencies are installed via versions of Python which\ndiffer from the version AWS is running. Those people are wrong.  I've already\nwasted the time to debunk this. They are liars.\n\nWith Python installed,  create a virtual environment inside any empty directory:\n\n$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n\n\nWith the environment active, install pandas via pip3 install pandas. This will\nsave pandas and all its dependencies to the site-packages  folder our\nenvironment is running from, resulting in a URL such as this: \npandasenv/lib/python3.6/site-packages.\n\nPandas is actually 5 packages total. We're going to add each of these libraries\nto a zip file by installing zip, and adding each folder to the zip file\none-by-one. Finally, we'll apply some liberal permissions to the zip file we\njust created so we can grab it via FTP.\n\n$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n\n\nThis should be ready for you to FTP in your instance and grab as a zip file now\n(assuming you want to work locally). Alternatively, we could always copy those\npackages into the directory we'd like to work out of and zip everything once\nwe're done.\n\nUpload Source Code to S3\nAt this point, you should have been able to grab the AWS friendly version of\nPandas which is ready to be included in the final source code which will become\nyour Lambda Function.  You might notice that pandas alone nearly 30Mb: which is\nroughly the file size of countless intelligent people creating their life's\nwork. When Lambda Functions go above this file size, it's best to upload our\nfinal package (with source and dependencies) as a zip file to S3, and link it to\nLambda that way. This is considerably faster than the alternative of uploading\nthe zip to Lambda directly.\n\nBonus Round: Saving Exports\nWhat? You want to save a CSV result of all the cool stuff you're doing in\nPandas? You really are needy.\n\nBecause AWS is invoking the function, any attempt to read_csv()  will be\nworthless to us. To get around this, we can use boto3  to write files to an S3\nbucket instead:\n\nimport pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n\n\nWord of Advice\nThis isn't the prettiest process in the world, but we're somewhat at fault here.\nLambda functions are intended to be small tidbits of logic aimed to serve a\nsingle simple purpose. We just jammed 30Mbs of Python libraries into that simple\npurpose.\n\nThere are alternatives to Pandas that are better suited for usage in Lambda,\nsuch as Toolz  (thanks to Snkia for the heads up). Enjoy your full Pandas\nlibrary for now, but remember to feel bad about what you’ve done for next time.","html":"<p>In one corner we have Pandas: Python's beloved data analysis library. In the other, AWS: the unstoppable cloud provider we're obligated to use for all eternity. We should have known this day would come.</p><p>While not the prettiest workflow, uploaded Python package dependencies for usage in AWS Lambda is typically straightforward. We install the packages locally to a virtual env, package them with our app logic, and upload a neat CSV to Lambda. In some cases this doesn't always work: some packages result in a cryptic error message with absolutely no helpful instruction. Pandas is one of those packages.</p><p>Why is this? I can't exactly speak to that, but I can speak to how to fix it.</p><h2 id=\"spin-up-an-ec2-instance\">Spin up an EC2 Instance</h2><p>Certain Python packages need to be installed and compiled on an EC2 instance in order to work properly with AWS microservices. I wish I could say that this fun little fact is well-documented somewhere in AWS with a perfectly good explanation. It's not, and it doesn't.  It's probably best not to ask questions.</p><p>Spin up a free tier EC2 instance, update your system packages, and make sure Python3 is installed. Some people theorize that the Python dependency package errors happen when said dependencies are installed via versions of Python which differ from the version AWS is running. <em>Those people are wrong.</em> I've already wasted the time to debunk this. They are liars.</p><p>With Python installed,  create a virtual environment inside any empty directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n</code></pre>\n<!--kg-card-end: markdown--><p>With the environment active, install pandas via <code>pip3 install pandas</code>. This will save pandas and all its dependencies to the <em>site-packages</em> folder our environment is running from, resulting in a URL such as this: <code>pandasenv/lib/python3.6/site-packages</code>.</p><p>Pandas is actually 5 packages total. We're going to add each of these libraries to a zip file by installing <code>zip</code>, and adding each folder to the zip file one-by-one. Finally, we'll apply some liberal permissions to the zip file we just created so we can grab it via FTP.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n</code></pre>\n<!--kg-card-end: markdown--><p>This should be ready for you to FTP in your instance and grab as a zip file now (assuming you want to work locally). Alternatively, we could always copy those packages into the directory we'd like to work out of and zip everything once we're done.</p><h3 id=\"upload-source-code-to-s3\">Upload Source Code to S3</h3><p>At this point, you should have been able to grab the <em>AWS friendly </em>version of Pandas which is ready to be included in the final source code which will become your Lambda Function.  You might notice that pandas alone nearly <em>30Mb</em>: which is roughly the file size of countless intelligent people creating their life's work. When Lambda Functions go above this file size, it's best to upload our final package (with source and dependencies) as a zip file to S3, and link it to Lambda that way. This is considerably faster than the alternative of uploading the zip to Lambda directly.</p><h2 id=\"bonus-round-saving-exports\">Bonus Round: Saving Exports</h2><p>What? You want to save a CSV result of all the cool stuff you're doing in Pandas? You really are needy.</p><p>Because AWS is invoking the function, any attempt to <code>read_csv()</code> will be worthless to us. To get around this, we can use <strong>boto3</strong> to write files to an S3 bucket instead:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"word-of-advice\">Word of Advice</h3><p>This isn't the prettiest process in the world, but we're somewhat at fault here. Lambda functions are intended to be small tidbits of logic aimed to serve a single simple purpose. We just jammed 30Mbs of Python libraries into that simple purpose.</p><p>There are alternatives to Pandas that are better suited for usage in Lambda, such as <em>Toolz</em> (thanks to Snkia for the heads up). Enjoy your full Pandas library for now, but remember to feel bad about what you’ve done for next time.</p>","url":"https://hackersandslackers.com/using-pandas-with-aws-lambda/","uuid":"3d2d6592-5614-4485-b9cd-15d905a28c46","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2ad36cded32f5af8fd674d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673636","title":"Dropping Rows of Data Using Pandas","slug":"pandas-dataframe-drop","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","custom_excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","created_at_pretty":"22 November, 2017","published_at_pretty":"18 April, 2018","updated_at_pretty":"08 March, 2019","created_at":"2017-11-22T01:21:36.000-05:00","published_at":"2018-04-18T15:00:00.000-04:00","updated_at":"2019-03-08T14:23:37.000-05:00","meta_title":"Dropping Rows Using Pandas | Hackers and Slackers","meta_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","og_title":"Dropping Rows Using Pandas","twitter_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","twitter_title":"Dropping Rows Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"You've heard the cliché before: it is often cited that roughly %80~ of a data\nscientist's role is dedicated to cleaning data sets. I Personally haven't looked\nin to the papers or clinical trials which prove this number (that was a joke),\nbut the idea holds true: in the data profession, we find ourselves doing away\nwith blatantly corrupt or useless data. The simplistic approach is to discard\nsuch data entirely, thus here we are.\n\nWhat constitutes 'filthy' data is project-specific, and at times borderline\nsubjective. Occasionally, the offenders are more obvious: these might include\nchunks of data which are empty, poorly formatted, or simply irrelevant. While\n'bad' data can occasionally be fixed or salvaged via transforms, in many cases\nit's best to do away with rows entirely to ensure that only the fittest survive.\n\nDrop Empty Rows or Columns\nIf you're looking to drop rows (or columns) containing empty data, you're in\nluck: Pandas' dropna()  method is specifically for this. \n\nUsing dropna()  is a simple one-liner which accepts a number of useful\narguments:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\nTechnically you could run MyDataFrame.dropna()  without any parameters, and this\n would default to dropping all rows where are completely empty. If thats all you\nneeded, well, I guess you're done already. Otherwise, here are the parameters\nyou can include:\n\n * Axis: Specifies to drop by row  or column. 0  means row, 1  means column.\n * How: Accepts one of two possible values: any  or all. This will either drop\n   an axis which is completely empty (all), or an axis with even just a single\n   empty cell (any).\n * Thresh: Here's an interesting one: thresh  accepts an integer, and will drop\n   an axis only if that number threshold of empty cells is breached.\n * Subset: Accepts an array of which axis' to consider, as opposed to\n   considering all by default.\n * Inplace: If you haven't come across inplace  yet, learn this now: changes\n   will NOT be made to the DataFrame you're touching unless this is set to True.\n   It's False  by default.\n\nPandas' .drop() Method\nThe pandas .drop()  method is used to remove entire rows or columns based on\ntheir name. If we can see that our DataFrame contains extraneous information\n(perhaps for example, the HR team is storing a preferred_icecream_flavor  in\ntheir master records), we can destroy the column (or row) outright.\n\nUsing drop()  looks something like this:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n\nWe'll attempt to cover the usage of these parameters in plain English before\ninevitably falling into useless lingo which you have not yet learned.\n\n *   Axis: Similar to the above, setting the axis specifies if you're trying to\n   drop rows or columns. \n *   Labels: May refer to either the name (string) of the target axis, or its\n   index (int). Of course, whether this is referring to columns or rows in the\n   DataFrame is dependent on the value of the axis parameter. Labels are always\n   defined in the 0th axis of the target DataFrame, and may accept multiple\n   values in the form of an array when dropping multiple rows/columns at once. \n\nDrop by Index:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n\n\nDrop by Label:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n\n\n *   Index, Columns: An alternative method for specifying the same as the above.\n   Accepts single or multiple values. Setting columns=labels  is equivalent to \n   labels, axis=1.  index=0* is equivalent to *labels=0.  \n *   Levels: Used in sets of data which contain multiple hierarchical levels,\n   similar to that of nested arrays. A high-level few of Hierarchical indexing\n   can be found here\n   [https://pandas.pydata.org/pandas-docs/stable/advanced.html]. \n *   Inplace: Again, drop methods are not carried out on the target Dataframe\n   unless explicitly stated. The purpose of this is to presumably preserve the\n   original set of data during ad hoc manipulation.This adheres to the Python\n   style-guide which states that actions should not be performed on live sets of\n   data unless explicitly stated. Here\n   [https://www.youtube.com/watch?v=XaCSdr7pPmY]  is a video of some guy\n   describing this for some reason. \n *   Errors: Accepts either ignore  or raise, with 'raise' set as default. When \n   errors='ignore'  is set, no errors will be thrown and existing labels are\n   dropped. \n\nDrop by Criteria\nWe can also remove rows or columns based on whichever criteria your little heart\ndesires. For example, if you really hate people named Chad, you can drop all\nrows in your Customer database who have the name Chad. Screw Chad.\n\nUnlike previous methods, the popular way of handling this is simply by saving\nyour Dataframe over itself give a passed value. Here's how we'd get rid of Chad:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n\n\nThe syntax may seem a bit off-putting to newcomers (note the repetition of \nmy_dataframe  3 times). The format of my_dataframe[CONDITION]  simply returns a\nmodified version of my_dataframe, where only the data matching the given\ncondition is affected. \n\nSince we're purging this data altogether, statingmy_dataframe =\nmy_dataframe[CONDITION]  is an easy (albeit destructive) method for shedding\ndata and moving on with our lives.","html":"<p>You've heard the cliché before: it is often cited that roughly %80~ of a data scientist's role is dedicated to cleaning data sets. I Personally haven't looked in to the papers or clinical trials which prove this number (that was a joke), but the idea holds true: in the data profession, we find ourselves doing away with blatantly corrupt or useless data. The simplistic approach is to discard such data entirely, thus here we are.</p><p>What constitutes 'filthy' data is project-specific, and at times borderline subjective. Occasionally, the offenders are more obvious: these might include chunks of data which are empty, poorly formatted, or simply irrelevant. While 'bad' data can occasionally be fixed or salvaged via transforms, in many cases it's best to do away with rows entirely to ensure that only the fittest survive.</p><h2 id=\"drop-empty-rows-or-columns\">Drop Empty Rows or Columns</h2><p>If you're looking to drop rows (or columns) containing empty data, you're in luck: Pandas' <code>dropna()</code> method is specifically for this. </p><p>Using <code>dropna()</code> is a simple one-liner which accepts a number of useful arguments:</p><!--kg-card-begin: code--><pre><code>import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)</code></pre><!--kg-card-end: code--><p>Technically you could run <code>MyDataFrame.dropna()</code> without any parameters, and this  would default to dropping all rows where are completely empty. If thats all you needed, well, I guess you're done already. Otherwise, here are the parameters you can include:</p><ul><li><strong>Axis</strong>: Specifies to drop by <em>row</em> or <em>column</em>. <code>0</code> means <em>row</em>, <code>1</code> means <em>column</em>.</li><li><strong>How</strong>: Accepts one of two possible values: <em>any</em> or <em>all</em>. This will either drop an axis which is completely empty (all), or an axis with even just a single empty cell (any).</li><li><strong>Thresh</strong>: Here's an interesting one: <em>thresh</em> accepts an integer, and will drop an axis only if that number threshold of empty cells is breached.</li><li><strong>Subset</strong>: Accepts an array of which axis' to consider, as opposed to considering all by default.</li><li><strong>Inplace</strong>: If you haven't come across <code>inplace</code> yet, learn this now: changes will NOT be made to the DataFrame you're touching unless this is set to <code>True</code>. It's <code>False</code> by default.</li></ul><h2 id=\"pandas-drop-method\">Pandas' .drop() Method</h2><p>The pandas <code>.drop()</code> method is used to remove entire rows or columns based on their name. If we can see that our DataFrame contains extraneous information (perhaps for example, the HR team is storing a <strong>preferred_icecream_flavor</strong> in their master records), we can destroy the column (or row) outright.</p><p>Using <code>drop()</code> looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n</code></pre>\n<!--kg-card-end: markdown--><p>We'll attempt to cover the usage of these parameters in plain English before inevitably falling into useless lingo which you have not yet learned.</p><ul><li> <strong>Axis</strong>: Similar to the above, setting the axis specifies if you're trying to drop rows or columns. </li><li> <strong>Labels</strong>: May refer to either the name (string) of the target axis, or its index (int). Of course, whether this is referring to columns or rows in the DataFrame is dependent on the value of the axis parameter. Labels are always defined in the 0th axis of the target DataFrame, and may accept multiple values in the form of an array when dropping multiple rows/columns at once. </li></ul><h3 id=\"drop-by-index-\">Drop by Index:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"drop-by-label-\">Drop by Label:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n</code></pre>\n<!--kg-card-end: markdown--><ul><li> <strong>Index, Columns</strong>: An alternative method for specifying the same as the above. Accepts single or multiple values. Setting <em>columns=labels</em> is equivalent to <em>labels, axis=1.</em> <em>index=0</em>* is equivalent to *<em>labels=0.</em> </li><li> <strong>Levels</strong>: Used in sets of data which contain multiple hierarchical levels, similar to that of nested arrays. A high-level few of Hierarchical indexing can be found <a href=\"https://pandas.pydata.org/pandas-docs/stable/advanced.html\">here</a>. </li><li> <strong>Inplace</strong>: Again, drop methods are not carried out on the target Dataframe unless explicitly stated. The purpose of this is to presumably preserve the original set of data during ad hoc manipulation.This adheres to the Python style-guide which states that actions should not be performed on live sets of data unless explicitly stated. <a href=\"https://www.youtube.com/watch?v=XaCSdr7pPmY\">Here</a> is a video of some guy describing this for some reason. </li><li> <strong>Errors</strong>: Accepts either <em>ignore</em> or <em>raise</em>, with 'raise' set as default. When <em>errors='ignore'</em> is set, no errors will be thrown and existing labels are dropped. </li></ul><h2 id=\"drop-by-criteria\">Drop by Criteria</h2><p>We can also remove rows or columns based on whichever criteria your little heart desires. For example, if you really hate people named Chad, you can drop all rows in your Customer database who have the name Chad. Screw Chad.</p><p>Unlike previous methods, the popular way of handling this is simply by saving your Dataframe over itself give a passed value. Here's how we'd get rid of Chad:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n</code></pre>\n<!--kg-card-end: markdown--><p>The syntax may seem a bit off-putting to newcomers (note the repetition of <code>my_dataframe</code> 3 times). The format of <code>my_dataframe[CONDITION]</code> simply returns a modified version of <code>my_dataframe</code>, where only the data matching the given condition is affected. </p><p>Since we're purging this data altogether, stating  <code>my_dataframe = my_dataframe[CONDITION]</code> is an easy (albeit destructive) method for shedding data and moving on with our lives.</p>","url":"https://hackersandslackers.com/pandas-dataframe-drop/","uuid":"6f57d667-6bab-4d97-a62a-adfb2e887d6c","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a151770ade7aa41676efce7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673632","title":"Merge Sets of Data in Python Using Pandas","slug":"merge-dataframes-with-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","excerpt":"Perform SQL-like merges of data using Python's Pandas.","custom_excerpt":"Perform SQL-like merges of data using Python's Pandas.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"26 December, 2018","created_at":"2017-11-17T19:09:32.000-05:00","published_at":"2017-11-17T19:22:25.000-05:00","updated_at":"2018-12-26T04:29:22.000-05:00","meta_title":"Merging Dataframes with Pandas | Hackers and Slackers","meta_description":"Perform merges of data similar to SQL JOINs using Python's Pandas library: the essential library for data analysis in Oython. ","og_description":"Perform SQL-like merges of data using Python's Pandas","og_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","og_title":"Merging Dataframes with Pandas","twitter_description":"Perform SQL-like merges of data using Python's Pandas","twitter_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","twitter_title":"Merging Dataframes with Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you have two obscenely large sets of data. \n\nThese sets of data contain information on a similar topic, such as customers. \nDataset #1 might contain a high-level view of all customers of a business, while\n Datatset #2  contains a lifetime history of orders for a company.\nUnsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any\nbusiness' orders are made by customers.\n\nWelcome to Relational Databases\nWhat we just described is the core foundation for relational databases  which\nhave been running at the core of businesses since the 1970s. Starting with\nfamiliar names like MySQL,  Oracle, and Postgres,  the concept of maintaining\nmultiple -related- tables of data are the bare minimum technology stack for any\ncompany, regardless of what said company does.\n\nWhile our example of Datasets #1 and #2  can be thought of as isolated tables,\nthe process of 'joining' them (in SQL terms) or 'merging' them (in Pandas terms) \n is  trivial. What's more, we can do far more than with JOINS (or merges) than\nsimply combining our data into a single set.\n\nEnter The Panda\nPython happens to have an obscenely popular library for performing SQL-like\nlogic, dubbed Pandas. If it remains unclear as to what Pandas is, just remember:\n Databases are basically Excel spreadsheets are basically an interface for\nPandas. The technicality of that explanation may be horrendous to those who\nunderstand the differences, but the fundamental truth remains: we're dealing\nwith information, inside of cells, on a two-dimensional grid. When you hear the\nnext idiot spew a catch phrase like \"data is the new oil\", the \"data\" they're\nreferring to is akin to that sick Excel sheet you made at work.\n\nScenario: Finding Mismatches in Data\nThis scenario actually stems from a real-life example which, sure enough, was my\nfirst encounter with Pandas. One could argue I owe much 0f my data career to a\n3am Google Hangout with Snkia.\n\nIn our scenario, our company has signed up for a very expensive software product\nwhich charges by individual license. To our surprise, the number of licenses for\nthis software totaled over 1000  seats!  After giving this data a quick glance,\nhowever, it's clear that many of these employees have actually been terminated,\nthus resulting in unspeakable loss in revenue. \n\nThe good news is we have another dataset called active employees (aka: employees\nwhich have not been terminated... yet). So, how do we use these two sets of data\nto determine which software licenses are valid? First, let's look at the types \nof ways we can merge data in Pandas.\n\nTerminology\nMERGE\nSets of data can be merged in a number of ways. Merges can either be used to\nfind similarities in two Dataframes and merge associated information, or may be\nentirely non-destructive in the way that two sets of data are merged.\n\nKEY\nIn many cases (such as the one in this tutorial) you'd likely want to merge two\nDataframes based on the value of a key. A key is the authoritative column by\nwhich the Dataframes will be merged. When merging Dataframes in this way, keys\nwill stay in tact as an identifier while the values of columns in the same row\nassociated to that key.\n\nThis type of merge can be used when two Dataframes hold differing fields for\nsimilar rows. If Dataframe 1 contains the phone numbers of customers by name,\nand Dataframe 2 contains emails of a similar grouping of people, these two may\nbe merged to create a single collection of data with all of this information.\n\nAXIS\nA parameter of pandas functions which determines whether the function should be\nrun against a Dataframe's columns or rows. An axis of 0 determines that the\naction will be taken on a per-row basis, where an axis of 1 denotes column.\n\nFor example, performing a drop with axis 0 on key X will drop the row where\nvalue of a cell is equal to X.\n\nLEFT/RIGHT MERGE\nAn example of a left/right merge can be seen below:\n\nJoin on keys found in left Dataframe.The two data frames above hold similar keys\nwith different associated information per axis, thus the result is a combination\nof these two Dataframes where the keys remain intact.\n\n\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from\nboth Dataframes do not match 1-to-1, specifying a left/right merge determines\nwhich Dataframe's keys will be considered the authority to be preserved in the\nmerge.\n\nJoin on keys found in right Dataframe.INNER MERGE\nAn inner merge will merge two Dataframes based on overlap of values between keys\nin both Dataframes:\n\nJoin on keys found in right Dataframe.OUTER MERGE\nAn outer merge will preserve the most data by not dropping keys which are\nuncommon to both Dataframes. Keys which exist in a single Dataframe will be\nadded to the resulting Dataframe, with empty values populated for any columns\nbrought in by the other Dataframe:\n\nBack to our Scenario: Merging Two Dataframes via Left Merge\nLet's get it going. Enter the iPython shell.\n\nImport Pandas and read both of your CSV files.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"csv1.csv\")  \ndf2 = pd.read_csv(\"csv2.csv\")\n\n\nThe above opens the CSVs as Dataframes recognizable by pandas.\nNext, we'll merge the two CSV files.\n\nHow  specifies the type of merge, and on  specifies the column to merge by\n(key). The key must be present in both Dataframes.\n\nFor the purpose of this exercise we'll be merging left, as that is the CSV which\ncontains the keys we'd like to maintain.\n\nmergedDF = df2.merge(df, how=“left”, on=\"email\")\n\nprint(mergedDF)\n\n\nThis should return a dataset of all common rows, with columns from both CSVs\nincluded in the merge.\n\nScenario 2: Missing Data\nBefore we go, let's toss in another scenario for good measure.\n\nThis time around we have two datasets which should actually probably be a single\ndataset. Dataset #1  contains all customers once again, but for some reason, \nDataset #1  contains email address where set Dataset #2  does not. Similarly, \nDataset #2  contains addresses which are  missing in Dataset #1. We assume there\nis no reason to keep these sets of data isolated other than human error.\n\nIn the case where we are confident that employees exist in both datasets but\ncontain different information, performing an inner  merge will join these two\nsets by a key such as customer ID or email. If all goes well, the final dataset\nshould equal the same number of rows found in both Datasets #1 and #2.\n\nDocumentation\nFor more on merging, check out the official Pandas documentation here\n[https://pandas.pydata.org/pandas-docs/stable/merging.html].","html":"<style>\n    img {\n        border: 0 !important;\n    }\n </style>   <p>Let's say you have two obscenely large sets of data. </p><p>These sets of data contain information on a similar topic, such as customers. <strong>Dataset #1 </strong>might contain a high-level view of all customers of a business, while <strong>Datatset #2</strong> contains a lifetime history of orders for a company. Unsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any business' orders are made by customers.</p><h2 id=\"welcome-to-relational-databases\">Welcome to Relational Databases</h2><p>What we just described is the core foundation for <em>relational databases</em> which have been running at the core of businesses since the 1970s. Starting with familiar names like <strong>MySQL</strong>,<strong> Oracle</strong>, and <strong>Postgres,</strong> the concept of maintaining multiple -<em>related- </em>tables of data are the bare minimum technology stack for any company, regardless of what said company does.</p><p>While our example of <strong>Datasets #1 and #2</strong> can be thought of as isolated tables, the process of 'joining' them <em>(in SQL terms) </em>or 'merging' them <em>(in Pandas terms)</em> is  trivial. What's more, we can do far more than with JOINS (or merges) than simply combining our data into a single set.</p><h3 id=\"enter-the-panda\">Enter The Panda</h3><p>Python happens to have an obscenely popular library for performing SQL-like logic, dubbed <strong>Pandas. </strong>If it remains unclear as to what Pandas is, just remember: <em>Databases are basically Excel spreadsheets are basically an interface for Pandas</em>. The technicality of that explanation may be horrendous to those who understand the differences, but the fundamental truth remains: we're dealing with information, inside of cells, on a two-dimensional grid. When you hear the next idiot spew a catch phrase like <em>\"data is the new oil\"</em>, the \"data\" they're referring to is akin to that sick Excel sheet you made at work.</p><h3 id=\"scenario-finding-mismatches-in-data\">Scenario: Finding Mismatches in Data</h3><p>This scenario actually stems from a real-life example which, sure enough, was my first encounter with Pandas. One could argue I owe much 0f my data career to a 3am Google Hangout with Snkia.</p><p>In our scenario, our company has signed up for a very expensive software product which charges by individual license. To our surprise, the number of licenses for this software totaled <strong>over 1000</strong> seats!<strong> </strong>After giving this data a quick glance, however, it's clear that many of these employees have actually been terminated, thus resulting in unspeakable loss in revenue. </p><p>The good news is we have another dataset called <em>active employees </em>(aka: employees which have not been terminated... yet). So, how do we use these two sets of data to determine which software licenses are valid? First, let's look at the <em>types</em> of ways we can merge data in Pandas.</p><h2 id=\"terminology\">Terminology</h2><h3 id=\"merge\">MERGE</h3><p>Sets of data can be merged in a number of ways. Merges can either be used to find similarities in two Dataframes and merge associated information, or may be entirely non-destructive in the way that two sets of data are merged.</p><h3 id=\"key\">KEY</h3><p>In many cases (such as the one in this tutorial) you'd likely want to merge two Dataframes based on the value of a key. A key is the authoritative column by which the Dataframes will be merged. When merging Dataframes in this way, keys will stay in tact as an identifier while the values of columns in the same row associated to that key.</p><p>This type of merge can be used when two Dataframes hold differing fields for similar rows. If Dataframe 1 contains the phone numbers of customers by name, and Dataframe 2 contains emails of a similar grouping of people, these two may be merged to create a single collection of data with all of this information.</p><h3 id=\"axis\">AXIS</h3><p>A parameter of pandas functions which determines whether the function should be run against a Dataframe's columns or rows. An axis of 0 determines that the action will be taken on a per-row basis, where an axis of 1 denotes column.</p><p>For example, performing a drop with axis 0 on key X will drop the row where value of a cell is equal to X.</p><h3 id=\"left-right-merge\">LEFT/RIGHT MERGE</h3><p>An example of a left/right merge can be seen below:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasleftjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in left Dataframe.</figcaption></figure><p>The two data frames above hold similar keys with different associated information per axis, thus the result is a combination of these two Dataframes where the keys remain intact.</p><p>\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from both Dataframes do not match 1-to-1, specifying a left/right merge determines which Dataframe's keys will be considered the authority to be preserved in the merge.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasrightjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"inner-merge\">INNER MERGE</h3><p>An inner merge will merge two Dataframes based on overlap of values between keys in both Dataframes:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasinnerjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"outer-merge\">OUTER MERGE</h3><p>An outer merge will preserve the most data by not dropping keys which are uncommon to both Dataframes. Keys which exist in a single Dataframe will be added to the resulting Dataframe, with empty values populated for any columns brought in by the other Dataframe:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasouterjoin.png\" class=\"kg-image\"></figure><h2 id=\"back-to-our-scenario-merging-two-dataframes-via-left-merge\">Back to our Scenario: Merging Two Dataframes via Left Merge</h2><p>Let's get it going. Enter the iPython shell.</p><p>Import Pandas and read both of your CSV files.</p><pre><code class=\"language-python\">import pandas as pd\n\ndf = pd.read_csv(&quot;csv1.csv&quot;)  \ndf2 = pd.read_csv(&quot;csv2.csv&quot;)\n</code></pre>\n<p>The above opens the CSVs as Dataframes recognizable by pandas.<br>Next, we'll merge the two CSV files.</p><p><strong>How</strong> specifies the type of merge, and <strong>on</strong> specifies the column to merge by (key). The key must be present in both Dataframes.</p><p>For the purpose of this exercise we'll be merging left, as that is the CSV which contains the keys we'd like to maintain.</p><pre><code class=\"language-python\">mergedDF = df2.merge(df, how=“left”, on=&quot;email&quot;)\n\nprint(mergedDF)\n</code></pre>\n<p>This should return a dataset of all common rows, with columns from both CSVs included in the merge.</p><h2 id=\"scenario-2-missing-data\">Scenario 2: Missing Data</h2><p>Before we go, let's toss in another scenario for good measure.</p><p>This time around we have two datasets which should actually probably be a single dataset. <strong>Dataset #1</strong> contains all customers once again, but for some reason, <strong>Dataset #1</strong> contains email address where set <strong>Dataset #2</strong> does not. Similarly, <strong>Dataset #2</strong> contains addresses which are  missing in <strong>Dataset #1</strong>. We assume there is no reason to keep these sets of data isolated other than human error.</p><p>In the case where we are confident that employees exist in both datasets but contain different information, performing an <em>inner</em> merge will join these two sets by a key such as customer ID or email. If all goes well, the final dataset should equal the same number of rows found in both <strong>Datasets #1 and #2</strong>.</p><h2 id=\"documentation\">Documentation</h2><p>For more on merging, check out the official Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/merging.html\">here</a>.</p>","url":"https://hackersandslackers.com/merge-dataframes-with-pandas/","uuid":"d2c59476-d879-484c-b719-55f53b3d4980","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f7a3ce38d612cc8261316"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867363f","title":"Another \"Intro to Data Analysis in Python Using Pandas\" Post","slug":"intro-to-data-analysis-in-python-using-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","custom_excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","created_at_pretty":"19 April, 2018","published_at_pretty":"16 November, 2017","updated_at_pretty":"10 April, 2019","created_at":"2018-04-18T21:18:27.000-04:00","published_at":"2017-11-16T10:52:00.000-05:00","updated_at":"2019-04-10T10:33:17.000-04:00","meta_title":"Intro to Data Analysis in Python Using Pandas | Hackers and Slackers","meta_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","og_title":"Intro to Data Analysis in Python Using Pandas","twitter_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","twitter_title":"Intro to Data Analysis in Python Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"}],"plaintext":"Let’s face it: the last thing the world needs is another “Intro to Pandas” post.\nAnybody strange enough to read this blog surely had the same reaction to\ndiscovering Pandas as I did: a manic euphoria that can only be described as love\nat first sight. We wanted to tell the world, and that we did. A lot. Yet here I\nam, about to helplessly sing cliche praises one more time. \n\nI’m a prisoner of circumstance here. As it turns out, the vast (and I mean vast)\nmajority of our fans have a raging Pandas addiction. They come to our humble\nmom-and-pop shop here at Hackers and Slackers foaming at the mouth, going on\nraging benders for all Pandas-related content. If I had half a brain, I’d rename\nthis site Pandas and Pandas  and delete all non-Pandas-related content. Talk\nabout cash money. \n\nAs a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro\nto Pandas” post was an unflattering belligerent mess jotted into a Confluence\ninstance long ago during a Friday night  pregame. That mess snuck its way on to\nthis blog, and has gone unnoticed for a year now. I've decided that this\nprobably wasn't the best way to open up a series about the most influential\nPython library of all time. We're going to try this again... For the Pandas.\n\nIntro to Pandas Rereleased: in IMAX 8k 4D \nPandas is used to analyze and modify tabular data in Python. When we say\n“tabular data,” we mean any instance in life where data is represented in a\ntable format. Excel, SQL databases, shitty HTML tables.... they’ve all been the\nsame thing with different syntax this whole time. Pandas can achieve anything\nthat any other table can. \n\nIf you’re reasonably green to data analysis and are experiencing the \n“oh-my-God-all-data-professions-are-kinda-just-Excel”  realization as we speak,\nfeel free to take a moment. Great, that’s behind us now.\n\nThe Anatomy of a DataFrame\nTabular data in Pandas is referred to as a “DataFrame.” We can’t call everything \n “tables-” otherwise, our choice of vague terminology would grow horribly\nconfusing when we refer to data in different systems. Between you and me though,\nDataFrames are basically tables.\n\nSo how do we represent two-dimensional data via command line: a concept which\ninherently interprets and displays information one-dimensionally? \n\n\"Oh nothing, we think it's cute.\"DataFrames consist of individual parts which\nare easy-to-understand at face value. It’s the complexity of these things\ntogether, creating a sum greater than the whole of its parts, which fuels the\nseemingly endless power of DataFrames. If we want any  hope of contributing to\nthe field of Data Science, we need to not only understand the terminology but at\nleast be aware of core concepts of what a DataFrame is beneath the hood. This\nunderstanding is what separates engineers from Excel monkeys. \n\nEngineers\n1\nWinExcel Nerds\n0\nLoseParts of a DataFrame\nWere you expecting this post just to be a bunch of one-liners in Pandas? Good, I\nhope you're disappointed. Strap yourself in, we might actually learn something\ntoday. Class is now in session, baby. Let's break apart what makes a DataFrame,\npiece-by-piece:\n\nThe most basic description of any table would be a collection of columns and \nrows. Despite looking at a two-dimensional grid, columns are in fact very\ndifferent from rows. Unlike rows, all data in a column  typically abides by the\nsame data type. In the above example, any value saved in the startTime  column\nwill always be a time. Rows, on the other hand, are simply an entry- an instance\nof a \"thing\", where each thing is described by attributes stored horizontally\nacross columns.\n\nThis seems like really elementary stuff, but I mention it for a reason. By our\ndefinition, columns  are more pivotal to structuring a table than rows, because\neven empty columns have meaning, whereas an empty row with no columns will\nalways equal infinite nothingness. Rows are made up of values in columns, not\nthe other way around. Thus, columns in Pandas are actually their own type of\nobject called a Series. \n\n * Series' are objects native to Pandas (and Numpy) which refer to\n   one-dimensional sequences of data. Another example of a one-dimensional\n   sequence of data could be an array, but series' are much more than arrays:\n   they're a class of their own for many powerful reasons, which we'll see in a\n   moment.\n * Axis  refers to the 'direction' of a series, or in other words \"column\" or\n   \"row\". A series with an axis of 0  is a row, whereas a series with an axis of\n    1  is a column. \n * A series contains labels, which are given visual names for a row/column\n   specifying labels allows us to call upon any labeled series in the same way\n   we would access a value in a Python dictionary. For instance, accessing \n   dataframe['awayTeamName']  returns the entire column matching the header \n   \"awayTeamName\".\n * Every row and column has a numerical index. Most of the time, a row's label \n   will be equivalent to the row's index. While it's common practice to define\n   headers for columns, columns have indexes as well, which simply aren't shown.\n   In this regard, Series share an attribute with lists/arrays, in that they are\n   a collection of indexed values\n\nConsider the last two points: we just described a series to work the same way as\na Python dictionary, but also the same way as a Python list. That's right:\nseries' objects are like the biracial offspring of lists and dicts. We can\naccess any column by either its name or its index, and the same goes for rows.\nEven if we rip a column out from a DataFrame, each cell in that series will\nstill retain the row labels for every cell. This means we can say things like \nget me column #3, and then find me the value for whatever was in the row labeled\n\"Y\".  Of course, this works in the reverse as well. It's crazy how things get\nexponentially more powerful and complicated when we add entire dimensions, isn't\nit?\n\nLoading Data Into Pandas\nIf you've made it this far, you've earned the right to start getting hands-on.\nLuckily, Pandas has plenty of methods to load tabular data into DataFrames,\nregardless if you're using static files, SQL, or quirkier methods, Pandas has\nyou covered. Here are some of my favorite examples:\n\nimport pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n\n\nAll of these achieve the same result of creating a DataFrame. No matter what\nhorrible data sources you may have been forced to inherit, Pandas is here to\nhelp. Pandas knows our pain. Pandas is love. Pandas is life.\n\nWith data loaded, let's see how we can apply our new knowledge of series'  to\ninteract with out data.\n\nFinding Data in Our Dataframe\nPandas has a method for finding a series by label, as well as a separate method\nfor finding a series by index. These methods are .iloc  and .loc, respectively.\nLet's say our DataFrame from the example above is stored as a variable named \nbaseball_df. To get the values of a column by name, we would do the following:\n\nbaseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n\n\nThis would return the following:\n\n0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n\n\nThat's our column! We can see the row labels being listed alongside each row's\nvalue. Told ya so. Getting a column will also return the column's dtype, or data\ntype. Data types can be set on columns explicitly. If they aren't, Pandas will\ngenerally either default to detecting that the data in the column is a float \n(returned for any column which only holds numerical values, despite number of\ndecimal points) or an 'object', which is a fancy catch-all meaning \"fuck if I\nknow, there's letters and shit in there, it could be anything probably.\" Pandas\ndoesn't try hard on its own to discern the types of data in each field.\n\nIf you're thinking ahead, you might see a looming conflict of interest with iloc\n. Since we've established that columns and rows are the same, and we're\naccessing series' based on criteria that is met by both  columns and rows (every\ntable has a first row and a first column), how does Pandas know what we want\nwith .loc()? Short answer: It doesn't, so it just returns both! \n\nbaseball_df = baseball_df.loc[3]\nprint(baseball_df)\n\n\n    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n\n\nAhhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a\nclever, intentional way. \"Clever and intentional\"  is actually a great way to\ndescribe Pandas as a library. This combination of ease and power is what makes\nPandas so magnetic to curious newcomers. \n\nWant another example? How about leveraging the unique attributes of series'  to\nsplice DataFrames as though they were arrays?\n\nsliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n\n\n    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n\n\n...Did we just do that? We totally did. We were able to slice a two-dimensional\nset of data by using the same syntax that we'd used to slice arrays, thanks to\nthe power of the series object.\n\nWelcome to the Club\nThere are a lot more entertaining, mind-blowing ways to introduce people to\nPandas. If our goal had been sheer amusement, we would have leveraged the\ncookie-cutter route to Pandas tutorials: overloading readers with Pandas\n\"tricks\" displaying immense power in minimal effort. Unfortunately, we took the\napplicable approach to actually retaining information. Surely this model of\n\"informational and time consuming\" will beat out \"useless but instantly\ngratifying,\" right? RIGHT? \n\nWhatever. I’ll schedule the Pandas and Pandas rebrand for next week. From now on\nwhen people want that quick fix, you can call me Pablo Escobar. Join us next\ntime when we use Pandas data analysis to determine which private Caribbean\nisland offers the best return on investment with all the filthy money we’ll\nmake.\n\nHint: it’s definitely not the Fyre festival one.","html":"<p>Let’s face it: the last thing the world needs is another “<strong>Intro to Pandas</strong>” post. Anybody strange enough to read this blog surely had the same reaction to discovering Pandas as I did: a manic euphoria that can only be described as love at first sight. We wanted to tell the world, and that we did. A lot. Yet here I am, about to helplessly sing cliche praises one more time. </p><p>I’m a prisoner of circumstance here. As it turns out, the vast (and I mean <em><strong>vast</strong></em>) majority of our fans have a raging Pandas addiction. They come to our humble mom-and-pop shop here at <strong>Hackers and Slackers </strong>foaming at the mouth, going on raging benders for all Pandas-related content. If I had half a brain, I’d rename this site <strong>Pandas and Pandas</strong> and delete all non-Pandas-related content. Talk about cash money. </p><p>As a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro to Pandas” post was an unflattering belligerent mess jotted into a Confluence instance long ago during a <a>Friday night</a> pregame. That mess snuck its way on to this blog, and has gone unnoticed for a year now. I've decided that this probably wasn't the best way to open up a series about the most influential Python library of all time. We're going to try this again... For the Pandas.</p><h2 id=\"intro-to-pandas-rereleased-in-imax-8k-4d\">Intro to Pandas Rereleased: in IMAX 8k 4D </h2><p>Pandas is used to analyze and modify tabular data in Python. When we say “tabular data,” we mean any instance in life where data is represented in a table format. Excel, SQL databases, shitty HTML tables.... they’ve all been the same thing with different syntax this whole time. Pandas can achieve anything that any other table can. </p><p>If you’re reasonably green to data analysis and are experiencing the <em>“oh-my-God-all-data-professions-are-kinda-just-Excel”</em> realization as we speak, feel free to take a moment. Great, that’s behind us now.</p><h2 id=\"the-anatomy-of-a-dataframe\">The Anatomy of a DataFrame</h2><p>Tabular data in Pandas is referred to as a “DataFrame.” We can’t call <em>everything</em> “tables-” otherwise, our choice of vague terminology would grow horribly confusing when we refer to data in different systems. Between you and me though, DataFrames are basically tables.</p><p>So how do we represent two-dimensional data via command line: a concept which inherently interprets and displays information one-dimensionally? </p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/moon.jpg\" class=\"kg-image\"><figcaption>\"Oh nothing, we think it's cute.\"</figcaption></figure><!--kg-card-end: image--><p>DataFrames consist of individual parts which are easy-to-understand at face value. It’s the complexity of these things together, creating a sum greater than the whole of its parts, which fuels the seemingly endless power of DataFrames. If we want <em>any</em> hope of contributing to the field of Data Science, we need to not only understand the terminology but <em>at least </em>be aware of core concepts of what a DataFrame is beneath the hood. This understanding is what separates engineers from Excel monkeys. </p><!--kg-card-begin: html--><div class=\"scoreboard\">\n\t<!-- Score Header -->\n\t<div class=\"score-header\">\n\t\t<!-- Background -->\n\t\t<div class=\"score-header-background\">\n\t\t\t<div class=\"score-header-background__left\"></div>\n\t\t\t<div class=\"score-header-background__right\"></div>\n\t\t\t<div class=\"score-header-background__logo\"></div>\n\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"score-header-foreground\">\n\t\t\t<div class=\"score-header-foreground__left\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Engineers</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">1</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Win</span>\n\t\t\t</div>\n\t\t\t<div class=\"score-header-foreground__right\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Excel Nerds</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">0</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Lose</span>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</div><!--kg-card-end: html--><h2 id=\"parts-of-a-dataframe\">Parts of a DataFrame</h2><p>Were you expecting this post just to be a bunch of one-liners in Pandas? Good, I hope you're disappointed. Strap yourself in, we might actually learn something today. Class is now in session, baby. Let's break apart what makes a DataFrame, piece-by-piece:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/03/dataframe2.jpg\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>The most basic description of any table would be a collection of <strong>columns </strong>and <strong>rows. </strong>Despite looking at a two-dimensional grid, columns are in fact very different from rows. Unlike rows, all data in a <em>column</em> typically abides by the same data type. In the above example, any value saved in the <strong>startTime</strong> column will always be a <strong>time</strong>. Rows, on the other hand, are simply an entry- an instance of a \"thing\", where each thing is described by attributes stored horizontally across columns.</p><p>This seems like really elementary stuff, but I mention it for a reason. By our definition, <em>columns</em> are more pivotal to structuring a table than rows, because even empty columns have meaning, whereas an empty row with no columns will always equal infinite nothingness. <strong>Rows are made up of values in columns, not the other way around. </strong> Thus, columns in Pandas are actually their own type of object called a <strong>Series</strong>. </p><ul><li><strong>Series</strong>' are objects native to Pandas (and Numpy) which refer to one-dimensional sequences of data. Another example of a one-dimensional sequence of data could be an <strong><em>array</em></strong><em>, </em>but series' are much more than arrays: they're a class of their own for many powerful reasons, which we'll see in a moment.</li><li><strong>Axis</strong> refers to the 'direction' of a series, or in other words \"column\" or \"row\". A series with an axis of <code>0</code> is a <em>row</em>, whereas a series with an axis of <code>1</code> is a <em>column</em>. </li><li>A series contains <strong>labels</strong>, which are given visual names for a row/column specifying labels allows us to call upon any labeled series in the same way we would access a value in a Python dictionary. For instance, accessing <code>dataframe['awayTeamName']</code> returns the entire column matching the header <em>\"awayTeamName\"</em>.</li><li>Every row and column has a numerical <strong>index. </strong>Most of the time, a row's <strong>label</strong> will be equivalent to the row's <strong>index. </strong>While it's common practice to define headers for columns, columns have indexes as well, which simply aren't shown. In this regard, Series share an attribute with lists/arrays, in that they are a collection of indexed values</li></ul><p>Consider the last two points: we just described a series to work the same way as a Python dictionary, but also the same way as a Python list. That's right: series' objects are like the biracial offspring of lists and dicts. We can access any column by either its name or its index, and the same goes for rows. Even if we rip a column out from a DataFrame, each cell in that series will still retain the row labels for every cell. This means we can say things like <strong>get me column #3, and then find me the value for whatever was in the row labeled \"Y\".</strong> Of course, this works in the reverse as well. It's crazy how things get exponentially more powerful and complicated when we add entire dimensions, isn't it?</p><h2 id=\"loading-data-into-pandas\">Loading Data Into Pandas</h2><p>If you've made it this far, you've earned the right to start getting hands-on. Luckily, Pandas has plenty of methods to load tabular data into DataFrames, regardless if you're using static files, SQL, or quirkier methods, Pandas has you covered. Here are some of my favorite examples:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n</code></pre>\n<!--kg-card-end: markdown--><p>All of these achieve the same result of creating a DataFrame. No matter what horrible data sources you may have been forced to inherit, Pandas is here to help. Pandas knows our pain. Pandas is love. Pandas is life.</p><p>With data loaded, let's see how we can apply our new knowledge of <strong>series'</strong> to interact with out data.</p><h2 id=\"finding-data-in-our-dataframe\">Finding Data in Our Dataframe</h2><p>Pandas has a method for finding a series by label, as well as a separate method for finding a series by index. These methods are <code>.iloc</code> and <code>.loc</code>, respectively. Let's say our DataFrame from the example above is stored as a variable named <code>baseball_df</code>. To get the values of a column by name, we would do the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><p>This would return the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n</code></pre>\n<!--kg-card-end: markdown--><p>That's our column! We can see the row labels being listed alongside each row's value. Told ya so. Getting a column will also return the column's <strong>dtype</strong>, or <em>data type</em>. Data types can be set on columns explicitly. If they aren't, Pandas will generally either default to detecting that the data in the column is a <strong>float</strong> (returned for any column which only holds numerical values, despite number of decimal points) or an '<strong>object'</strong>, which is a fancy catch-all meaning \"fuck if I know, there's letters and shit in there, it could be anything probably.\" Pandas doesn't try hard on its own to discern the types of data in each field.</p><p>If you're thinking ahead, you might see a looming conflict of interest with <code>iloc</code>. Since we've established that columns and rows are the same, and we're accessing series' based on criteria that is met by <em>both</em> columns and rows (every table has a first row and a first column), how does Pandas know what we want with <code>.loc()</code>? Short answer: It doesn't, so it just returns both! </p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.loc[3]\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n</code></pre>\n<!--kg-card-end: markdown--><p>Ahhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a clever, intentional way. \"<strong>Clever and intentional\"</strong> is actually a great way to describe Pandas as a library. This combination of ease and power is what makes Pandas so magnetic to curious newcomers. </p><p>Want another example? How about leveraging the unique attributes of <strong>series'</strong> to splice DataFrames as though they were arrays?</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">sliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n</code></pre>\n<!--kg-card-end: markdown--><p>...Did we just do that? We totally did. We were able to slice a two-dimensional set of data by using the same syntax that we'd used to slice arrays, thanks to the power of the series object.</p><h2 id=\"welcome-to-the-club\">Welcome to the Club</h2><p>There are a lot more entertaining, mind-blowing ways to introduce people to Pandas. If our goal had been sheer amusement, we would have leveraged the cookie-cutter route to Pandas tutorials: overloading readers with Pandas \"tricks\" displaying immense power in minimal effort. Unfortunately, we took the applicable approach to actually retaining information. Surely this model of \"informational and time consuming\" will beat out \"useless but instantly gratifying,\" right? <em>RIGHT? </em></p><p>Whatever. I’ll schedule the <strong>Pandas and Pandas </strong>rebrand for next week. From now on when people want that quick fix, you can call me Pablo Escobar. Join us next time when we use Pandas data analysis to determine which private Caribbean island offers the best return on investment with all the filthy money we’ll make.</p><p>Hint: it’s definitely not the Fyre festival one.</p>","url":"https://hackersandslackers.com/intro-to-data-analysis-in-python-using-pandas/","uuid":"828b4a6f-e6f2-446f-b51e-64fe19e05ba0","page":false,"codeinjection_foot":null,"codeinjection_head":"<style>\n  *,\n  *::before,\n  *::after {\n    box-sizing: inherit;\n  }\n\n  .scoreboard {\n    margin: auto;\n    max-width: 980px;\n    box-shadow: 0 0 10px #b0bddd;\n  }\n\n  .score-header {\n    position: relative;\n    height: 74px;\n    overflow: hidden;\n  }\n\n  .score-header-background {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: calc(100% + 63px + 4px);\n    left: -33.5px;\n  }\n\n  .score-header-background .score-header-background__left,\n  .score-header-background .score-header-background__right {\n    position: relative;\n    flex: 1 1 100%;\n    overflow: hidden;\n    border-bottom: 4px solid #fff;\n    transform: skewX(-40deg);\n  }\n\n  .score-header-background .score-header-background__left::before,\n  .score-header-background .score-header-background__right::before {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    transform: skewX(40deg);\n  }\n\n  .score-header-background .score-header-background__left::after,\n  .score-header-background .score-header-background__right::after {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    opacity: 0.35;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeAgMAAABGXkYxAAAACVBMVEUAAAD///8AAABzxoNxAAAAAnRSTlMAAHaTzTgAAAAlSURBVHhe3cmhEQAACMPAjIhhv+pOiahAsAGvchc6asMhjvdrAFlGOgM9VYUmAAAAAElFTkSuQmCC');\n  }\n\n  .score-header-background .score-header-background__left {\n    margin-right: 5px;\n    border-color: #19d9ff;\n  }\n\n  .score-header-background .score-header-background__left::before {\n    right: -31.5px;\n    background: linear-gradient(to left, #19d9ff 31.5px, #a9f6ff 60%);\n  }\n\n  .score-header-background .score-header-background__right {\n    margin-left: 5px;\n    border-color: #ff1979;\n  }\n\n  .score-header-background .score-header-background__right::before {\n    left: -31.5px;\n    background: linear-gradient(to right, #ff1979 31.5px, #ffb5ee 60%);\n  }\n\n  .score-header-background .score-header-background__logo {\n    position: absolute;\n    top: 0;\n    right: 0;\n    bottom: 0;\n    left: 0;\n    margin: auto;\n    width: 60px;\n    height: 60px;\n    border: 5px solid #ffffff;\n    border-radius: 100%;\n    background-color: rgba(111, 77, 238, 0.51);\n    background-image: url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiP…A3NDcsNTQ2LjMgDQoJCQkJCQkJIi8+DQoJCTwvZz4NCgk8L2c+DQo8L2c+DQo8L3N2Zz4NCg==);\n  }\n\n  .score-header-foreground {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: 100%;\n  }\n\n  .score-header-foreground .score-header-foreground__left,\n  .score-header-foreground .score-header-foreground__right {\n    display: flex;\n    margin: 0 20px;\n    flex: 1 1 100%;\n    align-items: baseline;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__score {\n    margin-right: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__right {\n    flex-direction: row-reverse;\n    text-align: right;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n    margin-left: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__title,\n  .score-header-foreground .score-header-foreground__score,\n  .score-header-foreground .score-header-foreground__win {\n    margin: 0 10px;\n    line-height: 69px;\n    text-transform: uppercase;\n  }\n\n  .score-header-foreground .score-header-foreground__title {\n    margin: 0;\n    flex: 1 1 auto;\n    font-size: 30px;\n  }\n\n  .score-header-foreground .score-header-foreground__score {\n    order: 1;\n    text-shadow: 0 0 4px rgba(255, 255, 255, 0.75), 0 0 8px rgba(255, 255, 255, 0.45);\n    font-size: 30px;\n    color: white;\n  }\n\n  .score-header-foreground .score-header-foreground__win {\n    font-size: 18px;\n    font-weight: 600;\n    font-family: 'TTNorms-Medium', sans-serif;\n    color: white;\n    color: #4a47a2;\n    mix-blend-mode: color-burn;\n  }\n\n  .player-list {\n    display: flex;\n    padding: 0;\n    flex-flow: column;\n    list-style: none;\n  }\n\n  .player-row {\n    display: flex;\n    margin: 20px 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player {\n    display: flex;\n    padding: 20px;\n    min-width: 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player.player--left {\n    padding-left: 20px;\n  }\n\n  .player.player--left .player__avatar {\n    margin-right: 20px;\n    border-color: #19d9ff;\n  }\n\n  .player.player--right {\n    padding-right: 20px;\n    text-align: right;\n    flex-flow: row-reverse;\n  }\n\n  .player.player--right .player__avatar {\n    margin-left: 20px;\n    border-color: #ff1979;\n  }\n\n  .player .player__avatar {\n    position: relative;\n    min-height: 60px;\n    min-width: 60px;\n    overflow: hidden;\n    border: 3px solid #fff;\n    border-radius: 100%;\n    background-color: #222;\n  }\n\n  .player .player__avatar::before {\n    content: \"\";\n    position: absolute;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    opacity: 0.1;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA8CAMAAAANIilAAAABTVBMVEU7PUP////4+Pj7+/v29vZAQkj6+vpCRErx8fL+/v5BQ0lNT1T9/f3t7u5HSU/8/Pz09PRGSE5aW2BOUFY8PkRJSlB/gITk5eWqq64+QEWPkJSVlppoam5KTFHZ2tvDw8XBwsPa29zP0NGdnqH6+/vExMVVV1z+/v/LzMxRUlidn6HKy8zFxce3uLuVlpmBgYZSVFnz8/SvsLOUlZhkZmv5+flRU1jg4OFDRUrExMepqqyWl5uAgYXMzc7r6+xZWl/AwMKenqFjZWnn6OjBwcJ5en7U1dZ+gIPNzs8/QUbi4+Smp6pSU1lUVVuLjI/j5OWwsLNpam9BQ0hLTFK7vL7p6ep0dXmIio3LzM339/fi4+OrrK5cXmI9P0Wlpqny8/PLy8xzdHh6e3/ExMZVVlzs7e2hoqU+QEbR0tO4ubyHiIzs7O1FR01bXWLz8/PjR6/UAAABIklEQVR4Xu3UxW7FMBCGUU+Sy8xFZmZmZmZmhvdfVuqi9rTprf8uIlXKtz+yNGON+Ke5uQ30NQb/JoMlozGimtopD259DWH6yCxfRG1pGX1WFQVxNSkVYbbOUHF8G8JNUuJPN3NcAeEcx5UQtjgeQ6yHeBaCM8TzOoZP6zmOC6Rzjqch3M/xHIRbOG6FcBvH7RDuMFXb6RNQXSruFlg9MWl7X0EcfZM4Al+xQYmHBNqwxCMwLpY46ygen5B4ElpzOjFDapHZQm06v+AlXmBpeUWHrq6tk13GxubWLzS0Y9JPBXb38sjE/sEh5e3o+MSeJs/8pNHFpc1nvTJIr/D1t9Hf3JJ2d4J3/0D6WSGOkwYBPXL8REjPLwwXQNifcgK72MXv0xEfs26TMDAAAAAASUVORK5CYII=');\n    background-size: cover;\n  }\n\n  .player .player__username {\n    font-size: 24px;\n    text-overflow: ellipsis;\n    overflow: hidden;\n    white-space: nowrap;\n  }\n\n  .language {\n    font-size: 24px;\n  }\n\n  .language.language--html {\n    color: #f69c24;\n  }\n\n  .language.language--css {\n    color: #299bf7;\n  }\n\n  .language.language--js {\n    color: #ffce22;\n  }\n\n  @media (max-width:600px) {\n    .score-header-foreground__title {\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n    }\n\n    .score-header-foreground__left,\n    .score-header-foreground__right {\n      margin: 10px 11px 0;\n      display: block;\n      position: relative;\n    }\n\n    .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n          right: 10px;\n    }\n\n    .score-header-foreground .score-header-foreground__left, .score-header-foreground .score-header-foreground__right {\n          margin: 0 10px;\n          display: block;\n    }\n\n    .score-header-foreground__left .score-header-foreground__title {\n      left:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground__right .score-header-foreground__title {\n      right:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground .score-header-foreground__win {\n      font-size: 12px;\n      font-weight: 600;\n      font-family: 'TTNorms-Medium', sans-serif;\n      color: white;\n      color: #4a47a2;\n      mix-blend-mode: multiply;\n      position: absolute;\n      width: fit-content;\n      bottom: 32px;\n    }\n\n    .score-header-foreground__score {\n      padding: 0;\n      display: block;\n      position: absolute;\n      top: 2px;\n      padding: 0 !important;\n      margin: 0 !important;\n      line-height: 1 !important;\n      height: fit-content;\n          top: 14px;\n    }\n\n    .score-header-foreground__win {\n      position: absolute;\n      bottom: 35px;\n      width: fit-content;\n    }\n\n    .score-header-foreground__right .score-header-foreground__win {\n          right: 36px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground__left .score-header-foreground__win {\n      left: 19px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground .score-header-foreground__title {\n      right: 10px;\n    }\n  }\n</style>","comment_id":"5ad7ee6365cd784d6288cb03"}}]}},"pageContext":{"slug":"pandas","limit":12,"skip":12,"numberOfPages":2,"humanPageNumber":2,"prevPageNumber":1,"nextPageNumber":null,"previousPagePath":"/tag/pandas/","nextPagePath":null}}