{"data":{"ghostTag":{"slug":"data-analysis-pandas","name":"#Data Analysis with Pandas","visibility":"internal","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673731","title":"Liberating Data from PDFs with Tabula and Pandas","slug":"liberating-data-from-pdfs-with-tabula-and-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","excerpt":"Making 'open' data more open.","custom_excerpt":"Making 'open' data more open.","created_at_pretty":"03 November, 2018","published_at_pretty":"04 November, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-11-03T13:04:50.000-04:00","published_at":"2018-11-04T14:23:41.000-05:00","updated_at":"2019-02-02T08:19:55.000-05:00","meta_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","meta_description":"Making 'open' data more open: use Python's Pandas library and Tabula to extract data from PDFs.","og_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","og_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","og_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","twitter_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","twitter_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Check out the accompanying GitHub repo for this article here\n[https://github.com/grahamalama/school_budget_aggregator].\n\nTechnically, the School District of Philadelphia's budget data for the 2019\nfiscal year is \"open\". It is, after all, made available through the district's \nOpen Data portal  and is freely available to download.\n\nBut just because data is freely available, doesn't mean it's easy to work with.\nThat's what found out when I downloaded the zipped folder, opened it up, and\nfound a heap of PDFs. Joy.\n\nAs a member of Code for Philly [https://codeforphilly.org/], I thought of my\ncompatriots who might want to use school district data in their projects. I knew\nwith a bit of data munging, I could provide a data set that would be more easily\nusable.\n\nData Liberation\nThe first hurdle was to find a way to get the data from the PDFs. After a bit\nGoogling, I came across tabula-py [https://github.com/chezou/tabula-py], a\nPython wrapper for Tabula [https://tabula.technology/].\n\nEach budget is composed of 5 tables:\n\n * General information about the school\n * Enrollment information\n * Operating Funded budget allotments\n * Grant Funded budget allotments\n * A summary table of allotment totals\n\nExtracting these tables from a budget with Tabula was as simple as:\n\ntabula.read_pdf(path_to_budget, multiple_tables=True)\n\n\nWhich returned a list of DataFrames, one for each table mentioned above.\nPerfect! \nSo, I iterated over all of the files in folder and appended them to a list:\n\nimport os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f\"{directory}/{filename}\", \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(\"SY1819_School_Budgets\")\n\n\nInitial Cleaning\nWhile this gave me a good start, I knew it wouldn't be that easy to liberate the\ndata from the PDFs. I took a look at each of the DataFrames to see what I'd be\nworking with. \n\n# an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n\n\nAfter I saw the output, I wrote a function to perform the same cleaning\noperation for each table in each budget.\n\nFor each table below, first I'll introduce the \"raw\" output that Tabula\nreturned, then I'll show the function that I wrote to fix that output.\n\nBasic Information\nRaw Output\nbasic_information = sample_budget[0] #basic information\nbasic_information\n\n\n0\n 1\n 0\n Basic Information\n NaN\n 1\n Council District\n 2nd\n 2\n Organization Code\n 1380\n 3\n School Level\n Elementary School\n 4\n Economically\n NaN\n 5\n Disadvantaged Rate*\n NaN\n 6\n NaN\n 83.44%\n Cleanup Function\ndef generate_basic_information_table(df):\n    '''Series representing the \"basic information\" table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [\"Economically Disadvantaged Rate\", df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n\n\nCleaned\nbasic_information = generate_basic_information_table(basic_information)\nbasic_information\n\n\n# Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n\n\nEnrollment\nRaw Output\n# Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n\n\n0\n 1\n 2\n 3\n 0\n NaN\n FY14\n FY18\n FY19 Projected\n 1\n Enrollment**\n 842\n 640\n 602\n Cleanup Function\ndef generate_enrollment_table(df):\n    '''returns a series representing the \"enrollment\" table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=\"enrollment\")\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n\n\nCleaned\n# Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n\n\nAllotments\nLuckily, both allotment tables were identical, so I could apply to the same\ncleanup steps to both.\n\nRaw Output\noperating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n\n\n0\n 1\n 2\n 3\n 0\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n 2\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n 3\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n 4\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n 5\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n 6\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n 7\n Secretaries\n 1.0\n 1.0\n 1.0\n 8\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n 9\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n 10\n Other\n 0.0\n 1.2\n 1.0\n 11\n Total Positions\n 60.0\n 43.8\n 50.1\n 12\n Supplies/Equipment/Non Full‐Time Salaries/Other\n $32,272\n $100,159\n $97,553\n grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n\n\nCleanup Function\nI decided to merge the two allotment tables into one DataFrame while building a\nMultiIndex to keep things in order. This would allow me to ask some more\ninteresting questions further on down the road.\n\ndef generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + \"ed_allotments\"\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(\"org_code\", \"fund\", \"allotment\")\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n\n\nCleaned\npd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, \"1410\", \"operating_fund\"\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, \"1410\", \"grant_fund\"\n    )\n])\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment\n 1410\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n Totals\nSince the final \"totals\" table could be calculated from the data already in the\nnew allotment table, I didn't bother transforming it in any way.\n\n# same data can be derived from the allotments table directly\nsample_budget[4]\n\n\n0\n 1\n 2\n 3\n 4\n 0\n NaN\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Total Positions\n NaN\n 76.1\n 64.8\n 67.8\n 2\n Total Supplies/Equipment/Non Full‐Time Salarie...\n NaN\n $230,726\n $120,136\n $112,719\n Once I figured out what transformations I needed for each table, I combined\nthem into a function so that, given a list of DataFames from Tabula, I'd get\nthose same tables back neatly formatted.\n\ndef generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n\n\nAggregation Time\nNow that I had cleaned the tables that Tabula produced, it was time to combine\nthem into some aggregated tables.\n\nFirst I wrote a function that would output a Series (representing one row) of\ninformation from all tables for a given school in a given fiscal year. \n\ndef generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +\": \" + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == \"FY19\" else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == \"FY19\" else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(\"FY18\", basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n\n\n# Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n\n\nThen, I applied this function to each list of budgets in the collection and\ncompiled them into a DataFrame.\n\ndef generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = \"FY14\"\nfy14.to_csv(\"output/combined_fy14.csv\")\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = \"FY18\"\nfy18.to_csv(\"output/combined_fy18.csv\")\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = \"FY19\"\nfy19.to_csv(\"output/combined_fy19.csv\")\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(\"output/all_budgets_tabular.csv\")\n\n\nFinally, I wanted to output a CSV that would preserve some of the multi-indexed\nnature of the allotment tables. Here's what I wrote for that.\n\ndef generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(\"output/all_budgets_hierarchical.csv\")\n\nhierarchical_budget\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment  \n 1380\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n grant_fund\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n 5160 rows × 3 columnsThis makes it easier to aggregate in interesting ways:\n\nhierarchical_budget.groupby('allotment').sum()\n\n\nFY14\n FY18\n FY19\n allotment\n Classroom Assistants/Teacher Assistants\n 2365.0\n 1720.0\n 1935.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 258.0\n 215.0\n 258.0\n Nurses/Health Services\n 129.0\n 215.0\n 215.0\n Other\n 215.0\n 258.0\n 215.0\n Principals/Assistant Principals\n 430.0\n 215.0\n 215.0\n Secretaries\n 215.0\n 215.0\n 215.0\n Student Climate Staff\n 1720.0\n 1720.0\n 1505.0\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 49606090.0\n 25829240.0\n 24234585.0\n Support Services Assistants\n 1505.0\n 1505.0\n 1720.0\n Teachers ‐ Regular Education\n 8234.5\n 7224.0\n 7224.0\n Teachers ‐ Special Education\n 1290.0\n 645.0\n 1075.0\n Total Positions\n 16361.5\n 13932.0\n 14577.0\n More Cleaning to be Done\nMy work here is done. I saved the data from their not-so-accessible PDF prisons.\nBut now it's time for someone with some domain-specific knowledge to make it\nactionable.\n\nThe biggest weakness with the data in its current form is that there is some\namount of ambiguity as to what the different allotments numbers represent in\nreal-dollar amounts. Only the Supplies/Equipment/Non Full‐Time Salaries/Other \nallotment category came in currency notation – the rest of the allotments were\nrepresented as simple decimal amounts with no context to help interpret what\nthey mean. Do they represent FTE\n[https://en.wikipedia.org/wiki/Full-time_equivalent]? Dollar amounts in\nscientific notation? I'm not sure, but I hope by handing this work off to the\nright people, these questions and more can be answered more easily thanks to a\ncleaner, more accessible data set.","html":"<p><em>Check out the accompanying GitHub repo for this article <a href=\"https://github.com/grahamalama/school_budget_aggregator\">here</a>.</em></p><p>Technically, the School District of Philadelphia's budget data for the 2019 fiscal year is \"open\". It is, after all, made available through the district's <a href=\"https://www.philasd.org/performance/programsservices/open-data/district-information/#budget\">Open Data portal</a> and is freely available to download.</p><p>But just because data is freely available, doesn't mean it's easy to work with. That's what found out when I downloaded the zipped folder, opened it up, and found a heap of PDFs. Joy.</p><p>As a member of <a href=\"https://codeforphilly.org/\">Code for Philly</a>, I thought of my compatriots who might want to use school district data in their projects. I knew with a bit of data munging, I could provide a data set that would be more easily usable.</p><h2 id=\"data-liberation\">Data Liberation</h2><p>The first hurdle was to find a way to get the data from the PDFs. After a bit Googling, I came across <a href=\"https://github.com/chezou/tabula-py\"><strong>tabula-py</strong></a>, a Python wrapper for <a href=\"https://tabula.technology/\">Tabula</a>.</p><p>Each budget is composed of 5 tables:</p><ul><li>General information about the school</li><li>Enrollment information</li><li>Operating Funded budget allotments</li><li>Grant Funded budget allotments</li><li>A summary table of allotment totals</li></ul><p>Extracting these tables from a budget with Tabula was as simple as:</p><pre><code class=\"language-python\">tabula.read_pdf(path_to_budget, multiple_tables=True)\n</code></pre>\n<p>Which returned a list of DataFrames, one for each table mentioned above. Perfect! <br>So, I iterated over all of the files in folder and appended them to a list:</p><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f&quot;{directory}/{filename}&quot;, \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(&quot;SY1819_School_Budgets&quot;)\n</code></pre>\n<h2 id=\"initial-cleaning\">Initial Cleaning</h2><p>While this gave me a good start, I knew it wouldn't be that easy to liberate the data from the PDFs. I took a look at each of the DataFrames to see what I'd be working with. </p><pre><code class=\"language-python\"># an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n</code></pre>\n<p>After I saw the output, I wrote a function to perform the same cleaning operation for each table in each budget.</p><p>For each table below, first I'll introduce the \"raw\" output that Tabula returned, then I'll show the function that I wrote to fix that output.</p><h2 id=\"basic-information\">Basic Information</h2><h3 id=\"raw-output\">Raw Output</h3><pre><code class=\"language-python\">basic_information = sample_budget[0] #basic information\nbasic_information\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Basic Information</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Council District</td>\n      <td>2nd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Organization Code</td>\n      <td>1380</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>School Level</td>\n      <td>Elementary School</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Economically</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Disadvantaged Rate*</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>83.44%</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_basic_information_table(df):\n    '''Series representing the &quot;basic information&quot; table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [&quot;Economically Disadvantaged Rate&quot;, df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n</code></pre>\n<h3 id=\"cleaned\">Cleaned</h3><pre><code class=\"language-python\">basic_information = generate_basic_information_table(basic_information)\nbasic_information\n</code></pre>\n<pre><code class=\"language-python\"># Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n</code></pre>\n<h2 id=\"enrollment\">Enrollment</h2><h4 id=\"raw-output-1\">Raw Output</h4><pre><code class=\"language-python\"># Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>FY14</td>\n      <td>FY18</td>\n      <td>FY19 Projected</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Enrollment**</td>\n      <td>842</td>\n      <td>640</td>\n      <td>602</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function-1\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_enrollment_table(df):\n    '''returns a series representing the &quot;enrollment&quot; table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=&quot;enrollment&quot;)\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n</code></pre>\n<h4 id=\"cleaned-1\">Cleaned</h4><pre><code class=\"language-python\"># Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n</code></pre>\n<h2 id=\"allotments\">Allotments</h2><p>Luckily, both allotment tables were identical, so I could apply to the same cleanup steps to both.</p><h4 id=\"raw-output-2\">Raw Output</h4><pre><code class=\"language-python\">operating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Principals/Assistant Principals</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Teachers ‐ Regular Education</td>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Teachers ‐ Special Education</td>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Counselors/Student Adv./ Soc. Serv. Liaisons</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nurses/Health Services</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Classroom Assistants/Teacher Assistants</td>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Secretaries</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Support Services Assistants</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Student Climate Staff</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Other</td>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Total Positions</td>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Supplies/Equipment/Non Full‐Time Salaries/Other</td>\n      <td>$32,272</td>\n      <td>$100,159</td>\n      <td>$97,553</td>\n    </tr>\n  </tbody>\n</table>\n</div><pre><code class=\"language-python\">grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n</code></pre>\n<h3 id=\"cleanup-function-2\">Cleanup Function</h3><p>I decided to merge the two allotment tables into one DataFrame while building a MultiIndex to keep things in order. This would allow me to ask some more interesting questions further on down the road.</p><pre><code class=\"language-python\">def generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + &quot;ed_allotments&quot;\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(&quot;org_code&quot;, &quot;fund&quot;, &quot;allotment&quot;)\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n</code></pre>\n<h4 id=\"cleaned-2\">Cleaned</h4><pre><code class=\"language-python\">pd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, &quot;1410&quot;, &quot;operating_fund&quot;\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, &quot;1410&quot;, &quot;grant_fund&quot;\n    )\n])\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"24\" valign=\"top\">1410</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"totals\">Totals</h2><p>Since the final \"totals\" table could be calculated from the data already in the new allotment table, I didn't bother transforming it in any way.</p><pre><code class=\"language-python\"># same data can be derived from the allotments table directly\nsample_budget[4]\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Total Positions</td>\n      <td>NaN</td>\n      <td>76.1</td>\n      <td>64.8</td>\n      <td>67.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total Supplies/Equipment/Non Full‐Time Salarie...</td>\n      <td>NaN</td>\n      <td>$230,726</td>\n      <td>$120,136</td>\n      <td>$112,719</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Once I figured out what transformations I needed for each table, I combined them into a function so that, given a list of DataFames from Tabula, I'd get those same tables back neatly formatted.</p><pre><code class=\"language-python\">def generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n</code></pre>\n<h2 id=\"aggregation-time\">Aggregation Time</h2><p>Now that I had cleaned the tables that Tabula produced, it was time to combine them into some aggregated tables.</p><p>First I wrote a function that would output a Series (representing one row) of information from all tables for a given school in a given fiscal year. </p><pre><code class=\"language-python\">def generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +&quot;: &quot; + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == &quot;FY19&quot; else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == &quot;FY19&quot; else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(&quot;FY18&quot;, basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n</code></pre>\n<pre><code class=\"language-python\"># Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n</code></pre>\n<p>Then, I applied this function to each list of budgets in the collection and compiled them into a DataFrame.</p><pre><code class=\"language-python\">def generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = &quot;FY14&quot;\nfy14.to_csv(&quot;output/combined_fy14.csv&quot;)\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = &quot;FY18&quot;\nfy18.to_csv(&quot;output/combined_fy18.csv&quot;)\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = &quot;FY19&quot;\nfy19.to_csv(&quot;output/combined_fy19.csv&quot;)\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(&quot;output/all_budgets_tabular.csv&quot;)\n</code></pre>\n<p>Finally, I wanted to output a CSV that would preserve some of the multi-indexed nature of the allotment tables. Here's what I wrote for that.</p><pre><code class=\"language-python\">def generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(&quot;output/all_budgets_hierarchical.csv&quot;)\n\nhierarchical_budget\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>       \n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"61\" valign=\"top\">1380</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">grant_fund</th>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n<div style=\"text-align: right;\n    width: 100%;\n    font-family: Gordita-Medium,sans-serif;\n    font-size: .9em;\n    margin-top: -20px;\">5160 rows × 3 columns</div>\n</div><p>This makes it easier to aggregate in interesting ways:</p><pre><code class=\"language-python\">hierarchical_budget.groupby('allotment').sum()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>2365.0</td>\n      <td>1720.0</td>\n      <td>1935.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>258.0</td>\n      <td>215.0</td>\n      <td>258.0</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>129.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>215.0</td>\n      <td>258.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Principals/Assistant Principals</th>\n      <td>430.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>215.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>1720.0</td>\n      <td>1720.0</td>\n      <td>1505.0</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>49606090.0</td>\n      <td>25829240.0</td>\n      <td>24234585.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>1505.0</td>\n      <td>1505.0</td>\n      <td>1720.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8234.5</td>\n      <td>7224.0</td>\n      <td>7224.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>1290.0</td>\n      <td>645.0</td>\n      <td>1075.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16361.5</td>\n      <td>13932.0</td>\n      <td>14577.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"more-cleaning-to-be-done\">More Cleaning to be Done</h2><p>My work here is done. I saved the data from their not-so-accessible PDF prisons. But now it's time for someone with some domain-specific knowledge to make it actionable.</p><p>The biggest weakness with the data in its current form is that there is some amount of ambiguity as to what the different allotments numbers represent in real-dollar amounts. Only the <strong>Supplies/Equipment/Non Full‐Time Salaries/Other</strong> allotment category came in currency notation – the rest of the allotments were represented as simple decimal amounts with no context to help interpret what they mean. Do they represent <a href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\">FTE</a>? Dollar amounts in scientific notation? I'm not sure, but I hope by handing this work off to the right people, these questions and more can be answered more easily thanks to a cleaner, more accessible data set.</p>","url":"https://hackersandslackers.com/liberating-data-from-pdfs-with-tabula-and-pandas/","uuid":"ab1a4ee3-9cc3-43a6-9ebe-5a885ae264a2","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bddd5323ea1e4769817c4c9"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c8","title":"Lazy Pandas and Dask","slug":"cutting-a-file-down-to-size-with-dask","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","excerpt":"Increase the performance of Pandas with Dask.","custom_excerpt":"Increase the performance of Pandas with Dask.","created_at_pretty":"05 August, 2018","published_at_pretty":"06 August, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-08-05T17:09:51.000-04:00","published_at":"2018-08-06T07:30:00.000-04:00","updated_at":"2019-02-19T05:19:53.000-05:00","meta_title":"Picking Low-Hanging Fruit With Dask | Hackers And Slackers","meta_description":"Dask is library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core. ","og_description":"Lazy Pandas and Dask","og_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","og_title":"Lazy Pandas and Dask","twitter_description":"Picking Low-Hanging Fruit With Dask","twitter_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","twitter_title":"Lazy Pandas and Dask","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Ah, laziness.  You love it, I love it, everyone agrees it's just better.\n\nFlesh-and-blood are famously lazy.  Pandas the package, however, uses Eager\nEvaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging\nout on the street corner and being fierce to the style choices of people walking\nby?  Well, yes, but that's not the most relevant sense in which I mean it here.\n\nEager evaluation means that once you call pd.read_csv(), Pandas immediately\njumps to read the whole CSV into memory.\n\n\"Wait!\" I hear you ask.\"Isn't that what we want?  Why would I call the function\nif I didn't want it to happen?\"\n\nEventually, yes that is what you want.  But sometimes you want some time in\nbetween when you give the command and when the computer hops to it.  That's why\nit's Lazy and not Inactive - it will get to the job at some point, it'll just\nprocrastinate a bit.\n\nFor example, last week I was tasked with searching through the output of a\ngovernment API.  It had records since the 90s, and was about 300MB.  Now, this\nisn't actually outside the realm of what Pandas can handle - it's quite\noptimized, and as long as the file can fit into memory, Pandas can mess with.\n However, it won't necessarily be fast.  Furthermore, my laptop is old and I\ndidn't feel like offloading what I was doing to a remote machine.\n\nFurthermore, I knew I actually only needed a subset of the file.  Here's where\nLaziness comes in handy.  With Eager evaluation, Pandas would have to load the\nwhole thing into memory, then filter based on my criteria.\n\nEnter Dask:  Dask is a very cool little library that seamlessly allows you to\nparallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed\nto only work on one core.  Dask, on the other hand, lets you split the work\nbetween different cores - both on a single machine, or on a distributed system.\n It doesn't implement every single thing you can do with Pandas, though, so only\nuse it when you have to.\n\n  I probably should have titled this post \"Parallel Pandas\", but hey, too late\nnow - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.\n Anyway, Laziness is  part of the process.  Because Dask is lazy by default\n(much like your humble narrator), we can define our fileout loading it, like so:\n\nimport dask.dataframe as dd\n\ndf = dd.read_csv(\"giantThing.csv\")\n\n\nPandas was taking a long time to parse the file.  What's more is that this file\nhad a few quirks - I'd figured out that it needed a special text encoding, and I\nwasn't sure if there was other weirdness going on.  Was my computer just taking\na long time to nom the file, or was I going to wait there for a few minutes to\nfind an error message?  It's a catch-22 - I needed to figure out how to cut the\nfile down to size, but in order to do that I would have needed to be able to\nload it into memory.  Dask to the rescue!\n\nThis file wasn't terribly well-documented - I had an inkling as to what column\nwould tell me which rows I wanted, but I wasn't sure.  So, first thing I did was\ncheck out the first few rows.  Remember, in order to see these first 5 rows in\nPandas, I'd have to load the whole thing into memory (which might or might not\neven work!).\n\ndf.head()\n\nWith that, I was able to do a simple spot-check and see if there were any weird\ncolumns that might get in the way of parsing.  Furthermore, I confirmed that the\nID columns I was looking at contained something vaguely like what I was looking\nfor.  Even MORE interestingly, I found that it was formatted slightly\nirregularly.  Even more use for laziness!  Let's load just that one column into\nmemory (you could do this with a loop, sure - but selecting a single column is a\nlot clumsier)\n\ndf[\"ORG_NAME\"].compute()\n\nNote the .compute()  method at the end.  That's necessary because of the Lazy\nEvaluation - just calling a column name doesn't make Dask think you necessarily\nwant the thing now.  I'm not sure why I didn't have to call it with df.head(),\nthough (that's the Hackers & Slackers Codeblogging vérité style!).\n\nSo, now that I've seen the formatting, I found out that I'm going to have to\nfilter it with a call of a str.contains()  method instead of an exact value.\n Let's poke around a teensy bit more.\n\norgDF = df[\"ORG_NAME\"]\norgFiltered = corp[corp.str.contains(\"baseName\", na=False)].compute().shape\n\n\nTurns out it was only about 800 rows!So, let's filter that and make a regular\nPandas Dataframe (and probably a new CSV for later!)\n\ndf = dd.read_csv(\"giantThing.csv\")\n\norgFiltered = df[df[\"ORG_NAME\"].str.contains(\"baseName\", na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(\"filteredThing.csv\")\n\n\nNote that I actually could have done this with base Pandas, through use of the\niterator flag.  However, I didn't realize that it's only wind up being so few\nrows.  It also would have been slower - and the speed difference makes a huge\ndifference in terms of how fluidly you can explore.\n\nFor instance, the na=False  flag was something I discovered would be needed\nbecause of a quirk in the file - again, this sort of thing becomes a lot easier\ndo diagnose when you can iterate quickly, and you know you're not going to just\ntimeout from running out of memory.\n\nFor comparison's sake, here's the code for filtering on the fly and loading into\nPandas:\n\niter_csv = pd.read_csv(\"giantThing.csv\",\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[\"ORG_NAME\"].str.contains(\"baseName\", na=False)] \n                for chunk in iter_csv])\n\n\nOn my computer, that took a little over 3 minutes.  While the Dask code took\nabout a minute.","html":"<p>Ah, laziness.  You love it, I love it, everyone agrees it's just better.</p><p>Flesh-and-blood are famously lazy.  Pandas the package, however, uses Eager Evaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging out on the street corner and being fierce to the style choices of people walking by?  Well, yes, but that's not the most relevant sense in which I mean it here.  </p><p>Eager evaluation means that once you call <code>pd.read_csv()</code>, Pandas immediately jumps to read the whole CSV into memory.</p><p><strong>\"Wait!\" </strong>I hear you ask.  <strong>\"Isn't that what we want?  Why would I call the function if I didn't want it to happen?\"</strong></p><p><em>Eventually</em>, yes that is what you want.  But sometimes you want some time in between when you give the command and when the computer hops to it.  That's why it's Lazy and not Inactive - it will get to the job at some point, it'll just procrastinate a bit.</p><p>For example, last week I was tasked with searching through the output of a government API.  It had records since the 90s, and was about 300MB.  Now, this isn't actually outside the realm of what Pandas can handle - it's quite optimized, and as long as the file can fit into memory, Pandas can mess with.  However, it won't necessarily be fast.  Furthermore, my laptop is old and I didn't feel like offloading what I was doing to a remote machine.  </p><p>Furthermore, I knew I actually only needed a subset of the file.  Here's where Laziness comes in handy.  With Eager evaluation, Pandas would have to load the whole thing into memory, then filter based on my criteria.</p><p>Enter Dask:  Dask is a very cool little library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core.  Dask, on the other hand, lets you split the work between different cores - both on a single machine, or on a distributed system.  It doesn't implement every single thing you can do with Pandas, though, so only use it when you have to.</p><p> I probably should have titled this post \"Parallel Pandas\", but hey, too late now - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.  Anyway, Laziness <em>is</em> part of the process.  Because Dask is lazy by default (much like your humble narrator), we can define our fileout loading it, like so:</p><pre><code class=\"language-python\">import dask.dataframe as dd\n\ndf = dd.read_csv(&quot;giantThing.csv&quot;)\n</code></pre>\n<p>Pandas was taking a long time to parse the file.  What's more is that this file had a few quirks - I'd figured out that it needed a special text encoding, and I wasn't sure if there was other weirdness going on.  Was my computer just taking a long time to nom the file, or was I going to wait there for a few minutes to find an error message?  It's a catch-22 - I needed to figure out how to cut the file down to size, but in order to do that I would have needed to be able to load it into memory.  Dask to the rescue!</p><p>This file wasn't terribly well-documented - I had an inkling as to what column would tell me which rows I wanted, but I wasn't sure.  So, first thing I did was check out the first few rows.  Remember, in order to see these first 5 rows in Pandas, I'd have to load the whole thing into memory (which might or might not even work!).</p><p><code>df.head()</code></p><p>With that, I was able to do a simple spot-check and see if there were any weird columns that might get in the way of parsing.  Furthermore, I confirmed that the ID columns I was looking at contained something vaguely like what I was looking for.  Even MORE interestingly, I found that it was formatted slightly irregularly.  Even more use for laziness!  Let's load just that one column into memory (you could do this with a loop, sure - but selecting a single column is a lot clumsier)</p><p><code>df[\"ORG_NAME\"].compute()</code></p><p>Note the <code>.compute()</code> method at the end.  That's necessary because of the Lazy Evaluation - just calling a column name doesn't make Dask think you necessarily want the thing now.  I'm not sure why I didn't have to call it with <code>df.head()</code>, though (that's the Hackers &amp; Slackers Codeblogging vérité style!).</p><p>So, now that I've seen the formatting, I found out that I'm going to have to filter it with a call of a <code>str.contains()</code> method instead of an exact value.  Let's poke around a teensy bit more.</p><pre><code class=\"language-python\">orgDF = df[&quot;ORG_NAME&quot;]\norgFiltered = corp[corp.str.contains(&quot;baseName&quot;, na=False)].compute().shape\n</code></pre>\n<blockquote>Turns out it was only about 800 rows!</blockquote><p>So, let's filter that and make a regular Pandas Dataframe (and probably a new CSV for later!)</p><pre><code class=\"language-python\">df = dd.read_csv(&quot;giantThing.csv&quot;)\n\norgFiltered = df[df[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(&quot;filteredThing.csv&quot;)\n</code></pre>\n<p>Note that I actually could have done this with base Pandas, through use of the iterator flag.  However, I didn't realize that it's only wind up being so few rows.  It also would have been slower - and the speed difference makes a huge difference in terms of how fluidly you can explore.</p><p>For instance, the <code>na=False</code> flag was something I discovered would be needed because of a quirk in the file - again, this sort of thing becomes a lot easier do diagnose when you can iterate quickly, and you know you're not going to just timeout from running out of memory.</p><p>For comparison's sake, here's the code for filtering on the fly and loading into Pandas:</p><pre><code class=\"language-python\">iter_csv = pd.read_csv(&quot;giantThing.csv&quot;,\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)] \n                for chunk in iter_csv])\n</code></pre>\n<p>On my computer, that took a little over 3 minutes.  While the Dask code took about a minute.</p>","url":"https://hackersandslackers.com/cutting-a-file-down-to-size-with-dask/","uuid":"d4270325-d03f-46fd-a0ae-1b3cbfe1b527","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b67679f17f6083e60a44c5d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c3","title":"Automagically Turn JSON into Pandas DataFrames","slug":"json-into-pandas-dataframes","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","custom_excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","created_at_pretty":"25 July, 2018","published_at_pretty":"28 July, 2018","updated_at_pretty":"21 February, 2019","created_at":"2018-07-25T10:52:35.000-04:00","published_at":"2018-07-28T08:00:00.000-04:00","updated_at":"2019-02-20T21:35:53.000-05:00","meta_title":"Turn JSON into Pandas DataFrames | Hackers And Slackers","meta_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","og_title":"Automagically Turn JSON into Pandas DataFrames","twitter_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","twitter_title":"Automagically Turn JSON into Pandas DataFrames","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"In his post about extracting data from APIs\n[https://hackersandslackers.com/extracting-massive-datasets-from-apis/], Todd\n[https://hackersandslackers.com/author/todd/]  demonstrated a nice way to\nmassage JSON into a pandas DataFrame. This method works great when our JSON\nresponse is flat, because dict.keys()  only gets the keys on the first \"level\"\nof a dictionary. It gets a little trickier when our JSON starts to become nested\nthough, as I experienced when working with Spotify's API\n[https://developer.spotify.com/documentation/web-api/]  via the Spotipy\n[https://spotipy.readthedocs.io/en/latest/]  library. For example, take a look\nat a response from their https://api.spotify.com/v1/tracks/{id}  endpoint:\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n\ntrack_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n\n\nOutput:\n{\n  \"album\": {\n    \"album_type\": \"album\",\n    \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n        \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n        \"name\": \"Stephen Malkmus\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n        \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n        \"name\": \"The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n      }\n    ],\n    \"available_markets\": [\"AR\",\n      \"BO\",\n      \"BR\",\n      \"CA\",\n      \"...\",\n      \"US\",\n      \"UY\",\n      \"VN\"\n    ],\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n    },\n    \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n    \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n    \"images\": [{\n        \"height\": 640,\n        \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n        \"width\": 640\n      },\n      {\n        \"height\": 300,\n        \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n        \"width\": 300\n      },\n      {\n        \"height\": 64,\n        \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n        \"width\": 64\n      }\n    ],\n    \"name\": \"Real Emotional Trash\",\n    \"release_date\": \"2008-03-04\",\n    \"release_date_precision\": \"day\",\n    \"type\": \"album\",\n    \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n  },\n  \"artists\": [{\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n      \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n      \"name\": \"Stephen Malkmus & The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n      \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n      \"name\": \"Stephen Malkmus\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n      \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n      \"name\": \"The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n    }\n  ],\n  \"available_markets\": [\"AR\",\n    \"BO\",\n    \"BR\",\n    \"CA\",\n    \"...\",\n    \"US\",\n    \"UY\",\n    \"VN\"\n  ],\n  \"disc_number\": 1,\n  \"duration_ms\": 608826,\n  \"explicit\": False,\n  \"external_ids\": {\n    \"isrc\": \"USMTD0877204\"\n  },\n  \"external_urls\": {\n    \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n  },\n  \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n  \"id\": \"0BDYBajZydY54OTgQsH940\",\n  \"is_local\": False,\n  \"name\": \"Real Emotional Trash\",\n  \"popularity\": 21,\n  \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n  \"track_number\": 4,\n  \"type\": \"track\",\n  \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n}\n\n\nIn addition to plenty of information about the track, Spotify also includes\ninformation about the album that contains the track. If we were to just use the \ndict.keys()  method to turn this response into a DataFrame, we'd be missing out\non all that extra album information. Well, it would be there, just not readily\naccessible.\n\ntrack_response.keys()\n\n\nOutput:\ndict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n\n\nSo how do we get around this? Well, we could write our own function, but because\npandas is amazing, it already has a built in tool that takes care of this for\nus.\n\nData Normalization\nMeet json_normalize():\n\nimport pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n\n\nOutput:\nalbum.album_type\n album.artists\n album.available_markets\n album.external_urls.spotify\n album.href\n album.id\n album.images\n album.name\n album.release_date\n album.release_date_precision\n ...\n external_urls.spotify\n href\n id\n is_local\n name\n popularity\n preview_url\n track_number\n type\n uri\n 0\n album\n [{'external_urls': {'spotify': 'https://open.s...\n [AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...\n https://open.spotify.com/album/6pWpb4IdPu9vp9m...\n https://api.spotify.com/v1/albums/6pWpb4IdPu9v...\n 6pWpb4IdPu9vp9mOdh5DjY\n [{'height': 640, 'url': 'https://i.scdn.co/ima...\n Real Emotional Trash\n 2008-03-04\n day\n ...\n https://open.spotify.com/track/0BDYBajZydY54OT...\n https://api.spotify.com/v1/tracks/0BDYBajZydY5...\n 0BDYBajZydY54OTgQsH940\n False\n Real Emotional Trash\n 21\n https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...\n 4\n track\n spotify:track:0BDYBajZydY54OTgQsH940\n Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and\nturns it into a DataFrame.\n\nThis makes our life easier when we're dealing with one record, but it really \ncomes in handy when we're dealing with a response that contains multiple\nrecords.\n\ntracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n\n\nOutput:\n{\n  \"tracks\": [{\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 608826,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877204\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n      \"id\": \"0BDYBajZydY54OTgQsH940\",\n      \"is_local\": False,\n      \"name\": \"Real Emotional Trash\",\n      \"popularity\": 21,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 4,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 222706,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877203\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs\",\n      \"id\": \"7fdUqrzb8oCcIoKvFuzMrs\",\n      \"is_local\": False,\n      \"name\": \"Cold Son\",\n      \"popularity\": 25,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 3,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:7fdUqrzb8oCcIoKvFuzMrs\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 416173,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877202\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj\",\n      \"id\": \"0islTY4Fw6lhYbfqi8Qtdj\",\n      \"is_local\": False,\n      \"name\": \"Hopscotch Willie\",\n      \"popularity\": 24,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 2,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0islTY4Fw6lhYbfqi8Qtdj\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 308146,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877201\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH\",\n      \"id\": \"3jyFLbljUTKjE13nIWXchH\",\n      \"is_local\": False,\n      \"name\": \"Dragonfly Pie\",\n      \"popularity\": 26,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 1,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:3jyFLbljUTKjE13nIWXchH\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        }],\n        \"available_markets\": [\"AR\",\n          \"AU\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"DO\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"JP\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"NZ\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\",\n          \"ZA\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG\",\n        \"id\": \"5DMvSCwRqfNVlMB5LjHOwG\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Sparkle Hard\",\n        \"release_date\": \"2018-05-18\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:5DMvSCwRqfNVlMB5LjHOwG\"\n      },\n      \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      }],\n      \"available_markets\": [\"AR\",\n        \"AU\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"DO\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"JP\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"NZ\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\",\n        \"ZA\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 423275,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD1710380\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs\",\n      \"id\": \"6dNmC2YWtWbVOFOdTuRDQs\",\n      \"is_local\": False,\n      \"name\": \"Difficulties - Let Them Eat Vowels\",\n      \"popularity\": 35,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 11,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:6dNmC2YWtWbVOFOdTuRDQs\"\n    }\n  ]\n}\n\n\n\njson_normalise(tracks_response)\n\n\nOutput:\nalbum.album_typealbum.artistsalbum.available_marketsalbum.external_urls.spotify\nalbum.hrefalbum.idalbum.imagesalbum.namealbum.release_date\nalbum.release_date_precision...external_urls.spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsSeparate Ways (Worlds Apart)\nBy default, json_normalize()  uses periods .  to indicate nested levels of the\nJSON object (which is actually converted to a Python dict  by Spotipy). In our\ncase, the album id is found in track['album']['id'], hence the period between\nalbum and id in the DataFrame. This makes things slightly annoying if we want to\ngrab a Series from our new DataFrame. In pandas, we can grab a Series from a\nDataFrame in many ways. To grab the album.id  column, for example:\n\ntracks_df['album.id']\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\nor\n\ntracks_df.loc[:,'album.id']\n\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\npandas also allows us to use dot notation (i.e. dataframe.column_name) to grab a\ncolumn as a Series, but only if our column name doesn't include a period\nalready. Since json_normalize()  uses a period as a separator by default, this\nruins that method. Never fear though – overriding this behavior is as simple as\noverriding the default argument in the function call:\n\ntracks_df = json_normalize(tracks_response['tracks'],sep=\"_\")\ntracks_df\n\n\nOutput:\nalbum_album_typealbum_artistsalbum_available_marketsalbum_external_urls_spotify\nalbum_hrefalbum_idalbum_imagesalbum_namealbum_release_date\nalbum_release_date_precision...external_urls_spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsNow we can go back to using dot notation to\naccess a column as a Series. This saves us some typing every time we want to\ngrab a column, and it looks a bit nicer (to me, at least). I say worth it.\n\ntracks_df.album_id\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n\n\nI Need That Record\nBy including more parameters when we use json_normlize(), we can really extract\njust the data that we want from our API response.\n\nFrom our responses above, we can see that the artist  property contains a list\nof artists that are associated with a track:\n\ntracks_response['tracks'][0]['artists']\n\n\nOutput:\n[{\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n    \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n    \"name\": \"Stephen Malkmus & The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n    \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n    \"name\": \"Stephen Malkmus\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n    \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n    \"name\": \"The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n  }\n]\n\n\nLet's say I want to load this data into a database later. It would be nice to\nhave a join table that maps each of the artists that are associated with each\ntrack. Luckily, this is possible with json_normalize()'s record_path  and meta \nparameters.\n\nrecord_path  tells json_normalize()  what path of keys leads to each individual\nrecord in the JSON object. In our case, we want to grab every artist id, so our\nfunction call will look like:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],sep=\"_\")\n\n\n\nexternal_urls href id name type uri\n 1 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 1 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 2 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 3 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 4 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 5 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 6 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 7 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 8 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 9 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 10 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 11 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 12 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPeCool – we're\nalmost there. Now we want to use the meta  parameter to specify what data we\nwant to include from the rest of the JSON object. In our case, we want to keep\nthe track id and map it to the artist id. If we look back at our API response,\nthe name of the column that included the track is is called, appropriately, id,\nso our full function call should look like this:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n\nOutput:\n-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    <ipython-input-14-77e00a98c3c0> in <module>()\n    ----> 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --> 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n\n\nUh oh – an error! What's going on? Well, it turns out that both the album id and\ntrack id were given the key id. pandas doesn't like that, and it gives us a\nhelpful error to tell us so: ValueError: Conflicting metadata name id, need\ndistinguishing prefix.\n\nThere are two more parameters we can use to overcome this error: record_prefix \nand meta_prefix. These are strings we'll add to the beginning of our records and\nmetadata to prevent these naming conflicts. Since we're dealing with Spotify\nartist ids for our records and Spotify track ids as the metadata, I'll use \nsp_artist_  and sp_track_  respectively. When that's done, I'll select only the\ncolumns that we're interested in.\n\nartist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=\"_\"\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n\n\nOutput:\nsp_track_id sp_artist_id\n 00BDYBajZydY54OTgQsH940 7wyRA7deGRxozTyBc6QXPe\n 10BDYBajZydY54OTgQsH940 0WISkx0PwT6lYWdPqKUJY8\n 20BDYBajZydY54OTgQsH940 7uStwCeP54Za8gXUFCf5L7\n 37fdUqrzb8oCcIoKvFuzMrs 7wyRA7deGRxozTyBc6QXPe\n 47fdUqrzb8oCcIoKvFuzMrs 0WISkx0PwT6lYWdPqKUJY8\n 57fdUqrzb8oCcIoKvFuzMrs 7uStwCeP54Za8gXUFCf5L7\n 60islTY4Fw6lhYbfqi8Qtdj 7wyRA7deGRxozTyBc6QXPe\n 70islTY4Fw6lhYbfqi8Qtdj 0WISkx0PwT6lYWdPqKUJY8\n 80islTY4Fw6lhYbfqi8Qtdj 7uStwCeP54Za8gXUFCf5L7\n 93jyFLbljUTKjE13nIWXchH 7wyRA7deGRxozTyBc6QXPe\n 103jyFLbljUTKjE13nIWXchH 0WISkx0PwT6lYWdPqKUJY8\n 113jyFLbljUTKjE13nIWXchH 7uStwCeP54Za8gXUFCf5L7\n 126dNmC2YWtWbVOFOdTuRDQs 7wyRA7deGRxozTyBc6QXPeTL;DR\n * Use pd.io.json.json_normalize()  to automagically flatten a nested JSON\n   object into a DataFrame\n * Make your life slightly easier when it comes to selecting columns by\n   overriding the default sep  parameter\n * Specify what data constitutes a record with the record_path  parameter\n * Include data from outside of the record path with the meta  parameter\n * Fix naming conflicts if they arise with the record_prefix  and meta_prefix \n   parameters","html":"<p>In his post about <a href=\"https://hackersandslackers.com/extracting-massive-datasets-from-apis/\">extracting data from APIs</a>, <a href=\"https://hackersandslackers.com/author/todd/\">Todd</a> demonstrated a nice way to massage JSON into a pandas DataFrame. This method works great when our JSON response is flat, because <code>dict.keys()</code> only gets the keys on the first \"level\" of a dictionary. It gets a little trickier when our JSON starts to become nested though, as I experienced when working with <a href=\"https://developer.spotify.com/documentation/web-api/\">Spotify's API</a> via the <a href=\"https://spotipy.readthedocs.io/en/latest/\">Spotipy</a> library. For example, take a look at a response from their <code>https://api.spotify.com/v1/tracks/{id}</code> endpoint:</p><pre><code class=\"language-python\">import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n</code></pre>\n<pre><code class=\"language-python\">track_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n</code></pre>\n<h3 id=\"output-\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;album&quot;: {\n    &quot;album_type&quot;: &quot;album&quot;,\n    &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;name&quot;: &quot;The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n      }\n    ],\n    &quot;available_markets&quot;: [&quot;AR&quot;,\n      &quot;BO&quot;,\n      &quot;BR&quot;,\n      &quot;CA&quot;,\n      &quot;...&quot;,\n      &quot;US&quot;,\n      &quot;UY&quot;,\n      &quot;VN&quot;\n    ],\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;images&quot;: [{\n        &quot;height&quot;: 640,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n        &quot;width&quot;: 640\n      },\n      {\n        &quot;height&quot;: 300,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n        &quot;width&quot;: 300\n      },\n      {\n        &quot;height&quot;: 64,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n        &quot;width&quot;: 64\n      }\n    ],\n    &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n    &quot;release_date&quot;: &quot;2008-03-04&quot;,\n    &quot;release_date_precision&quot;: &quot;day&quot;,\n    &quot;type&quot;: &quot;album&quot;,\n    &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n  },\n  &quot;artists&quot;: [{\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;name&quot;: &quot;The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n    }\n  ],\n  &quot;available_markets&quot;: [&quot;AR&quot;,\n    &quot;BO&quot;,\n    &quot;BR&quot;,\n    &quot;CA&quot;,\n    &quot;...&quot;,\n    &quot;US&quot;,\n    &quot;UY&quot;,\n    &quot;VN&quot;\n  ],\n  &quot;disc_number&quot;: 1,\n  &quot;duration_ms&quot;: 608826,\n  &quot;explicit&quot;: False,\n  &quot;external_ids&quot;: {\n    &quot;isrc&quot;: &quot;USMTD0877204&quot;\n  },\n  &quot;external_urls&quot;: {\n    &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n  },\n  &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n  &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n  &quot;is_local&quot;: False,\n  &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n  &quot;popularity&quot;: 21,\n  &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n  &quot;track_number&quot;: 4,\n  &quot;type&quot;: &quot;track&quot;,\n  &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n}\n</code></pre>\n<p>In addition to plenty of information about the track, Spotify also includes information about the album that contains the track. If we were to just use the <code>dict.keys()</code> method to turn this response into a DataFrame, we'd be missing out on all that extra album information. Well, it would be there, just not readily accessible.</p><pre><code class=\"language-python\">track_response.keys()\n</code></pre>\n<h3 id=\"output--1\">Output:</h3><pre><code class=\"language-python\">dict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n</code></pre>\n<p>So how do we get around this? Well, we could write our own function, but because pandas is amazing, it already has a built in tool that takes care of this for us.</p><h2 id=\"data-normalization\">Data Normalization</h2><p>Meet <code>json_normalize()</code>:</p><pre><code class=\"language-python\">import pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n</code></pre>\n<h3 id=\"output--2\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>album.album_type</th>\n      <th>album.artists</th>\n      <th>album.available_markets</th>\n      <th>album.external_urls.spotify</th>\n      <th>album.href</th>\n      <th>album.id</th>\n      <th>album.images</th>\n      <th>album.name</th>\n      <th>album.release_date</th>\n      <th>album.release_date_precision</th>\n      <th>...</th>\n      <th>external_urls.spotify</th>\n      <th>href</th>\n      <th>id</th>\n      <th>is_local</th>\n      <th>name</th>\n      <th>popularity</th>\n      <th>preview_url</th>\n      <th>track_number</th>\n      <th>type</th>\n      <th>uri</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>album</td>\n      <td>[{'external_urls': {'spotify': 'https://open.s...</td>\n      <td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td>\n      <td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td>\n      <td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td>\n      <td>6pWpb4IdPu9vp9mOdh5DjY</td>\n      <td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td>\n      <td>Real Emotional Trash</td>\n      <td>2008-03-04</td>\n      <td>day</td>\n      <td>...</td>\n      <td>https://open.spotify.com/track/0BDYBajZydY54OT...</td>\n      <td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td>\n      <td>0BDYBajZydY54OTgQsH940</td>\n      <td>False</td>\n      <td>Real Emotional Trash</td>\n      <td>21</td>\n      <td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td>\n      <td>4</td>\n      <td>track</td>\n      <td>spotify:track:0BDYBajZydY54OTgQsH940</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and turns it into a DataFrame.</p><p>This makes our life easier when we're dealing with one record, but it <em>really</em> comes in handy when we're dealing with a response that contains multiple records.</p><pre><code class=\"language-python\">tracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n</code></pre>\n<h3 id=\"output--3\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;tracks&quot;: [{\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 608826,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877204&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n      &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n      &quot;popularity&quot;: 21,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 4,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 222706,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877203&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;id&quot;: &quot;7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Cold Son&quot;,\n      &quot;popularity&quot;: 25,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 3,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:7fdUqrzb8oCcIoKvFuzMrs&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 416173,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877202&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;id&quot;: &quot;0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Hopscotch Willie&quot;,\n      &quot;popularity&quot;: 24,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 2,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0islTY4Fw6lhYbfqi8Qtdj&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 308146,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877201&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;id&quot;: &quot;3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Dragonfly Pie&quot;,\n      &quot;popularity&quot;: 26,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 1,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:3jyFLbljUTKjE13nIWXchH&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        }],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;AU&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;DO&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;JP&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;NZ&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;,\n          &quot;ZA&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;id&quot;: &quot;5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Sparkle Hard&quot;,\n        &quot;release_date&quot;: &quot;2018-05-18&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:5DMvSCwRqfNVlMB5LjHOwG&quot;\n      },\n      &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      }],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;AU&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;DO&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;JP&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;NZ&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;,\n        &quot;ZA&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 423275,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD1710380&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;id&quot;: &quot;6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Difficulties - Let Them Eat Vowels&quot;,\n      &quot;popularity&quot;: 35,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 11,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:6dNmC2YWtWbVOFOdTuRDQs&quot;\n    }\n  ]\n}\n\n</code></pre>\n<pre><code class=\"language-python\">json_normalise(tracks_response)\n</code></pre>\n<h3 id=\"output--4\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album.album_type</th><th>album.artists</th><th>album.available_markets</th><th>album.external_urls.spotify</th><th>album.href</th><th>album.id</th><th>album.images</th><th>album.name</th><th>album.release_date</th><th>album.release_date_precision</th><th>...</th><th>external_urls.spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><h2 id=\"separate-ways-worlds-apart-\">Separate Ways (Worlds Apart)</h2><p>By default, <code>json_normalize()</code> uses periods <code>.</code> to indicate nested levels of the JSON object (which is actually converted to a Python <code>dict</code> by Spotipy). In our case, the album id is found in <code>track['album']['id']</code>, hence the period between album and id in the DataFrame. This makes things slightly annoying if we want to grab a Series from our new DataFrame. In pandas, we can grab a Series from a DataFrame in many ways. To grab the <code>album.id</code> column, for example:</p><pre><code class=\"language-python\">tracks_df['album.id']\n</code></pre>\n<h3 id=\"output--5\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>or</p><pre><code class=\"language-python\">tracks_df.loc[:,'album.id']\n\n</code></pre>\n<h3 id=\"output--6\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>pandas also allows us to use dot notation (i.e. <code>dataframe.column_name</code>) to grab a column as a Series, but only if our column name doesn't include a period already. Since <code>json_normalize()</code> uses a period as a separator by default, this ruins that method. Never fear though – overriding this behavior is as simple as overriding the default argument in the function call:</p><pre><code class=\"language-python\">tracks_df = json_normalize(tracks_response['tracks'],sep=&quot;_&quot;)\ntracks_df\n</code></pre>\n<h3 id=\"output--7\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album_album_type</th><th>album_artists</th><th>album_available_markets</th><th>album_external_urls_spotify</th><th>album_href</th><th>album_id</th><th>album_images</th><th>album_name</th><th>album_release_date</th><th>album_release_date_precision</th><th>...</th><th>external_urls_spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><p>Now we can go back to using dot notation to access a column as a Series. This saves us some typing every time we want to grab a column, and it looks a bit nicer (to me, at least). I say worth it.</p><pre><code class=\"language-python\">tracks_df.album_id\n</code></pre>\n<h3 id=\"output--8\">Output:</h3><pre><code class=\"language-shell\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n</code></pre>\n<h2 id=\"i-need-that-record\">I Need That Record</h2><p>By including more parameters when we use <code>json_normlize()</code>, we can really extract just the data that we want from our API response.</p><p>From our responses above, we can see that the <code>artist</code> property contains a list of artists that are associated with a track:</p><pre><code class=\"language-python\">tracks_response['tracks'][0]['artists']\n</code></pre>\n<h3 id=\"output--9\">Output:</h3><pre><code class=\"language-json\">[{\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;name&quot;: &quot;The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n  }\n]\n</code></pre>\n<p>Let's say I want to load this data into a database later. It would be nice to have a join table that maps each of the artists that are associated with each track. Luckily, this is possible with <code>json_normalize()</code>'s <code>record_path</code> and <code>meta</code> parameters.</p><p><code>record_path</code> tells <code>json_normalize()</code> what path of keys leads to each individual record in the JSON object. In our case, we want to grab every artist id, so our function call will look like:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],sep=&quot;_&quot;)\n</code></pre>\n<h3></h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th> </th><th>external_urls </th><th>href </th><th>id </th><th>name </th><th>type </th><th>uri</th></tr></thead><tbody>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><p>Cool – we're almost there. Now we want to use the <code>meta</code> parameter to specify what data we want to include from the rest of the JSON object. In our case, we want to keep the track id and map it to the artist id. If we look back at our API response, the name of the column that included the track is is called, appropriately, <code>id</code>, so our full function call should look like this:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n</code></pre>\n<h3 id=\"output--10\">Output:</h3><pre><code class=\"language-python\">-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    &lt;ipython-input-14-77e00a98c3c0&gt; in &lt;module&gt;()\n    ----&gt; 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --&gt; 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n</code></pre>\n<p>Uh oh – an error! What's going on? Well, it turns out that both the album id and track id were given the key <code>id</code>. pandas doesn't like that, and it gives us a helpful error to tell us so: <code>ValueError: Conflicting metadata name id, need distinguishing prefix</code>.</p><p>There are two more parameters we can use to overcome this error: <code>record_prefix</code> and <code>meta_prefix</code>. These are strings we'll add to the beginning of our records and metadata to prevent these naming conflicts. Since we're dealing with Spotify artist ids for our records and Spotify track ids as the metadata, I'll use <code>sp_artist_</code> and <code>sp_track_</code> respectively. When that's done, I'll select only the columns that we're interested in.</p><pre><code class=\"language-python\">artist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=&quot;_&quot;\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n</code></pre>\n<h3 id=\"output--11\">Output:</h3><div class=\"tableshadow tableContainer\">\n<table>\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>sp_track_id </th><th>sp_artist_id</th></tr></thead><tbody>\n <tr><td>0</td><td>0BDYBajZydY54OTgQsH940 </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1</td><td>0BDYBajZydY54OTgQsH940 </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2</td><td>0BDYBajZydY54OTgQsH940 </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10</td><td>3jyFLbljUTKjE13nIWXchH </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12</td><td>6dNmC2YWtWbVOFOdTuRDQs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><h2 id=\"tl-dr\">TL;DR</h2><ul><li>Use <code>pd.io.json.json_normalize()</code> to automagically flatten a nested JSON object into a DataFrame</li><li>Make your life slightly easier when it comes to selecting columns by overriding the default <code>sep</code> parameter</li><li>Specify what data constitutes a record with the <code>record_path</code> parameter</li><li>Include data from outside of the record path with the <code>meta</code> parameter</li><li>Fix naming conflicts if they arise with the <code>record_prefix</code> and <code>meta_prefix</code> parameters</li></ul>","url":"https://hackersandslackers.com/json-into-pandas-dataframes/","uuid":"172cef40-4545-4c14-8488-a86a891ef47d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b588eb363c4cc21a000cf51"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ac","title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","slug":"trash-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","excerpt":"(And a way to clean it up with SQLAlchemy).","custom_excerpt":"(And a way to clean it up with SQLAlchemy).","created_at_pretty":"19 July, 2018","published_at_pretty":"23 July, 2018","updated_at_pretty":"17 November, 2018","created_at":"2018-07-18T20:26:25.000-04:00","published_at":"2018-07-23T08:30:00.000-04:00","updated_at":"2018-11-16T20:50:25.000-05:00","meta_title":"(And a way to clean it up with SQLAlchemy) | Hackers And Slackers","meta_description":"Python has an extremely handy little tool called f-strings that make string templating a snap!  ","og_description":"Trash Pandas: Messy, Convenient DB Operations via Pandas","og_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","og_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","twitter_description":"(And a way to clean it up with SQLAlchemy)","twitter_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","twitter_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you were continuing our task from last week\n[https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/]\n: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a\ndatabase.\n\nLet's say you've been given a new CSV that conflicts with some rows you've\nalready entered, and you're told that these rows are the correct values.\n\nWhy Not Use Pandas' Built-in Method?\nPandas' built-in to_sql  DataFrame method won't be useful here.  Remember, it\nwrites as a block - if you set the if_exists  flag to \"replace\", that'll make it\nreplace the entire DB table with a new one based on the DF you're uploading.\n And if you're doing this piecemeal, you presumably don't want that.\n\nLast week, we just made a new DataFrame out of each row and appended it to the\nDB table.  That won't work here - we need an Update.  Writing each update by\nhand would be annoying, though - luckily we can use code to generate more code!\n Python has an extremely handy little tool called f-strings  that make string\ntemplating a snap!\n\ndef updateStr(row):\n    return (f\"UPDATE books \"\n            f\"\"\"SET author = '{x.author}' \"\"\"\n            f\"\"\"WHERE id = {x.id};\"\"\")\n\n\nLet's walk through that.  It takes a row from a Dataframe - note that we're\nusing dot notation here instead of the bracket notation.  The reason we're doing\nthat is because, instead of using iterrows()  like last week, we'll be using \nitertuples  because the docstring for iterrows()  said I should.  One reason for\nthis is that iterrows()  gives a pandas Series, which will store everything as\nthe same datatype, which will be annoying in some cases.  I think it's supposed\nto be faster too?itertuples()  instead gives us Named Tuples, which is kind of\nlike a dictionary, except we have to use dot notation instead of square\nbrackets.\n\nSooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly\njust a convenient way of formatting strings with variables - any code inside\ncurly parentheses will be evaluated.  They're convenient, flexible, and\nsupposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a\nDataFrame df2  that only contains the rows to be updated...\n\ncnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n\n\nSweet!  Now let's actually execute it.  We'll be using the execute()  function\nin Pandas' io.sql  module.  I get the feeling I'm not supposed to, primarily\nbecause it doesn't have an entry in the official Pandas documentation, and I\nonly found it by poking around the module code.  But hey, it works!  (Warning\nfrom last time applies super-duper-extra-double this time!)\n\nfor x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n\n\nAnd now let's see if it worked...\n\npd.io.sql.read_sql_table(\"books\", cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n\n\nSweet!\n\nNow all that's well and good, but surely we're not the first person to try to\nmake SQL statements by calling Python functions!  How about a slightly less\nerror-prone way of doing this?\n\nSQLAlchemy\nI'll level with you - I've never actually used SQLAlchemy for anything but\nconnecting Pandas to databases before via the create_engine()  function.  But\nthat's why blogging's great - gives you an excuse to finally check out that\nthing you knew was gonna be useful!\n\nSQLAlchemy first needs some information about our table, then it'll let us\ncreate statements doing things to said table automagically.  We can either\ndefine it ourselves (maybe in a future post!) or read an existing table.  I\nfound the default way of doing this a little to\n\"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of\nthe command that worked into a little function.  I encourage you all to do the\nsame!\n\ndef loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n\n\nThere we go!  Now, let's load our books  table...\n\nbooks = loadTable(cnx, \"books\")\n\n\nAnd here's the cool part!  Now that we have our table object, it has a bunch of\nbuilt-in methods for doing SQL things!  We can print an example...\n\nstr(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n\n\nIf we call books.update, it'll do exactly that.  It also has a handy string\nrepresentation, for debugging and sanity checks.\n\nSQLAlchemy wants us to have a Connection  in addition to our Engine.  Well,\nalright then.\n\nconn = cnx.connect()\n\n\nFine, happy now?  Good.\n\nSQLAlchemy lets us build SQL statements by chain methods, which is fantastically\nuseful.  Less error-prone, easier to pass collections.  Our basic pattern would\nbe, based on iterating with itertuples...\n\nfor x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n\n\nSuccess!","html":"<p>Let's say you were continuing our task from <em><a href=\"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/\">last week</a></em>: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a database.</p><p>Let's say you've been given a new CSV that conflicts with some rows you've already entered, and you're told that these rows are the correct values.</p><h2 id=\"why-not-use-pandas-built-in-method\">Why Not Use Pandas' Built-in Method?</h2><p>Pandas' built-in <code>to_sql</code> DataFrame method won't be useful here.  Remember, it writes as a block - if you set the <code>if_exists</code> flag to <code>\"replace\"</code>, that'll make it replace the entire DB table with a new one based on the DF you're uploading.  And if you're doing this piecemeal, you presumably don't want that.</p><p>Last week, we just made a new DataFrame out of each row and appended it to the DB table.  That won't work here - we need an Update.  Writing each update by hand would be annoying, though - luckily we can use code to generate more code!  Python has an extremely handy little tool called <code>f-strings</code> that make string templating a snap!  </p><pre><code class=\"language-python\">def updateStr(row):\n    return (f&quot;UPDATE books &quot;\n            f&quot;&quot;&quot;SET author = '{x.author}' &quot;&quot;&quot;\n            f&quot;&quot;&quot;WHERE id = {x.id};&quot;&quot;&quot;)\n</code></pre>\n<p>Let's walk through that.  It takes a row from a Dataframe - note that we're using dot notation here instead of the bracket notation.  The reason we're doing that is because, instead of using <code>iterrows()</code> like last week, we'll be using <code>itertuples</code> because the docstring for <code>iterrows()</code> said I should.  One reason for this is that <code>iterrows()</code> gives a pandas Series, which will store everything as the same datatype, which will be annoying in some cases.  I think it's supposed to be faster too?  <code>itertuples()</code> instead gives us Named Tuples, which is kind of like a dictionary, except we have to use dot notation instead of square brackets.  </p><p>Sooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly just a convenient way of formatting strings with variables - any code inside curly parentheses will be evaluated.  They're convenient, flexible, and supposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a DataFrame <code>df2</code> that only contains the rows to be updated...</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n</code></pre>\n<p>Sweet!  Now let's actually execute it.  We'll be using the <code>execute()</code> function in Pandas' <code>io.sql</code> module.  I get the feeling I'm not supposed to, primarily because it doesn't have an entry in the official Pandas documentation, and I only found it by poking around the module code.  But hey, it works!  (Warning from last time applies super-duper-extra-double this time!)</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n</code></pre>\n<p>And now let's see if it worked...</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n</code></pre>\n<p>Sweet!</p><p>Now all that's well and good, but surely we're not the first person to try to make SQL statements by calling Python functions!  How about a slightly less error-prone way of doing this?</p><h1 id=\"sqlalchemy\">SQLAlchemy</h1><p>I'll level with you - I've never actually used SQLAlchemy for anything but connecting Pandas to databases before via the <code>create_engine()</code> function.  But that's why blogging's great - gives you an excuse to finally check out that thing you knew was gonna be useful!</p><p>SQLAlchemy first needs some information about our table, then it'll let us create statements doing things to said table automagically.  We can either define it ourselves (maybe in a future post!) or read an existing table.  I found the default way of doing this a little to \"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of the command that worked into a little function.  I encourage you all to do the same!</p><pre><code class=\"language-python\">def loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n</code></pre>\n<p>There we go!  Now, let's load our <code>books</code> table...</p><pre><code class=\"language-python\">books = loadTable(cnx, &quot;books&quot;)\n</code></pre>\n<p>And here's the cool part!  Now that we have our table object, it has a bunch of built-in methods for doing SQL things!  We can print an example...</p><pre><code class=\"language-python\">str(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n</code></pre>\n<p>If we call <code>books.update</code>, it'll do exactly that.  It also has a handy string representation, for debugging and sanity checks.</p><p>SQLAlchemy wants us to have a <code>Connection</code> in addition to our <code>Engine</code>.  Well, alright then.</p><pre><code class=\"language-python\">conn = cnx.connect()\n</code></pre>\n<p>Fine, happy now?  Good.</p><p>SQLAlchemy lets us build SQL statements by chain methods, which is fantastically useful.  Less error-prone, easier to pass collections.  Our basic pattern would be, based on iterating with <code>itertuples</code>...</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n</code></pre>\n<p>Success!</p>","url":"https://hackersandslackers.com/trash-pandas/","uuid":"8a789f92-fbde-48b5-8bdf-01206e340cc1","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b4fdab10dda8433e079043f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369a","title":"Using Pandas and SQLAlchemy to Simplify Databases","slug":"using-pandas-to-make-dealing-with-dbs-less-of-a-hassle","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2.jpg","excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","custom_excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","created_at_pretty":"02 July, 2018","published_at_pretty":"03 July, 2018","updated_at_pretty":"04 April, 2019","created_at":"2018-07-02T14:54:34.000-04:00","published_at":"2018-07-03T07:00:00.000-04:00","updated_at":"2019-04-04T00:35:09.000-04:00","meta_title":"Using Pandas and SQLAlchemy to Simplify Databases | Hackers and Slackers","meta_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-2.jpg","og_title":"Using Pandas and SQLAlchemy to Simplify Databases","twitter_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","twitter_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-1.jpg","twitter_title":"Using Pandas and SQLAlchemy to Simplify Databases","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"Manually opening and closing cursors? Iterating through DB output by hand?\nRemembering which function is the actual one that matches the Python data\nstructure you're gonna be using?\n\nThere has to be a better way!\n\nThere totally is.\n\nOne of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an\nExcel file, or a database - Pandas gets you what you want painlessly. In\nfact,I'd say that even if you don't have the spare bandwidth at the moment to\nrewire your brain to learn all the wonderful ways Pandas lets you manipulate\ndata (array-based programming generally isn't the first paradigm people learn),\nthen it's STILL worth using just for the I/O. Grab your data with one of its\ndelicious one-liners, and once it's in a Dataframe there's have a method to\nconvert it to practically any combination of native Python data structures that\nyou want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've\ngot it!).\n\nHere we'll be looking at interacting with a database with Pandas, because that's\nwhat I was finishing up when I saw Todd texted me that a lot of the people who\nwind up on the site are searching for Pandas stuff.\n\nSo, for our purposes, let's say you're doing Python stuff with a MySQL database.\nI'm also assuming you're using Linux (including via Windows Subsystem for Linux,\nwhich is what I use in order to have Ubuntu on my Windows laptop), because I've\nnever done this with Windows.\n\nAnnoyingly, there's more than one potential connector - I generally use PyMySQL,\nfor no particular reason. Installing this involves an extra step - telling pip\nor Conda to install it won't do the whole thing. Annoying, but them's the\nbreaks.\n\nsudo apt-get install python3-pymysql -y\n\n\nShould get you there. I remember fussing with this every time I have to do it on\na new machine.\n\nYou also have to install a mysql-connector, for reasons I won't pretend to\nunderstand.\n\nconda install mysql-connector-python -y\n\n\nI've never actually done this with pip, but I assume it'd be\n\npip install mysql-connector\n\n\nIf that doesn't work, try\n\npip install --allow-external mysql-connector-python mysql-connector-python\n\n\nWE'RE ALMOST DONE INSTALLING THINGS!\nLast, but not least, is a lovely little thing called SQLAlchemy. Remember when I\nsaid I didn't actually know anything about PyMySQL and its relative advantages\nand disadvantages to other MySQL connectors for Python? Part of the reason for\nthat is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap\nPython SQL connectors at a higher level of abstraction and not have to deal with\nthe particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's\nactually a default Anaconda package. BUT, if you're in a fresh environment (or\nnot using Conda)...\n\n(Always remember the little -y   at the end. That ensures that you can walk away\nwhile it installs and it'll be done when you get back, instead of finishing the\nlong process of whatever's happening when you first tell it to install and then\nwaiting to start on the next long process that takes place after you hit y.)\n\nOR\n\npip install sqlalchemy   (that IS one of the nice things about pip! Doesn't need\nyou to confirm).\n\nSo, to review, before we start Pythoning...\n\n$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n\n\nOR\n\n$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n\n\nCool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!\n We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what\nPandas uses to connect to databases.\n\nAaaand one (possible) last step. If you're doing this with a locally-installed\ndb, you might have to sudo service mysql start.\n\n(Like, every time you do dev work on it. At least on my machine. Your mileage\nmay vary)\n\nOn to the Actual Python\nSo, what does all this do for us? Well, first off, it wasn't THAT much - and you\nonly have to do it once (per machine (really per environment per machine (you\nare  taking advantage of Conda envs or virtualenvs, right?))). But what it\nreally lets us skip all the implementation  details and focus on the actual\nlogic of what we're doing. That's really what both SQL and Pandas are all about-\nthey wed short, declarative  bits of code to highly optimized libraries\nunderneath the hood. Relational Database Management Systems sit on top of code\nthat's been optimized for decades, turning your short queries into highly\nefficient compiled code. Pandas takes advantage of scientific computing\nlibraries for doing matrix operations that have been around since before there\nwas an internet. Drill down deep enough in Pandas and you'll actually find\nFORTRAN code that was originally written to simulate nuclear explosions. Our\nwhole goal is to just focus on our data manipulation via SQL and Python's\nscientific computing family - and we don't want to add a bunch of additional\ncognitive load from worrying about cursors.\n\nOkay, I Lied - NOW on to the Actual Python\nimport pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n\n\nFrom all the database connector stuff we installed, we actually just need ONE\nfunction - and then it's pure blissful operations on data from here on out.\ncreate_engine()  will create a connection for us, and then we'll use that as an\nargument to Pandas functions that talk to the DB. You might have to reopen it\nsometimes. Whatever.\n\nFirst, let's make a string that describes our database. Note that normally you\nshouldn't actually store this directly in your code (IT will totally be mad at\nyou), but for our purposes let's include it.\n\nThe format is:\n\ndialect+driver://username:password@host:port/database\n\n\nSo, since we're using mysql, and the PyMySQL package, we'd start with:\n\nmysql+pymysql://username:password@host:port/database\n\n\nThe rest is fairly straightforward. So, if we've got the user \"analyst\"  with\nthe password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty\nsure you don't have to specify a port if it's localhost, but bear with me), and\nthe database itself is named terrible_purchases, it'd be:\n\nmysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n\n\nAnd now let's actually make our connection!\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n\n\nDone! Now Pandas can read and write from our database.\nIf you don't wanna have to see a whole bunch of extraneous stuff, I'd also add\nthe flag echo=False  (create_engine  has a ton of useful little flags, letting\nyou do stuff like automatically strip unicode that your database might not be\nable to read).\nBehold! We can write an SQL query and then get a Pandas Dataframe with those\nexact parameters. We can rapidly prototype without worrying that the reason our\nquery isn't working is that we forgot some fussy intermediary stage. We can get\nstuff wrong and experiment!\n\nReading a table is easy as:\n\ndf = pd.io.sql.sql_query(\"SELECT * FROM cursed_items\", cnx)\n\n\nDone in one line!\n\nWriting is just as easy (with a teeeensy little asterisk, that I'll probably\ntalk about in the future). Let's say we dusted off an old CSV with more cursed\nitems that aren't in our database. After we've loaded it (and maybe done some\ncleaning)\n\nnewCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n\n\nThe if_exists  flag adds a ton of flexibility. You can create a whole new DB\ntable from a Dataframe, and that's actually the default behavior. For our case,\nwe appended - but there's other times when we might want to replace, for\ninstance.\n\nFinal Addendum\nSo, this is a super useful workflow for doing interactive analytics work with\nstuff that's on a database. It's also quite useful for quick-and-dirty scripts,\nor for prototyping the logic of something that'll be expanded upon later.  And,\nah, bigger stuff too - especially if there's gonna be really complex\ntransformations that. As long as the data can fit in memory (RAM), Pandas is\npretty awesome.\n\n\nThat being said, there are  times when you'd actually want to deal with the\nlevels we bypassed here. Everything has tradeoffs. One example would be that all\nthose compiled Linear Algebra libraries that Pandas sit on top of have a massive\nmemory footprint, and AWS Lambda charges you according to memory usage. I'm\npretty sure there are also use cases where this method of handling the\nlower-level nitty-gritty of database interactions (ie, by mostly not  handling\nit) will cause problems. But at that point, we're dealing with the difference\nbetween Data Science & Data Engineering.","html":"<p>Manually opening and closing cursors? Iterating through DB output by hand? Remembering which function is the actual one that matches the Python data structure you're gonna be using?</p><p>There has to be a better way!</p><p>There totally is.</p><p>One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. In fact,I'd say that even if you don't have the spare bandwidth at the moment to rewire your brain to learn all the wonderful ways Pandas lets you manipulate data (array-based programming generally isn't the first paradigm people learn), then it's STILL worth using just for the I/O. Grab your data with one of its delicious one-liners, and once it's in a Dataframe there's have a method to convert it to practically any combination of native Python data structures that you want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've got it!).</p><p>Here we'll be looking at interacting with a database with Pandas, because that's what I was finishing up when I saw Todd texted me that a lot of the people who wind up on the site are searching for Pandas stuff.</p><p>So, for our purposes, let's say you're doing Python stuff with a MySQL database. I'm also assuming you're using Linux (including via Windows Subsystem for Linux, which is what I use in order to have Ubuntu on my Windows laptop), because I've never done this with Windows.</p><p>Annoyingly, there's more than one potential connector - I generally use PyMySQL, for no particular reason. Installing this involves an extra step - telling pip or Conda to install it won't do the whole thing. Annoying, but them's the breaks.</p><pre><code class=\"language-bash\">sudo apt-get install python3-pymysql -y\n</code></pre>\n<p>Should get you there. I remember fussing with this every time I have to do it on a new machine.</p><p>You also have to install a mysql-connector, for reasons I won't pretend to understand.</p><pre><code class=\"language-bash\">conda install mysql-connector-python -y\n</code></pre>\n<p>I've never actually done this with pip, but I assume it'd be</p><pre><code class=\"language-bash\">pip install mysql-connector\n</code></pre>\n<p>If that doesn't work, try</p><pre><code class=\"language-bash\">pip install --allow-external mysql-connector-python mysql-connector-python\n</code></pre>\n<h3 id=\"we-re-almost-done-installing-things-\">WE'RE ALMOST DONE INSTALLING THINGS!</h3><p>Last, but not least, is a lovely little thing called SQLAlchemy. Remember when I said I didn't actually know anything about PyMySQL and its relative advantages and disadvantages to other MySQL connectors for Python? Part of the reason for that is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap Python SQL connectors at a higher level of abstraction and not have to deal with the particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's actually a default Anaconda package. BUT, if you're in a fresh environment (or not using Conda)...</p><p>(Always remember the little <code>-y</code>  at the end. That ensures that you can walk away while it installs and it'll be done when you get back, instead of finishing the long process of whatever's happening when you first tell it to install and then waiting to start on the next long process that takes place after you hit y.)</p><p>OR</p><p><code>pip install sqlalchemy</code>  (that IS one of the nice things about pip! Doesn't need you to confirm).</p><p>So, to review, before we start Pythoning...</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n</code></pre>\n<p>OR</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n</code></pre>\n<p>Cool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!  We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what Pandas uses to connect to databases.</p><p>Aaaand one (possible) last step. If you're doing this with a locally-installed db, you might have to <code>sudo service mysql start</code>.</p><p>(Like, every time you do dev work on it. At least on my machine. Your mileage may vary)</p><h2 id=\"on-to-the-actual-python\">On to the Actual Python</h2><p>So, what does all this do for us? Well, first off, it wasn't THAT much - and you only have to do it once (per machine (really per environment per machine (you are  taking advantage of Conda envs or virtualenvs, right?))). But what it really lets us skip all the implementation  details and focus on the actual logic of what we're doing. That's really what both SQL and Pandas are all about- they wed short, declarative  bits of code to highly optimized libraries underneath the hood. Relational Database Management Systems sit on top of code that's been optimized for decades, turning your short queries into highly efficient compiled code. Pandas takes advantage of scientific computing libraries for doing matrix operations that have been around since before there was an internet. Drill down deep enough in Pandas and you'll actually find FORTRAN code that was originally written to simulate nuclear explosions. Our whole goal is to just focus on our data manipulation via SQL and Python's scientific computing family - and we don't want to add a bunch of additional cognitive load from worrying about cursors.</p><h2 id=\"okay-i-lied-now-on-to-the-actual-python\">Okay, I Lied - NOW on to the Actual Python</h2><pre><code class=\"language-python\">import pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n</code></pre>\n<p>From all the database connector stuff we installed, we actually just need ONE function - and then it's pure blissful operations on data from here on out. create_engine()  will create a connection for us, and then we'll use that as an argument to Pandas functions that talk to the DB. You might have to reopen it sometimes. Whatever.</p><p>First, let's make a string that describes our database. Note that normally you shouldn't actually store this directly in your code (IT will totally be mad at you), but for our purposes let's include it.</p><p>The format is:</p><pre><code class=\"language-bash\">dialect+driver://username:password@host:port/database\n</code></pre>\n<p>So, since we're using mysql, and the PyMySQL package, we'd start with:</p><pre><code class=\"language-bash\">mysql+pymysql://username:password@host:port/database\n</code></pre>\n<p>The rest is fairly straightforward. So, if we've got the user \"analyst\"  with the password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty sure you don't have to specify a port if it's localhost, but bear with me), and the database itself is named terrible_purchases, it'd be:</p><pre><code class=\"language-bash\">mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n</code></pre>\n<p>And now let's actually make our connection!</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n</code></pre>\n<p>Done! Now Pandas can read and write from our database.<br>If you don't wanna have to see a whole bunch of extraneous stuff, I'd also add the flag echo=False  (create_engine  has a ton of useful little flags, letting you do stuff like automatically strip unicode that your database might not be able to read).<br>Behold! We can write an SQL query and then get a Pandas Dataframe with those exact parameters. We can rapidly prototype without worrying that the reason our query isn't working is that we forgot some fussy intermediary stage. We can get stuff wrong and experiment!</p><p>Reading a table is easy as:</p><pre><code class=\"language-python\">df = pd.io.sql.sql_query(&quot;SELECT * FROM cursed_items&quot;, cnx)\n</code></pre>\n<p>Done in one line!</p><p>Writing is just as easy (with a teeeensy little asterisk, that I'll probably talk about in the future). Let's say we dusted off an old CSV with more cursed items that aren't in our database. After we've loaded it (and maybe done some cleaning)</p><pre><code class=\"language-python\">newCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n</code></pre>\n<p>The <code>if_exists</code> flag adds a ton of flexibility. You can create a whole new DB table from a Dataframe, and that's actually the default behavior. For our case, we appended - but there's other times when we might want to replace, for instance.</p><h2 id=\"final-addendum\">Final Addendum</h2><p>So, this is a super useful workflow for doing interactive analytics work with stuff that's on a database. It's also quite useful for quick-and-dirty scripts, or for prototyping the logic of something that'll be expanded upon later.  And, ah, bigger stuff too - especially if there's gonna be really complex transformations that. As long as the data can fit in memory (RAM), Pandas is pretty awesome.</p><p><br>That being said, there are  times when you'd actually want to deal with the levels we bypassed here. Everything has tradeoffs. One example would be that all those compiled Linear Algebra libraries that Pandas sit on top of have a massive memory footprint, and AWS Lambda charges you according to memory usage. I'm pretty sure there are also use cases where this method of handling the lower-level nitty-gritty of database interactions (ie, by mostly not  handling it) will cause problems. But at that point, we're dealing with the difference between Data Science &amp; Data Engineering.</p>","url":"https://hackersandslackers.com/using-pandas-to-make-dealing-with-dbs-less-of-a-hassle/","uuid":"859fc3cb-daff-4c4b-9b10-ee206c25bb9f","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3a74ead0ac8a143588f35a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673636","title":"Dropping Rows of Data Using Pandas","slug":"pandas-dataframe-drop","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","custom_excerpt":"Square one of cleaning your Pandas Dataframes: dropping empty or problematic data.","created_at_pretty":"22 November, 2017","published_at_pretty":"18 April, 2018","updated_at_pretty":"08 March, 2019","created_at":"2017-11-22T01:21:36.000-05:00","published_at":"2018-04-18T15:00:00.000-04:00","updated_at":"2019-03-08T14:23:37.000-05:00","meta_title":"Dropping Rows Using Pandas | Hackers and Slackers","meta_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","og_title":"Dropping Rows Using Pandas","twitter_description":"Cleaning your Pandas Dataframes: dropping empty or problematic data. Learn the basic methods to get our data workable in a timely fashion.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandasmerge-2.jpg","twitter_title":"Dropping Rows Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"You've heard the cliché before: it is often cited that roughly %80~ of a data\nscientist's role is dedicated to cleaning data sets. I Personally haven't looked\nin to the papers or clinical trials which prove this number (that was a joke),\nbut the idea holds true: in the data profession, we find ourselves doing away\nwith blatantly corrupt or useless data. The simplistic approach is to discard\nsuch data entirely, thus here we are.\n\nWhat constitutes 'filthy' data is project-specific, and at times borderline\nsubjective. Occasionally, the offenders are more obvious: these might include\nchunks of data which are empty, poorly formatted, or simply irrelevant. While\n'bad' data can occasionally be fixed or salvaged via transforms, in many cases\nit's best to do away with rows entirely to ensure that only the fittest survive.\n\nDrop Empty Rows or Columns\nIf you're looking to drop rows (or columns) containing empty data, you're in\nluck: Pandas' dropna()  method is specifically for this. \n\nUsing dropna()  is a simple one-liner which accepts a number of useful\narguments:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n\nTechnically you could run MyDataFrame.dropna()  without any parameters, and this\n would default to dropping all rows where are completely empty. If thats all you\nneeded, well, I guess you're done already. Otherwise, here are the parameters\nyou can include:\n\n * Axis: Specifies to drop by row  or column. 0  means row, 1  means column.\n * How: Accepts one of two possible values: any  or all. This will either drop\n   an axis which is completely empty (all), or an axis with even just a single\n   empty cell (any).\n * Thresh: Here's an interesting one: thresh  accepts an integer, and will drop\n   an axis only if that number threshold of empty cells is breached.\n * Subset: Accepts an array of which axis' to consider, as opposed to\n   considering all by default.\n * Inplace: If you haven't come across inplace  yet, learn this now: changes\n   will NOT be made to the DataFrame you're touching unless this is set to True.\n   It's False  by default.\n\nPandas' .drop() Method\nThe pandas .drop()  method is used to remove entire rows or columns based on\ntheir name. If we can see that our DataFrame contains extraneous information\n(perhaps for example, the HR team is storing a preferred_icecream_flavor  in\ntheir master records), we can destroy the column (or row) outright.\n\nUsing drop()  looks something like this:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n\n\nWe'll attempt to cover the usage of these parameters in plain English before\ninevitably falling into useless lingo which you have not yet learned.\n\n *   Axis: Similar to the above, setting the axis specifies if you're trying to\n   drop rows or columns. \n *   Labels: May refer to either the name (string) of the target axis, or its\n   index (int). Of course, whether this is referring to columns or rows in the\n   DataFrame is dependent on the value of the axis parameter. Labels are always\n   defined in the 0th axis of the target DataFrame, and may accept multiple\n   values in the form of an array when dropping multiple rows/columns at once. \n\nDrop by Index:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n\n\nDrop by Label:\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n\n\n *   Index, Columns: An alternative method for specifying the same as the above.\n   Accepts single or multiple values. Setting columns=labels  is equivalent to \n   labels, axis=1.  index=0* is equivalent to *labels=0.  \n *   Levels: Used in sets of data which contain multiple hierarchical levels,\n   similar to that of nested arrays. A high-level few of Hierarchical indexing\n   can be found here\n   [https://pandas.pydata.org/pandas-docs/stable/advanced.html]. \n *   Inplace: Again, drop methods are not carried out on the target Dataframe\n   unless explicitly stated. The purpose of this is to presumably preserve the\n   original set of data during ad hoc manipulation.This adheres to the Python\n   style-guide which states that actions should not be performed on live sets of\n   data unless explicitly stated. Here\n   [https://www.youtube.com/watch?v=XaCSdr7pPmY]  is a video of some guy\n   describing this for some reason. \n *   Errors: Accepts either ignore  or raise, with 'raise' set as default. When \n   errors='ignore'  is set, no errors will be thrown and existing labels are\n   dropped. \n\nDrop by Criteria\nWe can also remove rows or columns based on whichever criteria your little heart\ndesires. For example, if you really hate people named Chad, you can drop all\nrows in your Customer database who have the name Chad. Screw Chad.\n\nUnlike previous methods, the popular way of handling this is simply by saving\nyour Dataframe over itself give a passed value. Here's how we'd get rid of Chad:\n\nimport pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n\n\nThe syntax may seem a bit off-putting to newcomers (note the repetition of \nmy_dataframe  3 times). The format of my_dataframe[CONDITION]  simply returns a\nmodified version of my_dataframe, where only the data matching the given\ncondition is affected. \n\nSince we're purging this data altogether, statingmy_dataframe =\nmy_dataframe[CONDITION]  is an easy (albeit destructive) method for shedding\ndata and moving on with our lives.","html":"<p>You've heard the cliché before: it is often cited that roughly %80~ of a data scientist's role is dedicated to cleaning data sets. I Personally haven't looked in to the papers or clinical trials which prove this number (that was a joke), but the idea holds true: in the data profession, we find ourselves doing away with blatantly corrupt or useless data. The simplistic approach is to discard such data entirely, thus here we are.</p><p>What constitutes 'filthy' data is project-specific, and at times borderline subjective. Occasionally, the offenders are more obvious: these might include chunks of data which are empty, poorly formatted, or simply irrelevant. While 'bad' data can occasionally be fixed or salvaged via transforms, in many cases it's best to do away with rows entirely to ensure that only the fittest survive.</p><h2 id=\"drop-empty-rows-or-columns\">Drop Empty Rows or Columns</h2><p>If you're looking to drop rows (or columns) containing empty data, you're in luck: Pandas' <code>dropna()</code> method is specifically for this. </p><p>Using <code>dropna()</code> is a simple one-liner which accepts a number of useful arguments:</p><!--kg-card-begin: code--><pre><code>import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop rows with any empty cells\nmy_dataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)</code></pre><!--kg-card-end: code--><p>Technically you could run <code>MyDataFrame.dropna()</code> without any parameters, and this  would default to dropping all rows where are completely empty. If thats all you needed, well, I guess you're done already. Otherwise, here are the parameters you can include:</p><ul><li><strong>Axis</strong>: Specifies to drop by <em>row</em> or <em>column</em>. <code>0</code> means <em>row</em>, <code>1</code> means <em>column</em>.</li><li><strong>How</strong>: Accepts one of two possible values: <em>any</em> or <em>all</em>. This will either drop an axis which is completely empty (all), or an axis with even just a single empty cell (any).</li><li><strong>Thresh</strong>: Here's an interesting one: <em>thresh</em> accepts an integer, and will drop an axis only if that number threshold of empty cells is breached.</li><li><strong>Subset</strong>: Accepts an array of which axis' to consider, as opposed to considering all by default.</li><li><strong>Inplace</strong>: If you haven't come across <code>inplace</code> yet, learn this now: changes will NOT be made to the DataFrame you're touching unless this is set to <code>True</code>. It's <code>False</code> by default.</li></ul><h2 id=\"pandas-drop-method\">Pandas' .drop() Method</h2><p>The pandas <code>.drop()</code> method is used to remove entire rows or columns based on their name. If we can see that our DataFrame contains extraneous information (perhaps for example, the HR team is storing a <strong>preferred_icecream_flavor</strong> in their master records), we can destroy the column (or row) outright.</p><p>Using <code>drop()</code> looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\nmy_dataframe.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n</code></pre>\n<!--kg-card-end: markdown--><p>We'll attempt to cover the usage of these parameters in plain English before inevitably falling into useless lingo which you have not yet learned.</p><ul><li> <strong>Axis</strong>: Similar to the above, setting the axis specifies if you're trying to drop rows or columns. </li><li> <strong>Labels</strong>: May refer to either the name (string) of the target axis, or its index (int). Of course, whether this is referring to columns or rows in the DataFrame is dependent on the value of the axis parameter. Labels are always defined in the 0th axis of the target DataFrame, and may accept multiple values in the form of an array when dropping multiple rows/columns at once. </li></ul><h3 id=\"drop-by-index-\">Drop by Index:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by row or column index\nmy_dataframe.drop([0, 1])\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"drop-by-label-\">Drop by Label:</h3><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop by column name\nmy_dataframe.drop(['B', 'C'])\n</code></pre>\n<!--kg-card-end: markdown--><ul><li> <strong>Index, Columns</strong>: An alternative method for specifying the same as the above. Accepts single or multiple values. Setting <em>columns=labels</em> is equivalent to <em>labels, axis=1.</em> <em>index=0</em>* is equivalent to *<em>labels=0.</em> </li><li> <strong>Levels</strong>: Used in sets of data which contain multiple hierarchical levels, similar to that of nested arrays. A high-level few of Hierarchical indexing can be found <a href=\"https://pandas.pydata.org/pandas-docs/stable/advanced.html\">here</a>. </li><li> <strong>Inplace</strong>: Again, drop methods are not carried out on the target Dataframe unless explicitly stated. The purpose of this is to presumably preserve the original set of data during ad hoc manipulation.This adheres to the Python style-guide which states that actions should not be performed on live sets of data unless explicitly stated. <a href=\"https://www.youtube.com/watch?v=XaCSdr7pPmY\">Here</a> is a video of some guy describing this for some reason. </li><li> <strong>Errors</strong>: Accepts either <em>ignore</em> or <em>raise</em>, with 'raise' set as default. When <em>errors='ignore'</em> is set, no errors will be thrown and existing labels are dropped. </li></ul><h2 id=\"drop-by-criteria\">Drop by Criteria</h2><p>We can also remove rows or columns based on whichever criteria your little heart desires. For example, if you really hate people named Chad, you can drop all rows in your Customer database who have the name Chad. Screw Chad.</p><p>Unlike previous methods, the popular way of handling this is simply by saving your Dataframe over itself give a passed value. Here's how we'd get rid of Chad:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Create a Dataframe from CSV\nmy_dataframe = pd.read_csv('example.csv')\n\n# Drop via logic: similar to SQL 'WHERE' clause\nmy_dataframe = my_dataframe[my_dataframe.employee_name != 'chad')]\n</code></pre>\n<!--kg-card-end: markdown--><p>The syntax may seem a bit off-putting to newcomers (note the repetition of <code>my_dataframe</code> 3 times). The format of <code>my_dataframe[CONDITION]</code> simply returns a modified version of <code>my_dataframe</code>, where only the data matching the given condition is affected. </p><p>Since we're purging this data altogether, stating  <code>my_dataframe = my_dataframe[CONDITION]</code> is an easy (albeit destructive) method for shedding data and moving on with our lives.</p>","url":"https://hackersandslackers.com/pandas-dataframe-drop/","uuid":"6f57d667-6bab-4d97-a62a-adfb2e887d6c","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a151770ade7aa41676efce7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673632","title":"Merge Sets of Data in Python Using Pandas","slug":"merge-dataframes-with-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","excerpt":"Perform SQL-like merges of data using Python's Pandas.","custom_excerpt":"Perform SQL-like merges of data using Python's Pandas.","created_at_pretty":"18 November, 2017","published_at_pretty":"18 November, 2017","updated_at_pretty":"26 December, 2018","created_at":"2017-11-17T19:09:32.000-05:00","published_at":"2017-11-17T19:22:25.000-05:00","updated_at":"2018-12-26T04:29:22.000-05:00","meta_title":"Merging Dataframes with Pandas | Hackers and Slackers","meta_description":"Perform merges of data similar to SQL JOINs using Python's Pandas library: the essential library for data analysis in Oython. ","og_description":"Perform SQL-like merges of data using Python's Pandas","og_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","og_title":"Merging Dataframes with Pandas","twitter_description":"Perform SQL-like merges of data using Python's Pandas","twitter_image":"https://hackersandslackers.com/content/images/2017/11/pandasmerge@2x.jpg","twitter_title":"Merging Dataframes with Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you have two obscenely large sets of data. \n\nThese sets of data contain information on a similar topic, such as customers. \nDataset #1 might contain a high-level view of all customers of a business, while\n Datatset #2  contains a lifetime history of orders for a company.\nUnsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any\nbusiness' orders are made by customers.\n\nWelcome to Relational Databases\nWhat we just described is the core foundation for relational databases  which\nhave been running at the core of businesses since the 1970s. Starting with\nfamiliar names like MySQL,  Oracle, and Postgres,  the concept of maintaining\nmultiple -related- tables of data are the bare minimum technology stack for any\ncompany, regardless of what said company does.\n\nWhile our example of Datasets #1 and #2  can be thought of as isolated tables,\nthe process of 'joining' them (in SQL terms) or 'merging' them (in Pandas terms) \n is  trivial. What's more, we can do far more than with JOINS (or merges) than\nsimply combining our data into a single set.\n\nEnter The Panda\nPython happens to have an obscenely popular library for performing SQL-like\nlogic, dubbed Pandas. If it remains unclear as to what Pandas is, just remember:\n Databases are basically Excel spreadsheets are basically an interface for\nPandas. The technicality of that explanation may be horrendous to those who\nunderstand the differences, but the fundamental truth remains: we're dealing\nwith information, inside of cells, on a two-dimensional grid. When you hear the\nnext idiot spew a catch phrase like \"data is the new oil\", the \"data\" they're\nreferring to is akin to that sick Excel sheet you made at work.\n\nScenario: Finding Mismatches in Data\nThis scenario actually stems from a real-life example which, sure enough, was my\nfirst encounter with Pandas. One could argue I owe much 0f my data career to a\n3am Google Hangout with Snkia.\n\nIn our scenario, our company has signed up for a very expensive software product\nwhich charges by individual license. To our surprise, the number of licenses for\nthis software totaled over 1000  seats!  After giving this data a quick glance,\nhowever, it's clear that many of these employees have actually been terminated,\nthus resulting in unspeakable loss in revenue. \n\nThe good news is we have another dataset called active employees (aka: employees\nwhich have not been terminated... yet). So, how do we use these two sets of data\nto determine which software licenses are valid? First, let's look at the types \nof ways we can merge data in Pandas.\n\nTerminology\nMERGE\nSets of data can be merged in a number of ways. Merges can either be used to\nfind similarities in two Dataframes and merge associated information, or may be\nentirely non-destructive in the way that two sets of data are merged.\n\nKEY\nIn many cases (such as the one in this tutorial) you'd likely want to merge two\nDataframes based on the value of a key. A key is the authoritative column by\nwhich the Dataframes will be merged. When merging Dataframes in this way, keys\nwill stay in tact as an identifier while the values of columns in the same row\nassociated to that key.\n\nThis type of merge can be used when two Dataframes hold differing fields for\nsimilar rows. If Dataframe 1 contains the phone numbers of customers by name,\nand Dataframe 2 contains emails of a similar grouping of people, these two may\nbe merged to create a single collection of data with all of this information.\n\nAXIS\nA parameter of pandas functions which determines whether the function should be\nrun against a Dataframe's columns or rows. An axis of 0 determines that the\naction will be taken on a per-row basis, where an axis of 1 denotes column.\n\nFor example, performing a drop with axis 0 on key X will drop the row where\nvalue of a cell is equal to X.\n\nLEFT/RIGHT MERGE\nAn example of a left/right merge can be seen below:\n\nJoin on keys found in left Dataframe.The two data frames above hold similar keys\nwith different associated information per axis, thus the result is a combination\nof these two Dataframes where the keys remain intact.\n\n\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from\nboth Dataframes do not match 1-to-1, specifying a left/right merge determines\nwhich Dataframe's keys will be considered the authority to be preserved in the\nmerge.\n\nJoin on keys found in right Dataframe.INNER MERGE\nAn inner merge will merge two Dataframes based on overlap of values between keys\nin both Dataframes:\n\nJoin on keys found in right Dataframe.OUTER MERGE\nAn outer merge will preserve the most data by not dropping keys which are\nuncommon to both Dataframes. Keys which exist in a single Dataframe will be\nadded to the resulting Dataframe, with empty values populated for any columns\nbrought in by the other Dataframe:\n\nBack to our Scenario: Merging Two Dataframes via Left Merge\nLet's get it going. Enter the iPython shell.\n\nImport Pandas and read both of your CSV files.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"csv1.csv\")  \ndf2 = pd.read_csv(\"csv2.csv\")\n\n\nThe above opens the CSVs as Dataframes recognizable by pandas.\nNext, we'll merge the two CSV files.\n\nHow  specifies the type of merge, and on  specifies the column to merge by\n(key). The key must be present in both Dataframes.\n\nFor the purpose of this exercise we'll be merging left, as that is the CSV which\ncontains the keys we'd like to maintain.\n\nmergedDF = df2.merge(df, how=“left”, on=\"email\")\n\nprint(mergedDF)\n\n\nThis should return a dataset of all common rows, with columns from both CSVs\nincluded in the merge.\n\nScenario 2: Missing Data\nBefore we go, let's toss in another scenario for good measure.\n\nThis time around we have two datasets which should actually probably be a single\ndataset. Dataset #1  contains all customers once again, but for some reason, \nDataset #1  contains email address where set Dataset #2  does not. Similarly, \nDataset #2  contains addresses which are  missing in Dataset #1. We assume there\nis no reason to keep these sets of data isolated other than human error.\n\nIn the case where we are confident that employees exist in both datasets but\ncontain different information, performing an inner  merge will join these two\nsets by a key such as customer ID or email. If all goes well, the final dataset\nshould equal the same number of rows found in both Datasets #1 and #2.\n\nDocumentation\nFor more on merging, check out the official Pandas documentation here\n[https://pandas.pydata.org/pandas-docs/stable/merging.html].","html":"<style>\n    img {\n        border: 0 !important;\n    }\n </style>   <p>Let's say you have two obscenely large sets of data. </p><p>These sets of data contain information on a similar topic, such as customers. <strong>Dataset #1 </strong>might contain a high-level view of all customers of a business, while <strong>Datatset #2</strong> contains a lifetime history of orders for a company. Unsurprisingly, the customers in Dataset #1 appear in Dataset x#2, as any business' orders are made by customers.</p><h2 id=\"welcome-to-relational-databases\">Welcome to Relational Databases</h2><p>What we just described is the core foundation for <em>relational databases</em> which have been running at the core of businesses since the 1970s. Starting with familiar names like <strong>MySQL</strong>,<strong> Oracle</strong>, and <strong>Postgres,</strong> the concept of maintaining multiple -<em>related- </em>tables of data are the bare minimum technology stack for any company, regardless of what said company does.</p><p>While our example of <strong>Datasets #1 and #2</strong> can be thought of as isolated tables, the process of 'joining' them <em>(in SQL terms) </em>or 'merging' them <em>(in Pandas terms)</em> is  trivial. What's more, we can do far more than with JOINS (or merges) than simply combining our data into a single set.</p><h3 id=\"enter-the-panda\">Enter The Panda</h3><p>Python happens to have an obscenely popular library for performing SQL-like logic, dubbed <strong>Pandas. </strong>If it remains unclear as to what Pandas is, just remember: <em>Databases are basically Excel spreadsheets are basically an interface for Pandas</em>. The technicality of that explanation may be horrendous to those who understand the differences, but the fundamental truth remains: we're dealing with information, inside of cells, on a two-dimensional grid. When you hear the next idiot spew a catch phrase like <em>\"data is the new oil\"</em>, the \"data\" they're referring to is akin to that sick Excel sheet you made at work.</p><h3 id=\"scenario-finding-mismatches-in-data\">Scenario: Finding Mismatches in Data</h3><p>This scenario actually stems from a real-life example which, sure enough, was my first encounter with Pandas. One could argue I owe much 0f my data career to a 3am Google Hangout with Snkia.</p><p>In our scenario, our company has signed up for a very expensive software product which charges by individual license. To our surprise, the number of licenses for this software totaled <strong>over 1000</strong> seats!<strong> </strong>After giving this data a quick glance, however, it's clear that many of these employees have actually been terminated, thus resulting in unspeakable loss in revenue. </p><p>The good news is we have another dataset called <em>active employees </em>(aka: employees which have not been terminated... yet). So, how do we use these two sets of data to determine which software licenses are valid? First, let's look at the <em>types</em> of ways we can merge data in Pandas.</p><h2 id=\"terminology\">Terminology</h2><h3 id=\"merge\">MERGE</h3><p>Sets of data can be merged in a number of ways. Merges can either be used to find similarities in two Dataframes and merge associated information, or may be entirely non-destructive in the way that two sets of data are merged.</p><h3 id=\"key\">KEY</h3><p>In many cases (such as the one in this tutorial) you'd likely want to merge two Dataframes based on the value of a key. A key is the authoritative column by which the Dataframes will be merged. When merging Dataframes in this way, keys will stay in tact as an identifier while the values of columns in the same row associated to that key.</p><p>This type of merge can be used when two Dataframes hold differing fields for similar rows. If Dataframe 1 contains the phone numbers of customers by name, and Dataframe 2 contains emails of a similar grouping of people, these two may be merged to create a single collection of data with all of this information.</p><h3 id=\"axis\">AXIS</h3><p>A parameter of pandas functions which determines whether the function should be run against a Dataframe's columns or rows. An axis of 0 determines that the action will be taken on a per-row basis, where an axis of 1 denotes column.</p><p>For example, performing a drop with axis 0 on key X will drop the row where value of a cell is equal to X.</p><h3 id=\"left-right-merge\">LEFT/RIGHT MERGE</h3><p>An example of a left/right merge can be seen below:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasleftjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in left Dataframe.</figcaption></figure><p>The two data frames above hold similar keys with different associated information per axis, thus the result is a combination of these two Dataframes where the keys remain intact.</p><p>\"Left\" or \"right\" refer to the left or right Dataframes above. If the keys from both Dataframes do not match 1-to-1, specifying a left/right merge determines which Dataframe's keys will be considered the authority to be preserved in the merge.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasrightjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"inner-merge\">INNER MERGE</h3><p>An inner merge will merge two Dataframes based on overlap of values between keys in both Dataframes:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasinnerjoin.png\" class=\"kg-image\"><figcaption>Join on keys found in right Dataframe.</figcaption></figure><h3 id=\"outer-merge\">OUTER MERGE</h3><p>An outer merge will preserve the most data by not dropping keys which are uncommon to both Dataframes. Keys which exist in a single Dataframe will be added to the resulting Dataframe, with empty values populated for any columns brought in by the other Dataframe:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/pandasouterjoin.png\" class=\"kg-image\"></figure><h2 id=\"back-to-our-scenario-merging-two-dataframes-via-left-merge\">Back to our Scenario: Merging Two Dataframes via Left Merge</h2><p>Let's get it going. Enter the iPython shell.</p><p>Import Pandas and read both of your CSV files.</p><pre><code class=\"language-python\">import pandas as pd\n\ndf = pd.read_csv(&quot;csv1.csv&quot;)  \ndf2 = pd.read_csv(&quot;csv2.csv&quot;)\n</code></pre>\n<p>The above opens the CSVs as Dataframes recognizable by pandas.<br>Next, we'll merge the two CSV files.</p><p><strong>How</strong> specifies the type of merge, and <strong>on</strong> specifies the column to merge by (key). The key must be present in both Dataframes.</p><p>For the purpose of this exercise we'll be merging left, as that is the CSV which contains the keys we'd like to maintain.</p><pre><code class=\"language-python\">mergedDF = df2.merge(df, how=“left”, on=&quot;email&quot;)\n\nprint(mergedDF)\n</code></pre>\n<p>This should return a dataset of all common rows, with columns from both CSVs included in the merge.</p><h2 id=\"scenario-2-missing-data\">Scenario 2: Missing Data</h2><p>Before we go, let's toss in another scenario for good measure.</p><p>This time around we have two datasets which should actually probably be a single dataset. <strong>Dataset #1</strong> contains all customers once again, but for some reason, <strong>Dataset #1</strong> contains email address where set <strong>Dataset #2</strong> does not. Similarly, <strong>Dataset #2</strong> contains addresses which are  missing in <strong>Dataset #1</strong>. We assume there is no reason to keep these sets of data isolated other than human error.</p><p>In the case where we are confident that employees exist in both datasets but contain different information, performing an <em>inner</em> merge will join these two sets by a key such as customer ID or email. If all goes well, the final dataset should equal the same number of rows found in both <strong>Datasets #1 and #2</strong>.</p><h2 id=\"documentation\">Documentation</h2><p>For more on merging, check out the official Pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/merging.html\">here</a>.</p>","url":"https://hackersandslackers.com/merge-dataframes-with-pandas/","uuid":"d2c59476-d879-484c-b719-55f53b3d4980","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f7a3ce38d612cc8261316"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867363f","title":"Another \"Intro to Data Analysis in Python Using Pandas\" Post","slug":"intro-to-data-analysis-in-python-using-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","custom_excerpt":"Obligatory Pandas tutorial by a questionably qualified stranger.","created_at_pretty":"19 April, 2018","published_at_pretty":"16 November, 2017","updated_at_pretty":"10 April, 2019","created_at":"2018-04-18T21:18:27.000-04:00","published_at":"2017-11-16T10:52:00.000-05:00","updated_at":"2019-04-10T10:33:17.000-04:00","meta_title":"Intro to Data Analysis in Python Using Pandas | Hackers and Slackers","meta_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","og_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","og_title":"Intro to Data Analysis in Python Using Pandas","twitter_description":"Obligatory introduction to Pandas by a questionably qualified stranger. Learn the anatomy of a DataFrame, how to load data, and selecting subsets of data.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/intropandas-1-4@2x.jpg","twitter_title":"Intro to Data Analysis in Python Using Pandas","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"}],"plaintext":"Let’s face it: the last thing the world needs is another “Intro to Pandas” post.\nAnybody strange enough to read this blog surely had the same reaction to\ndiscovering Pandas as I did: a manic euphoria that can only be described as love\nat first sight. We wanted to tell the world, and that we did. A lot. Yet here I\nam, about to helplessly sing cliche praises one more time. \n\nI’m a prisoner of circumstance here. As it turns out, the vast (and I mean vast)\nmajority of our fans have a raging Pandas addiction. They come to our humble\nmom-and-pop shop here at Hackers and Slackers foaming at the mouth, going on\nraging benders for all Pandas-related content. If I had half a brain, I’d rename\nthis site Pandas and Pandas  and delete all non-Pandas-related content. Talk\nabout cash money. \n\nAs a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro\nto Pandas” post was an unflattering belligerent mess jotted into a Confluence\ninstance long ago during a Friday night  pregame. That mess snuck its way on to\nthis blog, and has gone unnoticed for a year now. I've decided that this\nprobably wasn't the best way to open up a series about the most influential\nPython library of all time. We're going to try this again... For the Pandas.\n\nIntro to Pandas Rereleased: in IMAX 8k 4D \nPandas is used to analyze and modify tabular data in Python. When we say\n“tabular data,” we mean any instance in life where data is represented in a\ntable format. Excel, SQL databases, shitty HTML tables.... they’ve all been the\nsame thing with different syntax this whole time. Pandas can achieve anything\nthat any other table can. \n\nIf you’re reasonably green to data analysis and are experiencing the \n“oh-my-God-all-data-professions-are-kinda-just-Excel”  realization as we speak,\nfeel free to take a moment. Great, that’s behind us now.\n\nThe Anatomy of a DataFrame\nTabular data in Pandas is referred to as a “DataFrame.” We can’t call everything \n “tables-” otherwise, our choice of vague terminology would grow horribly\nconfusing when we refer to data in different systems. Between you and me though,\nDataFrames are basically tables.\n\nSo how do we represent two-dimensional data via command line: a concept which\ninherently interprets and displays information one-dimensionally? \n\n\"Oh nothing, we think it's cute.\"DataFrames consist of individual parts which\nare easy-to-understand at face value. It’s the complexity of these things\ntogether, creating a sum greater than the whole of its parts, which fuels the\nseemingly endless power of DataFrames. If we want any  hope of contributing to\nthe field of Data Science, we need to not only understand the terminology but at\nleast be aware of core concepts of what a DataFrame is beneath the hood. This\nunderstanding is what separates engineers from Excel monkeys. \n\nEngineers\n1\nWinExcel Nerds\n0\nLoseParts of a DataFrame\nWere you expecting this post just to be a bunch of one-liners in Pandas? Good, I\nhope you're disappointed. Strap yourself in, we might actually learn something\ntoday. Class is now in session, baby. Let's break apart what makes a DataFrame,\npiece-by-piece:\n\nThe most basic description of any table would be a collection of columns and \nrows. Despite looking at a two-dimensional grid, columns are in fact very\ndifferent from rows. Unlike rows, all data in a column  typically abides by the\nsame data type. In the above example, any value saved in the startTime  column\nwill always be a time. Rows, on the other hand, are simply an entry- an instance\nof a \"thing\", where each thing is described by attributes stored horizontally\nacross columns.\n\nThis seems like really elementary stuff, but I mention it for a reason. By our\ndefinition, columns  are more pivotal to structuring a table than rows, because\neven empty columns have meaning, whereas an empty row with no columns will\nalways equal infinite nothingness. Rows are made up of values in columns, not\nthe other way around. Thus, columns in Pandas are actually their own type of\nobject called a Series. \n\n * Series' are objects native to Pandas (and Numpy) which refer to\n   one-dimensional sequences of data. Another example of a one-dimensional\n   sequence of data could be an array, but series' are much more than arrays:\n   they're a class of their own for many powerful reasons, which we'll see in a\n   moment.\n * Axis  refers to the 'direction' of a series, or in other words \"column\" or\n   \"row\". A series with an axis of 0  is a row, whereas a series with an axis of\n    1  is a column. \n * A series contains labels, which are given visual names for a row/column\n   specifying labels allows us to call upon any labeled series in the same way\n   we would access a value in a Python dictionary. For instance, accessing \n   dataframe['awayTeamName']  returns the entire column matching the header \n   \"awayTeamName\".\n * Every row and column has a numerical index. Most of the time, a row's label \n   will be equivalent to the row's index. While it's common practice to define\n   headers for columns, columns have indexes as well, which simply aren't shown.\n   In this regard, Series share an attribute with lists/arrays, in that they are\n   a collection of indexed values\n\nConsider the last two points: we just described a series to work the same way as\na Python dictionary, but also the same way as a Python list. That's right:\nseries' objects are like the biracial offspring of lists and dicts. We can\naccess any column by either its name or its index, and the same goes for rows.\nEven if we rip a column out from a DataFrame, each cell in that series will\nstill retain the row labels for every cell. This means we can say things like \nget me column #3, and then find me the value for whatever was in the row labeled\n\"Y\".  Of course, this works in the reverse as well. It's crazy how things get\nexponentially more powerful and complicated when we add entire dimensions, isn't\nit?\n\nLoading Data Into Pandas\nIf you've made it this far, you've earned the right to start getting hands-on.\nLuckily, Pandas has plenty of methods to load tabular data into DataFrames,\nregardless if you're using static files, SQL, or quirkier methods, Pandas has\nyou covered. Here are some of my favorite examples:\n\nimport pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n\n\nAll of these achieve the same result of creating a DataFrame. No matter what\nhorrible data sources you may have been forced to inherit, Pandas is here to\nhelp. Pandas knows our pain. Pandas is love. Pandas is life.\n\nWith data loaded, let's see how we can apply our new knowledge of series'  to\ninteract with out data.\n\nFinding Data in Our Dataframe\nPandas has a method for finding a series by label, as well as a separate method\nfor finding a series by index. These methods are .iloc  and .loc, respectively.\nLet's say our DataFrame from the example above is stored as a variable named \nbaseball_df. To get the values of a column by name, we would do the following:\n\nbaseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n\n\nThis would return the following:\n\n0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n\n\nThat's our column! We can see the row labels being listed alongside each row's\nvalue. Told ya so. Getting a column will also return the column's dtype, or data\ntype. Data types can be set on columns explicitly. If they aren't, Pandas will\ngenerally either default to detecting that the data in the column is a float \n(returned for any column which only holds numerical values, despite number of\ndecimal points) or an 'object', which is a fancy catch-all meaning \"fuck if I\nknow, there's letters and shit in there, it could be anything probably.\" Pandas\ndoesn't try hard on its own to discern the types of data in each field.\n\nIf you're thinking ahead, you might see a looming conflict of interest with iloc\n. Since we've established that columns and rows are the same, and we're\naccessing series' based on criteria that is met by both  columns and rows (every\ntable has a first row and a first column), how does Pandas know what we want\nwith .loc()? Short answer: It doesn't, so it just returns both! \n\nbaseball_df = baseball_df.loc[3]\nprint(baseball_df)\n\n\n    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n\n\nAhhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a\nclever, intentional way. \"Clever and intentional\"  is actually a great way to\ndescribe Pandas as a library. This combination of ease and power is what makes\nPandas so magnetic to curious newcomers. \n\nWant another example? How about leveraging the unique attributes of series'  to\nsplice DataFrames as though they were arrays?\n\nsliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n\n\n    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n\n\n...Did we just do that? We totally did. We were able to slice a two-dimensional\nset of data by using the same syntax that we'd used to slice arrays, thanks to\nthe power of the series object.\n\nWelcome to the Club\nThere are a lot more entertaining, mind-blowing ways to introduce people to\nPandas. If our goal had been sheer amusement, we would have leveraged the\ncookie-cutter route to Pandas tutorials: overloading readers with Pandas\n\"tricks\" displaying immense power in minimal effort. Unfortunately, we took the\napplicable approach to actually retaining information. Surely this model of\n\"informational and time consuming\" will beat out \"useless but instantly\ngratifying,\" right? RIGHT? \n\nWhatever. I’ll schedule the Pandas and Pandas rebrand for next week. From now on\nwhen people want that quick fix, you can call me Pablo Escobar. Join us next\ntime when we use Pandas data analysis to determine which private Caribbean\nisland offers the best return on investment with all the filthy money we’ll\nmake.\n\nHint: it’s definitely not the Fyre festival one.","html":"<p>Let’s face it: the last thing the world needs is another “<strong>Intro to Pandas</strong>” post. Anybody strange enough to read this blog surely had the same reaction to discovering Pandas as I did: a manic euphoria that can only be described as love at first sight. We wanted to tell the world, and that we did. A lot. Yet here I am, about to helplessly sing cliche praises one more time. </p><p>I’m a prisoner of circumstance here. As it turns out, the vast (and I mean <em><strong>vast</strong></em>) majority of our fans have a raging Pandas addiction. They come to our humble mom-and-pop shop here at <strong>Hackers and Slackers </strong>foaming at the mouth, going on raging benders for all Pandas-related content. If I had half a brain, I’d rename this site <strong>Pandas and Pandas</strong> and delete all non-Pandas-related content. Talk about cash money. </p><p>As a middle-ground, I’ve decided to do a bit of housekeeping. My previous “Intro to Pandas” post was an unflattering belligerent mess jotted into a Confluence instance long ago during a <a>Friday night</a> pregame. That mess snuck its way on to this blog, and has gone unnoticed for a year now. I've decided that this probably wasn't the best way to open up a series about the most influential Python library of all time. We're going to try this again... For the Pandas.</p><h2 id=\"intro-to-pandas-rereleased-in-imax-8k-4d\">Intro to Pandas Rereleased: in IMAX 8k 4D </h2><p>Pandas is used to analyze and modify tabular data in Python. When we say “tabular data,” we mean any instance in life where data is represented in a table format. Excel, SQL databases, shitty HTML tables.... they’ve all been the same thing with different syntax this whole time. Pandas can achieve anything that any other table can. </p><p>If you’re reasonably green to data analysis and are experiencing the <em>“oh-my-God-all-data-professions-are-kinda-just-Excel”</em> realization as we speak, feel free to take a moment. Great, that’s behind us now.</p><h2 id=\"the-anatomy-of-a-dataframe\">The Anatomy of a DataFrame</h2><p>Tabular data in Pandas is referred to as a “DataFrame.” We can’t call <em>everything</em> “tables-” otherwise, our choice of vague terminology would grow horribly confusing when we refer to data in different systems. Between you and me though, DataFrames are basically tables.</p><p>So how do we represent two-dimensional data via command line: a concept which inherently interprets and displays information one-dimensionally? </p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/moon.jpg\" class=\"kg-image\"><figcaption>\"Oh nothing, we think it's cute.\"</figcaption></figure><!--kg-card-end: image--><p>DataFrames consist of individual parts which are easy-to-understand at face value. It’s the complexity of these things together, creating a sum greater than the whole of its parts, which fuels the seemingly endless power of DataFrames. If we want <em>any</em> hope of contributing to the field of Data Science, we need to not only understand the terminology but <em>at least </em>be aware of core concepts of what a DataFrame is beneath the hood. This understanding is what separates engineers from Excel monkeys. </p><!--kg-card-begin: html--><div class=\"scoreboard\">\n\t<!-- Score Header -->\n\t<div class=\"score-header\">\n\t\t<!-- Background -->\n\t\t<div class=\"score-header-background\">\n\t\t\t<div class=\"score-header-background__left\"></div>\n\t\t\t<div class=\"score-header-background__right\"></div>\n\t\t\t<div class=\"score-header-background__logo\"></div>\n\t\t</div>\n\t\t<!-- Foreground -->\n\t\t<div class=\"score-header-foreground\">\n\t\t\t<div class=\"score-header-foreground__left\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Engineers</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">1</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Win</span>\n\t\t\t</div>\n\t\t\t<div class=\"score-header-foreground__right\">\n\t\t\t\t<h1 class=\"score-header-foreground__title\">Excel Nerds</h1>\n\t\t\t\t<h2 class=\"score-header-foreground__score\">0</h2>\n\t\t\t\t<span class=\"score-header-foreground__win\">Lose</span>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</div><!--kg-card-end: html--><h2 id=\"parts-of-a-dataframe\">Parts of a DataFrame</h2><p>Were you expecting this post just to be a bunch of one-liners in Pandas? Good, I hope you're disappointed. Strap yourself in, we might actually learn something today. Class is now in session, baby. Let's break apart what makes a DataFrame, piece-by-piece:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/03/dataframe2.jpg\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>The most basic description of any table would be a collection of <strong>columns </strong>and <strong>rows. </strong>Despite looking at a two-dimensional grid, columns are in fact very different from rows. Unlike rows, all data in a <em>column</em> typically abides by the same data type. In the above example, any value saved in the <strong>startTime</strong> column will always be a <strong>time</strong>. Rows, on the other hand, are simply an entry- an instance of a \"thing\", where each thing is described by attributes stored horizontally across columns.</p><p>This seems like really elementary stuff, but I mention it for a reason. By our definition, <em>columns</em> are more pivotal to structuring a table than rows, because even empty columns have meaning, whereas an empty row with no columns will always equal infinite nothingness. <strong>Rows are made up of values in columns, not the other way around. </strong> Thus, columns in Pandas are actually their own type of object called a <strong>Series</strong>. </p><ul><li><strong>Series</strong>' are objects native to Pandas (and Numpy) which refer to one-dimensional sequences of data. Another example of a one-dimensional sequence of data could be an <strong><em>array</em></strong><em>, </em>but series' are much more than arrays: they're a class of their own for many powerful reasons, which we'll see in a moment.</li><li><strong>Axis</strong> refers to the 'direction' of a series, or in other words \"column\" or \"row\". A series with an axis of <code>0</code> is a <em>row</em>, whereas a series with an axis of <code>1</code> is a <em>column</em>. </li><li>A series contains <strong>labels</strong>, which are given visual names for a row/column specifying labels allows us to call upon any labeled series in the same way we would access a value in a Python dictionary. For instance, accessing <code>dataframe['awayTeamName']</code> returns the entire column matching the header <em>\"awayTeamName\"</em>.</li><li>Every row and column has a numerical <strong>index. </strong>Most of the time, a row's <strong>label</strong> will be equivalent to the row's <strong>index. </strong>While it's common practice to define headers for columns, columns have indexes as well, which simply aren't shown. In this regard, Series share an attribute with lists/arrays, in that they are a collection of indexed values</li></ul><p>Consider the last two points: we just described a series to work the same way as a Python dictionary, but also the same way as a Python list. That's right: series' objects are like the biracial offspring of lists and dicts. We can access any column by either its name or its index, and the same goes for rows. Even if we rip a column out from a DataFrame, each cell in that series will still retain the row labels for every cell. This means we can say things like <strong>get me column #3, and then find me the value for whatever was in the row labeled \"Y\".</strong> Of course, this works in the reverse as well. It's crazy how things get exponentially more powerful and complicated when we add entire dimensions, isn't it?</p><h2 id=\"loading-data-into-pandas\">Loading Data Into Pandas</h2><p>If you've made it this far, you've earned the right to start getting hands-on. Luckily, Pandas has plenty of methods to load tabular data into DataFrames, regardless if you're using static files, SQL, or quirkier methods, Pandas has you covered. Here are some of my favorite examples:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\n\n# Reads a local CSV file.\ncsv_df = pd.read_csv('data.csv')\n\n# Similar to above\nexcel_df = pd.read_excel('data.xlsx')\n\n# Creating tabular data from non-tabular JSON\njson_df = pd.read_json('data.json')\n\n# Direct db access utilizing SQLAlchemy\nread_sql = read_sql('SELECT * FROM blah', conn=sqlalchemy_engine)\n\n# My personal ridiculous favorite: HTML table to DataFrame.\nread_html = read_html('examplePageWithTable.html')\n\n# The strength of Google BigQuery: already officially supported by Pandas\nread_gbq = read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n</code></pre>\n<!--kg-card-end: markdown--><p>All of these achieve the same result of creating a DataFrame. No matter what horrible data sources you may have been forced to inherit, Pandas is here to help. Pandas knows our pain. Pandas is love. Pandas is life.</p><p>With data loaded, let's see how we can apply our new knowledge of <strong>series'</strong> to interact with out data.</p><h2 id=\"finding-data-in-our-dataframe\">Finding Data in Our Dataframe</h2><p>Pandas has a method for finding a series by label, as well as a separate method for finding a series by index. These methods are <code>.iloc</code> and <code>.loc</code>, respectively. Let's say our DataFrame from the example above is stored as a variable named <code>baseball_df</code>. To get the values of a column by name, we would do the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.iloc['homeTeamName']\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><p>This would return the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">0   Cubs\n1   Indians\n2   Padres\n3   Diamondbacks\n4   Giants\n5   Blue Jays\n6   Reds\n7   Cubs\n8   Rockies\n9   Yankees\nName: homeTeamName, dtype: object\n</code></pre>\n<!--kg-card-end: markdown--><p>That's our column! We can see the row labels being listed alongside each row's value. Told ya so. Getting a column will also return the column's <strong>dtype</strong>, or <em>data type</em>. Data types can be set on columns explicitly. If they aren't, Pandas will generally either default to detecting that the data in the column is a <strong>float</strong> (returned for any column which only holds numerical values, despite number of decimal points) or an '<strong>object'</strong>, which is a fancy catch-all meaning \"fuck if I know, there's letters and shit in there, it could be anything probably.\" Pandas doesn't try hard on its own to discern the types of data in each field.</p><p>If you're thinking ahead, you might see a looming conflict of interest with <code>iloc</code>. Since we've established that columns and rows are the same, and we're accessing series' based on criteria that is met by <em>both</em> columns and rows (every table has a first row and a first column), how does Pandas know what we want with <code>.loc()</code>? Short answer: It doesn't, so it just returns both! </p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">baseball_df = baseball_df.loc[3]\nprint(baseball_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName   startTime      duration_minutes\n0   Cubs            Reds           18:20:00 UTC   188\n1   Indians         Astros         18:20:00 UTC   194\n2   Padres          Giants         18:20:00 UTC   185\n3   Diamondbacks    Brewers        18:20:00 UTC   211\n</code></pre>\n<!--kg-card-end: markdown--><p>Ahhh, a 4x4 grid! This does, in fact, satisfy what we asked for- albiet in a clever, intentional way. \"<strong>Clever and intentional\"</strong> is actually a great way to describe Pandas as a library. This combination of ease and power is what makes Pandas so magnetic to curious newcomers. </p><p>Want another example? How about leveraging the unique attributes of <strong>series'</strong> to splice DataFrames as though they were arrays?</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">sliced_df = df.loc['homeTeamName':'awayTeamName']\nprint(sliced_df)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">    homeTeamName    awayTeamName\n0   Cubs            Reds        \n1   Indians         Astros      \n2   Padres          Giants      \n3   Diamondbacks    Brewers     \n</code></pre>\n<!--kg-card-end: markdown--><p>...Did we just do that? We totally did. We were able to slice a two-dimensional set of data by using the same syntax that we'd used to slice arrays, thanks to the power of the series object.</p><h2 id=\"welcome-to-the-club\">Welcome to the Club</h2><p>There are a lot more entertaining, mind-blowing ways to introduce people to Pandas. If our goal had been sheer amusement, we would have leveraged the cookie-cutter route to Pandas tutorials: overloading readers with Pandas \"tricks\" displaying immense power in minimal effort. Unfortunately, we took the applicable approach to actually retaining information. Surely this model of \"informational and time consuming\" will beat out \"useless but instantly gratifying,\" right? <em>RIGHT? </em></p><p>Whatever. I’ll schedule the <strong>Pandas and Pandas </strong>rebrand for next week. From now on when people want that quick fix, you can call me Pablo Escobar. Join us next time when we use Pandas data analysis to determine which private Caribbean island offers the best return on investment with all the filthy money we’ll make.</p><p>Hint: it’s definitely not the Fyre festival one.</p>","url":"https://hackersandslackers.com/intro-to-data-analysis-in-python-using-pandas/","uuid":"828b4a6f-e6f2-446f-b51e-64fe19e05ba0","page":false,"codeinjection_foot":null,"codeinjection_head":"<style>\n  *,\n  *::before,\n  *::after {\n    box-sizing: inherit;\n  }\n\n  .scoreboard {\n    margin: auto;\n    max-width: 980px;\n    box-shadow: 0 0 10px #b0bddd;\n  }\n\n  .score-header {\n    position: relative;\n    height: 74px;\n    overflow: hidden;\n  }\n\n  .score-header-background {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: calc(100% + 63px + 4px);\n    left: -33.5px;\n  }\n\n  .score-header-background .score-header-background__left,\n  .score-header-background .score-header-background__right {\n    position: relative;\n    flex: 1 1 100%;\n    overflow: hidden;\n    border-bottom: 4px solid #fff;\n    transform: skewX(-40deg);\n  }\n\n  .score-header-background .score-header-background__left::before,\n  .score-header-background .score-header-background__right::before {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    transform: skewX(40deg);\n  }\n\n  .score-header-background .score-header-background__left::after,\n  .score-header-background .score-header-background__right::after {\n    content: \"\";\n    position: absolute;\n    width: 100%;\n    height: 100%;\n    opacity: 0.35;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeAgMAAABGXkYxAAAACVBMVEUAAAD///8AAABzxoNxAAAAAnRSTlMAAHaTzTgAAAAlSURBVHhe3cmhEQAACMPAjIhhv+pOiahAsAGvchc6asMhjvdrAFlGOgM9VYUmAAAAAElFTkSuQmCC');\n  }\n\n  .score-header-background .score-header-background__left {\n    margin-right: 5px;\n    border-color: #19d9ff;\n  }\n\n  .score-header-background .score-header-background__left::before {\n    right: -31.5px;\n    background: linear-gradient(to left, #19d9ff 31.5px, #a9f6ff 60%);\n  }\n\n  .score-header-background .score-header-background__right {\n    margin-left: 5px;\n    border-color: #ff1979;\n  }\n\n  .score-header-background .score-header-background__right::before {\n    left: -31.5px;\n    background: linear-gradient(to right, #ff1979 31.5px, #ffb5ee 60%);\n  }\n\n  .score-header-background .score-header-background__logo {\n    position: absolute;\n    top: 0;\n    right: 0;\n    bottom: 0;\n    left: 0;\n    margin: auto;\n    width: 60px;\n    height: 60px;\n    border: 5px solid #ffffff;\n    border-radius: 100%;\n    background-color: rgba(111, 77, 238, 0.51);\n    background-image: url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiP…A3NDcsNTQ2LjMgDQoJCQkJCQkJIi8+DQoJCTwvZz4NCgk8L2c+DQo8L2c+DQo8L3N2Zz4NCg==);\n  }\n\n  .score-header-foreground {\n    position: absolute;\n    display: flex;\n    height: 100%;\n    width: 100%;\n  }\n\n  .score-header-foreground .score-header-foreground__left,\n  .score-header-foreground .score-header-foreground__right {\n    display: flex;\n    margin: 0 20px;\n    flex: 1 1 100%;\n    align-items: baseline;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__left .score-header-foreground__score {\n    margin-right: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__right {\n    flex-direction: row-reverse;\n    text-align: right;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__title {\n    color: #fff;\n  }\n\n  .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n    margin-left: 20px;\n  }\n\n  .score-header-foreground .score-header-foreground__title,\n  .score-header-foreground .score-header-foreground__score,\n  .score-header-foreground .score-header-foreground__win {\n    margin: 0 10px;\n    line-height: 69px;\n    text-transform: uppercase;\n  }\n\n  .score-header-foreground .score-header-foreground__title {\n    margin: 0;\n    flex: 1 1 auto;\n    font-size: 30px;\n  }\n\n  .score-header-foreground .score-header-foreground__score {\n    order: 1;\n    text-shadow: 0 0 4px rgba(255, 255, 255, 0.75), 0 0 8px rgba(255, 255, 255, 0.45);\n    font-size: 30px;\n    color: white;\n  }\n\n  .score-header-foreground .score-header-foreground__win {\n    font-size: 18px;\n    font-weight: 600;\n    font-family: 'TTNorms-Medium', sans-serif;\n    color: white;\n    color: #4a47a2;\n    mix-blend-mode: color-burn;\n  }\n\n  .player-list {\n    display: flex;\n    padding: 0;\n    flex-flow: column;\n    list-style: none;\n  }\n\n  .player-row {\n    display: flex;\n    margin: 20px 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player {\n    display: flex;\n    padding: 20px;\n    min-width: 0;\n    flex: 1 1 auto;\n    align-items: center;\n  }\n\n  .player.player--left {\n    padding-left: 20px;\n  }\n\n  .player.player--left .player__avatar {\n    margin-right: 20px;\n    border-color: #19d9ff;\n  }\n\n  .player.player--right {\n    padding-right: 20px;\n    text-align: right;\n    flex-flow: row-reverse;\n  }\n\n  .player.player--right .player__avatar {\n    margin-left: 20px;\n    border-color: #ff1979;\n  }\n\n  .player .player__avatar {\n    position: relative;\n    min-height: 60px;\n    min-width: 60px;\n    overflow: hidden;\n    border: 3px solid #fff;\n    border-radius: 100%;\n    background-color: #222;\n  }\n\n  .player .player__avatar::before {\n    content: \"\";\n    position: absolute;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    opacity: 0.1;\n    background: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADwAAAA8CAMAAAANIilAAAABTVBMVEU7PUP////4+Pj7+/v29vZAQkj6+vpCRErx8fL+/v5BQ0lNT1T9/f3t7u5HSU/8/Pz09PRGSE5aW2BOUFY8PkRJSlB/gITk5eWqq64+QEWPkJSVlppoam5KTFHZ2tvDw8XBwsPa29zP0NGdnqH6+/vExMVVV1z+/v/LzMxRUlidn6HKy8zFxce3uLuVlpmBgYZSVFnz8/SvsLOUlZhkZmv5+flRU1jg4OFDRUrExMepqqyWl5uAgYXMzc7r6+xZWl/AwMKenqFjZWnn6OjBwcJ5en7U1dZ+gIPNzs8/QUbi4+Smp6pSU1lUVVuLjI/j5OWwsLNpam9BQ0hLTFK7vL7p6ep0dXmIio3LzM339/fi4+OrrK5cXmI9P0Wlpqny8/PLy8xzdHh6e3/ExMZVVlzs7e2hoqU+QEbR0tO4ubyHiIzs7O1FR01bXWLz8/PjR6/UAAABIklEQVR4Xu3UxW7FMBCGUU+Sy8xFZmZmZmZmhvdfVuqi9rTprf8uIlXKtz+yNGON+Ke5uQ30NQb/JoMlozGimtopD259DWH6yCxfRG1pGX1WFQVxNSkVYbbOUHF8G8JNUuJPN3NcAeEcx5UQtjgeQ6yHeBaCM8TzOoZP6zmOC6Rzjqch3M/xHIRbOG6FcBvH7RDuMFXb6RNQXSruFlg9MWl7X0EcfZM4Al+xQYmHBNqwxCMwLpY46ygen5B4ElpzOjFDapHZQm06v+AlXmBpeUWHrq6tk13GxubWLzS0Y9JPBXb38sjE/sEh5e3o+MSeJs/8pNHFpc1nvTJIr/D1t9Hf3JJ2d4J3/0D6WSGOkwYBPXL8REjPLwwXQNifcgK72MXv0xEfs26TMDAAAAAASUVORK5CYII=');\n    background-size: cover;\n  }\n\n  .player .player__username {\n    font-size: 24px;\n    text-overflow: ellipsis;\n    overflow: hidden;\n    white-space: nowrap;\n  }\n\n  .language {\n    font-size: 24px;\n  }\n\n  .language.language--html {\n    color: #f69c24;\n  }\n\n  .language.language--css {\n    color: #299bf7;\n  }\n\n  .language.language--js {\n    color: #ffce22;\n  }\n\n  @media (max-width:600px) {\n    .score-header-foreground__title {\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n    }\n\n    .score-header-foreground__left,\n    .score-header-foreground__right {\n      margin: 10px 11px 0;\n      display: block;\n      position: relative;\n    }\n\n    .score-header-foreground .score-header-foreground__right .score-header-foreground__score {\n          right: 10px;\n    }\n\n    .score-header-foreground .score-header-foreground__left, .score-header-foreground .score-header-foreground__right {\n          margin: 0 10px;\n          display: block;\n    }\n\n    .score-header-foreground__left .score-header-foreground__title {\n      left:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground__right .score-header-foreground__title {\n      right:0;\n      margin: 0;\n      font-size: 16px;\n      height: fit-content;\n      width: fit-content;\n      /* display: block; */\n      text-shadow: #12183d 1px 1px 1px;\n      position: absolute;\n      bottom: 9px;\n          line-height: 1;\n    }\n\n    .score-header-foreground .score-header-foreground__win {\n      font-size: 12px;\n      font-weight: 600;\n      font-family: 'TTNorms-Medium', sans-serif;\n      color: white;\n      color: #4a47a2;\n      mix-blend-mode: multiply;\n      position: absolute;\n      width: fit-content;\n      bottom: 32px;\n    }\n\n    .score-header-foreground__score {\n      padding: 0;\n      display: block;\n      position: absolute;\n      top: 2px;\n      padding: 0 !important;\n      margin: 0 !important;\n      line-height: 1 !important;\n      height: fit-content;\n          top: 14px;\n    }\n\n    .score-header-foreground__win {\n      position: absolute;\n      bottom: 35px;\n      width: fit-content;\n    }\n\n    .score-header-foreground__right .score-header-foreground__win {\n          right: 36px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground__left .score-header-foreground__win {\n      left: 19px;\n      bottom: 32px;\n      height: fit-content;\n      line-height: 1;\n      margin: 0;\n    }\n\n    .score-header-foreground .score-header-foreground__title {\n      right: 10px;\n    }\n  }\n</style>","comment_id":"5ad7ee6365cd784d6288cb03"}}]}},"pageContext":{"slug":"data-analysis-pandas","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}