{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673734","title":"Scraping Data on the Web with BeautifulSoup","slug":"scraping-urls-with-beautifulsoup","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","excerpt":"The honest act of systematically stealing data without permission.","custom_excerpt":"The honest act of systematically stealing data without permission.","created_at_pretty":"11 November, 2018","published_at_pretty":"11 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-11T04:53:44.000-05:00","published_at":"2018-11-11T08:35:09.000-05:00","updated_at":"2019-01-05T13:21:06.000-05:00","meta_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","meta_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","og_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","twitter_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","twitter_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"There are plenty of reliable and open sources of data on the web. Datasets are\nfreely released to the public domain by the likes of Kaggle, Google Cloud, and\nof course local & federal government. Like most things free and open, however,\nfollowing the rules to obtain public data can be a bit... boring. I'm not\nsuggesting we go and blatantly break some grey-area laws by stealing data, but\nthis blog isn't exactly called People Who Play It Safe And Slackers, either. \n\nMy personal Python roots can actually be traced back to an ambitious\nside-project: to aggregate all new music from across the web and deliver it the\nmasses. While that project may have been abandoned (after realizing it already\nexisted), BeautifulSoup  was more-or-less my first ever experience with Python. \n\nThe Tool(s) for the Job(s)\nBefore going any further, we'd be ill-advised to not at least mention Python's\nother web-scraping behemoth, Scrapy [https://scrapy.org/]. BeautifulSoup  and \nScrapy  have two very different agendas. BeautifulSoup is intended to parse or\nextract data one page at a time, with each page being served up via the requests \n library or equivalent. Scrapy,  on the other hand, is for creating crawlers: or\nrather absolute monstrosities unleashed upon the web like a swarm, loosely\nfollowing links and haste-fully grabbing data where data exists to be grabbed.\nTo put this in perspective, Google Cloud functions will not even let you import\nScrapy as a usable library.\n\nThis isn't to say that BeautifulSoup  can't be made into a similar monstrosity\nof its own. For now, we'll focus on a modest task: generating link previews for\nURLs by grabbing their metadata.\n\nStep 1: Stalk Your Prey\nBefore we steal any data, we should take a look at the data we're hoping to\nsteal.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape URLs to generate previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n\n\nThe above is the minimum needed to retrieve the DOM structure of an HTML page. \nBeautifulSoup  accepts the .content  output from a request, from which we can\ninvestigate the contents.\n\nUsing BeauitfulSoup will often result in different results for your scaper than\nyou might see as a human, such as 403 errors or blocked content. An easy way\naround this faking your headers into looking like normal browser agents, as we\ndo here: \nheaders.update({\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101\nFirefox/52.0',\n})`The result of print(soup.prettify())  will predictably output a \"pretty\" printed\nversion of your target DOM structure:\n\n<html class=\"gr__example_com\"><head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta property=\"og:site_name\" content=\"Example dot com\">\n    <meta property=\"og:type\" content=\"website\">\n    <meta property=\"og:title\" content=\"Example\">\n    <meta property=\"og:description\" content=\"An Example website.\">\n    <meta property=\"og:image\" content=\"http://example.com/img/image.jpg\">\n    <meta name=\"twitter:title\" content=\"Hackers and Slackers\">\n    <meta name=\"twitter:description\" content=\"An Example website.\">\n    <meta name=\"twitter:url\" content=\"http://example.com/\">\n    <meta name=\"twitter:image\" content=\"http://example.com/img/image.jpg\">\n</head>\n\n<body data-gr-c-s-loaded=\"true\">\n  <div>\n    <h1>Example Domain</h1>\n      <p>This domain is established to be used for illustrative examples in documents.</p>\n      <p>You may use this domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n  </div>\n</body>\n    \n</html>\n\n\nStep 2: The Extraction\nAfter turning our request content into a BeautifulSoup object, we access items\nin the DOM via dot notation as such:\n\ntitle = soup.title.string\n\n\n.string  gives us the actual content of the tag which is Example Domain, whereas\n soup.title  would return the entirety of the tag as <title>Example\nDomain</title>. \n\nDot notation is fine when pages have predictable hierarchies or structures, but\nbecomes much less useful for extracting patterns we see in the document. soup.a \nwill only return the first instance of a link, and probably isn't what we want.\n\nIf we wanted to extract all  <a>  tags of a page's content while avoiding the\nnoise of nav links etc, we can use CSS selectors to return a list of all\nelements matching the selection. soup.select('body p > a')  retrieves all links\nembedded in paragraph text, limited to the body of the page. \n\nSome other methods of grabbing elements:\n\n * soup.find(id=\"example\"): Useful for when a single element is expected.\n * soup.find_all('a'):  Returns a list of all elements matching the selection\n   after searching the document recursively.\n * .parent and .child: Relative selectors to a currently engaged element.\n\nGet Some Attributes\nChances are we'll almost always want the contents or the attributes of a tag, as\nopposed to the entire <a>  tag's HTML. A common example of going after a tag's\nattributes would be in the cases of img  and a  tags. Chances are we're most\ninterested in the src  and href  attributes of such tags, respectively. \n\nThe .get  method refers specifically to getting the value of attributes on a\ntag. For example, soup.find('.logo').get('href')  would find an element with the\nclass \"logo\", and return the url to that image.\n\nPesky Tags to Deal With\nIn our example of creating link previews, a good first source of information\nwould obviously be the page's meta tags: specifically the og  tags they've\nspecified to openly provide the bite-sized information we're looking for.\nGrabbing these tags are a bit more difficult to deal with:\n\nsoup.find(\"meta\", property=\"og:description\").get('content')\n\n\nOh yeah, now that's some ugly shit right there. Meta tags are especially\ninteresting because they're all uselessly dubbed 'meta', thus we need a second\ndifferentiator in addition to the tag name to specify which meta tag we care\nabout. Only then can we bother to get  the actual content of said tag.\n\nStep 3: Realizing Something Will Always Break\nIf we were to try the above selector on an HTML page which did not contain an \nog:description, our script would break unforgivingly. Not only do we miss this\ndata, but we miss out on everything entirely - this means we always need to\nbuild in a plan B, and at the very least deal with a lack of tag altogether.\n\nIt's best to break out this logic one tag at a time. First, let's look at an\nexample for a base scraper with all the knowledge we have so far:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p > a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n\n\nGreat - there's a base function for snatching all links out of the body of a\npage. Ultimately we'll create a JSON object for each of these links containing\npreview data, link_preview_dict.\n\nTo handle each value of our dict, we have individual functions:\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get a preview image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\nIn case you're wondering:\n\n * getTitle tries to get the <title>  tag, and falls back to the page's first \n   <h1>  tag (surprisingly enough some pages are in fact missing a title).\n * getDescription  looks for the OG description, and falls back to the content\n   of the page's first paragraph.\n * getImage looks for the OG image, and falls back to the page's first image.\n * getSiteName similarly tries to grab the OG attribute, otherwise it does it's\n   best to extract the domain name from the URL string under the assumption that\n   this is the origin's name (look, it ain't perfect).\n\nWhat Did We Just Build?\nBelieve it or not, the above is considered to be enough logic to be a paid\nservice with a monthly fee. Go ahead and Google it; or better yet, just steal my\nsource code entirely:\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p > a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)","html":"<p>There are plenty of reliable and open sources of data on the web. Datasets are freely released to the public domain by the likes of Kaggle, Google Cloud, and of course local &amp; federal government. Like most things free and open, however, following the rules to obtain public data can be a bit... boring. I'm not suggesting we go and blatantly break some grey-area laws by stealing data, but this blog isn't exactly called <strong>People Who Play It Safe And Slackers</strong>, either. </p><p>My personal Python roots can actually be traced back to an ambitious side-project: to aggregate all new music from across the web and deliver it the masses. While that project may have been abandoned (after realizing it already existed), <strong>BeautifulSoup</strong> was more-or-less my first ever experience with Python. </p><h2 id=\"the-tool-s-for-the-job-s-\">The Tool(s) for the Job(s)</h2><p>Before going any further, we'd be ill-advised to not at least mention Python's other web-scraping behemoth, <strong><a href=\"https://scrapy.org/\">Scrapy</a></strong>. <strong>BeautifulSoup</strong> and <strong>Scrapy</strong> have two very different agendas. BeautifulSoup is intended to parse or extract data one page at a time, with each page being served up via the <strong>requests</strong> library or equivalent. <strong>Scrapy,</strong> on the other hand, is for creating crawlers: or rather absolute monstrosities unleashed upon the web like a swarm, loosely following links and haste-fully grabbing data where data exists to be grabbed. To put this in perspective, Google Cloud functions will not even let you import Scrapy as a usable library.</p><p>This isn't to say that <strong>BeautifulSoup</strong> can't be made into a similar monstrosity of its own. For now, we'll focus on a modest task: generating link previews for URLs by grabbing their metadata.</p><h2 id=\"step-1-stalk-your-prey\">Step 1: Stalk Your Prey</h2><p>Before we steal any data, we should take a look at the data we're hoping to steal.</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape URLs to generate previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n</code></pre>\n<p>The above is the minimum needed to retrieve the DOM structure of an HTML page. <strong>BeautifulSoup</strong> accepts the <code>.content</code> output from a request, from which we can investigate the contents.</p><div class=\"protip\">\n    Using BeauitfulSoup will often result in different results for your scaper than you might see as a human, such as 403 errors or blocked content. An easy way around this faking your headers into looking like normal browser agents, as we do here: <br><code>headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })`</code>\n</div><p>The result of <code>print(soup.prettify())</code> will predictably output a \"pretty\" printed version of your target DOM structure:</p><pre><code class=\"language-html\">&lt;html class=&quot;gr__example_com&quot;&gt;&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n    &lt;meta charset=&quot;utf-8&quot;&gt;\n    &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;\n    &lt;meta property=&quot;og:site_name&quot; content=&quot;Example dot com&quot;&gt;\n    &lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n    &lt;meta property=&quot;og:title&quot; content=&quot;Example&quot;&gt;\n    &lt;meta property=&quot;og:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n    &lt;meta name=&quot;twitter:title&quot; content=&quot;Hackers and Slackers&quot;&gt;\n    &lt;meta name=&quot;twitter:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta name=&quot;twitter:url&quot; content=&quot;http://example.com/&quot;&gt;\n    &lt;meta name=&quot;twitter:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n&lt;/head&gt;\n\n&lt;body data-gr-c-s-loaded=&quot;true&quot;&gt;\n  &lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n      &lt;p&gt;This domain is established to be used for illustrative examples in documents.&lt;/p&gt;\n      &lt;p&gt;You may use this domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&quot;http://www.iana.org/domains/example&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n    \n&lt;/html&gt;\n</code></pre>\n<h2 id=\"step-2-the-extraction\">Step 2: The Extraction</h2><p>After turning our request content into a BeautifulSoup object, we access items in the DOM via dot notation as such:</p><pre><code class=\"language-python\">title = soup.title.string\n</code></pre>\n<p><code>.string</code> gives us the actual content of the tag which is <code>Example Domain</code>, whereas <code>soup.title</code> would return the entirety of the tag as <code>&lt;title&gt;Example Domain&lt;/title&gt;</code>. </p><p>Dot notation is fine when pages have predictable hierarchies or structures, but becomes much less useful for extracting patterns we see in the document. <code>soup.a</code> will only return the first instance of a link, and probably isn't what we want.</p><p>If we wanted to extract <em>all</em> <code>&lt;a&gt;</code> tags of a page's content while avoiding the noise of nav links etc, we can use CSS selectors to return a list of all elements matching the selection. <code>soup.select('body p &gt; a')</code> retrieves all links embedded in paragraph text, limited to the body of the page. </p><p>Some other methods of grabbing elements:</p><ul><li><strong>soup.find(id=\"example\")</strong>: Useful for when a single element is expected.</li><li><strong>soup.find_all('a')</strong>:<strong> </strong>Returns a list of all elements matching the selection after searching the document recursively.</li><li><strong>.parent </strong>and <strong>.child</strong>: Relative selectors to a currently engaged element.</li></ul><h3 id=\"get-some-attributes\">Get Some Attributes</h3><p>Chances are we'll almost always want the contents or the attributes of a tag, as opposed to the entire <code>&lt;a&gt;</code> tag's HTML. A common example of going after a tag's attributes would be in the cases of <code>img</code> and <code>a</code> tags. Chances are we're most interested in the <code>src</code> and <code>href</code> attributes of such tags, respectively. </p><p>The <code>.get</code> method refers specifically to getting the value of attributes on a tag. For example, <code>soup.find('.logo').get('href')</code> would find an element with the class \"logo\", and return the url to that image.</p><h3 id=\"pesky-tags-to-deal-with\">Pesky Tags to Deal With</h3><p>In our example of creating link previews, a good first source of information would obviously be the page's meta tags: specifically the <code>og</code> tags they've specified to openly provide the bite-sized information we're looking for. Grabbing these tags are a bit more difficult to deal with:</p><pre><code class=\"language-python\">soup.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n</code></pre>\n<p>Oh yeah, now that's some ugly shit right there. Meta tags are especially interesting because they're all uselessly dubbed 'meta', thus we need a second differentiator in addition to the tag name to specify <em>which </em>meta tag we care about. Only then can we bother to <em>get</em> the actual content of said tag.</p><h2 id=\"step-3-realizing-something-will-always-break\">Step 3: Realizing Something Will Always Break</h2><p>If we were to try the above selector on an HTML page which did not contain an <code>og:description</code>, our script would break unforgivingly. Not only do we miss this data, but we miss out on everything entirely - this means we always need to build in a plan B, and at the very least deal with a lack of tag altogether.</p><p>It's best to break out this logic one tag at a time. First, let's look at an example for a base scraper with all the knowledge we have so far:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p &gt; a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n</code></pre>\n<p>Great - there's a base function for snatching all links out of the body of a page. Ultimately we'll create a JSON object for each of these links containing preview data, <code>link_preview_dict</code>.</p><p>To handle each value of our dict, we have individual functions:</p><pre><code class=\"language-python\">def getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get a preview image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n</code></pre>\n<p>In case you're wondering:</p><ul><li><strong>getTitle </strong>tries to get the <code>&lt;title&gt;</code> tag, and falls back to the page's first <code>&lt;h1&gt;</code> tag (surprisingly enough some pages are in fact missing a title).</li><li><strong>getDescription</strong> looks for the OG description, and falls back to the content of the page's first paragraph.</li><li><strong>getImage </strong>looks for the OG image, and falls back to the page's first image.</li><li><strong>getSiteName </strong>similarly tries to grab the OG attribute, otherwise it does it's best to extract the domain name from the URL string under the assumption that this is the origin's name (look, it ain't perfect).</li></ul><h2 id=\"what-did-we-just-build\">What Did We Just Build?</h2><p>Believe it or not, the above is considered to be enough logic to be a paid service with a monthly fee. Go ahead and Google it; or better yet, just steal my source code entirely:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p &gt; a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)\n</code></pre>\n","url":"https://hackersandslackers.com/scraping-urls-with-beautifulsoup/","uuid":"c933218e-6bbf-44b7-8f01-bfd188c71d89","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be7fc282ec6e0035b4b16bc"}},"pageContext":{"slug":"scraping-urls-with-beautifulsoup"}}