{"data":{"ghostAuthor":{"slug":"todd","name":"Todd Birchard","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","cover_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/fox_o_o.jpg","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","location":"New York City","website":"https://toddbirchard.com","twitter":"@ToddRBirchard","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5cacc6440a88d417374aaf43","title":"Upgrading to JupyterLab on Ubuntu","slug":"upgrading-to-jupyter-lab-on-ubuntu","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-1.jpg","excerpt":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","custom_excerpt":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","created_at_pretty":"09 April, 2019","published_at_pretty":"14 April, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-04-09T12:20:20.000-04:00","published_at":"2019-04-14T13:25:33.000-04:00","updated_at":"2019-04-14T13:25:33.000-04:00","meta_title":"Upgrading to Jupyter Lab on Ubuntu | Hackers and Slackers","meta_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","og_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","og_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-3.jpg","og_title":"Upgrading to Jupyter Lab on Ubuntu","twitter_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-2.jpg","twitter_title":"Upgrading to Jupyter Lab on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Last we chatted, we walked through the process of getting started with Jupyter\nnotebooks on a Ubuntu server\n[https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/].\nThe classic Jupyter notebook interface is certainly well-suited to the job done.\nThat said, it only takes some time of getting lost in the interface to start\nthinking we can do better. That's where Jupyter Lab comes in.\n\nWhy JupyterLab?\nJupyterLab is sometimes referred to as \"the next generation of Jupyter\nnotebooks,\" which is a statement I can attest to. JupyterLab offers an improved\ninterface for Jupyter notebooks which is both available out of the box, as well\nas highly customizable for your workflow.\n\nOut of the box, the traditional Jupyter interface is extended to include a tree\nfile manager: similar to what you might expect from an IDE. This allows you to\neasily browse all  available notebooks on your server. In addition, the notebook\ninterface has been simplified to reduce noise brought on by (mostly useless)\ntoolbars and excessive buttons. Take a look at the interface prior to any\ncustomization:\n\nEven more appealing than an updated interface is JupyterLab's openness to \ncustomization. JupyterLab has a strongly growing ecosystem for extension\ndevelopment: we'll be getting a taste of some of those goodies in just a moment.\n\nGetting Started\nIf you're starting from scratch, go ahead and follow the same steps in the \nJupyter Notebook setup tutorial\n[https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/] \nup until Conda is set up and activated. \n\nWith a Ubuntu server prepped and ready, install Jupyter Lab with the following\ncommand:\n\n$ conda install -c conda-forge jupyterlab\n\n\nBefore we go any further, make sure you're tunneled into your server - we need\nto do this in order to launch notebooks, remember?:\n\n$ ssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nStart up the Jupyter Lab environment like this:\n\n$ conda activate your_env\n$ jupyter lab\n\n\nIf everything went well, you should be greeted with a fancy Jupyter Lab loading\nscreen and then thrown into the Jupyer Lab environment.\n\nHome sweet home.Things are looking good right off the bat. Without any added\nlibraries, we've already beefed up our Jupyter Notebook workspace. The tree view\nis available, we can launch terminals, and don't forget: we can split screen by\ndragging snapping windows where we see fit. \n\nCustomizing Your Workspace\nJupyterLab uses NodeJS to enable some of its cooler functionality and\nextensions. Â Go ahead and install Node:\n\n$ cd /tmp\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n\n\nNice! Now we have all we need to go nuts with extensions. Here are the ones I\nrecommend:\n\nStatus Bar\n\n--------------------------------------------------------------------------------\n\nKeep an eye on your environment's vitals.Repository found here [https://github.com/jupyterlab/jupyterlab-statusbar].\nInstallation steps:\n\n$ pip install nbresuse\n$ jupyter serverextension enable --py nbresuse\n$ jupyter labextension install @jupyterlab/statusbar\n\n\nTable of Contents\nAuto-generate table of contents to help navigate and organize your notebooks.\nRepository found here [https://github.com/jupyterlab/jupyterlab-toc].\nInstallation steps:\n\n$ jupyter labextension install @jupyterlab/toc\n\n\nVariable Inspector\n\n--------------------------------------------------------------------------------\n\nKeep tabs on every variable used in your notebook and their respective values.\nRepository found here [https://github.com/lckr/jupyterlab-variableInspector].\nInstallation steps:\n\n$ jupyter labextension install @lckr/jupyterlab_variableinspector\n\n\nGit Integration\n\n--------------------------------------------------------------------------------\n\nA visual approach to version control.Repository found here [https://github.com/jupyterlab/jupyterlab-git].\nInstallation steps:\n\n$ jupyter labextension install @jupyterlab/git\n$ pip install jupyterlab-git\n$ jupyter serverextension enable --py jupyterlab_git\n\n\nDraw.io\n\n--------------------------------------------------------------------------------\n\nCreate Draw.io diagrams right from your notebook.Repository found here [https://github.com/QuantStack/jupyterlab-drawio].\nInstallation steps:\n\n$ jupyter labextension install jupyterlab-drawio\n\n\nAdditional Resources\nYou have everything you need to go nuts from here forward. INSTALL ALL THE\nEXTENSIONS!\n\nIf you're looking for more extension goodness, I'd start with the Awesome\nJupyter [https://github.com/markusschanta/awesome-jupyter#jupyterlab-extensions] \n repo on Github- there's a section specifically for JupyterLab.\n\nIf you're totally into JupyterLab now and want to join a gang, the community\npage can be found here [https://github.com/topics/jupyterlab-extension].\n\nLastly, if you've gone totally off the deep end and already want to start\ncreating extensions of your own, check out the extension documentation\n[https://jupyterlab.readthedocs.io/en/stable/user/extensions.html]. That's all\nfolks!","html":"<p>Last we chatted, we walked through the process of getting started with <a href=\"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/\">Jupyter notebooks on a Ubuntu server</a>. The classic Jupyter notebook interface is certainly well-suited to the job done. That said, it only takes some time of getting lost in the interface to start thinking we can do better. That's where Jupyter Lab comes in.</p><h2 id=\"why-jupyterlab\">Why JupyterLab?</h2><p>JupyterLab is sometimes referred to as \"the next generation of Jupyter notebooks,\" which is a statement I can attest to. JupyterLab offers an improved interface for Jupyter notebooks which is both available out of the box, as well as highly customizable for your workflow.</p><p>Out of the box, the traditional Jupyter interface is extended to include a tree file manager: similar to what you might expect from an IDE. This allows you to easily browse <em>all</em> available notebooks on your server. In addition, the notebook interface has been simplified to reduce noise brought on by (mostly useless) toolbars and excessive buttons. Take a look at the interface prior to any customization:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-no-customization.png\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>Even more appealing than an updated interface is JupyterLab's openness to <em>customization</em>. JupyterLab has a strongly growing ecosystem for extension development: we'll be getting a taste of some of those goodies in just a moment.</p><h2 id=\"getting-started\">Getting Started</h2><p>If you're starting from scratch, go ahead and follow the same steps in the <a href=\"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/\">Jupyter Notebook setup tutorial</a> up until Conda is set up and activated. </p><p>With a Ubuntu server prepped and ready, install Jupyter Lab with the following command:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda install -c conda-forge jupyterlab\n</code></pre>\n<!--kg-card-end: markdown--><p>Before we go any further, make sure you're tunneled into your server - we need to do this in order to launch notebooks, remember?:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Start up the Jupyter Lab environment like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda activate your_env\n$ jupyter lab\n</code></pre>\n<!--kg-card-end: markdown--><p>If everything went well, you should be greeted with a fancy Jupyter Lab loading screen and then thrown into the Jupyer Lab environment.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyter-lab-home.png\" class=\"kg-image\"><figcaption>Home sweet home.</figcaption></figure><!--kg-card-end: image--><p>Things are looking good right off the bat. Without any added libraries, we've already beefed up our Jupyter Notebook workspace. The tree view is available, we can launch terminals, and don't forget: we can split screen by dragging snapping windows where we see fit. </p><h2 id=\"customizing-your-workspace\">Customizing Your Workspace</h2><p>JupyterLab uses NodeJS to enable some of its cooler functionality and extensions. Â Go ahead and install Node:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd /tmp\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice! Now we have all we need to go nuts with extensions. Here are the ones I recommend:</p><h3 id=\"status-bar\">Status Bar</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-statusbar.gif\" class=\"kg-image\"><figcaption>Keep an eye on your environment's vitals.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-statusbar\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ pip install nbresuse\n$ jupyter serverextension enable --py nbresuse\n$ jupyter labextension install @jupyterlab/statusbar\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"table-of-contents\">Table of Contents</h3><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-toc.gif\" class=\"kg-image\"><figcaption>Auto-generate table of contents to help navigate and organize your notebooks.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-toc\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @jupyterlab/toc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"variable-inspector\">Variable Inspector</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-vars.gif\" class=\"kg-image\"><figcaption>Keep tabs on every variable used in your notebook and their respective values.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/lckr/jupyterlab-variableInspector\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @lckr/jupyterlab_variableinspector\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"git-integration\">Git Integration</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyter-git.gif\" class=\"kg-image\"><figcaption>A visual approach to version control.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-git\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @jupyterlab/git\n$ pip install jupyterlab-git\n$ jupyter serverextension enable --py jupyterlab_git\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"draw-io\">Draw.io</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/drawio.gif\" class=\"kg-image\"><figcaption>Create Draw.io diagrams right from your notebook.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/QuantStack/jupyterlab-drawio\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install jupyterlab-drawio\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"additional-resources\">Additional Resources</h2><p>You have everything you need to go nuts from here forward. INSTALL ALL THE EXTENSIONS!</p><p>If you're looking for more extension goodness, I'd start with the <a href=\"https://github.com/markusschanta/awesome-jupyter#jupyterlab-extensions\">Awesome Jupyter</a> repo on Github- there's a section specifically for JupyterLab.</p><p>If you're totally into JupyterLab now and want to join a gang, the community page can be found <a href=\"https://github.com/topics/jupyterlab-extension\">here</a>.</p><p>Lastly, if you've gone totally off the deep end and already want to start creating extensions of your own, check out the <a href=\"https://jupyterlab.readthedocs.io/en/stable/user/extensions.html\">extension documentation</a>. That's all folks!</p><p></p>","url":"https://hackersandslackers.com/upgrading-to-jupyter-lab-on-ubuntu/","uuid":"58669f89-0b58-4689-bc6d-03f93d843919","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5cacc6440a88d417374aaf43"}},{"node":{"id":"Ghost__Post__5c779ffbc380a221de39c7cf","title":"Using Flask-Login to Handle User Accounts","slug":"authenticating-users-with-flask-login","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin.jpg","excerpt":"Add user authentication to your Flask app with Flask-Login","custom_excerpt":"Add user authentication to your Flask app with Flask-Login","created_at_pretty":"28 February, 2019","published_at_pretty":"04 April, 2019","updated_at_pretty":"11 April, 2019","created_at":"2019-02-28T03:46:51.000-05:00","published_at":"2019-04-04T18:36:51.000-04:00","updated_at":"2019-04-10T22:01:31.000-04:00","meta_title":"Authenticating Users With Flask-Login | Hackers and Slackers","meta_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","og_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","og_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin-2.jpg","og_title":"Authenticating Users With Flask-Login","twitter_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin-1.jpg","twitter_title":"Authenticating Users With Flask-Login","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Pythonâs fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Pythonâs fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"Weâve covered a lot of Flask goodness in this series thus far. We fully\nunderstand how to structure a sensible application; we can serve up complex\npage\ntemplates\n[https://hackersandslackers.com/powerful-page-templates-in-flask-with-jinja/],\nand have dived into interacting with databases using Flask-SQLAlchemy\n[https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/].\nFor our next challenge, weâre going to need all  of the knowledge we've acquired\nthus far and much, much more. Welcome to the Super Bowl of Flask development.\nThis. Is. Flask-Login.\n\nFlask-Login [https://flask-login.readthedocs.io/en/latest/]  is a dope library\nwhich handles all aspects of user management, including vital nuances you might\nnot expect. Some noteworthy features include securing parts of our app behind\nlogin walls, encrypting passwords, and handling sessions. Moreover, It plays\nnicely with other Flask libraries weâre already familiar with: Flask-SQLAlchemy\n[https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/] \nto create and fetch accounts, and Flask-WTForms\n[https://hackersandslackers.com/guide-to-building-forms-in-flask/]  for handling\nintelligent sign-up & log-in forms. This tutorial assumes you have some working\nknowledge of these things.\n\nFlask-Login is shockingly quite easy to use after the initial learning curve...\nbut therein lies the catch. Perhaps Iâm not the only one to have noticed, but\nmost Flask-related documentation tends to be, well, God-awful. The community is\nriddled with helplessly outdated information; if you ever come across flask.ext \nin a tutorial, it is inherently worthless to anybody developing in 2019. To make\nmatters worse, official Flask-Login documentation contains some artifacts which\nare just plain wrong. The documentation contradicts itself (Iâll show you what I\nmean), and offers little to no code examples to speak of. My only hope is that I\nmight save somebody the endless headaches Iâve experienced myself.\n\nStructuring Our Application\nLetâs start with installing dependencies. This should give you an idea of what\nyouâre in for:\n\n$ pip3 install flask flask-login flask-sqlalchemy psycopg2-binary python-dotenv\n\n\nSweet. Letâs take this one step at a time, starting with our project file\nstructure:\n\nflasklogin-tutorial\nâââ /login_tutorial\nâ   âââ __init__.py\nâ   âââ auth.py\nâ   âââ forms.py\nâ   âââ models.py\nâ   âââ routes.py\nâ   âââ /static\nâ   â   âââ /dist\nâ   â   â   âââ /css\nâ   â   â   â   âââ account.css\nâ   â   â   â   âââ dashboard.css\nâ   â   â   âââ /js\nâ   â   â       âââ main.min.js\nâ   â   âââ /src\nâ   â       âââ /js\nâ   â       â   âââ main.js\nâ   â       âââ /less\nâ   â           âââ account.less\nâ   â           âââ dashboard.less\nâ   â           âââ vars.less\nâ   âââ /templates\nâ       âââ dashboard.html\nâ       âââ layout.html\nâ       âââ login.html\nâ       âââ meta.html\nâ       âââ scripts.html\nâ       âââ signup.html\nâââ config.py\nâââ requirements.txt\nâââ setup.py\nâââ start.sh\nâââ wsgi.py\n\n\nOf course, I wouldn't be a gentleman unless I revealed my config.py  as well:\n\nimport os\n\n\nclass Config:\n    \"\"\"Set Flask configuration vars from .env file.\"\"\"\n\n    # General Config\n    SECRET_KEY = os.environ.get('SECRET_KEY')\n    FLASK_APP = os.environ.get('FLASK_APP')\n    FLASK_ENV = os.environ.get('FLASK_ENV')\n    FLASK_DEBUG = os.environ.get('FLASK_DEBUG')\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get('SQLALCHEMY_TRACK_MODIFICATIONS')\n\n\nThe configuration values live in a .env  file, a practice I highly encourage. Of\nthese configuration variables, SECRET_KEY  is where we should turn our\nattention. SECRET_KEY is the equivalent of a password used to secure our app; it\nshould be as long, nonsensical, and impossible-to-remember as humanly possible.\nSeriously: having your secret key compromised is the equivalent of feeding\ngremlins after midnight.\n\nInitializing Flask-Login\nWith a standard \"application factory\" app, setting up Flask-Login is no\ndifferent from other Flask plugins (or whatever they're called now). This makes\nsetting up easy; all we need to do is make sure Flask-Login  is initialized in \n__init__.py  along with the rest of our plugins:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\n\n\ndb = SQLAlchemy()\nlogin_manager = LoginManager()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n\n    # Application Configuration\n    app.config.from_object('config.Config')\n\n    # Initialize Plugins\n    db.init_app(app)\n    login_manager.init_app(app)\n\n    with app.app_context():\n        # Import parts of our application\n        from . import routes\n        from . import login\n        app.register_blueprint(routes.main_bp)\n        app.register_blueprint(login.login_bp)\n\n        # Initialize Global db\n        db.create_all()\n\n        return app\n\n\nIn the above example, we're using the minimal number of plug-ins to get logins\nworking: Flask-SQLAlchemy  and Flask-Login.\n\nTo keep our sanity, we're going to separate our login routes from our main\napplication routes and logic. This is why we register a Blueprint called auth_bp\n, imported from a file called auth.py. Our âmainâ application (AKA anything that\nisnât logging in) will instead live in routes.py, in a Blueprint called main_bp.\nWe'll come back to these in a moment\n\nCreating a User Model\nWe'll save our User  model in models.py. There are a few things to keep in mind\nwhen creating models compatible with Flask-Login- the most important being the\nutilization of UserMixin  from the flask_login  library. When we inherit our\nclass from UserMixin,  our model is immediately extended to include all the\nmethods necessary for Flask-Login to work. This is by far the easiest way of\ncreating a User model. I won't bother getting into details of what these methods\ndo, because if you simply begin your class with class User(UserMixin, db.Model):\n, you genuinely don't need to understand any of it:\n\nfrom . import db\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n\nclass User(UserMixin, db.Model):\n    \"\"\"Model for user accounts.\"\"\"\n\n    __tablename__ = 'flasklogin-users'\n\n    id = db.Column(db.Integer,\n                   primary_key=True,\n                   )\n    name = db.Column(db.String,\n                     nullable=False,\n                     unique=False)\n    email = db.Column(db.String(40),\n                      unique=True,\n                      nullable=False\n                      )\n    password = db.Column(db.String(200),\n                         primary_key=False,\n                         unique=False,\n                         nullable=False\n                         )\n    website = db.Column(db.String(60),\n                        index=False,\n                        unique=False,\n                        nullable=True\n                        )\n    created_on = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n    last_login = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n\n    def set_password(self, password):\n        \"\"\"Create hashed password.\"\"\"\n        self.password = generate_password_hash(password, method='sha256')\n\n    def check_password(self, password):\n        \"\"\"Check hashed password.\"\"\"\n        return check_password_hash(self.password, password)\n\n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n\nThe set_password  and check_password  methods don't necessarily need to live\ninside our User model, but it's nice to keep related logic bundled together and\nout of our routes.\n\nYou may notice that our password field explicitly allows 200 characters: this is\nbecause our database will be storing hashed passwords. Thus, even if a user's\npassword is 8 characters long, the string in our database will look much\ndifferent.\n\nCreating Log-in and Sign-up Forms\nIf you're well versed in WTForms, our form logic in forms.py  probably looks as\nyou'd expect. Of course, the constraints we set here are to handle front-end\nvalidation only:\n\nfrom wtforms import Form, StringField, PasswordField, validators, SubmitField\nfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo, Length, Optional\n\n\nclass SignupForm(Form):\n    \"\"\"User Signup Form.\"\"\"\n\n    name = StringField('Name',\n                        validators=[DataRequired(message=('Enter a fake name or something.'))])\n    email = StringField('Email',\n                        validators=[Length(min=6, message=('Please enter a valid email address.')),\n                                    Email(message=('Please enter a valid email address.')),\n                                    DataRequired(message=('Please enter a valid email address.'))])\n    password = PasswordField('Password',\n                             validators=[DataRequired(message='Please enter a password.'),\n                                         Length(min=6, message=('Please select a stronger password.')),\n                                         EqualTo('confirm', message='Passwords must match')])\n    confirm = PasswordField('Confirm Your Password',)\n    website = StringField('Website',\n                          validators=[Optional()])\n    submit = SubmitField('Register')\n\n\nclass LoginForm(Form):\n    \"\"\"User Login Form.\"\"\"\n\n    email = StringField('Email', validators=[DataRequired('Please enter a valid email address.'),\n                                             Email('Please enter a valid email address.')])\n    password = PasswordField('Password', validators=[DataRequired('Uhh, your password tho?')])\n    submit = SubmitField('Log In')\n\n\nWith those out of the way, let's look at how we implement these on the Jinja\nside.\n\nsignup.html\n{% extends \"layout.html\" %}\n\n{% block pagestyles %}\n    <link href=\"{{ url_for('static', filename='dist/css/account.css') }}\" rel=\"stylesheet\" type=\"text/css\">\n{% endblock %}\n\n{% block content %}\n  <div class=\"formwrapper\">\n    <form method=post>\n      <div class=\"logo\">\n        <img src=\"{{ url_for('static', filename='dist/img/logo.png') }}\">\n      </div>\n      {% for message in get_flashed_messages() %}\n        <div class=\"alert alert-warning\">\n            <button type=\"button\" class=\"close\" data-dismiss=\"alert\">&times;</button>\n            {{ message }}\n        </div>\n      {% endfor %}\n      <h1>Sign Up</h1>\n      <div class=\"name\">\n        {{ form.name.label }}\n        {{ form.name(placeholder='John Smith') }}\n        {% if form.name.errors %}\n          <ul class=\"errors\">\n            {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"email\">\n        {{ form.email.label }}\n        {{ form.email(placeholder='youremail@example.com') }}\n        {% if form.email.errors %}\n          <ul class=\"errors\">\n            {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"password\">\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.password.errors %}\n          <ul class=\"errors\">\n            {% for error in form.password.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"confirm\">\n        {{ form.confirm.label }}\n        {{ form.confirm }}\n        {% if form.confirm.errors %}\n          <ul class=\"errors\">\n            {% for error in form.confirm.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"website\">\n        {{ form.website.label }}\n        {{ form.website(placeholder='http://example.com') }}\n      </div>\n      <div class=\"submitbutton\">\n        <input id=\"submit\" type=\"submit\" value=\"Submit\">\n      </div>\n    </form>\n    <div class=\"loginsignup\">\n      <span>Already have an account? <a href=\"{{ url_for('auth_bp.login_page') }}\">Log in.</a><span>\n    </div>\n  </div>\n{% endblock %}\n\n\n\nlogin.py\n{% extends \"layout.html\" %}\n\n{% block pagestyles %}\n  <link href=\"{{ url_for('static', filename='dist/css/account.css') }}\" rel=\"stylesheet\" type=\"text/css\">\n{% endblock %}\n\n{% block content %}\n  <div class=\"formwrapper\">\n    <form method=post>\n      <div class=\"logo\">\n        <img src=\"{{ url_for('static', filename='dist/img/logo.png') }}\">\n      </div>\n      {% for message in get_flashed_messages() %}\n        <div class=\"alert alert-warning\">\n            <button type=\"button\" class=\"close\" data-dismiss=\"alert\">&times;</button>\n            {{ message }}\n        </div>\n      {% endfor %}\n      <h1>Log In</h1>\n      <div class=\"email\">\n         {{ form.email.label }}\n         {{ form.email(placeholder='youremail@example.com') }}\n         {% if form.email.errors %}\n           <ul class=\"errors\">\n             {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n           </ul>\n         {% endif %}\n      </div>\n      <div class=\"password\">\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.email.errors %}\n          <ul class=\"errors\">\n            {% for error in form.password.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"submitbutton\">\n        <input id=\"submit\" type=\"submit\" value=\"Submit\">\n      </div>\n      <div class=\"loginsignup\">\n        <span>Don't have an account? <a href=\"{{ url_for('auth_bp.signup_page') }}\">Sign up.</a><span>\n        </div>\n    </form>\n  </div>\n{% endblock %}\n\n\n\nExcellent: the stage is set to start kicking some ass.\n\nCreating Our Login Routes\nLet us turn our attention to the heart of the logic we'll be writing in auth.py:\n\nimport os\nfrom flask import redirect, render_template, flash, Blueprint, request, session, url_for\nfrom flask_login import login_required, logout_user, current_user, login_user\nfrom flask import current_app as app\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom .forms import LoginForm, SignupForm\nfrom .models import db, User\nfrom . import login_manager\n\n\n# Blueprint Configuration\nauth_bp = Blueprint('auth_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    \"\"\"User login page.\"\"\"\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        ...\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=\"Log in with your User account.\")\n\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    \"\"\"User sign-up page.\"\"\"\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        ...\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=\"Sign up for a user account.\")\n\n\nHere we find two separate skeleton routes for Sign up  and Log in. Without the\nauthentication logic added quite yet, these routes look almost identical thus\nfar.\n\nEach time a user visits a page in your app, the corresponding route is sent a \nrequest  object. This object contains contextual information about the request\nmade by the user, such as the type of request (GET or POST), any form data which\nwas submitted, etc. We leverage this to see whether the user is just arriving at\nthe page for the first time (a GET request), or if they're attempting to sign in\n(a POST request). The fairly clever takeaway here is that our login pages verify\nusers by making POST requests to themselves: this allows us to keep all logic\nrelated to logging in or signing up in a single route.\n\nSigning Up\nWe're able to validate the submitted form by importing the SignupForm  class and\npassing request.form  as the form in question. if signup_form.validate()  checks\nthe information submitted by the user against all the form's validators. If any\nof the validators are not met, the user is redirected back to the signup form\nwith error messages present.\n\nAssuming that our user isn't inept, we can move on with our logic. First, we\nneed to make sure a user with the provided email doesn't already exist:\n\n...\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    \"\"\"User sign-up page.\"\"\"\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        if signup_form.validate():\n            # Get Form Fields\n            name = request.form.get('name')\n            email = request.form.get('email')\n            password = request.form.get('password')\n            website = request.form.get('website')\n            existing_user = User.query.filter_by(email=email).first()\n            if existing_user is None:\n                user = User(name=name,\n                            email=email,\n                            password=generate_password_hash(password, method='sha256'),\n                            website=website)\n                db.session.add(user)\n                db.session.commit()\n                login_user(user)\n                return redirect(url_for('main_bp.dashboard'))\n            flash('A user already exists with that email address.')\n            return redirect(url_for('auth_bp.signup_page'))\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=\"Sign up for a user account.\")\n\n\nIf existing_user is None, we're all clear to actually clear to create a new user\nrecord. We create an instance of our User model via user = User(...). We then\nadd the user via standard SQLAlchemy syntax and finally use the imported method \nlogin_user()  to log the user in.\n\nIf everything goes well, the user will finally be redirected to the main\napplication, which is handled by return redirect(url_for('main_bp.dashboard')):\n\nA successful user log in.And here's what will happen if we log out and try to\nsign up with the same information:\n\nAttempting to sign up with an existing emailLogging In\nMoving on to our login route:\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    \"\"\"User login page.\"\"\"\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        if login_form.validate():\n            # Get Form Fields\n            email = request.form.get('email')\n            password = request.form.get('password')\n            # Validate Login Attempt\n            user = User.query.filter_by(email=email).first()\n            if user:\n                if user.check_password(password=password):\n                    login_user(user)\n                    next = request.args.get('next')\n                    return redirect(next or url_for('main_bp.dashboard'))\n        flash('Invalid username/password combination')\n        return redirect(url_for('auth_bp.login_page'))\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=\"Log in with your User account.\")\n\n\nThis should mostly look the same! our logic is identical up until the point\nwhere we check to see if the user exists. This time, a match results in success\nas opposed to a failure. Continuing, we then use user.check_password()  to check\nthe hashed password we created earlier with user.generate_password_hash(). Both\nof these methods handle the encrypting and decrypting of passwords on their own\n(based on that SECRET_KEY we created earlier) to ensure that nobody (not even\nus) has any business looking at user passwords.\n\nAs with last time, a successful login ends in login_user(user). Our redirect\nlogic is little more sophisticated this time around: instead of always sending\nthe user back to the dashboard, we check for next, which is a parameter stored\nin the query string of the current user. If the user attempted to access our app\nbefore logging in, next  would equal the page they had attempted to reach: this\nallows us wall-off our app from unauthorized users, and then drop users off at\nthe page they attempted to reach before they logged in:\n\nA successful log inIMPORTANT: Login Helpers\nBefore your app can work like the above, we need to finish auth.py  by providing\na few more routes:\n\n@auth_bp.route(\"/logout\")\n@login_required\ndef logout_page():\n    \"\"\"User log-out logic.\"\"\"\n    logout_user()\n    return redirect(url_for('auth_bp.login_page'))\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    \"\"\"Check if user is logged-in on every page load.\"\"\"\n    if user_id is not None:\n        return User.query.get(user_id)\n    return None\n\n\n@login_manager.unauthorized_handler\ndef unauthorized():\n    \"\"\"Redirect unauthorized users to Login page.\"\"\"\n    flash('You must be logged in to view that page.')\n    return redirect(url_for('auth_bp.login_page'))\n\n\nOur first route, logout_page, handles the logic of users logging out. This will\nsimply end the user's session and redirect them to the login screen.\n\nload_user  is critical for making our app work: before every page load, our app\nmust verify whether or not the user is logged in (or still  logged in after time\nhas elapsed). user_loader  loads users by their unique ID. If a user is\nreturned, this signifies a logged-out user. Otherwise, when None  is returned,\nthe user is logged out.\n\nLastly, we have the unauthorized  route, which uses the unauthorized_handler \ndecorator for dealing with unauthorized users. Any time a user attempts to hit\nour app and is unauthorized, this route will fire.\n\nThe Last Piece: routes.py\nThe last thing we'll cover is how to protect parts of our app from unauthorized\nusers. Here's what we have in routes.py:\n\nimport os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask_login import current_user\nfrom flask import current_app as app\nfrom .models import User\nfrom flask_login import login_required\n\n\n# Blueprint Configuration\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@main_bp.route('/', methods=['GET'])\n@login_required\ndef dashboard():\n    \"\"\"Serve logged in Dashboard.\"\"\"\n    return render_template('dashboard.html',\n                           title='Flask-Login Tutorial.',\n                           template='dashboard-template',\n                           current_user=current_user,\n                           body=\"You are now logged in!\")\n\n\nThe magic here is all contained within the @login_required  decorator. When this\ndecorator is present on a route, the following things happen:\n\n * The @login_manager.user_loader  route we created determines whether or not\n   the user is authorized to view the page (logged in). If the user is logged\n   in, they'll be permitted to view the page.\n * If the user is not logged in, the user will be redirected as per the logic in\n   the route decorated with @login_manager.unauthorized_handler.\n * The name of the route the user attempted to access will be stored in the URL\n   as ?url=[name-of-route]. This what allows next  to work.\n\nThere You Have It\nIf you've made it this far, I commend you for your courage. To reward your\naccomplishments, I've published the source code for this tutorial on Github\n[https://github.com/toddbirchard/flasklogin-tutorial]  for your reference.\nGodspeed, brave adventurer.","html":"<p>Weâve covered a lot of Flask goodness in this series thus far. We fully understand how to structure a sensible application; we can serve up <a href=\"https://hackersandslackers.com/powerful-page-templates-in-flask-with-jinja/\"><strong>complex page templates</strong></a>, and have dived into <a href=\"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/\"><strong>interacting with databases using Flask-SQLAlchemy</strong></a>. For our next challenge, weâre going to need <em>all</em> of the knowledge we've acquired thus far and much, much more. Welcome to the Super Bowl of Flask development. This. Is. Flask-Login.</p><p><a href=\"https://flask-login.readthedocs.io/en/latest/\"><strong>Flask-Login</strong></a> is a dope library which handles all aspects of user management, including vital nuances you might not expect. Some noteworthy features include securing parts of our app behind login walls, encrypting passwords, and handling sessions. Moreover, It plays nicely with other Flask libraries weâre already familiar with: <a href=\"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/\"><strong>Flask-SQLAlchemy</strong></a> to create and fetch accounts, and <a href=\"https://hackersandslackers.com/guide-to-building-forms-in-flask/\"><strong>Flask-WTForms</strong></a> for handling intelligent sign-up &amp; log-in forms. This tutorial assumes you have <em>some </em>working knowledge of these things.</p><p>Flask-Login is shockingly quite easy to use after the initial learning curve... but therein lies the catch. Perhaps Iâm not the only one to have noticed, but most Flask-related documentation tends to be, well, God-awful. The community is riddled with helplessly outdated information; if you ever come across <code>flask.ext</code> in a tutorial, it is inherently worthless to anybody developing in 2019. To make matters worse, official Flask-Login documentation contains some artifacts which are just plain wrong. The documentation contradicts itself (Iâll show you what I mean), and offers little to no code examples to speak of. My only hope is that I might save somebody the endless headaches Iâve experienced myself.</p><h2 id=\"structuring-our-application\">Structuring Our Application</h2><p>Letâs start with installing dependencies. This should give you an idea of what youâre in for:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ pip3 install flask flask-login flask-sqlalchemy psycopg2-binary python-dotenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Sweet. Letâs take this one step at a time, starting with our project file structure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">flasklogin-tutorial\nâââ /login_tutorial\nâ   âââ __init__.py\nâ   âââ auth.py\nâ   âââ forms.py\nâ   âââ models.py\nâ   âââ routes.py\nâ   âââ /static\nâ   â   âââ /dist\nâ   â   â   âââ /css\nâ   â   â   â   âââ account.css\nâ   â   â   â   âââ dashboard.css\nâ   â   â   âââ /js\nâ   â   â       âââ main.min.js\nâ   â   âââ /src\nâ   â       âââ /js\nâ   â       â   âââ main.js\nâ   â       âââ /less\nâ   â           âââ account.less\nâ   â           âââ dashboard.less\nâ   â           âââ vars.less\nâ   âââ /templates\nâ       âââ dashboard.html\nâ       âââ layout.html\nâ       âââ login.html\nâ       âââ meta.html\nâ       âââ scripts.html\nâ       âââ signup.html\nâââ config.py\nâââ requirements.txt\nâââ setup.py\nâââ start.sh\nâââ wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><p>Of course, I wouldn't be a gentleman unless I revealed my <strong>config.py</strong> as well:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\n\n\nclass Config:\n    &quot;&quot;&quot;Set Flask configuration vars from .env file.&quot;&quot;&quot;\n\n    # General Config\n    SECRET_KEY = os.environ.get('SECRET_KEY')\n    FLASK_APP = os.environ.get('FLASK_APP')\n    FLASK_ENV = os.environ.get('FLASK_ENV')\n    FLASK_DEBUG = os.environ.get('FLASK_DEBUG')\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get('SQLALCHEMY_TRACK_MODIFICATIONS')\n</code></pre>\n<!--kg-card-end: markdown--><p>The configuration values live in a <code>.env</code> file, a practice I highly encourage. Of these configuration variables, <strong>SECRET_KEY</strong> is where we should turn our attention. SECRET_KEY is the equivalent of a password used to secure our app; it should be as long, nonsensical, and impossible-to-remember as humanly possible. Seriously: having your secret key compromised is the equivalent of feeding gremlins after midnight.</p><h2 id=\"initializing-flask-login\">Initializing Flask-Login</h2><p>With a standard \"application factory\" app, setting up Flask-Login is no different from other Flask plugins (or whatever they're called now). This makes setting up easy; all we need to do is make sure <strong>Flask-Login</strong> is initialized in <code>__init__.py</code> along with the rest of our plugins:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\n\n\ndb = SQLAlchemy()\nlogin_manager = LoginManager()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n\n    # Application Configuration\n    app.config.from_object('config.Config')\n\n    # Initialize Plugins\n    db.init_app(app)\n    login_manager.init_app(app)\n\n    with app.app_context():\n        # Import parts of our application\n        from . import routes\n        from . import login\n        app.register_blueprint(routes.main_bp)\n        app.register_blueprint(login.login_bp)\n\n        # Initialize Global db\n        db.create_all()\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>In the above example, we're using the minimal number of plug-ins to get logins working: <strong>Flask-SQLAlchemy</strong> and <strong>Flask-Login</strong>.</p><p>To keep our sanity, we're going to separate our login routes from our main application routes and logic. This is why we register a Blueprint called <strong>auth_bp</strong>, imported from a file called <code>auth.py</code>. Our âmainâ application (AKA anything that isnât logging in) will instead live in <code>routes.py</code>, in a Blueprint called <strong>main_bp</strong>. We'll come back to these in a moment</p><h2 id=\"creating-a-user-model\">Creating a User Model</h2><p>We'll save our <strong>User</strong> model in <code>models.py</code>. There are a few things to keep in mind when creating models compatible with Flask-Login- the most important being the utilization of <code>UserMixin</code> from the <code>flask_login</code> library. When we inherit our class from <strong>UserMixin,</strong> our model is immediately extended to include all the methods necessary for Flask-Login to work. This is by far the easiest way of creating a User model. I won't bother getting into details of what these methods do, because if you simply begin your class with <code>class User(UserMixin, db.Model):</code>, you genuinely don't need to understand any of it:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from . import db\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n\nclass User(UserMixin, db.Model):\n    &quot;&quot;&quot;Model for user accounts.&quot;&quot;&quot;\n\n    __tablename__ = 'flasklogin-users'\n\n    id = db.Column(db.Integer,\n                   primary_key=True,\n                   )\n    name = db.Column(db.String,\n                     nullable=False,\n                     unique=False)\n    email = db.Column(db.String(40),\n                      unique=True,\n                      nullable=False\n                      )\n    password = db.Column(db.String(200),\n                         primary_key=False,\n                         unique=False,\n                         nullable=False\n                         )\n    website = db.Column(db.String(60),\n                        index=False,\n                        unique=False,\n                        nullable=True\n                        )\n    created_on = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n    last_login = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n\n    def set_password(self, password):\n        &quot;&quot;&quot;Create hashed password.&quot;&quot;&quot;\n        self.password = generate_password_hash(password, method='sha256')\n\n    def check_password(self, password):\n        &quot;&quot;&quot;Check hashed password.&quot;&quot;&quot;\n        return check_password_hash(self.password, password)\n\n    def __repr__(self):\n        return '&lt;User {}&gt;'.format(self.username)\n</code></pre>\n<!--kg-card-end: markdown--><p>The <code>set_password</code> and <code>check_password</code> methods don't necessarily need to live inside our User model, but it's nice to keep related logic bundled together and out of our routes.</p><p>You may notice that our password field explicitly allows 200 characters: this is because our database will be storing hashed passwords. Thus, even if a user's password is 8 characters long, the string in our database will look much different.</p><h2 id=\"creating-log-in-and-sign-up-forms\">Creating Log-in and Sign-up Forms</h2><p>If you're well versed in <strong>WTForms</strong>, our form logic in <strong>forms.py</strong> probably looks as you'd expect. Of course, the constraints we set here are to handle front-end validation only:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from wtforms import Form, StringField, PasswordField, validators, SubmitField\nfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo, Length, Optional\n\n\nclass SignupForm(Form):\n    &quot;&quot;&quot;User Signup Form.&quot;&quot;&quot;\n\n    name = StringField('Name',\n                        validators=[DataRequired(message=('Enter a fake name or something.'))])\n    email = StringField('Email',\n                        validators=[Length(min=6, message=('Please enter a valid email address.')),\n                                    Email(message=('Please enter a valid email address.')),\n                                    DataRequired(message=('Please enter a valid email address.'))])\n    password = PasswordField('Password',\n                             validators=[DataRequired(message='Please enter a password.'),\n                                         Length(min=6, message=('Please select a stronger password.')),\n                                         EqualTo('confirm', message='Passwords must match')])\n    confirm = PasswordField('Confirm Your Password',)\n    website = StringField('Website',\n                          validators=[Optional()])\n    submit = SubmitField('Register')\n\n\nclass LoginForm(Form):\n    &quot;&quot;&quot;User Login Form.&quot;&quot;&quot;\n\n    email = StringField('Email', validators=[DataRequired('Please enter a valid email address.'),\n                                             Email('Please enter a valid email address.')])\n    password = PasswordField('Password', validators=[DataRequired('Uhh, your password tho?')])\n    submit = SubmitField('Log In')\n</code></pre>\n<!--kg-card-end: markdown--><p>With those out of the way, let's look at how we implement these on the Jinja side.</p><h3 id=\"signup-html\">signup.html</h3><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block pagestyles %}\n    &lt;link href=&quot;{{ url_for('static', filename='dist/css/account.css') }}&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;div class=&quot;formwrapper&quot;&gt;\n    &lt;form method=post&gt;\n      &lt;div class=&quot;logo&quot;&gt;\n        &lt;img src=&quot;{{ url_for('static', filename='dist/img/logo.png') }}&quot;&gt;\n      &lt;/div&gt;\n      {% for message in get_flashed_messages() %}\n        &lt;div class=&quot;alert alert-warning&quot;&gt;\n            &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot;&gt;&amp;times;&lt;/button&gt;\n            {{ message }}\n        &lt;/div&gt;\n      {% endfor %}\n      &lt;h1&gt;Sign Up&lt;/h1&gt;\n      &lt;div class=&quot;name&quot;&gt;\n        {{ form.name.label }}\n        {{ form.name(placeholder='John Smith') }}\n        {% if form.name.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;email&quot;&gt;\n        {{ form.email.label }}\n        {{ form.email(placeholder='youremail@example.com') }}\n        {% if form.email.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;password&quot;&gt;\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.password.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.password.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;confirm&quot;&gt;\n        {{ form.confirm.label }}\n        {{ form.confirm }}\n        {% if form.confirm.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.confirm.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;website&quot;&gt;\n        {{ form.website.label }}\n        {{ form.website(placeholder='http://example.com') }}\n      &lt;/div&gt;\n      &lt;div class=&quot;submitbutton&quot;&gt;\n        &lt;input id=&quot;submit&quot; type=&quot;submit&quot; value=&quot;Submit&quot;&gt;\n      &lt;/div&gt;\n    &lt;/form&gt;\n    &lt;div class=&quot;loginsignup&quot;&gt;\n      &lt;span&gt;Already have an account? &lt;a href=&quot;{{ url_for('auth_bp.login_page') }}&quot;&gt;Log in.&lt;/a&gt;&lt;span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"login-py\">login.py</h3><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block pagestyles %}\n  &lt;link href=&quot;{{ url_for('static', filename='dist/css/account.css') }}&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;div class=&quot;formwrapper&quot;&gt;\n    &lt;form method=post&gt;\n      &lt;div class=&quot;logo&quot;&gt;\n        &lt;img src=&quot;{{ url_for('static', filename='dist/img/logo.png') }}&quot;&gt;\n      &lt;/div&gt;\n      {% for message in get_flashed_messages() %}\n        &lt;div class=&quot;alert alert-warning&quot;&gt;\n            &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot;&gt;&amp;times;&lt;/button&gt;\n            {{ message }}\n        &lt;/div&gt;\n      {% endfor %}\n      &lt;h1&gt;Log In&lt;/h1&gt;\n      &lt;div class=&quot;email&quot;&gt;\n         {{ form.email.label }}\n         {{ form.email(placeholder='youremail@example.com') }}\n         {% if form.email.errors %}\n           &lt;ul class=&quot;errors&quot;&gt;\n             {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n           &lt;/ul&gt;\n         {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;password&quot;&gt;\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.email.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.password.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;submitbutton&quot;&gt;\n        &lt;input id=&quot;submit&quot; type=&quot;submit&quot; value=&quot;Submit&quot;&gt;\n      &lt;/div&gt;\n      &lt;div class=&quot;loginsignup&quot;&gt;\n        &lt;span&gt;Don't have an account? &lt;a href=&quot;{{ url_for('auth_bp.signup_page') }}&quot;&gt;Sign up.&lt;/a&gt;&lt;span&gt;\n        &lt;/div&gt;\n    &lt;/form&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n</code></pre>\n<!--kg-card-end: markdown--><p>Excellent: the stage is set to start kicking some ass.</p><h2 id=\"creating-our-login-routes\">Creating Our Login Routes</h2><p>Let us turn our attention to the heart of the logic we'll be writing in <strong>auth.py</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nfrom flask import redirect, render_template, flash, Blueprint, request, session, url_for\nfrom flask_login import login_required, logout_user, current_user, login_user\nfrom flask import current_app as app\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom .forms import LoginForm, SignupForm\nfrom .models import db, User\nfrom . import login_manager\n\n\n# Blueprint Configuration\nauth_bp = Blueprint('auth_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    &quot;&quot;&quot;User login page.&quot;&quot;&quot;\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        ...\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=&quot;Log in with your User account.&quot;)\n\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    &quot;&quot;&quot;User sign-up page.&quot;&quot;&quot;\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        ...\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=&quot;Sign up for a user account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Here we find two separate skeleton routes for <strong>Sign up</strong> and <strong>Log in</strong>. Without the authentication logic added quite yet, these routes look almost identical thus far.</p><p>Each time a user visits a page in your app, the corresponding route is sent a <code>request</code> object. This object contains contextual information about the request made by the user, such as the type of request (GET or POST), any form data which was submitted, etc. We leverage this to see whether the user is just arriving at the page for the first time (a GET request), or if they're attempting to sign in (a POST request). The fairly clever takeaway here is that our login pages verify users by making POST requests to themselves: this allows us to keep all logic related to logging in or signing up in a single route.</p><h3 id=\"signing-up\">Signing Up</h3><p>We're able to validate the submitted form by importing the <code>SignupForm</code> class and passing <code>request.form</code> as the form in question. <code>if signup_form.validate()</code> checks the information submitted by the user against all the form's validators. If any of the validators are not met, the user is redirected back to the signup form with error messages present.</p><p>Assuming that our user isn't inept, we can move on with our logic. First, we need to make sure a user with the provided email doesn't already exist:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">...\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    &quot;&quot;&quot;User sign-up page.&quot;&quot;&quot;\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        if signup_form.validate():\n            # Get Form Fields\n            name = request.form.get('name')\n            email = request.form.get('email')\n            password = request.form.get('password')\n            website = request.form.get('website')\n            existing_user = User.query.filter_by(email=email).first()\n            if existing_user is None:\n                user = User(name=name,\n                            email=email,\n                            password=generate_password_hash(password, method='sha256'),\n                            website=website)\n                db.session.add(user)\n                db.session.commit()\n                login_user(user)\n                return redirect(url_for('main_bp.dashboard'))\n            flash('A user already exists with that email address.')\n            return redirect(url_for('auth_bp.signup_page'))\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=&quot;Sign up for a user account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>If <code>existing_user is None</code>, we're all clear to actually clear to create a new user record. We create an instance of our User model via <code>user = User(...)</code>. We then add the user via standard SQLAlchemy syntax and finally use the imported method <code>login_user()</code> to log the user in.</p><p>If everything goes well, the user will finally be redirected to the main application, which is handled by <code>return redirect(url_for('main_bp.dashboard'))</code>:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/signup.gif\" class=\"kg-image\"><figcaption>A successful user log in.</figcaption></figure><!--kg-card-end: image--><p>And here's what will happen if we log out and try to sign up with the same information:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/signupfailed.gif\" class=\"kg-image\"><figcaption>Attempting to sign up with an existing email</figcaption></figure><!--kg-card-end: image--><h3 id=\"logging-in\">Logging In</h3><p>Moving on to our login route:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    &quot;&quot;&quot;User login page.&quot;&quot;&quot;\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        if login_form.validate():\n            # Get Form Fields\n            email = request.form.get('email')\n            password = request.form.get('password')\n            # Validate Login Attempt\n            user = User.query.filter_by(email=email).first()\n            if user:\n                if user.check_password(password=password):\n                    login_user(user)\n                    next = request.args.get('next')\n                    return redirect(next or url_for('main_bp.dashboard'))\n        flash('Invalid username/password combination')\n        return redirect(url_for('auth_bp.login_page'))\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=&quot;Log in with your User account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>This should mostly look the same! our logic is identical up until the point where we check to see if the user exists. This time, a match results in success as opposed to a failure. Continuing, we then use <code>user.check_password()</code> to check the hashed password we created earlier with <code>user.generate_password_hash()</code>. Both of these methods handle the encrypting and decrypting of passwords on their own (based on that SECRET_KEY we created earlier) to ensure that nobody (not even us) has any business looking at user passwords.</p><p>As with last time, a successful login ends in <code>login_user(user)</code>. Our redirect logic is little more sophisticated this time around: instead of always sending the user back to the dashboard, we check for <code>next</code>, which is a parameter stored in the query string of the current user. If the user attempted to access our app before logging in, <code>next</code> would equal the page they had attempted to reach: this allows us wall-off our app from unauthorized users, and then drop users off at the page they attempted to reach before they logged in:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/login.gif\" class=\"kg-image\"><figcaption>A successful log in</figcaption></figure><!--kg-card-end: image--><h3 id=\"important-login-helpers\">IMPORTANT: Login Helpers</h3><p>Before your app can work like the above, we need to finish <strong>auth.py</strong> by providing a few more routes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">@auth_bp.route(&quot;/logout&quot;)\n@login_required\ndef logout_page():\n    &quot;&quot;&quot;User log-out logic.&quot;&quot;&quot;\n    logout_user()\n    return redirect(url_for('auth_bp.login_page'))\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    &quot;&quot;&quot;Check if user is logged-in on every page load.&quot;&quot;&quot;\n    if user_id is not None:\n        return User.query.get(user_id)\n    return None\n\n\n@login_manager.unauthorized_handler\ndef unauthorized():\n    &quot;&quot;&quot;Redirect unauthorized users to Login page.&quot;&quot;&quot;\n    flash('You must be logged in to view that page.')\n    return redirect(url_for('auth_bp.login_page'))\n</code></pre>\n<!--kg-card-end: markdown--><p>Our first route, <code>logout_page</code>, handles the logic of users logging out. This will simply end the user's session and redirect them to the login screen.</p><p><code>load_user</code> is critical for making our app work: before every page load, our app must verify whether or not the user is logged in (or <em>still</em> logged in after time has elapsed). <code>user_loader</code> loads users by their unique ID. If a user is returned, this signifies a logged-out user. Otherwise, when <code>None</code> is returned, the user is logged out.</p><p>Lastly, we have the <code>unauthorized</code> route, which uses the <code>unauthorized_handler</code> decorator for dealing with unauthorized users. Any time a user attempts to hit our app and is unauthorized, this route will fire.</p><h2 id=\"the-last-piece-routes-py\">The Last Piece: routes.py</h2><p>The last thing we'll cover is how to protect parts of our app from unauthorized users. Here's what we have in <strong>routes.py</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask_login import current_user\nfrom flask import current_app as app\nfrom .models import User\nfrom flask_login import login_required\n\n\n# Blueprint Configuration\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@main_bp.route('/', methods=['GET'])\n@login_required\ndef dashboard():\n    &quot;&quot;&quot;Serve logged in Dashboard.&quot;&quot;&quot;\n    return render_template('dashboard.html',\n                           title='Flask-Login Tutorial.',\n                           template='dashboard-template',\n                           current_user=current_user,\n                           body=&quot;You are now logged in!&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>The magic here is all contained within the <code>@login_required</code> decorator. When this decorator is present on a route, the following things happen:</p><ul><li>The <code>@login_manager.user_loader</code> route we created determines whether or not the user is authorized to view the page (logged in). If the user is logged in, they'll be permitted to view the page.</li><li>If the user is not logged in, the user will be redirected as per the logic in the route decorated with <code>@login_manager.unauthorized_handler</code>.</li><li>The name of the route the user attempted to access will be stored in the URL as <code>?url=[name-of-route]</code>. This what allows <code>next</code> to work.</li></ul><h3 id=\"there-you-have-it\">There You Have It</h3><p>If you've made it this far, I commend you for your courage. To reward your accomplishments, I've published the <a href=\"https://github.com/toddbirchard/flasklogin-tutorial\">source code for this tutorial on Github</a> for your reference. Godspeed, brave adventurer.</p>","url":"https://hackersandslackers.com/authenticating-users-with-flask-login/","uuid":"23a82e0a-31e7-49ea-8cc1-fecdd466bcfd","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c779ffbc380a221de39c7cf"}},{"node":{"id":"Ghost__Post__5c95e08ef654036aa06c6a02","title":"Building an ETL Pipeline: From JIRA to SQL","slug":"building-an-etl-pipeline-from-jira-to-postgresql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-3.jpg","excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","custom_excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","created_at_pretty":"23 March, 2019","published_at_pretty":"28 March, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-03-23T03:30:22.000-04:00","published_at":"2019-03-28T04:15:00.000-04:00","updated_at":"2019-04-08T23:34:47.000-04:00","meta_title":"Building an ETL Pipeline: From JIRA to SQL | Hackers and Slackers","meta_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-2.jpg","og_title":"Building an ETL Pipeline: From JIRA to SQL","twitter_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-1.jpg","twitter_title":"Building an ETL Pipeline: From JIRA to SQL","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Atlassian","slug":"atlassian","description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","feature_image":null,"meta_description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","meta_title":"Atlassian Development for JIRA and Confluence. | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we haven't done just yet on this site is walking through the humble\nprocess of creating data pipelines: the art of taking a bunch of data, changing\nsaid data, and putting it somewhere else. It's kind of a weird thing to be into,\nhence why the MoMA has been rejecting my submissions of Github repositories.\nDon't worry; I'll keep at it.\n\nSomething you don't see every day are people sharing their pipelines, which is\nunderstandable. Presumably, the other people who do this kind of stuff do it for\nwork; nobody is happily building stupid pipelines in their free time begging to\nbe open sourced. Except me.\n\nWe've recently revamped our projects [https://hackersandslackers.com/projects/] \npage to include a public-facing Kanban board using GraphQL. To achieve this, we\nneed to extract JIRA data from the JIRA Cloud REST API and place it securely in\nour database.\n\nStructuring our Pipeline\nAn ETL pipeline which is considered 'well-structured' is in the eyes of the\nbeholder. There are a million different ways to pull and mess with data, so\nthere isn't a \"template\" for building these things out. In my case, the\nstructure of my script just so happened to end up as three modules: one for \nextracting, one for loading, and one for transforming. This was unplanned, but\nit's a good sign when our app matches our mindset. Here's the breakdown:\n\njira-database-etl\nâââ __main__.py\nâââ jira_etl\nâ   âââ __init__.py\nâ   âââ fetch.py\nâ   âââ data.py\nâ   âââ db.py\nâââ LICENSE\nâââ MANIFEST.in\nâââ Pipfile\nâââ Pipfile.lock\nâââ README.md\nâââ requirements.txt\nâââ setup.cfg\nâââ setup.py\n\n\nmain.py  is our application entry point. The logic of our pipeline is stored in\nthree parts under the jira_etl  directory:\n\n * fetch.py  grabs the data from the source (JIRA Cloud's REST API) and handles\n   fetching all JIRA issues.\n * data.py  transforms the data we've fetched and constructs a neat DataFrame\n   containing only the information we're after.\n * db.py  finally loads the data into a SQL database.\n\nDon't look into it too much, but here's our entry point:\n\nfrom jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    \"\"\"Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    \"\"\"\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n\n\nWithout further adieu, let's dig into the logic!\n\nExtracting Our Data\nBefore doing anything, it's essential we become familiar with the data we're\nabout to pull. Firstly, JIRA's REST API returns paginated results which max out\nat 100 results per page. This means we'll have to loop through the pages\nrecursively until all results are loaded.\n\nNext, let's look at an example of a single  JIRA issue JSON object returned by\nthe API:\n\n{\n    \"expand\": \"names,schema\",\n    \"startAt\": 0,\n    \"maxResults\": 1,\n    \"total\": 888,\n    \"issues\": [\n        {\n            \"expand\": \"operations,versionedRepresentations,editmeta,changelog,renderedFields\",\n            \"id\": \"11718\",\n            \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/11718\",\n            \"key\": \"HACK-756\",\n            \"fields\": {\n                \"issuetype\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014\",\n                    \"id\": \"10014\",\n                    \"description\": \"Placeholder item for \\\"holy shit this is going to be a lot of work\\\"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&avatarId=10311&avatarType=issuetype\",\n                    \"name\": \"Major Functionality\",\n                    \"subtask\": false,\n                    \"avatarId\": 10311\n                },\n                \"customfield_10070\": null,\n                \"customfield_10071\": null,\n                \"customfield_10073\": null,\n                \"customfield_10074\": null,\n                \"customfield_10075\": null,\n                \"project\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/project/10015\",\n                    \"id\": \"10015\",\n                    \"key\": \"HACK\",\n                    \"name\": \"Hackers and Slackers\",\n                    \"projectTypeKey\": \"software\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&avatarId=10535\",\n                        \"24x24\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&pid=10015&avatarId=10535\",\n                        \"16x16\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&pid=10015&avatarId=10535\",\n                        \"32x32\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&pid=10015&avatarId=10535\"\n                    }\n                },\n                \"fixVersions\": [],\n                \"resolution\": null,\n                \"resolutiondate\": null,\n                \"workratio\": -1,\n                \"lastViewed\": \"2019-03-24T02:01:31.355-0400\",\n                \"watches\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers\",\n                    \"watchCount\": 1,\n                    \"isWatching\": true\n                },\n                \"created\": \"2019-02-03T00:47:36.677-0500\",\n                \"customfield_10062\": null,\n                \"customfield_10063\": null,\n                \"customfield_10064\": null,\n                \"customfield_10065\": null,\n                \"customfield_10066\": null,\n                \"priority\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/priority/2\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg\",\n                    \"name\": \"High\",\n                    \"id\": \"2\"\n                },\n                \"customfield_10067\": null,\n                \"customfield_10068\": null,\n                \"customfield_10069\": [],\n                \"labels\": [],\n                \"versions\": [],\n                \"issuelinks\": [],\n                \"assignee\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"updated\": \"2019-03-24T02:01:30.724-0400\",\n                \"status\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/status/10004\",\n                    \"description\": \"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/\",\n                    \"name\": \"To Do\",\n                    \"id\": \"10004\",\n                    \"statusCategory\": {\n                        \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2\",\n                        \"id\": 2,\n                        \"key\": \"new\",\n                        \"colorName\": \"blue-gray\",\n                        \"name\": \"To Do\"\n                    }\n                },\n                \"components\": [],\n                \"description\": {\n                    \"version\": 1,\n                    \"type\": \"doc\",\n                    \"content\": [\n                        {\n                            \"type\": \"paragraph\",\n                            \"content\": [\n                                {\n                                    \"type\": \"text\",\n                                    \"text\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\",\n                                    \"marks\": [\n                                        {\n                                            \"type\": \"link\",\n                                            \"attrs\": {\n                                                \"href\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\"\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                \"customfield_10010\": null,\n                \"customfield_10011\": \"0|i0064j:i\",\n                \"customfield_10012\": null,\n                \"customfield_10013\": null,\n                \"security\": null,\n                \"customfield_10008\": \"HACK-143\",\n                \"customfield_10009\": {\n                    \"hasEpicLinkFieldDependency\": false,\n                    \"showField\": false,\n                    \"nonEditableReason\": {\n                        \"reason\": \"PLUGIN_LICENSE_ERROR\",\n                        \"message\": \"Portfolio for Jira must be licensed for the Parent Link to be available.\"\n                    }\n                },\n                \"summary\": \"Automate newsletter\",\n                \"creator\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"subtasks\": [],\n                \"reporter\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"customfield_10000\": \"{}\",\n                \"customfield_10001\": null,\n                \"customfield_10004\": null,\n                \"environment\": null,\n                \"duedate\": null,\n                \"votes\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes\",\n                    \"votes\": 0,\n                    \"hasVoted\": false\n                }\n            }\n        }\n    ]\n}\n\n\nWhoa, mama! That's a ton of BS for a single issue. You can see now why we'd want\nto transform this data before importing ten million fields into any database.\nMake note of these important fields:\n\n * startAt: An integer which tells us which issue number the paginated results\n   start at.\n * maxResults: Denotes the maximum number of results page - maxes out at 100\n   issues.\n * total: The total number of issues across all pages.\n * issues: A list of objects which contain the information for exactly one JIRA\n   issue per object\n\nGreat. So the purpose of fetch.py will essentially consist of creating a list of\nall 888  issues (in my case), and passing that off for transformation. Check it\nthe source I came up with:\n\nimport os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    \"\"\"Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     \"\"\"\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        \"Accept\": \"application/json\"\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        \"\"\"Gets the total number of results.\"\"\"\n        params = {\n            \"jql\": cls.jql,\n            \"maxResults\": 0,\n            \"startAt\": 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        \"\"\"Recursively retrieve all pages of JIRA issues.\"\"\"\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            \"\"\"Fetch one page of results, and determine if another page exists.\"\"\"\n            params = {\n                \"jql\": cls.jql,\n                \"maxResults\": cls.results_per_page,\n                \"startAt\": len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n\n\nYep, I'm using classes. This class has two methods:\n\n * get_total_number_of_issues: All this does is essentially pull the number of\n   issues (888) from the REST API. We'll use this number in our next function to\n   check if additional pages exist.\n * fetch_all_results: This is where things start getting fun. fetch_all_results \n   is a @classmethod  which contains a function within itself. fetch_all_results \n    gets the total number of JIRA issues and then calls upon child function \n   fetch_single_page to pull JIRA issue JSON objects and dump them into a list\n   called issue_arr  until all issues are accounted for.\n\nBecause we have 888 issues and can retrieve 100 issues  at a time, our function\nfetch_single_page  should run 9 times. And it does!\n\nTransforming Our Data\nSo now we have a list of 888 messy JIRA issues. The scope of data.py  should be\nto pull out only the data we want, and make sure that data is clean:\n\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    \"\"\"Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    \"\"\"\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        \"\"\"Make DataFrame out of data received from JIRA API.\"\"\"\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        \"\"\"Convert dict to JSON to string.\"\"\"\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        \"\"\"Create a JSON body for each ticket.\"\"\"\n        updated_date = datetime.strptime(issue['fields']['updated'], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n\n\nAgain, let's see the methods at work:\n\n * construct_dataframe: The main function we invoke to build our DataFrame\n   (mostly just calls other methods). Once all transformations are completed,\n   creates a DataFrame called jira_df  by using the Pandas json_normalize() \n   method.\n * make_issue_body: Creates a new dictionary per singular JIRA issue. Extracts \n   only  the fields we want to be imported into our database. Converts each\n   field into either a string or an int as a lazy way of avoiding null values\n   (for example, if issue['fields']['priority']['name']  contained a null value,\n   the script would error out. Wrapping this in str() is a dirty way of\n   converting null  to an empty string).\n * dict_to_json_string  Takes each issue dictionary and converts it to a JSON\n   object, which is then turned into a string (this is done for Pandas).\n\nLoading Our Data\nAnd now for the final step! Thanks to the joyful marriage of Pandas and\nSQLAlchemy, turning DataFrames into SQL tables is super simple. We never make\nthings simple, though.\n\nimport os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    \"\"\"Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    \"\"\"\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=\"hackers$prod\")\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        \"\"\"Clear table of data.\"\"\"\n        sql = text('TRUNCATE TABLE \"hackers$prod\".\"JiraIssue\"')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        \"\"\"Merge epic metadata from existing SQL table.\"\"\"\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        \"\"\"Upload JIRA DataFrame to PostgreSQL database.\"\"\"\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={\"assignee\": String(30),\n                                     \"assignee_url\": Text,\n                                     \"epic_link\": String(50),\n                                     \"issuetype_name\": String(50),\n                                     \"issuetype_icon\": Text,\n                                     \"key\": String(10),\n                                     \"priority_name\": String(30),\n                                     \"priority_rank\": Integer,\n                                     \"priority_url\": Text,\n                                     \"project\": String(50),\n                                     \"status\": String(30),\n                                     \"summary\": Text,\n                                     \"updated\": Integer,\n                                     \"updatedAt\": TIMESTAMP,\n                                     \"createdAt\": TIMESTAMP,\n                                     \"epic_color\": String(20),\n                                     \"epic_name\": String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n\n\n * merge_epic_metadata: Due to the nature of the JIRA REST API, some metadata is\n   missing per issue. If you're interested, the data missing revolves around \n   Epics: JIRA's REST API does not include the Epic Name  or Epic Color  fields\n   of linked epics.\n * upload_dataframe: Uses Panda's to_sql()  method to upload our DataFrame into\n   a SQL table (our target happens to be PostgreSQL, so we pass schema  here).\n   To make things explicit, we set the data type of every column on upload.\n\nWell, let's see how we made out!\n\nA look at our resulting database table.Whoaaa nelly, we did it! With our data\nclean, we can now build something useful! Here's what I built:\n\nFruits of our labor!There we have it: a pipeline that takes a bunch of messy\ndata, cleans it, and puts it somewhere else for proper use.\n\nIf you're interested in how we created the frontend for our Kanban board, check\nout our series on building features with GraphQL\n[https://hackersandslackers.com/series/graphql-hype/]. For the source code,\ncheck out the Github repository\n[https://github.com/toddbirchard/jira-database-etl].","html":"<p>Something we haven't done just yet on this site is walking through the humble process of creating data pipelines: the art of taking a bunch of data, changing said data, and putting it somewhere else. It's kind of a weird thing to be into, hence why the MoMA has been rejecting my submissions of Github repositories. Don't worry; I'll keep at it.</p><p>Something you don't see every day are people sharing their pipelines, which is understandable. Presumably, the other people who do this kind of stuff do it for work; nobody is happily building stupid pipelines in their free time begging to be open sourced. Except me.</p><p>We've recently revamped our <strong><a href=\"https://hackersandslackers.com/projects/\">projects</a></strong> page to include a public-facing Kanban board using GraphQL. To achieve this, we need to extract JIRA data from the JIRA Cloud REST API and place it securely in our database.</p><h2 id=\"structuring-our-pipeline\">Structuring our Pipeline</h2><p>An ETL pipeline which is considered 'well-structured' is in the eyes of the beholder. There are a million different ways to pull and mess with data, so there isn't a \"template\" for building these things out. In my case, the structure of my script just so happened to end up as three modules: one for <em>extracting</em>, one for <em>loading</em>, and one for <em>transforming</em>. This was unplanned, but it's a good sign when our app matches our mindset. Here's the breakdown:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">jira-database-etl\nâââ __main__.py\nâââ jira_etl\nâ   âââ __init__.py\nâ   âââ fetch.py\nâ   âââ data.py\nâ   âââ db.py\nâââ LICENSE\nâââ MANIFEST.in\nâââ Pipfile\nâââ Pipfile.lock\nâââ README.md\nâââ requirements.txt\nâââ setup.cfg\nâââ setup.py\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>main.py</strong> is our application entry point. The logic of our pipeline is stored in three parts under the <strong>jira_etl</strong> directory:</p><ul><li><strong>fetch.py</strong> grabs the data from the source (JIRA Cloud's REST API) and handles fetching all JIRA issues.</li><li><strong>data.py</strong> transforms the data we've fetched and constructs a neat DataFrame containing only the information we're after.</li><li><strong>db.py</strong> finally loads the data into a SQL database.</li></ul><p>Don't look into it too much, but here's our entry point:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    &quot;&quot;&quot;Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    &quot;&quot;&quot;\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n</code></pre>\n<!--kg-card-end: markdown--><p>Without further adieu, let's dig into the logic!</p><h2 id=\"extracting-our-data\">Extracting Our Data</h2><p>Before doing anything, it's essential we become familiar with the data we're about to pull. Firstly, JIRA's REST API returns paginated results which max out at 100 results per page. This means we'll have to loop through the pages recursively until all results are loaded.</p><p>Next, let's look at an example of a <strong><em>single</em></strong> JIRA issue JSON object returned by the API:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n    &quot;expand&quot;: &quot;names,schema&quot;,\n    &quot;startAt&quot;: 0,\n    &quot;maxResults&quot;: 1,\n    &quot;total&quot;: 888,\n    &quot;issues&quot;: [\n        {\n            &quot;expand&quot;: &quot;operations,versionedRepresentations,editmeta,changelog,renderedFields&quot;,\n            &quot;id&quot;: &quot;11718&quot;,\n            &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/11718&quot;,\n            &quot;key&quot;: &quot;HACK-756&quot;,\n            &quot;fields&quot;: {\n                &quot;issuetype&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014&quot;,\n                    &quot;id&quot;: &quot;10014&quot;,\n                    &quot;description&quot;: &quot;Placeholder item for \\&quot;holy shit this is going to be a lot of work\\&quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&amp;avatarId=10311&amp;avatarType=issuetype&quot;,\n                    &quot;name&quot;: &quot;Major Functionality&quot;,\n                    &quot;subtask&quot;: false,\n                    &quot;avatarId&quot;: 10311\n                },\n                &quot;customfield_10070&quot;: null,\n                &quot;customfield_10071&quot;: null,\n                &quot;customfield_10073&quot;: null,\n                &quot;customfield_10074&quot;: null,\n                &quot;customfield_10075&quot;: null,\n                &quot;project&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/project/10015&quot;,\n                    &quot;id&quot;: &quot;10015&quot;,\n                    &quot;key&quot;: &quot;HACK&quot;,\n                    &quot;name&quot;: &quot;Hackers and Slackers&quot;,\n                    &quot;projectTypeKey&quot;: &quot;software&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&amp;avatarId=10535&quot;,\n                        &quot;24x24&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;16x16&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;32x32&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&amp;pid=10015&amp;avatarId=10535&quot;\n                    }\n                },\n                &quot;fixVersions&quot;: [],\n                &quot;resolution&quot;: null,\n                &quot;resolutiondate&quot;: null,\n                &quot;workratio&quot;: -1,\n                &quot;lastViewed&quot;: &quot;2019-03-24T02:01:31.355-0400&quot;,\n                &quot;watches&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers&quot;,\n                    &quot;watchCount&quot;: 1,\n                    &quot;isWatching&quot;: true\n                },\n                &quot;created&quot;: &quot;2019-02-03T00:47:36.677-0500&quot;,\n                &quot;customfield_10062&quot;: null,\n                &quot;customfield_10063&quot;: null,\n                &quot;customfield_10064&quot;: null,\n                &quot;customfield_10065&quot;: null,\n                &quot;customfield_10066&quot;: null,\n                &quot;priority&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/priority/2&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg&quot;,\n                    &quot;name&quot;: &quot;High&quot;,\n                    &quot;id&quot;: &quot;2&quot;\n                },\n                &quot;customfield_10067&quot;: null,\n                &quot;customfield_10068&quot;: null,\n                &quot;customfield_10069&quot;: [],\n                &quot;labels&quot;: [],\n                &quot;versions&quot;: [],\n                &quot;issuelinks&quot;: [],\n                &quot;assignee&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;updated&quot;: &quot;2019-03-24T02:01:30.724-0400&quot;,\n                &quot;status&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/status/10004&quot;,\n                    &quot;description&quot;: &quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/&quot;,\n                    &quot;name&quot;: &quot;To Do&quot;,\n                    &quot;id&quot;: &quot;10004&quot;,\n                    &quot;statusCategory&quot;: {\n                        &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2&quot;,\n                        &quot;id&quot;: 2,\n                        &quot;key&quot;: &quot;new&quot;,\n                        &quot;colorName&quot;: &quot;blue-gray&quot;,\n                        &quot;name&quot;: &quot;To Do&quot;\n                    }\n                },\n                &quot;components&quot;: [],\n                &quot;description&quot;: {\n                    &quot;version&quot;: 1,\n                    &quot;type&quot;: &quot;doc&quot;,\n                    &quot;content&quot;: [\n                        {\n                            &quot;type&quot;: &quot;paragraph&quot;,\n                            &quot;content&quot;: [\n                                {\n                                    &quot;type&quot;: &quot;text&quot;,\n                                    &quot;text&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;,\n                                    &quot;marks&quot;: [\n                                        {\n                                            &quot;type&quot;: &quot;link&quot;,\n                                            &quot;attrs&quot;: {\n                                                &quot;href&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                &quot;customfield_10010&quot;: null,\n                &quot;customfield_10011&quot;: &quot;0|i0064j:i&quot;,\n                &quot;customfield_10012&quot;: null,\n                &quot;customfield_10013&quot;: null,\n                &quot;security&quot;: null,\n                &quot;customfield_10008&quot;: &quot;HACK-143&quot;,\n                &quot;customfield_10009&quot;: {\n                    &quot;hasEpicLinkFieldDependency&quot;: false,\n                    &quot;showField&quot;: false,\n                    &quot;nonEditableReason&quot;: {\n                        &quot;reason&quot;: &quot;PLUGIN_LICENSE_ERROR&quot;,\n                        &quot;message&quot;: &quot;Portfolio for Jira must be licensed for the Parent Link to be available.&quot;\n                    }\n                },\n                &quot;summary&quot;: &quot;Automate newsletter&quot;,\n                &quot;creator&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;subtasks&quot;: [],\n                &quot;reporter&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;customfield_10000&quot;: &quot;{}&quot;,\n                &quot;customfield_10001&quot;: null,\n                &quot;customfield_10004&quot;: null,\n                &quot;environment&quot;: null,\n                &quot;duedate&quot;: null,\n                &quot;votes&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes&quot;,\n                    &quot;votes&quot;: 0,\n                    &quot;hasVoted&quot;: false\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Whoa, mama! That's a ton of BS for a single issue. You can see now why we'd want to transform this data before importing ten million fields into any database. Make note of these important fields:</p><ul><li><code>startAt</code>: An integer which tells us which issue number the paginated results start at.</li><li><code>maxResults</code>: Denotes the maximum number of results page - maxes out at 100 issues.</li><li><code>total</code>: The total number of issues across all pages.</li><li><code>issues</code>: A list of objects which contain the information for exactly one JIRA issue per object</li></ul><p>Great. So the purpose of <strong>fetch.py </strong>will essentially consist of creating a list of all <strong>888</strong> issues (in my case), and passing that off for transformation. Check it the source I came up with:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    &quot;&quot;&quot;Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     &quot;&quot;&quot;\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        &quot;Accept&quot;: &quot;application/json&quot;\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        &quot;&quot;&quot;Gets the total number of results.&quot;&quot;&quot;\n        params = {\n            &quot;jql&quot;: cls.jql,\n            &quot;maxResults&quot;: 0,\n            &quot;startAt&quot;: 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        &quot;&quot;&quot;Recursively retrieve all pages of JIRA issues.&quot;&quot;&quot;\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            &quot;&quot;&quot;Fetch one page of results, and determine if another page exists.&quot;&quot;&quot;\n            params = {\n                &quot;jql&quot;: cls.jql,\n                &quot;maxResults&quot;: cls.results_per_page,\n                &quot;startAt&quot;: len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n</code></pre>\n<!--kg-card-end: markdown--><p>Yep, I'm using classes. This class has two methods:</p><ul><li><code>get_total_number_of_issues</code>: All this does is essentially pull the number of issues (888) from the REST API. We'll use this number in our next function to check if additional pages exist.</li><li><code>fetch_all_results</code>: This is where things start getting fun. <strong>fetch_all_results</strong> is a <em>@classmethod</em> which contains a function within itself. <strong>fetch_all_results</strong> gets the total number of JIRA issues and then calls upon child function <strong>fetch_single_page </strong>to pull JIRA issue JSON objects and dump them into a list called <code>issue_arr</code> until all issues are accounted for.</li></ul><p>Because we have <em>888 issues </em>and can retrieve <em>100 issues</em> at a time, our function Â <code>fetch_single_page</code> should run <em>9 times</em>. And it does!</p><h2 id=\"transforming-our-data\">Transforming Our Data</h2><p>So now we have a list of 888 messy JIRA issues. The scope of <strong>data.py</strong> should be to pull out only the data we want, and make sure that data is clean:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    &quot;&quot;&quot;Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    &quot;&quot;&quot;\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        &quot;&quot;&quot;Make DataFrame out of data received from JIRA API.&quot;&quot;&quot;\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        &quot;&quot;&quot;Convert dict to JSON to string.&quot;&quot;&quot;\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        &quot;&quot;&quot;Create a JSON body for each ticket.&quot;&quot;&quot;\n        updated_date = datetime.strptime(issue['fields']['updated'], &quot;%Y-%m-%dT%H:%M:%S.%f%z&quot;)\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n</code></pre>\n<!--kg-card-end: markdown--><p>Again, let's see the methods at work:</p><ul><li><code>construct_dataframe</code>: The main function we invoke to build our DataFrame (mostly just calls other methods). Once all transformations are completed, creates a DataFrame called <strong>jira_df</strong> by using the Pandas <em>json_normalize()</em> method.</li><li><code>make_issue_body</code>: Creates a new dictionary per singular JIRA issue. Extracts <em>only</em> the fields we want to be imported into our database. Converts each field into either a string or an int as a lazy way of avoiding null values (for example, if <code>issue['fields']['priority']['name']</code> contained a null value, the script would error out. Wrapping this in <strong>str() </strong>is a dirty way of converting <em>null</em> to an empty string).</li><li><code>dict_to_json_string</code> Takes each issue dictionary and converts it to a JSON object, which is then turned into a string (this is done for Pandas).</li></ul><h2 id=\"loading-our-data\">Loading Our Data</h2><p>And now for the final step! Thanks to the joyful marriage of Pandas and SQLAlchemy, turning DataFrames into SQL tables is super simple. We never make things simple, though.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    &quot;&quot;&quot;Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    &quot;&quot;&quot;\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=&quot;hackers$prod&quot;)\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        &quot;&quot;&quot;Clear table of data.&quot;&quot;&quot;\n        sql = text('TRUNCATE TABLE &quot;hackers$prod&quot;.&quot;JiraIssue&quot;')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        &quot;&quot;&quot;Merge epic metadata from existing SQL table.&quot;&quot;&quot;\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        &quot;&quot;&quot;Upload JIRA DataFrame to PostgreSQL database.&quot;&quot;&quot;\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={&quot;assignee&quot;: String(30),\n                                     &quot;assignee_url&quot;: Text,\n                                     &quot;epic_link&quot;: String(50),\n                                     &quot;issuetype_name&quot;: String(50),\n                                     &quot;issuetype_icon&quot;: Text,\n                                     &quot;key&quot;: String(10),\n                                     &quot;priority_name&quot;: String(30),\n                                     &quot;priority_rank&quot;: Integer,\n                                     &quot;priority_url&quot;: Text,\n                                     &quot;project&quot;: String(50),\n                                     &quot;status&quot;: String(30),\n                                     &quot;summary&quot;: Text,\n                                     &quot;updated&quot;: Integer,\n                                     &quot;updatedAt&quot;: TIMESTAMP,\n                                     &quot;createdAt&quot;: TIMESTAMP,\n                                     &quot;epic_color&quot;: String(20),\n                                     &quot;epic_name&quot;: String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><code>merge_epic_metadata</code>: Due to the nature of the JIRA REST API, some metadata is missing per issue. If you're interested, the data missing revolves around <strong>Epics</strong>: JIRA's REST API does not include the <em>Epic Name</em> or <em>Epic Color</em> fields of linked epics.</li><li><code>upload_dataframe</code>: Uses Panda's <strong>to_sql()</strong> method to upload our DataFrame into a SQL table (our target happens to be PostgreSQL, so we pass <em>schema</em> here). To make things explicit, we set the data type of every column on upload.</li></ul><p>Well, let's see how we made out!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.03.51-PM.png\" class=\"kg-image\"><figcaption>A look at our resulting database table.</figcaption></figure><!--kg-card-end: image--><p>Whoaaa nelly, we did it! With our data clean, we can now build something useful! Here's what I built:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.29.20-PM.png\" class=\"kg-image\"><figcaption>Fruits of our labor!</figcaption></figure><!--kg-card-end: image--><p>There we have it: a pipeline that takes a bunch of messy data, cleans it, and puts it somewhere else for proper use.</p><p>If you're interested in how we created the frontend for our Kanban board, check out our series on <a href=\"https://hackersandslackers.com/series/graphql-hype/\">building features with GraphQL</a>. For the source code, check out the <a href=\"https://github.com/toddbirchard/jira-database-etl\">Github repository</a>.</p>","url":"https://hackersandslackers.com/building-an-etl-pipeline-from-jira-to-postgresql/","uuid":"23647abe-9b47-4f58-8206-cff1fb2ae891","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c95e08ef654036aa06c6a02"}},{"node":{"id":"Ghost__Post__5c82cfe75af763016e85082e","title":"Working With GraphQL Fragments and Mutations","slug":"creating-updating-and-deleting-data-via-graphql-mutations","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/graphql-mutations-1-3.jpg","excerpt":"Make your GraphQL queries more dynamic with Fragments, plus get started with Mutations.","custom_excerpt":"Make your GraphQL queries more dynamic with Fragments, plus get started with Mutations.","created_at_pretty":"08 March, 2019","published_at_pretty":"19 March, 2019","updated_at_pretty":"20 March, 2019","created_at":"2019-03-08T15:26:15.000-05:00","published_at":"2019-03-19T16:34:38.000-04:00","updated_at":"2019-03-20T18:39:27.000-04:00","meta_title":"Working With GraphQL Fragments and Mutations | Hackers and Slackers","meta_description":"Make your GraphQL queries more dynamic with Fragments, plus get started with Mutations.","og_description":"Make your GraphQL queries more dynamic with Fragments, plus get started with Mutations.","og_image":"https://hackersandslackers.com/content/images/2019/03/graphql-mutations-1-3.jpg","og_title":"Working With GraphQL Fragments and Mutations","twitter_description":"Make your GraphQL queries more dynamic with Fragments, plus get started with Mutations.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/graphql-mutations-1-2.jpg","twitter_title":"Working With GraphQL Fragments and Mutations","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"Last week we encountered a genuine scenario when working with GraphQL clients.\nWhen building real applications consuming data via GraphQL, we usually don't\nknow precisely the query we're going to want to run at runtime. Imagine a user\ncruising through your application, setting preferences, and arriving at core\npieces of functionality under a content which is specific only to them. Say\nwe're building a GrubHub knockoff (we hate profits and love entering\nimpenetrable parts of the market, it's not that uncommon really.) At its core,\nthe information we're serving will always be restaurants; we'll always want to\nreturn things like the restaurant address, name, rating, etc. Because we want\nour app to be intelligent, this means that circumstances in which User 1  makes\na query are vastly different than User 2. Aside from the obvious facts (residing\nin different locales), perhaps there's more metadata we can leverage from User 1\n's longterm app usage, versus User 2  who is a total noob to our knockoff app.\n\nYet, the information we're serving will always be restaurants. There's a core\nquery being reused at the heart of our requests: we need to be dynamic enough to\naccount for the fact that User 1  has checked off 13 different cuisines and\nstrict delivery time windows, whereas User 2  doesn't give a shit. User 2  just\nwants pizza.\n\nThis is where GraphQL Fragments  come in to play. We've already seen how we can\npass variables through our queries to receive contextual data: the next step is\ncreating blocks of reusable code which may never change, which become the\nfoundational building blocks of all future queries.\n\nWhen to Use Fragments\nBack to our JIRA example, I demonstrated precisely the sort of thing one should\nnever do: making more than one GraphQL request to serve a single purpose.\n\nTo recap, we're pulling in JIRA issues to a Kanban board. Our board has 4\ncolumns: one per \"status.\" Here's a god-awful way of hardcoding a query like\nthat:\n\nquery JiraIssuesByStatus($project: String, $status: String) {\n\tbacklog: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  todo: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  progress: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  done: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n }\n\nSeems like a lot of repetition, yeah? What if we could define chunks of queries\nto be reused to simplify things?\n\n# Write your query or mutation here\nfragment JiraFields on jiraIssue {\n  key\n  summary\n  epic_color\n  epic_name\n  status\n  priority_rank\n  priority_url\n  issuetype_name\n  issuetype_url\n  assignee_name\n  assignee_url \n}\n\nquery JiraIssuesViaFragments($project: String) {\n  backlog: jiraIssues(where: {status: \"Backlog\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  todo: jiraIssues(where: {status: \"To Do\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  progress: jiraIssues(where: {status: \"In Progress\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  done: jiraIssues(where: {status: \"Done\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n}\n\nProgress! Instead of reiterating the fields we want to query for each time, we\nset these once.  We do this by creating a fragment  named JiraFields  (naming\nconventions for fragments are totally up to you- these don't relate to\nanything). To make this easier to visualize, let's just look at the parts:\n\nfragment [GivenNameToYourFragment] on [DatamodelToQuery(SINGULAR)] {\n  [fields] \n}\n\nTake note of [nameOfDatamodelToQuery(SINGULAR)]. Our fragment will refer to data\nmodel in the singular syntax Â - this is important.\n\nOur New Query Using a Fragment\nAgain, let's simply what we're looking at:\n\nquery [GivenNameToYourQuery]($project: String) {\n  [subsetName]: [DatamodelToQuery(PLURAL)](where: {status: \"Backlog\", project: $project}) {\n    ...[GivenNameToYourFragment]\n  }\n}\n\n * [subsetName]  is the name of the embedded JSON object to be returned in the\n   response. The naming is up to us.\n * [DatamodelToQuery(PLURAL)]  contrasts the singular data model we specified in\n   our fragment.\n * Finally, ...[GivenNameToYourFragment]  is the syntax for dumping a fragment\n   into a query. Yes, the ...  is intentional.\n\nHere's how we managed to get on:\n\nNow we're talkin'.Implementing On The Client Side\nWith the big picture in hand, this is still all theoretical until we have some\nreal code making real dynamic queries. So which GraphQL client tools should we\nuse?!?! Sweet baby Jesus have mercy, I wish I had a straight answer.\n\nAs we all know, Apollo [https://github.com/apollographql/apollo-client]  is\ncrushing the game with their seemingly endless libraries doing... a lot of\nsimilar stuff? Then there's Prisma\n[https://www.prisma.io/docs/prisma-client/basic-data-access/reading-data-JAVASCRIPT-rsc2/]\n, the new hotshot looking to make a buck. But what about this repo\n[https://github.com/smooth-code/fraql]? It seems totally fine, but why won't it\nfreakin work?! And what about this Lokka [https://github.com/kadirahq/lokka] \nthing? Also, apparently you can just use node-fetch\n[https://www.apollographql.com/docs/react/advanced/fragments.html#fragment-matcher] \n anyway?\n\nFor somebody looking for simplicity, this gets very frustrating. Most clients\nare immediately concerned with integrating with React as fast as possible\n(totally understandable), but a small-town country boy like me just wants to\nstart with simple. I'm just trying to write a god damn tutorial!\n\nAnyway. The GraphQL ecosystem is was it is: we'd be foolish to think anything\nrelated to JavaScript could be cohesive or straightforward. Instead of wrestling\nwith that reality, now's as good a time as ever to move on to the part of\nGraphQL we've failed to speak of: modifying data.\n\nGraphQL Mutation Cheatsheet\nAny form of creating, changing, or deleting data in GraphQL falls under the\numbrella of mutations. The structure is similar to queries, except that we take\ndata in (presumably through variables) and spit out whichever fields you'd like\nto see as a result of that.\n\nCreating Records\nA functioning \"create\" mutation with the resulting response:\n\nSimple enough.And the mutation itself, just in case anybody is copy/pasting out\nthere:\n\nmutation CreateJiraIssue($key: String!, $summary: String!, $status: String!) {\n  createjiraissues(data: {key: $key, status: $status, summary: $summary}) {\n    key\n    status\n    summary\n  }\n}\n\nUpdating Records\nWe can update records (aka nodes) by specifying the target node using where: {},\nand the data to be updated within data: {}\n\nSyntax is just like creating nodes, but with an added where:{} statement.mutation UpdateJIRAIssue($summary: String, $status: String) {\nÂ Â updatejiraIssue(data: {status: $status, summary: $summary}, where:{key: \"HACK-10\"}) {\nÂ Â Â Â key\nÂ Â Â Â status\nÂ Â Â Â summary\nÂ Â Â Â project\nÂ Â Â Â issuetype_name\nÂ Â Â Â epic_name\nÂ Â }\n}\n\nDeleting Records\nYou can even specify which fields you want returned from the node you're in the\nact of ruthlessly murdering! \n\nAnd with its final breath, the node shouted out \"Hack-9999, Backlog, Test issue\nwith GraphQL\" to please its master once last time.mutation DeleteJiraIssue ($key: String!){\n    deletejiraissues(where: {key: $key}){\n    key\n    status\n    summary\n  }\n}\n\nEnough For Now\nHopefully, I'm not the only one to have bee deceived by the simplicity of\nGraphQL's syntax at first glance. The minimalism of GraphQL queries and\nmutations would lead one to believe that they're simple to understand right off\nthe bat. The problem with that logic is the syntax is so  simplistic, that\nthere's hardly any way of telling what nearly identical queries or mutations\nmight do from one character to the next. Even in JSON, the combination of \nexplicit quotations, key:value relationships, and  comma-separation  affords us\na lot of inferred knowledge we take for granted.\n\nI'm not saying GraphQL is wrong, or painstakingly difficult to pick up, as much\nas it can easily be frustrating to newcomers (and rightfully so). As long as\npeople keep reading, I'll keep posting, so let's chip away at this thing week by\nweek.","html":"<p>Last week we encountered a genuine scenario when working with GraphQL clients. When building real applications consuming data via GraphQL, we usually don't know precisely the query we're going to want to run at runtime. Imagine a user cruising through your application, setting preferences, and arriving at core pieces of functionality under a content which is specific only to them. Say we're building a GrubHub knockoff (we hate profits and love entering impenetrable parts of the market, it's not that uncommon really.) At its core, the information we're serving will always be restaurants; we'll always want to return things like the restaurant address, name, rating, etc. Because we want our app to be intelligent, this means that circumstances in which <strong>User 1</strong> makes a query are vastly different than <strong>User 2</strong>. Aside from the obvious facts (residing in different locales), perhaps there's more metadata we can leverage from <strong>User 1</strong>'s longterm app usage, versus <strong>User 2</strong> who is a total noob to our knockoff app.</p><p>Yet, <em>the information we're serving will always be restaurants</em>. There's a core query being reused at the heart of our requests: we need to be dynamic enough to account for the fact that <strong>User 1</strong> has checked off 13 different cuisines and strict delivery time windows, whereas <strong>User 2</strong> doesn't give a shit. <strong>User 2</strong> just wants pizza.</p><p>This is where GraphQL <em><strong>Fragments</strong></em> come in to play. We've already seen how we can pass variables through our queries to receive contextual data: the next step is creating blocks of reusable code which may never change, which become the foundational building blocks of all future queries.</p><h2 id=\"when-to-use-fragments\">When to Use Fragments</h2><p>Back to our JIRA example, I demonstrated precisely the sort of thing one should never do: making more than one GraphQL request to serve a single purpose.</p><p>To recap, we're pulling in JIRA issues to a Kanban board. Our board has 4 columns: one per \"status.\" Here's a god-awful way of hardcoding a query like that:</p><!--kg-card-begin: code--><pre><code>query JiraIssuesByStatus($project: String, $status: String) {\n\tbacklog: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  todo: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  progress: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n  done: jiraIssues(where: {project: $project, status: $status}, orderBy: updated_DESC, first: 6) {\n\tkey\n    summary\n    epic_color\n    epic_name\n    status\n    priority_rank\n    priority_url\n    issuetype_name\n    issuetype_url\n    assignee_name\n    assignee_url \n   }\n }</code></pre><!--kg-card-end: code--><p>Seems like a lot of repetition, yeah? What if we could define chunks of queries to be reused to simplify things?</p><!--kg-card-begin: code--><pre><code># Write your query or mutation here\nfragment JiraFields on jiraIssue {\n  key\n  summary\n  epic_color\n  epic_name\n  status\n  priority_rank\n  priority_url\n  issuetype_name\n  issuetype_url\n  assignee_name\n  assignee_url \n}\n\nquery JiraIssuesViaFragments($project: String) {\n  backlog: jiraIssues(where: {status: \"Backlog\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  todo: jiraIssues(where: {status: \"To Do\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  progress: jiraIssues(where: {status: \"In Progress\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n  done: jiraIssues(where: {status: \"Done\", project: $project}, orderBy: updated_DESC, first: 6) {\n    ...JiraFields\n  }\n}</code></pre><!--kg-card-end: code--><p>Progress! Instead of reiterating the fields we want to query for each time, we set these <em>once.</em> We do this by creating a <code>fragment</code> named <strong>JiraFields</strong> (naming conventions for fragments are totally up to you- these don't relate to anything). To make this easier to visualize, let's just look at the parts:</p><!--kg-card-begin: code--><pre><code>fragment [GivenNameToYourFragment] on [DatamodelToQuery(SINGULAR)] {\n  [fields] \n}</code></pre><!--kg-card-end: code--><p>Take note of <code>[nameOfDatamodelToQuery(SINGULAR)]</code>. Our fragment will refer to data model in the singular syntax Â - this is important.</p><h3 id=\"our-new-query-using-a-fragment\">Our New Query Using a Fragment</h3><p>Again, let's simply what we're looking at:</p><!--kg-card-begin: code--><pre><code>query [GivenNameToYourQuery]($project: String) {\n  [subsetName]: [DatamodelToQuery(PLURAL)](where: {status: \"Backlog\", project: $project}) {\n    ...[GivenNameToYourFragment]\n  }\n}</code></pre><!--kg-card-end: code--><ul><li><code>[subsetName]</code> is the name of the embedded JSON object to be returned in the response. The naming is up to us.</li><li><code>[DatamodelToQuery(PLURAL)]</code> contrasts the singular data model we specified in our fragment.</li><li>Finally, <code>...[GivenNameToYourFragment]</code> is the syntax for dumping a fragment into a query. Yes, the <code>...</code> is intentional.</li></ul><p>Here's how we managed to get on:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-18-at-8.15.19-AM.png\" class=\"kg-image\"><figcaption>Now we're talkin'.</figcaption></figure><!--kg-card-end: image--><h2 id=\"implementing-on-the-client-side\">Implementing On The Client Side</h2><p>With the big picture in hand, this is still all theoretical until we have some real code making real dynamic queries. So which GraphQL client tools should we use?!?! Sweet baby Jesus have mercy, I wish I had a straight answer.</p><p>As we all know, <a href=\"https://github.com/apollographql/apollo-client\"><strong>Apollo</strong></a> is crushing the game with their seemingly endless libraries doing... a lot of similar stuff? Then there's <strong><a href=\"https://www.prisma.io/docs/prisma-client/basic-data-access/reading-data-JAVASCRIPT-rsc2/\">Prisma</a></strong>, the new hotshot looking to make a buck. But what about <a href=\"https://github.com/smooth-code/fraql\"><strong>this repo</strong></a>? It seems totally fine, but why won't it freakin work?! And what about this <strong><a href=\"https://github.com/kadirahq/lokka\">Lokka</a></strong> thing? Also, <a href=\"https://www.apollographql.com/docs/react/advanced/fragments.html#fragment-matcher\">apparently you can just use <strong>node-fetch</strong></a> anyway?</p><p>For somebody looking for simplicity, this gets very frustrating. Most clients are immediately concerned with integrating with React as fast as possible (totally understandable), but a small-town country boy like me just wants to start with simple. I'm just trying to write a god damn tutorial!</p><p>Anyway. The GraphQL ecosystem is was it is: we'd be foolish to think anything related to JavaScript could be cohesive or straightforward. Instead of wrestling with that reality, now's as good a time as ever to move on to the part of GraphQL we've failed to speak of: modifying data.</p><h2 id=\"graphql-mutation-cheatsheet\">GraphQL Mutation Cheatsheet</h2><p>Any form of creating, changing, or deleting data in GraphQL falls under the umbrella of mutations. The structure is similar to queries, except that we take data in (presumably through variables) and spit out whichever fields you'd like to see as a result of that.</p><h3 id=\"creating-records\">Creating Records</h3><p>A functioning \"create\" mutation with the resulting response:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-08-at-4.02.41-PM.png\" class=\"kg-image\"><figcaption>Simple enough.</figcaption></figure><!--kg-card-end: image--><p>And the mutation itself, just in case anybody is copy/pasting out there:</p><!--kg-card-begin: code--><pre><code>mutation CreateJiraIssue($key: String!, $summary: String!, $status: String!) {\n  createjiraissues(data: {key: $key, status: $status, summary: $summary}) {\n    key\n    status\n    summary\n  }\n}</code></pre><!--kg-card-end: code--><h3 id=\"updating-records\">Updating Records</h3><p>We can update records (aka nodes) by specifying the target node using <code>where: {}</code>, and the data to be updated within <code>data: {}</code></p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-19-at-1.23.28-PM.png\" class=\"kg-image\"><figcaption>Syntax is just like creating nodes, but with an added where:{} statement.</figcaption></figure><!--kg-card-end: image--><!--kg-card-begin: code--><pre><code>mutation UpdateJIRAIssue($summary: String, $status: String) {\nÂ Â updatejiraIssue(data: {status: $status, summary: $summary}, where:{key: \"HACK-10\"}) {\nÂ Â Â Â key\nÂ Â Â Â status\nÂ Â Â Â summary\nÂ Â Â Â project\nÂ Â Â Â issuetype_name\nÂ Â Â Â epic_name\nÂ Â }\n}</code></pre><!--kg-card-end: code--><h3 id=\"deleting-records\">Deleting Records</h3><p>You can even specify which fields you want returned from the node you're in the act of ruthlessly murdering! </p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-08-at-4.02.20-PM.png\" class=\"kg-image\"><figcaption>And with its final breath, the node shouted out \"Hack-9999, Backlog, Test issue with GraphQL\" to please its master once last time.</figcaption></figure><!--kg-card-end: image--><!--kg-card-begin: code--><pre><code>mutation DeleteJiraIssue ($key: String!){\n    deletejiraissues(where: {key: $key}){\n    key\n    status\n    summary\n  }\n}</code></pre><!--kg-card-end: code--><h3 id=\"enough-for-now\">Enough For Now</h3><p>Hopefully, I'm not the only one to have bee deceived by the simplicity of GraphQL's syntax at first glance. The minimalism of GraphQL queries and mutations would lead one to believe that they're simple to understand right off the bat. The problem with that logic is the syntax is <em>so</em> simplistic, that there's hardly any way of telling what nearly identical queries or mutations might do from one character to the next. Even in JSON, the combination of <strong>explicit quotations</strong>, <strong>key:value relationships</strong>, and<strong> comma-separation</strong> affords us a lot of inferred knowledge we take for granted.</p><p>I'm not saying GraphQL is wrong, or painstakingly difficult to pick up, as much as it can easily be frustrating to newcomers (and rightfully so). As long as people keep reading, I'll keep posting, so let's chip away at this thing week by week.</p><p></p>","url":"https://hackersandslackers.com/creating-updating-and-deleting-data-via-graphql-mutations/","uuid":"a042692b-1812-49a1-a2fb-c0bd97973edf","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c82cfe75af763016e85082e"}},{"node":{"id":"Ghost__Post__5c654f34eab17b74dbf2d2c0","title":"Welcome to SQL 4: Aggregate Functions","slug":"welcome-to-sql-4-aggregate-functions","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","custom_excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","created_at_pretty":"14 February, 2019","published_at_pretty":"14 March, 2019","updated_at_pretty":"17 March, 2019","created_at":"2019-02-14T06:21:24.000-05:00","published_at":"2019-03-14T03:10:00.000-04:00","updated_at":"2019-03-17T17:25:34.000-04:00","meta_title":"Welcome to SQL 4: Aggregate Functions | Hackers and Slackers","meta_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","og_title":"Welcome to SQL 4: Aggregate Functions","twitter_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","twitter_title":"Welcome to SQL 4: Aggregate Functions","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Aggregate functions in SQL are super dope. When combining these functions with\nclauses such as GROUP BY  and HAVING, we discover ways to view our data from\ncompletely new perspectives. Instead of looking at the same old endless flat\ntable, we can use these functions to give us entirely new insights; aggregate\nfunctions help us to understand bigger-picture things.  Those things might\ninclude finding outliers in datasets, or simply figuring out which employee with\na family to feed should be terminated, based on some arbitrary metric such as\nsales numbers.\n\nWith the basics of JOINs under our belts, this is when SQL starts feel really,\nreally powerful. Our plain two-dimensional tables suddenly gain this power to be\ncombined, aggregated, folded on to themselves, expand infinitely outward as the\nuniverse itself, and even transcend into the fourth dimension.*\n\n*Needs citationOur Base Aggregation Functions\nFirst up, let's see what we mean by \"aggregate functions\" anyway. These simple\nfunctions provide us with a way to mathematically quantify what exactly is in\nour database. Aggregate functions are performed on table columns to give us the\nmake-up of said column. On their own, they seem quite simple:\n\n * AVG: The average of a set of values in a column.\n * COUNT: Number of rows a column contains in a specified table or view.\n * MIN: The minimum value in a set of values.\n * MAX: The maximum value in a set of values.\n * SUM: The sum of values.\n\nDISTINCT Aggregations\nA particularly useful way of using aggregate functions on their own is when we'd\nlike to know the number of DISTINCT  values. While aggregate values take all\nrecords into account, using DISTINCT  limits the data returned to specifically\nrefer to unique values. COUNT(column_name)  will return the number of all\nrecords in a column, where COUNT(DISTINCT column_name)  will ignore counting\nrecords where the value in the counted column is repeated.\n\nUsing GROUP BY\nThe GROUP BY  statement is often used with aggregate functions (COUNT, MAX, MIN,\nSUM, AVG) to group the result-set by one or more columns.\n\nTo demonstrate how aggregate functions work moving forward, I'll be using a\nfamiliar database: the database which contains all the content for this very\nblog. Let's look at how we can use aggregate functions to find which authors\nhave been posting most frequently:\n\nSELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n\n\nAnd the result:\n\nCount\n author_id\n 102\n 1\n 280\n 5c12c3821345c22dced9f591\n 17\n 5c12c3821345c22dced9f592\n 5\n 5c12c3821345c22dced9f593\n 2\n 5c12c3821345c22dced9f594\n 2\n 5c12c3821345c22dced9f595\n Oh look, a real-life data problem to solve! It seems like authors are\nrepresented in Ghost's posts  table simply by their IDs. This isn't very useful.\nLuckily, we've already learned enough about JOINs\n[https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/] \n to know we can fill in the missing information from the users  table!\n\nSELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n\n\nLet's see the results this time around:\n\nCount\n author_id\n 280\n Matthew Alhonte\n 102\n Todd Birchard\n 17\n Max Mileaf\n 5\n Ryan Rosado\n 2\n Graham Beckley\n 2\n David Aquino\n Now that's more like it! Matt is crushing the game with his Lynx Roundup \nseries, with myself in second place. Max had respectable numbers for a moment\nbut has presumably moved on to other hobbies, such as living his life.\n\nFor the remainder, well, I've got nothing to say other than we're hiring. We\ndon't pay though. In fact, there's probably zero benefits to joining us.\n\nConditional Grouping With \"HAVING\"\nHAVING  is like the WHERE  of aggregations. We can't use WHERE  on aggregate\nvalues, so that's why HAVING  exists. HAVING  can't accept any conditional\nvalue, but instead it must accept a numerical conditional derived from a GROUP\nBY. Perhaps this would be easier to visualize in a query:\n\nSELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n\n\nIn this scenario, we want to see which tags on our blog have the highest number\nof associated posts. The query is very similar to the one we made previously,\nonly this time we have a special guest:\n\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\n\n\nThis usage of HAVING  only gives us tags which have ten posts or more. This\nshould clean up our report by letting Darwinism takes its course. Here's how it\nworked out:\n\ntag\n Count\n Roundup\n 263\n Python\n 80\n Machine Learning\n 29\n DevOps\n 28\n Data Science\n 28\n Software Development\n 27\n Data Engineering\n 23\n Excel\n 19\n SQL\n 18\n Architecture\n 18\n REST APIs\n 16\n #Adventures in Excel\n 16\n Pandas\n 15\n Flask\n 14\n Data Analysis\n 12\n JavaScript\n 12\n AWS\n 11\n MySQL\n 11\n As expected, Matt's roundup posts take the lead (and if we compare this to\nprevious data, we can see Matt has made a total of 17  non-Lynx posts: meaning\nMax and Matt are officially TIED).\n\nIf we hadn't included our HAVING  statement, this list would be much longer,\nfilled with tags nobody cares about. Thanks to explicit omission, now we don't\nneed to experience the dark depression that comes when confronting those sad\npathetic tags. Out of sight, out of mind.\n\nGet Creative\nAggregate functions aren't just about counting values. Especially in Data\nScience, these functions are critical  to drawing any statistical conclusions\nfrom data. That said, attention spans only last so long, and I'm not a\nscientist. Perhaps that can be your job.","html":"<p>Aggregate functions in SQL are super dope. When combining these functions with clauses such as <code>GROUP BY</code> and <code>HAVING</code>, we discover ways to view our data from completely new perspectives. Instead of looking at the same old endless flat table, we can use these functions to give us entirely new insights; aggregate functions help us to understand bigger-picture things.<em> </em>Those things might include finding outliers in datasets, or simply figuring out which employee with a family to feed should be terminated, based on some arbitrary metric such as sales numbers.</p><p>With the basics of <code>JOIN</code>s under our belts, this is when SQL starts feel really, really powerful. Our plain two-dimensional tables suddenly gain this power to be combined, aggregated, folded on to themselves, expand infinitely outward as the universe itself, and even transcend into the fourth dimension.*</p><!--kg-card-begin: html--><div style=\"color:grey; text-align: right; font-style: italic;\">\n    *Needs citation\n</div><!--kg-card-end: html--><h2 id=\"our-base-aggregation-functions\">Our Base Aggregation Functions</h2><p>First up, let's see what we mean by \"aggregate functions\" anyway. These simple functions provide us with a way to mathematically quantify what exactly is in our database. Aggregate functions are performed on table columns to give us the make-up of said column. On their own, they seem quite simple:</p><ul><li><code>AVG</code>: The average of a set of values in a column.</li><li><code>COUNT</code>: Number of rows a column contains in a specified table or view.</li><li><code>MIN</code>: The minimum value in a set of values.</li><li><code>MAX</code>: The maximum value in a set of values.</li><li><code>SUM</code>: The sum of values.</li></ul><h3 id=\"distinct-aggregations\">DISTINCT Aggregations</h3><p>A particularly useful way of using aggregate functions on their own is when we'd like to know the number of <code>DISTINCT</code> values. While aggregate values take all records into account, using <code>DISTINCT</code> limits the data returned to specifically refer to unique values. <code>COUNT(column_name)</code> will return the number of all records in a column, where <code>COUNT(DISTINCT column_name)</code> will ignore counting records where the value in the counted column is repeated.</p><h2 id=\"using-group-by\">Using GROUP BY</h2><p>The <code>GROUP BY</code> statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns.</p><p>To demonstrate how aggregate functions work moving forward, I'll be using a familiar database: the database which contains all the content for this very blog. Let's look at how we can use aggregate functions to find which authors have been posting most frequently:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>And the result:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n  <table>\n    <thead>\n      <tr>\n        <th>Count</th>\n        <th>author_id</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>102</td>\n        <td>1</td>\n      </tr>\n      <tr>\n        <td>280</td>\n        <td>5c12c3821345c22dced9f591</td>\n      </tr>\n      <tr>\n        <td>17</td>\n        <td>5c12c3821345c22dced9f592</td>\n      </tr>\n      <tr>\n        <td>5</td>\n        <td>5c12c3821345c22dced9f593</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f594</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f595</td>\n      </tr>\n    </tbody>\n  </table>\n</div><!--kg-card-end: html--><p>Oh look, a real-life data problem to solve! It seems like authors are represented in Ghost's <strong><em>posts</em></strong> table simply by their IDs. This isn't very useful. Luckily, we've <a href=\"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/\">already learned enough about JOINs</a> to know we can fill in the missing information from the <strong><em>users</em></strong> table!</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see the results this time around:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>Count</th>\n      <th>author_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>280</td>\n      <td>Matthew Alhonte</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>Todd Birchard</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>Max Mileaf</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Ryan Rosado</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Graham Beckley</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>David Aquino</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Now that's more like it! Matt is crushing the game with his <strong>Lynx Roundup</strong> series, with myself in second place. Max had respectable numbers for a moment but has presumably moved on to other hobbies, such as living his life.</p><p>For the remainder, well, I've got nothing to say other than we're hiring. We don't pay though. In fact, there's probably zero benefits to joining us.</p><h3 id=\"conditional-grouping-with-having\">Conditional Grouping With \"HAVING\"</h3><p><code>HAVING</code> is like the <code>WHERE</code> of aggregations. We can't use <code>WHERE</code> on aggregate values, so that's why <code>HAVING</code> exists. <code>HAVING</code> can't accept any conditional value, but instead it <em>must </em>accept a numerical conditional derived from a <code>GROUP BY</code>. Perhaps this would be easier to visualize in a query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>In this scenario, we want to see which tags on our blog have the highest number of associated posts. The query is very similar to the one we made previously, only this time we have a special guest:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">HAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\n</code></pre>\n<!--kg-card-end: markdown--><p>This usage of <code>HAVING</code> only gives us tags which have ten posts or more. This should clean up our report by letting Darwinism takes its course. Here's how it worked out:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>tag</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Roundup</td>\n      <td>263</td>\n    </tr>\n    <tr>\n      <td>Python</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <td>Machine Learning</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <td>DevOps</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Data Science</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Software Development</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <td>Data Engineering</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <td>Excel</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <td>SQL</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>Architecture</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>REST APIs</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>#Adventures in Excel</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>Pandas</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <td>Flask</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <td>Data Analysis</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>JavaScript</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>AWS</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <td>MySQL</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>As expected, Matt's roundup posts take the lead (and if we compare this to previous data, we can see Matt has made a total of <strong>17</strong> non-Lynx posts: meaning Max and Matt are officially TIED).</p><p>If we hadn't included our <code>HAVING</code> statement, this list would be much longer, filled with tags nobody cares about. Thanks to explicit omission, now we don't need to experience the dark depression that comes when confronting those sad pathetic tags. Out of sight, out of mind.</p><h3 id=\"get-creative\">Get Creative</h3><p>Aggregate functions aren't just about counting values. Especially in Data Science, these functions are <em>critical</em> to drawing any statistical conclusions from data. That said, attention spans only last so long, and I'm not a scientist. Perhaps that can be your job.</p>","url":"https://hackersandslackers.com/welcome-to-sql-4-aggregate-functions/","uuid":"f45c0ccc-3efc-4963-a236-a23db74c2e96","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654f34eab17b74dbf2d2c0"}},{"node":{"id":"Ghost__Post__5c85a8da181da30210ceca9d","title":"Serve Docker Containers With A Custom Domain and SSL","slug":"serve-docker-containers-with-custom-dns-and-ssl","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","excerpt":"Do even less work to deploy your Docker apps to production.","custom_excerpt":"Do even less work to deploy your Docker apps to production.","created_at_pretty":"11 March, 2019","published_at_pretty":"11 March, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-03-10T20:16:26.000-04:00","published_at":"2019-03-11T06:15:00.000-04:00","updated_at":"2019-04-09T15:05:49.000-04:00","meta_title":"Serve Docker Containers With A Domain and SSL | Hackers and Slackers","meta_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","og_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","og_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","og_title":"Serve Docker Containers With A Custom Domain and SSL","twitter_description":"Do even less work to deploy your Docker apps to production. Caddy is a Fast, cross-platform HTTP/2 web server with automatic SSL.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/caddy.jpg","twitter_title":"Serve Docker Containers With A Custom Domain and SSL","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"The past few years of software development and architecture has witnessed\nmultiple revolutions. The rise of containers, unruly package management\necosystems, and one-click-deployments holds an unspoken narrative: most people\nprobably donât care about how  things work beneath the top layer. Sure,\nadvancements in application infrastructure has undoubtedly made our lives\neasier. I suppose I find this lack of curiosity and unwillingness to dig deeper\ninto the innards, an unrelatable trait. Yet I digress.\n\nIâve never found web server configurations to be particularly difficult, but\napparently most consider this enough of a nuisance to make something even easier\nto use. Thatâs where I came across Caddy [https://caddyserver.com/].\n\nCaddy  is a web server and free SSL service in which most of the actual work\nhappens via their download GUI [https://caddyserver.com/download]. Itâs great.\nEven though I never expected us to reach a place where apt install nginx  and \napt install certbot  is considered too much of a burden, it only took a few\nminutes of wrestling with a Docker container running on a VPS that I realized\nthere was a better way.\n\nServe Anything With Caddy\nIn my particular example, the Docker container I was running produced an API\nendpoint. For some reason, this service forcefully insists that this endpoint is\nyour machineâs localhost, or it simply wonât work. While scoffing at vanity URLs\nfor APIs is fine, what isnât  fine is you canât assign an SSL certificate to an\nIP address. That means whichever app consuming your API will fail because your\napp surely has a cert of its own, and HTTPS > HTTP  API calls just arenât gonna\nhappen.\n\nCaddy trivializes SSL certs to the point where you might not notice youâve\nacquired one. Any host specified in a Caddyfile  immediately receives an SSL\ncert, but we'll get to that in a moment.\n\nCaddyâs download page is like a shopping cart for which things you might want\nyour web server to do. Proxies, CORS, you name it: just put it in the (free)\nshopping cart:\n\nSoon we won't even have to write code at all!Selecting your platform, plugins, and license  will provide you with a\nconvenient one-liner which downloads your exact package, unpacks, and installs\nit on your machine. For example, a Caddy installation for Linux with no bells or\nwhistles looks like this:\n\ncurl https://getcaddy.com | bash\n\n\nThis will install Caddy, which leaves only some trivial configuration before\nyou're up and running.\n\nConfiguring Your Caddyfile\nCaddy is configured via what is simply named Caddyfile, a file which can\nconveniently live in your project folder, as opposed to a distant land called \n/etc/nginx/sites-enabled. Go ahead and create your Caddy file.\n\nThe first line in our Caddyfile config is both simple and magic. It contains\nmerely the domain youâre intending to listen on, such something like \nhackersandslackers.com.  No matter what else happens in your config, the mere\nexistence of this line will generate an SSL cert for you when you run caddy.\n\nYou can serve content via any method that Nginx  or Apache  can, albeit much\neasier. A few examples:\n\n * root path/to/project  points your DNS to serve HTTP out a root folder.\n * websocket path/to/socket command  will serve an application via the specified\n   websocket.\n * rewrite [/original/folder/path] [/new/folder/path]  will reroute internal\n   requests made to origin A to origin B,\n\nThe point Iâm trying to make here is that no matter what your desired\nconfiguration might be, itâs dead simple and likely wonât exceed more than 5\nlines.\n\nServing Our Docker Container via Proxy\nIf youâre using Node, chances are youâre going for a proxy configuration. In my\ncase I had no choice: I somehow needed to interact with an HTTP  url, while also\npassing the authentication headers necessary to make the app work. Luckily, this\nis trivial:\n\nexample.com\n\nproxy example.com proxy example.com localhost:4466/my_api/prod {\n transparent\n} \n\nerrors proxieserrors.log\n\nYes, really. Our proxy  block simply creates a proxy from  example.com, and\nserves localhost:4466/my_api/prod.\n\ntransparent  is a magic phrase which passes through all our headers to the\ntarget. It's shorthand for the following:\n\nheader_upstream Host {host}\nheader_upstream X-Real-IP {remote}\nheader_upstream X-Forwarded-For {remote}\nheader_upstream X-Forwarded-Port {server_port}\nheader_upstream X-Forwarded-Proto {scheme}\n\nDespite our Docker app requiring an authentication token to work hitting \nexample.com  will still result in a working endpoint thanks to the headers we're\npushing upstream.\n\nI even went the extra mile to include errors proxieserrors.log  as a way to log\nerrors. I didn't even need to. I only even got two errors total: Caddy works\nobnoxiously well.\n\nIn case you need anything more, Iâd recommend reading the documentation\n[https://caddyserver.com/docs/proxy]. Even then, this basically summarizes the\nthings you can potentially configure:\n\nproxy from to... {\n\tpolicy name [value]\n\tfail_timeout duration\n\tmax_fails integer\n\tmax_conns inâteger\n\ttry_duration duration\n\ttry_interval duration\n\thealth_check path\n\thealth_check_port port\n\thealth_check_interval interval_duration\n\thealth_check_timeout timeout_duration\n\tfallback_delay delay_duration\n\theader_upstream name value\n\theader_downstream name value\n\tkeepalive number\n\ttimeout duration\n\twithout prefix\n\texcept ignored_paths...\n\tupstream to\n\tca_certificates certs...\n\tinsecure_skip_verify\n\tpreset\n}\n\nRun Caddy And Never Worry About It Again\nSaving your Caddyfile  and running $ caddy  will issue your cert, and run Caddy\nas a process. This will result in a dialogue letting Â you know that Caddy is\nlistening on both ports 80  and 443.\n\nCaddy wonât run as a background process by default. To do this, simply use the\ncommand $ nohup caddy &  and you're good to go.","html":"<p>The past few years of software development and architecture has witnessed multiple revolutions. The rise of containers, unruly package management ecosystems, and one-click-deployments holds an unspoken narrative: most people probably donât care about <em>how</em> things work beneath the top layer. Sure, advancements in application infrastructure has undoubtedly made our lives easier. I suppose I find this lack of curiosity and unwillingness to dig deeper into the innards, an unrelatable trait. Yet I digress.</p><p>Iâve never found web server configurations to be particularly difficult, but apparently most consider this enough of a nuisance to make something even easier to use. Thatâs where I came across <a href=\"https://caddyserver.com/\" rel=\"noopener\">Caddy</a>.</p><p><strong>Caddy</strong> is a web server and free SSL service in which most of the actual work happens via their <a href=\"https://caddyserver.com/download\" rel=\"noopener\">download GUI</a>. Itâs great. Even though I never expected us to reach a place where <code>apt install nginx</code> and <code>apt install certbot</code> is considered too much of a burden, it only took a few minutes of wrestling with a Docker container running on a VPS that I realized there was a better way.</p><h2 id=\"serve-anything-with-caddy\">Serve Anything With Caddy</h2><p>In my particular example, the Docker container I was running produced an API endpoint. For some reason, this service forcefully insists that this endpoint is your machineâs <em>localhost</em>, or it simply wonât work. While scoffing at vanity URLs for APIs is fine, what <em>isnât</em> fine is <em>you canât assign an SSL certificate to an IP address. </em>That means whichever app consuming your API will fail because your app <em>surely </em>has a cert of its own, and <strong>HTTPS &gt; HTTP</strong> API calls just arenât gonna happen.</p><p>Caddy trivializes SSL certs to the point where you might not notice youâve acquired one. Any host specified in a <code>Caddyfile</code> immediately receives an SSL cert, but we'll get to that in a moment.</p><p>Caddyâs download page is like a shopping cart for which things you might want your web server to do. Proxies, CORS, you name it: just put it in the (free) shopping cart:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/caddy-download.png\" class=\"kg-image\"><figcaption>Soon we won't even have to write code at all!</figcaption></figure><!--kg-card-end: image--><p>Selecting your <strong>platform</strong>, <strong>plugins</strong>, and <strong>license</strong> will provide you with a convenient one-liner which downloads your exact package, unpacks, and installs it on your machine. For example, a Caddy installation for Linux with no bells or whistles looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">curl https://getcaddy.com | bash\n</code></pre>\n<!--kg-card-end: markdown--><p>This will install Caddy, which leaves only some trivial configuration before you're up and running.</p><h2 id=\"configuring-your-caddyfile\">Configuring Your Caddyfile</h2><p>Caddy is configured via what is simply named <code>Caddyfile</code>, a file which can conveniently live in your project folder, as opposed to a distant land called <code>/etc/nginx/sites-enabled</code>. Go ahead and create your Caddy file.</p><p>The first line in our Caddyfile config is both simple and magic. It contains merely the domain youâre intending to listen on, such something like <em>hackersandslackers.com.</em> No matter what else happens in your config, the mere existence of this line will generate an SSL cert for you when you run caddy.</p><p>You can serve content via any method that <strong>Nginx</strong> or <strong>Apache</strong> can, albeit much easier. A few examples:</p><ul><li><code>root path/to/project</code> points your DNS to serve HTTP out a root folder.</li><li><code>websocket path/to/socket command</code> will serve an application via the specified websocket.</li><li><code>rewrite [/original/folder/path] [/new/folder/path]</code> will reroute internal requests made to origin A to origin B,</li></ul><p>The point Iâm trying to make here is that no matter what your desired configuration might be, itâs dead simple and likely wonât exceed more than 5 lines.</p><h2 id=\"serving-our-docker-container-via-proxy\">Serving Our Docker Container via Proxy</h2><p>If youâre using Node, chances are youâre going for a proxy configuration. In my case I had no choice: I somehow needed to interact with an <strong>HTTP</strong> url, while also passing the authentication headers necessary to make the app work. Luckily, this is trivial:</p><!--kg-card-begin: code--><pre><code>example.com\n\nproxy example.com proxy example.com localhost:4466/my_api/prod {\n transparent\n} \n\nerrors proxieserrors.log</code></pre><!--kg-card-end: code--><p>Yes, really. Our <code>proxy</code> block simply creates a proxy <em>from</em> <code>example.com</code>, and serves <code>localhost:4466/my_api/prod</code>.</p><p><code>transparent</code> is a magic phrase which passes through all our headers to the target. It's shorthand for the following:</p><!--kg-card-begin: code--><pre><code>header_upstream Host {host}\nheader_upstream X-Real-IP {remote}\nheader_upstream X-Forwarded-For {remote}\nheader_upstream X-Forwarded-Port {server_port}\nheader_upstream X-Forwarded-Proto {scheme}</code></pre><!--kg-card-end: code--><p>Despite our Docker app requiring an authentication token to work hitting <code>example.com</code> will still result in a working endpoint thanks to the headers we're pushing upstream.</p><p>I even went the extra mile to include <code>errors proxieserrors.log</code> as a way to log errors. I didn't even need to. I only even got two errors total: Caddy works obnoxiously well.</p><p>In case you need anything more, Iâd recommend reading <a href=\"https://caddyserver.com/docs/proxy\" rel=\"noopener\">the documentation</a>. Even then, this basically summarizes the things you can potentially configure:</p><!--kg-card-begin: code--><pre><code>proxy from to... {\n\tpolicy name [value]\n\tfail_timeout duration\n\tmax_fails integer\n\tmax_conns inâteger\n\ttry_duration duration\n\ttry_interval duration\n\thealth_check path\n\thealth_check_port port\n\thealth_check_interval interval_duration\n\thealth_check_timeout timeout_duration\n\tfallback_delay delay_duration\n\theader_upstream name value\n\theader_downstream name value\n\tkeepalive number\n\ttimeout duration\n\twithout prefix\n\texcept ignored_paths...\n\tupstream to\n\tca_certificates certs...\n\tinsecure_skip_verify\n\tpreset\n}</code></pre><!--kg-card-end: code--><h2 id=\"run-caddy-and-never-worry-about-it-again\">Run Caddy And Never Worry About It Again</h2><p>Saving your <code>Caddyfile</code> and running <code>$ caddy</code> will issue your cert, and run Caddy as a process. This will result in a dialogue letting Â you know that Caddy is listening on both ports <code>80</code> and <code>443</code>.</p><p>Caddy wonât run as a background process by default. To do this, simply use the command <code>$ nohup caddy &amp;</code> and you're good to go.</p>","url":"https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/","uuid":"5b609f96-d76d-4b8b-943e-d470ee414d97","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c85a8da181da30210ceca9d"}},{"node":{"id":"Ghost__Post__5c838ee05af763016e85085b","title":"Building a Client For Your GraphQL API","slug":"interacting-with-your-graphql-api","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","excerpt":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","custom_excerpt":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","created_at_pretty":"09 March, 2019","published_at_pretty":"09 March, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-03-09T05:01:04.000-05:00","published_at":"2019-03-09T15:43:14.000-05:00","updated_at":"2019-04-14T05:36:35.000-04:00","meta_title":"Building a Client For Your GraphQL API | Hackers and Slackers","meta_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","og_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","og_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","og_title":"Building a Client For Your GraphQL API","twitter_description":"Now that we have an understanding of GraphQL queries and API setup, it's time to get that data.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/graphqlclient.jpg","twitter_title":"Building a Client For Your GraphQL API","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"If you had the pleasure of joining us last time, we had just completed a crash\ncourse in structuring GraphQL Queries\n[https://hackersandslackers.com/writing-your-first-graphql-queries/]. As much we\nall love studying abstract queries within the confines of a playground\nenvironment, the only real way to learn anything to overzealously attempt to\nbuild something way out of our skill level. Thus, we're going to shift gears and\nactually make something  with all the dry technical knowledge we've accumulated\nso far. Hooray!\n\nData Gone Wild: Exposing Your GraphQL Endpoint\nIf you're following along with Prisma as your GraphQL service, the endpoint for\nyour API defaults to [your_ip_address]:4466. What's more, you've probably\nnoticed it is publicly accessible. THIS IS VERY BAD.  Your server has full\nread/write access to whichever database you configured with it... if anybody\nfinds your endpoint hanging out in a Github commit somewhere, you've just lost\nyour database and everything in it. You're pretty much Equifax, and you should\nfeel bad.\n\nPrisma has a straightforward solution. While SSHed into\nwherever-you-set-up-your-server, check out the prisma.yaml  file which was\ngenerated as a result of when we first started getting set up. You know, this\ndirectory:\n\nmy-prisma\nâââ datamodel.prisma\nâââ docker-compose.yml\nâââ generated\nâ   âââ prisma-client\nâ       âââ index.ts\nâ       âââ prisma-schema.ts\nâââ prisma.yml\n\n\nprisma.yaml  seems inglorious, but that's because it's hiding a secret; or\nshould I say, it's not  hiding a secret! Hah!... (you know, like, credentials).\nAnyway. \n\nIn order to enable authorization on our endpoint, we need to add a secret to our\n prisma.yaml  file. The secret can be anything you like; this is simply a string\nwhich will be used to generate a token. Add a line which defines secret  like\nthis:\n\nendpoint: http://localhost:4466\ndatamodel: datamodel.prisma\nsecret: HIIHGUTFTUY$VK$G$YI&TUYCUY$DT$\n\ngenerate:\n  - generator: typescript-client\n    output: ./generated/prisma-client/\n\n\nWith your secret stashed away safely, the Prisma CLI can now use this secret to\ncreate the authentication token. This will be the value we pass in the headers\nof our requests to actually interact with our Prisma server remotely.\n\nType $ prisma token  in your project directory to get the work of art:\n\n$ prisma token\neyJhbGciOiJIUzI1NiIsInUYGFUJGSFKHFGSJFKSFJKSFGJdfSwiaWF0IjoxNTUyMTYwMDQ5LCJleHAiOjE1NTI3NjQ4NDl9.xrubUg_dRc93bqqR4f6jGt-KvQRS2Xq6lRi0a0uw-C0\n\n\nNice; believe it or not, that was the \"hard\" part.\n\nEXTRA CREDIT: Assign a DNS Record and Apply a Security Certificate\nIf really want to, you could already query against your insecure IP address and\nstart receiving some information. That said, making HTTP  requests as such from \nHTTPS  origins will fail. Not only that, but you kind of look shitty for not\neven bothering to name your API, much less apply a free SSL certificate. For the\neasiest possible way to do this, see our post on using Caddy as an HTTP server\n[https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/]\n.\n\nBuilding a Javascript Client to Consume Our API\nWith our API nice and secure, we can start hitting this baby from wherever we\nwant... as long as it's a Node app. We'll start by requiring two packages:\n\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\n\nGraphQLClient  is the magic behind our client- it's everything. It also happens\nto be very similar to existing npm  libraries for making requests, such as \nnode-fetch [https://hackersandslackers.com/making-api-requests-with-nodejs/].\n\nWe'll also leverage the dotenv  library to make sure our API endpoint  and \nBearer token  stay out of source code. Try not to be Equifax whenever possible. \ndotenv  allows us to load sensitive values from a .env  file. Just in case you\nneed a refresher, that file should look like this:\n\nNODE_ENV=Production\nENDPOINT=https://yourapiendpoint.com\nAUTH=Bearer eyJhbGciOBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHGUYFIERIBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHZl-UGnMrOk3w\n\nInitialize The GraphQL Client\nI like to set up a one-time client for our API that we can go back and reuse if\nneed be. After pulling the API endpoint and token from our .env  file, setting\nup the client is easy:\n\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n\nEMERGENCY MEETING: EVERYBODY HUDDLE UP\nOh I'm sorry, were you focusing on development? Unfortunately for you, I spent 8\nyears as a product manager, and I love  stopping everything suddenly to call\nemergency meetings.\n\nReal talk though, let's think back to the JIRA Kanban board example we've been\nusing for the last two posts. If you recall, we're going to write a query that\npopulates a 4-column Kanban board. The board represents a project (in this case,\n Hackers and Slackers) and each column represents a status  of ticket, like\nthis:\n\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n\nWe've previously established that GraphQL queries are friendly to drop-in\nvariables. Let's use this to build some logic into our client, as opposed to\nhardcoding a massive query, which is really just the same 4 queries stitched\ntogether. Here's what a query to populate a single JIRA column looks like:\n\n// Structured query\nconst query = `\n    query JiraIssuesByStatus($project: String, $status: String) {\n         jiraIssues(where: {project: $project, status: $status}, \n         orderBy: timestamp_DESC, \n         first: 6) {\n            key\n            summary\n            epic\n            status\n            project\n            priority\n            issuetype\n            timestamp\n            }\n         }\n       `\n\nWe're passing both the project  and the issue status  as variables to our query.\nWe can make things a bit dynamic here by looping through our statuses and\nexecuting this query four times: each time resulting in a callback filling the\nappropriate columns with JIRA issues.\n\nThis approach is certainly less clunky and more dynamic than a hardcoded query.\nThat said, this still  isn't the best solution. Remember: the strength of\nGraphQL is the ability to get obscene amounts of data across complex\nrelationships in a single call. The best approach here would probably be to\nbuild the query string itself dynamically using fragments,  which we'll review\nin the next post.Game On: Our Client in Action\nconst { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n// Structured query\nconst query = `\n   query JiraIssuesByStatus($project: String, $status: String) {\n      jiraIssues(where: {project: $project, status: $status}, orderBy: timestamp_DESC, first: 6) {\n         key\n         summary\n         epic\n         status\n         project\n         priority\n         issuetype\n         timestamp\n        }\n      }\n    `;\n\n// All Possible Issue Statuses\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n// Execute a query per issue status\nfor(var i = 0; i < statuses.length; i++){\n  var variables = {\n    project: \"Hackers and Slackers\",\n    status: statuses[i]\n  }\n\n  client.request(query, variables).then((data) => {\n    console.log(data)\n  }).catch(err => {\n    console.log(err.response.errors) // GraphQL response errors\n    console.log(err.response.data) // Response data if available\n  });\n}\n\n\nWorks like a charm. We only had one endpoint, only had to set one header, and\ndidn't spend any time reading through hundreds of pages of documentation to\nfigure out which combination of REST API endpoint, parameters, and methods\nactually get us what we want. It's almost as if we're writing SQL now, except...\nit looks a lot more like... NoSQL. Thanks for the inspiration, MongoDB! Hope\nthat whole selling-open-source-software  thing works out.\n\nOh, and of course, here were the results of my query:\n\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550194791,\n       project: 'Hackers and Slackers',\n       key: 'HACK-778',\n       epic: 'Code snippets',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'HLJS: set indentation level' },\n     { priority: 'Medium',\n       timestamp: 1550194782,\n\n       project: 'Hackers and Slackers',\n       key: 'HACK-555',\n       epic: 'Optimization',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Minify Babel' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-785',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Unix commands for data' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-251',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Using Ghost\\'s content filtering' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-302',\n       epic: 'Widgets',\n       status: 'Backlog',\n       issuetype: 'Integration',\n       summary: 'Discord channel signups ' },\n     { priority: 'Low',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-336',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Linux: Configuring your server to send SMTP emails' } ] }\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550224412,\n       project: 'Hackers and Slackers',\n       key: 'HACK-769',\n       epic: 'Projects Page',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'Fix projects dropdown' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-710',\n       epic: 'Lynx',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Implement auto text synopsis for Lynx posts' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-777',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Redesign footer to be informative; link-heavy' },\n     { priority: 'Highest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-779',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Changeover from cloudinary to DO' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-780',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Make mobile post title bold' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-781',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'This post consistently doesnât work on mobile' } ] }\n{ jiraIssues:\n   [ { priority: 'Low',\n       timestamp: 1550223282,\n       project: 'Hackers and Slackers',\n       key: 'HACK-782',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary:\n        'Lynx: on mobile, instead of full link, show domainname.com/...' },\n     { priority: 'High',\n       timestamp: 1550194799,\n       project: 'Hackers and Slackers',\n       key: 'HACK-774',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'New Widget: Next/Previous article in series' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-395',\n       epic: 'Page Templates',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Create fallback image for posts with no image' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-756',\n       epic: 'Newsletter',\n       status: 'To Do',\n       issuetype: 'Major Functionality',\n       summary: 'Automate newsletter' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-775',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Data & Analytics',\n       summary: 'Update issuetype icons' },\n     { priority: 'Lowest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-776',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Add fork icon to repos' } ] }\n{ jiraIssues:\n   [ { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-784',\n       epic: 'New Post',\n       status: 'In Progress',\n       issuetype: 'Content',\n       summary: 'Welcome to SQL part1' } ] }\n\n\nBefore we say \"GG, 2ez, 1v1 me,\" know that we're  only getting started \nuncovering what GraphQL can do. It's not all just creating and deleting records\neither; we're talking full-on database JOIN equivalent type shit here. Stick\naround folks, the bandwagon's just getting warmed up.","html":"<p>If you had the pleasure of joining us last time, we had just completed a <a href=\"https://hackersandslackers.com/writing-your-first-graphql-queries/\">crash course in structuring GraphQL Queries</a>. As much we all love studying abstract queries within the confines of a playground environment, the only real way to learn anything to overzealously attempt to build something way out of our skill level. Thus, we're going to shift gears and actually <em>make something</em> with all the dry technical knowledge we've accumulated so far. Hooray!</p><h2 id=\"data-gone-wild-exposing-your-graphql-endpoint\">Data Gone Wild: Exposing Your GraphQL Endpoint</h2><p>If you're following along with Prisma as your GraphQL service, the endpoint for your API defaults to <code>[your_ip_address]:4466</code>. What's more, you've probably noticed it is publicly accessible. <strong>THIS IS VERY BAD.</strong> Your server has full read/write access to whichever database you configured with it... if anybody finds your endpoint hanging out in a Github commit somewhere, you've just lost your database and everything in it. You're pretty much Equifax, and you should feel bad.</p><p>Prisma has a straightforward solution. While SSHed into wherever-you-set-up-your-server, check out the <code>prisma.yaml</code> file which was generated as a result of when we first started getting set up. You know, this directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-prisma\nâââ datamodel.prisma\nâââ docker-compose.yml\nâââ generated\nâ   âââ prisma-client\nâ       âââ index.ts\nâ       âââ prisma-schema.ts\nâââ prisma.yml\n</code></pre>\n<!--kg-card-end: markdown--><p><code>prisma.yaml</code> seems inglorious, but that's because it's hiding a secret; or should I say, it's <em>not</em> hiding a secret! Hah!... (you know, like, credentials). Anyway. </p><p>In order to enable authorization on our endpoint, we need to add a secret to our <code>prisma.yaml</code> file. The secret can be anything you like; this is simply a string which will be used to generate a token. Add a line which defines <code>secret</code> like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">endpoint: http://localhost:4466\ndatamodel: datamodel.prisma\nsecret: HIIHGUTFTUY$VK$G$YI&amp;TUYCUY$DT$\n\ngenerate:\n  - generator: typescript-client\n    output: ./generated/prisma-client/\n</code></pre>\n<!--kg-card-end: markdown--><p>With your secret stashed away safely, the <strong>Prisma CLI </strong>can now use this secret to create the authentication token. This will be the value we pass in the headers of our requests to actually interact with our Prisma server remotely.</p><p>Type <code>$ prisma token</code> in your project directory to get the work of art:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ prisma token\neyJhbGciOiJIUzI1NiIsInUYGFUJGSFKHFGSJFKSFJKSFGJdfSwiaWF0IjoxNTUyMTYwMDQ5LCJleHAiOjE1NTI3NjQ4NDl9.xrubUg_dRc93bqqR4f6jGt-KvQRS2Xq6lRi0a0uw-C0\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice; believe it or not, that was the \"hard\" part.</p><h3 id=\"extra-credit-assign-a-dns-record-and-apply-a-security-certificate\">EXTRA CREDIT: Assign a DNS Record and Apply a Security Certificate</h3><p>If really want to, you could already query against your insecure IP address and start receiving some information. That said, making <strong>HTTP</strong> requests as such from <strong>HTTPS</strong> origins will fail. Not only that, but you kind of look shitty for not even bothering to name your API, much less apply a free SSL certificate. For the easiest possible way to do this, see our post on <a href=\"https://hackersandslackers.com/serve-docker-containers-with-custom-dns-and-ssl/\">using Caddy as an HTTP server</a>.</p><h2 id=\"building-a-javascript-client-to-consume-our-api\">Building a Javascript Client to Consume Our API</h2><p>With our API nice and secure, we can start hitting this baby from wherever we want... as long as it's a Node app. We'll start by requiring two packages:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n</code></pre>\n<!--kg-card-end: markdown--><p><code>GraphQLClient</code> is the magic behind our client- it's everything. It also happens to be very similar to existing <strong>npm</strong> libraries for making requests, such as <a href=\"https://hackersandslackers.com/making-api-requests-with-nodejs/\">node-fetch</a>.</p><p>We'll also leverage the <code>dotenv</code> library to make sure our <strong>API endpoint</strong> and <strong>Bearer token</strong> stay out of source code. Try not to be Equifax whenever possible. <code>dotenv</code> allows us to load sensitive values from a <code>.env</code> file. Just in case you need a refresher, that file should look like this:</p><!--kg-card-begin: code--><pre><code>NODE_ENV=Production\nENDPOINT=https://yourapiendpoint.com\nAUTH=Bearer eyJhbGciOBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHGUYFIERIBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHBLAHZl-UGnMrOk3w</code></pre><!--kg-card-end: code--><h3 id=\"initialize-the-graphql-client\">Initialize The GraphQL Client</h3><p>I like to set up a one-time client for our API that we can go back and reuse if need be. After pulling the API endpoint and token from our <code>.env</code> file, setting up the client is easy:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"emergency-meeting-everybody-huddle-up\">EMERGENCY MEETING: EVERYBODY HUDDLE UP</h2><p>Oh I'm sorry, were you focusing on development? Unfortunately for you, I spent 8 years as a product manager, and I <em>love</em> stopping everything suddenly to call emergency meetings.</p><p>Real talk though, let's think back to the JIRA Kanban board example we've been using for the last two posts. If you recall, we're going to write a query that populates a 4-column Kanban board. The board represents a <em>project </em>(in this case, <strong>Hackers and Slackers</strong>) and each column represents a <em>status</em> of ticket, like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n</code></pre>\n<!--kg-card-end: markdown--><p>We've previously established that GraphQL queries are friendly to drop-in variables. Let's use this to build some logic into our client, as opposed to hardcoding a massive query, which is really just the same 4 queries stitched together. Here's what a query to populate a single JIRA column looks like:</p><!--kg-card-begin: code--><pre><code>// Structured query\nconst query = `\n    query JiraIssuesByStatus($project: String, $status: String) {\n         jiraIssues(where: {project: $project, status: $status}, \n         orderBy: timestamp_DESC, \n         first: 6) {\n            key\n            summary\n            epic\n            status\n            project\n            priority\n            issuetype\n            timestamp\n            }\n         }\n       `</code></pre><!--kg-card-end: code--><p>We're passing both the <em>project</em> and the <em>issue status</em> as variables to our query. We can make things a bit dynamic here by looping through our statuses and executing this query four times: each time resulting in a callback filling the appropriate columns with JIRA issues.</p><!--kg-card-begin: html--><div class=\"protip\">\nThis approach is certainly less clunky and more dynamic than a hardcoded query. That said, this <i>still</i> isn't the best solution. Remember: the strength of GraphQL is the ability to get obscene amounts of data across complex relationships in a single call. The best approach here would probably be to build the query string itself dynamically using <strong>fragments,</strong> which we'll review in the next post.\n</div><!--kg-card-end: html--><h2 id=\"game-on-our-client-in-action\">Game On: Our Client in Action</h2><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const { GraphQLClient } = require('graphql-request')\nconst { dotenv } = require('dotenv').config()\n\nconst endpoint = process.env.ENDPOINT;\nconst token = process.env.AUTH;\n\n// Initialize GraphQL Client\nconst client = new GraphQLClient(endpoint, {\n  headers: {\n    Authorization: token\n  }\n});\n\n// Structured query\nconst query = `\n   query JiraIssuesByStatus($project: String, $status: String) {\n      jiraIssues(where: {project: $project, status: $status}, orderBy: timestamp_DESC, first: 6) {\n         key\n         summary\n         epic\n         status\n         project\n         priority\n         issuetype\n         timestamp\n        }\n      }\n    `;\n\n// All Possible Issue Statuses\nconst statuses = ['Backlog', 'To Do', 'In Progress', 'Done'];\n\n// Execute a query per issue status\nfor(var i = 0; i &lt; statuses.length; i++){\n  var variables = {\n    project: &quot;Hackers and Slackers&quot;,\n    status: statuses[i]\n  }\n\n  client.request(query, variables).then((data) =&gt; {\n    console.log(data)\n  }).catch(err =&gt; {\n    console.log(err.response.errors) // GraphQL response errors\n    console.log(err.response.data) // Response data if available\n  });\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Works like a charm. We only had one endpoint, only had to set one header, and didn't spend any time reading through hundreds of pages of documentation to figure out which combination of REST API endpoint, parameters, and methods actually get us what we want. It's almost as if we're writing SQL now, except... it looks a lot more like... NoSQL. Thanks for the inspiration, <strong>MongoDB</strong>! Hope that whole <em>selling-open-source-software</em> thing works out.</p><p>Oh, and of course, here were the results of my query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550194791,\n       project: 'Hackers and Slackers',\n       key: 'HACK-778',\n       epic: 'Code snippets',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'HLJS: set indentation level' },\n     { priority: 'Medium',\n       timestamp: 1550194782,\n\n       project: 'Hackers and Slackers',\n       key: 'HACK-555',\n       epic: 'Optimization',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Minify Babel' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-785',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Task',\n       summary: 'Unix commands for data' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-251',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Using Ghost\\'s content filtering' },\n     { priority: 'Medium',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-302',\n       epic: 'Widgets',\n       status: 'Backlog',\n       issuetype: 'Integration',\n       summary: 'Discord channel signups ' },\n     { priority: 'Low',\n       timestamp: 1550016000,\n       project: 'Hackers and Slackers',\n       key: 'HACK-336',\n       epic: 'New Post',\n       status: 'Backlog',\n       issuetype: 'Content',\n       summary: 'Linux: Configuring your server to send SMTP emails' } ] }\n{ jiraIssues:\n   [ { priority: 'Medium',\n       timestamp: 1550224412,\n       project: 'Hackers and Slackers',\n       key: 'HACK-769',\n       epic: 'Projects Page',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'Fix projects dropdown' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-710',\n       epic: 'Lynx',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Implement auto text synopsis for Lynx posts' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-777',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Redesign footer to be informative; link-heavy' },\n     { priority: 'Highest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-779',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Changeover from cloudinary to DO' },\n     { priority: 'Medium',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-780',\n       epic: 'Creative',\n       status: 'Done',\n       issuetype: 'Task',\n       summary: 'Make mobile post title bold' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-781',\n       epic: 'Urgent',\n       status: 'Done',\n       issuetype: 'Bug',\n       summary: 'This post consistently doesnât work on mobile' } ] }\n{ jiraIssues:\n   [ { priority: 'Low',\n       timestamp: 1550223282,\n       project: 'Hackers and Slackers',\n       key: 'HACK-782',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary:\n        'Lynx: on mobile, instead of full link, show domainname.com/...' },\n     { priority: 'High',\n       timestamp: 1550194799,\n       project: 'Hackers and Slackers',\n       key: 'HACK-774',\n       epic: 'Widgets',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'New Widget: Next/Previous article in series' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-395',\n       epic: 'Page Templates',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Create fallback image for posts with no image' },\n     { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-756',\n       epic: 'Newsletter',\n       status: 'To Do',\n       issuetype: 'Major Functionality',\n       summary: 'Automate newsletter' },\n     { priority: 'Low',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-775',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Data &amp; Analytics',\n       summary: 'Update issuetype icons' },\n     { priority: 'Lowest',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-776',\n       epic: 'Projects Page',\n       status: 'To Do',\n       issuetype: 'Task',\n       summary: 'Add fork icon to repos' } ] }\n{ jiraIssues:\n   [ { priority: 'High',\n       timestamp: 1550102400,\n       project: 'Hackers and Slackers',\n       key: 'HACK-784',\n       epic: 'New Post',\n       status: 'In Progress',\n       issuetype: 'Content',\n       summary: 'Welcome to SQL part1' } ] }\n</code></pre>\n<!--kg-card-end: markdown--><p>Before we say \"GG, 2ez, 1v1 me,\" know that we're<em> only getting started</em> uncovering what GraphQL can do. It's not all just creating and deleting records either; we're talking full-on database JOIN equivalent type shit here. Stick around folks, the bandwagon's just getting warmed up.</p>","url":"https://hackersandslackers.com/interacting-with-your-graphql-api/","uuid":"34fef193-6a56-4754-a329-3d34571fcd15","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c838ee05af763016e85085b"}},{"node":{"id":"Ghost__Post__5c806baf199621174e904b03","title":"Writing Your First GraphQL Query","slug":"writing-your-first-graphql-queries","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/graphql-1-3.jpg","excerpt":"Begin to structure complex queries against your GraphQL API.","custom_excerpt":"Begin to structure complex queries against your GraphQL API.","created_at_pretty":"07 March, 2019","published_at_pretty":"07 March, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-03-06T19:54:07.000-05:00","published_at":"2019-03-07T10:37:00.000-05:00","updated_at":"2019-03-28T11:01:59.000-04:00","meta_title":"Writing Your First GraphQL Queries | Hackers and Slackers","meta_description":"Structure your first GraphQL Queries and begin to build a client.","og_description":"Structure your first GraphQL Queries and begin to build a client.","og_image":"https://hackersandslackers.com/content/images/2019/03/graphql-1-3.jpg","og_title":"Writing Your First GraphQL Queries","twitter_description":"Structure your first GraphQL Queries and begin to build a client.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/graphql-1-2.jpg","twitter_title":"Writing Your First GraphQL Queries","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"In our last run-in with GraphQL, we used Prisma  to assist in setting up a\nGraphQL server\n[https://hackersandslackers.com/easily-build-graphql-apis-with-prisma/]. This\neffectively gave us an endpoint to work with for making GraphQL requests against\nthe database we specified when getting started. If you're still in the business\nof setting up a GraphQL server, there are plenty of alternative services to\nPrisma you could explore. Apollo [https://www.apollographql.com/]  is perhaps\nthe most popular. A different approach could be to use GraphCMS\n[https://graphcms.com/]: a headless CMS for building GraphQL models with a\nbeautiful interface.\n\nWith our first models are created and deployed, weâre now able to explore\nGraphQL hands-on. Prisma (and just about any other service) gives us the luxury\nof a âplaygroundâ interface, where we can write all sorts of nonsensical and\notherwise dangerous shit. This is our opportunity to get comfortable before\nunleashing our ignorance upon the world in a production environment. To guide\nus, Iâll be using my own example of creating models, importing dummy data, and\nhow to write the queries to fetch said data.\n\nOur Example Model\nIn my case, I created a model for one of my favorite things: JIRA issues. I'll\nbe creating a Kanban widget using the data we play with here down the line, so\nthis is a real live use-case we'll be working with.\n\nHere are the contents of my datamodel.prisma  file:\n\ntype jiraissue {\n  id: ID! @unique,\n  key: String! @unique,\n  assignee: String,\n  summary: String,\n  status: String!,\n  priority: String,\n  issuetype: String,\n  epic_name: String,\n  updated: DateTime,\n  rank: Int,\n  timestamp: Int,\n  project: String\n}\n\nYou'll notice we have a good number of datatypes here, as well as two unique\nkeys. In case this point has been missed before, the exclamation marks in our\nmodel denote a required field.\n\nDeploying this model results in the following PostgreSQL query:\n\nCREATE TABLE \"default$default\".\"jiraissues\" (\n    \"id\" varchar(25) NOT NULL,\n    \"key\" text NOT NULL,\n    \"assignee\" text,\n    \"summary\" text,\n    \"status\" text NOT NULL,\n    \"priority\" text,\n    \"issuetype\" text,\n    \"epic_name\" text,\n    \"updated\" timestamp(3),\n    \"rank\" int4,\n    \"timestamp\" int4,\n    \"project\" text,\n    \"updatedAt\" timestamp(3) NOT NULL,\n    \"createdAt\" timestamp(3) NOT NULL,\n    PRIMARY KEY (\"id\")\n);\n\n\nLooks like everything lines up! The only caveat are the updatedAt  and createdAt \n fields: Prisma adds these to every database table for us.\n\nHere's a sample of the data I added by connecting to my database and importing a\nCSV:\n\nid\n key\n assignee\n summary\n status\n priority\n issuetype\n epic_name\n updated\n rank\n timestamp\n project\n updatedAt\n createdAt\n 430\n HACK-769\n Todd Birchard\n Fix projects dropdown\n Done\n Medium\n Bug\n Projects Page\n 2019-02-15 00:00:00\n 3\n 1550224412\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 465\n HACK-782\n Todd Birchard\n Lynx: on mobile, instead of full link, show domainname.com/...\n To Do\n Low\n Task\n Widgets\n 2019-02-15 00:00:00\n 4\n 1550223282\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 472\n HACK-774\n Todd Birchard\n New Widget: Next/Previous article in series\n To Do\n High\n Task\n Widgets\n 2019-02-14 00:00:00\n 2\n 1550194799\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 464\n HACK-778\n Todd Birchard\n HLJS: set indentation level\n Backlog\n Medium\n Task\n Code snippets\n 2019-02-14 00:00:00\n 3\n 1550194791\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 481\n HACK-555\n Todd Birchard\n Minify Babel\n Backlog\n Medium\n Task\n Optimization\n 2019-02-14 00:00:00\n 3\n 1550194782\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 432\n HACK-777\n Todd Birchard\n Redesign footer to be informative; link-heavy\n Done\n Medium\n Task\n Creative\n 2019-02-14 00:00:00\n 2\n 1550102400\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 433\n HACK-779\n Todd Birchard\n Changeover from cloudinary to DO\n Done\n Highest\n Task\n Urgent\n 2019-02-14 00:00:00\n 0\n 1550102400\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 428\n HACK-775\n Todd Birchard\n Update issuetype icons\n To Do\n Low\n Data & Analytics\n Projects Page\n 2019-02-14 00:00:00\n 3\n 1550102400\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 374\n HACK-710\n Todd Birchard\n Implement auto text synopsis for Lynx posts\n Done\n High\n Task\n Lynx\n 2019-02-14 00:00:00\n 1\n 1550102400\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n 185\n HACK-395\n Todd Birchard\n Create fallback image for posts with no image\n To Do\n Low\n Task\n Page Templates\n 2019-02-14 00:00:00\n 3\n 1550102400\n Hackers and Slackers\n 2019-03-02 15:43:59.419\n 2019-03-02 15:43:59.419\n A Few Things About GraphQL Queries\nBefore going any further, let's touch on a few concepts that are easy to stumble\nover.\n\nFirstly, a  GraphQL API only has a single endpoint. It makes sense: the logic of\nGraphQL API hits sit with the person creating the queries. That said, we've all\nbeen building REST APIs long enough to have this slip past us; I caught myself\nthinking through how to separate which endpoints I wanted before remembering\nthat's entirely not how this works.\n\nIt's import to understand that GraphQL is designed to be explicit. A significant\nadvantage of GraphQL is that we can be sure only to return the information which\nis essential to us.  For applications looking to optimize system resources (such\nas mobile apps), avoiding massive payloads is a feature, not a bug. This\nexplains many of the design decisions which went into designing GraphQL, as\nyou'll see it's intentionally difficult (but possible) to create a \"get all\nrecords\" query.\n\nLastly, GraphQL allows us to create queries in both shorthand and long-form  \nformats.  We'll take a look at both, starting with shorthand.\n\nGraphQL Shorthand Queries\nShorthand queries are an excellent place to start for beginners like us just\ntrying to get some data out of our database.\n\nThe structure of such a query looks like this:\n\n{\n  [model_name]s {\n    [desired_field_name_1]\n    [desired_field_name_2]\n    [desired_field_name_3]\n  }\n}\n\nUsing our example, our model_name  in this case would be jiraissue made plural, \nresulting in jiraissues. This is an important thing to note: when creating\nmodels, we should name them as a single entity, as things get confusing very\nfast otherwise. I initially made the mistake of naming my model jiraissues,\nwhich would then drive me to query jiraissueses. That was a fun little trip.\n\nWithin the brackets of our model, we must explicitly specify which fields (aka\ndatabase columns) we'd like returned with our query. Here's a full example of a\nshorthand query:\n\n{\n  jiraissues {\n    key\n    summary\n    epic_name\n  }\n}\n\n\nCheck out what this results in when entered in our \"playground\":\n\nQuery on the left, results on the right.Just like that, we have liftoff!\n\nThe \"Where\" Clause\nAs mentioned earlier, a major point of GraphQL is to return only the data which\nis necessary. Thus, we should almost always make queries with a where clause.\nThus, we can extend our simple query as such:\n\n{\n  jiraissueses(where: {status: \"Backlog\"}) {\n    key\n    summary\n    epic_name\n    status\n  }\n}\n\n\nAnd here's the result:\n\nFiltering results \"where\" certain criteria are met.Adding to Our Query\nJust like SQL or MongoDB queries, we can add more to our query to get more\nspecific:\n\n{\n  jiraissues(where: {status: \"Backlog\", project: \"Hackers and Slackers\"}, orderBy: updated_DESC,  first: 6) \n  {\n    key\n    summary\n    epic_name\n    status\n    updated\n  }\n}\n\n\nHere, we've expanded our filter to work on two  fields: now our query will only\nreturn issues which match our criteria for both status  and project. \n\nWe've also added a few other things to our query. With orderBy, we can set the\norder in which records will be returned to us by field, either in ascending\n(ASC) or descending (DESC) order. first  imposes a limit on our results, giving\nus the first 6 which meet our criteria. Alternatively, last  would give us the\nopposite.\n\nThere are plenty of more parameters we could add here. For example:\n\n * [fieldname]_contains: Filters results where the string field contains a\n   substring.\n * [fieldname]_in: Checks a list to return records where the value of the field\n   matches any substring in a provided list.\n * [fieldname]_starts_with: An expression to check for values that start with a\n   provided substring.\n * [fieldname]_ends_with: Similar to the above, only for ending with a\n   substring.\n\nNot only are there more to add to this list, but each as an accompanying reverse\nstatement which would return the opposite. For example, [fieldname]_not_contains \n is the opposite of [fieldname]_contains.\n\nGraphQL Longform Queries\nWhat we've seen so far is already pretty powerful, but we're far from seeing\njust how far GraphQL can go. \n\nTo demonstrate what a more complicated query is capable of, let's use out Kanban\nboard example. Our board is going to have 4 columns representing 4 statuses: \nBacklog, To Do, In Progress, and Done.  Check out how we can receive all of this\nwith a single query:\n\nquery KanbanJiraIssues {\n  backlog: jiraissues(where: {status: \"Backlog\", project: \"Hackers and Slackers\"}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  todo: jiraissues(where: {status: \"To Do\", project: \"Hackers and Slackers\"}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  inprogress: jiraissues(where: {status: \"In Progress\", project: \"Hackers and Slackers\"}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  done: jiraissues(where: {status: \"Done\", project: \"Hackers and Slackers\"}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n}\n\n\nUnlike our shorthand queries, we begin this query with the syntax query\n[your_query_name]. You can name your query anything you'd like.\n\nWithin that query, we can perform multiple individual queries which we too give\ndisplay names. In whole, the structure looks like this:\\\n\nquery [your_query_name] {\n  [subquery_name]: [model_name]s(where: {[your_criteria]}){\n    [desired_field_name_1]\n    [desired_field_name_2]\n    [desired_field_name_3]\n  }\n}\n\n\nCheck out the result:\n\nNow THAT's a query.This format has helped us accomplish something previously\nimpossible with REST APIs: we've used a single endpoint to give us exactly  the\ninformation we need while omitting the information we don't.\n\nPassing Variables Into Queries\nAs you can see, queries can get lengthy pretty quick. It would suck if we had to\nwrite the entirety of the query above every time we wanted to hit an API.\nLuckily, we don't don't have to: that's where GraphQL variables come in.\n\nVariables allow us to use the structure of a GraphQL query repeatedly, while\nproviding different values where we see fit. That means if we have a\nparticularly complicated query structure that we'd like to repurpose, we can\npass dynamic values into said query. This is where things start to get really\npowerful.\n\nLet's assume that finding JIRA issues by epic link  is a common task we'll have\nto deal with. This is how we'd pass a dynamic value for epic_link:\n\nquery JiraIssuesByEpicName($epic_name: String) {\n  jiraissues(where: {epic_name: $epic_name}) {\n    key\n    summary\n    epic_name\n    status\n    updated\n    project\n    priority\n    issuetype\n    timestamp\n  }\n}\n\n$epic_name  is the name of our variable, which we set in the object we pass to\nthe query. That object looks like this:\n\n{\n  \"epic_name\": \"SEO\"\n}\n\nSo what we're saying on line 1  is that we're passing a variable named \n$epic_name, and that variable will be a String. When $epic_name  appears again\non line 2, the variable is interpreted as its value, which is \"SEO\".\n\nLuckily, our playground has a place specifically for setting variables which get\npassed to our queries. Here's how it all looks:\n\nHeavy breathing intensifies.Unlimited Power?\nWhile GraphQL's syntax looks clean and simple at first glance, it's easy to see\nhow quickly simple queries evolve into complex behemoths. It's no coincidence\nthat all GraphQL services come with a playground. It's hard to imagine how\nanybody could internalize GraphQL syntax without trial and error, and we're only\ngetting started.\n\nSo far we've only queried existing data; we haven't even begun to touch on\nmutations yet. Catch us next time when we start modifying data and get ourselves\ninto a whole lot of trouble.","html":"<p>In our last run-in with GraphQL, we used <strong>Prisma</strong> to assist in <a href=\"https://hackersandslackers.com/easily-build-graphql-apis-with-prisma/\">setting up a GraphQL server</a>. This effectively gave us an endpoint to work with for making GraphQL requests against the database we specified when getting started. If you're still in the business of setting up a GraphQL server, there are plenty of alternative services to Prisma you could explore. <a href=\"https://www.apollographql.com/\"><strong>Apollo</strong></a> is perhaps the most popular. A different approach could be to use <a href=\"https://graphcms.com/\"><strong>GraphCMS</strong></a>: a headless CMS for building GraphQL models with a beautiful interface.</p><p>With our first models are created and deployed, weâre now able to explore GraphQL hands-on. Prisma (and just about any other service) gives us the luxury of a âplaygroundâ interface, where we can write all sorts of nonsensical and otherwise dangerous shit. This is our opportunity to get comfortable before unleashing our ignorance upon the world in a production environment. To guide us, Iâll be using my own example of creating models, importing dummy data, and how to write the queries to fetch said data.</p><h2 id=\"our-example-model\">Our Example Model</h2><p>In my case, I created a model for one of my favorite things: JIRA issues. I'll be creating a Kanban widget using the data we play with here down the line, so this is a real live use-case we'll be working with.</p><p>Here are the contents of my <code>datamodel.prisma</code> file:</p><!--kg-card-begin: code--><pre><code>type jiraissue {\n  id: ID! @unique,\n  key: String! @unique,\n  assignee: String,\n  summary: String,\n  status: String!,\n  priority: String,\n  issuetype: String,\n  epic_name: String,\n  updated: DateTime,\n  rank: Int,\n  timestamp: Int,\n  project: String\n}</code></pre><!--kg-card-end: code--><p>You'll notice we have a good number of datatypes here, as well as two unique keys. In case this point has been missed before, the exclamation marks in our model denote a required field.</p><p>Deploying this model results in the following PostgreSQL query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">CREATE TABLE &quot;default$default&quot;.&quot;jiraissues&quot; (\n    &quot;id&quot; varchar(25) NOT NULL,\n    &quot;key&quot; text NOT NULL,\n    &quot;assignee&quot; text,\n    &quot;summary&quot; text,\n    &quot;status&quot; text NOT NULL,\n    &quot;priority&quot; text,\n    &quot;issuetype&quot; text,\n    &quot;epic_name&quot; text,\n    &quot;updated&quot; timestamp(3),\n    &quot;rank&quot; int4,\n    &quot;timestamp&quot; int4,\n    &quot;project&quot; text,\n    &quot;updatedAt&quot; timestamp(3) NOT NULL,\n    &quot;createdAt&quot; timestamp(3) NOT NULL,\n    PRIMARY KEY (&quot;id&quot;)\n);\n</code></pre>\n<!--kg-card-end: markdown--><p>Looks like everything lines up! The only caveat are the <code>updatedAt</code> and <code>createdAt</code> fields: Prisma adds these to every database table for us.</p><p>Here's a sample of the data I added by connecting to my database and importing a CSV:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table>\n    <thead>\n       <tr>\n             <th>id</th>\n             <th>key</th>\n             <th>assignee</th>\n             <th>summary</th>\n             <th>status</th>\n             <th>priority</th>\n             <th>issuetype</th>\n             <th>epic_name</th>\n             <th>updated</th>\n             <th>rank</th>\n             <th>timestamp</th>\n             <th>project</th>\n             <th>updatedAt</th>\n             <th>createdAt</th>\n         </tr>\n    </thead>\n    <tbody>\n       <tr>\n              <td>430</td>\n              <td>HACK-769</td>\n              <td>Todd Birchard</td>\n              <td>Fix projects dropdown</td>\n              <td>Done</td>\n              <td>Medium</td>\n              <td>Bug</td>\n              <td>Projects Page</td>\n              <td>2019-02-15 00:00:00</td>\n              <td>3</td>\n              <td>1550224412</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>465</td>\n              <td>HACK-782</td>\n              <td>Todd Birchard</td>\n              <td>Lynx: on mobile, instead of full link, show domainname.com/...</td>\n              <td>To Do</td>\n              <td>Low</td>\n              <td>Task</td>\n              <td>Widgets</td>\n              <td>2019-02-15 00:00:00</td>\n              <td>4</td>\n              <td>1550223282</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>472</td>\n              <td>HACK-774</td>\n              <td>Todd Birchard</td>\n              <td>New Widget: Next/Previous article in series</td>\n              <td>To Do</td>\n              <td>High</td>\n              <td>Task</td>\n              <td>Widgets</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>2</td>\n              <td>1550194799</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>464</td>\n              <td>HACK-778</td>\n              <td>Todd Birchard</td>\n              <td>HLJS: set indentation level</td>\n              <td>Backlog</td>\n              <td>Medium</td>\n              <td>Task</td>\n              <td>Code snippets</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>3</td>\n              <td>1550194791</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>481</td>\n              <td>HACK-555</td>\n              <td>Todd Birchard</td>\n              <td>Minify Babel</td>\n              <td>Backlog</td>\n              <td>Medium</td>\n              <td>Task</td>\n              <td>Optimization</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>3</td>\n              <td>1550194782</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>432</td>\n              <td>HACK-777</td>\n              <td>Todd Birchard</td>\n              <td>Redesign footer to be informative; link-heavy</td>\n              <td>Done</td>\n              <td>Medium</td>\n              <td>Task</td>\n              <td>Creative</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>2</td>\n              <td>1550102400</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>433</td>\n              <td>HACK-779</td>\n              <td>Todd Birchard</td>\n              <td>Changeover from cloudinary to DO</td>\n              <td>Done</td>\n              <td>Highest</td>\n              <td>Task</td>\n              <td>Urgent</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>0</td>\n              <td>1550102400</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>428</td>\n              <td>HACK-775</td>\n              <td>Todd Birchard</td>\n              <td>Update issuetype icons</td>\n              <td>To Do</td>\n              <td>Low</td>\n              <td>Data & Analytics</td>\n              <td>Projects Page</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>3</td>\n              <td>1550102400</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>374</td>\n              <td>HACK-710</td>\n              <td>Todd Birchard</td>\n              <td>Implement auto text synopsis for Lynx posts</td>\n              <td>Done</td>\n              <td>High</td>\n              <td>Task</td>\n              <td>Lynx</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>1</td>\n              <td>1550102400</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n       <tr>\n              <td>185</td>\n              <td>HACK-395</td>\n              <td>Todd Birchard</td>\n              <td>Create fallback image for posts with no image</td>\n              <td>To Do</td>\n              <td>Low</td>\n              <td>Task</td>\n              <td>Page Templates</td>\n              <td>2019-02-14 00:00:00</td>\n              <td>3</td>\n              <td>1550102400</td>\n              <td>Hackers and Slackers</td>\n              <td>2019-03-02 15:43:59.419</td>\n              <td>2019-03-02 15:43:59.419</td>\n          </tr>\n    </tbody>\n   </table>\n</div><!--kg-card-end: html--><h2 id=\"a-few-things-about-graphql-queries\">A Few Things About GraphQL Queries</h2><p>Before going any further, let's touch on a few concepts that are easy to stumble over.</p><p>Firstly, a<strong> GraphQL API only has a single endpoint</strong>. It makes sense: the logic of GraphQL API hits sit with the person creating the queries. That said, we've all been building REST APIs long enough to have this slip past us; I caught myself thinking through how to separate which endpoints I wanted before remembering that's entirely not how this works.</p><p>It's import to understand that <strong>GraphQL is designed to be explicit</strong>. A significant advantage of GraphQL is that we can be sure <em>only to return the information which is essential to us.</em> For applications looking to optimize system resources (such as mobile apps), avoiding massive payloads is a feature, not a bug. This explains many of the design decisions which went into designing GraphQL, as you'll see it's intentionally difficult (but possible) to create a \"get all records\" query.</p><p>Lastly, GraphQL allows us to <strong>create queries in both shorthand and long-form</strong> <strong>formats</strong>.<strong> </strong>We'll take a look at both, starting with shorthand.</p><h2 id=\"graphql-shorthand-queries\">GraphQL Shorthand Queries</h2><p>Shorthand queries are an excellent place to start for beginners like us just trying to get some data out of our database.</p><p>The structure of such a query looks like this:</p><!--kg-card-begin: code--><pre><code>{\n  [model_name]s {\n    [desired_field_name_1]\n    [desired_field_name_2]\n    [desired_field_name_3]\n  }\n}</code></pre><!--kg-card-end: code--><p>Using our example, our <strong>model_name</strong> in this case would be <strong>jiraissue </strong><em>made plural,</em> resulting in <strong>jiraissues</strong>. This is an important thing to note: when creating models, we should name them as a single entity, as things get confusing very fast otherwise. I initially made the mistake of naming my model <strong>jiraissues</strong>, which would then drive me to query <strong>jiraissueses</strong>. That was a fun little trip.</p><p>Within the brackets of our model, we must explicitly specify which fields (aka database columns) we'd like returned with our query. Here's a full example of a shorthand query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">{\n  jiraissues {\n    key\n    summary\n    epic_name\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out what this results in when entered in our \"playground\":</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-06-at-8.50.53-PM.png\" class=\"kg-image\"><figcaption>Query on the left, results on the right.</figcaption></figure><!--kg-card-end: image--><p>Just like that, we have liftoff!</p><h3 id=\"the-where-clause\">The \"Where\" Clause</h3><p>As mentioned earlier, a major point of GraphQL is to return only the data which is necessary. Thus, we should almost always make queries with a <em>where </em>clause. Thus, we can extend our simple query as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">{\n  jiraissueses(where: {status: &quot;Backlog&quot;}) {\n    key\n    summary\n    epic_name\n    status\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>And here's the result:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-06-at-9.21.23-PM.png\" class=\"kg-image\"><figcaption>Filtering results \"where\" certain criteria are met.</figcaption></figure><!--kg-card-end: image--><h3 id=\"adding-to-our-query\">Adding to Our Query</h3><p>Just like SQL or MongoDB queries, we can add more to our query to get more specific:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">{\n  jiraissues(where: {status: &quot;Backlog&quot;, project: &quot;Hackers and Slackers&quot;}, orderBy: updated_DESC,  first: 6) \n  {\n    key\n    summary\n    epic_name\n    status\n    updated\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Here, we've expanded our filter to work on <em>two</em> fields: now our query will only return issues which match our criteria for both <code>status</code> and <code>project</code>. </p><p>We've also added a few other things to our query. With <code>orderBy</code>, we can set the order in which records will be returned to us by field, either in ascending (ASC) or descending (DESC) order. <code>first</code> imposes a limit on our results, giving us the first 6 which meet our criteria. Alternatively, <code>last</code> would give us the opposite.</p><p>There are plenty of more parameters we could add here. For example:</p><ul><li><code>[fieldname]_contains</code>: Filters results where the string field contains a substring.</li><li><code>[fieldname]_in</code>: Checks a list to return records where the value of the field matches any substring in a provided list.</li><li><code>[fieldname]_starts_with</code>: An expression to check for values that start with a provided substring.</li><li><code>[fieldname]_ends_with</code>: Similar to the above, only for ending with a substring.</li></ul><p>Not only are there more to add to this list, but each as an accompanying reverse statement which would return the opposite. For example, <code>[fieldname]_not_contains</code> is the opposite of <code>[fieldname]_contains</code>.</p><h2 id=\"graphql-longform-queries\">GraphQL Longform Queries</h2><p>What we've seen so far is already pretty powerful, but we're far from seeing just how far GraphQL can go. </p><p>To demonstrate what a more complicated query is capable of, let's use out Kanban board example. Our board is going to have 4 columns representing 4 statuses: <strong>Backlog, To Do, In Progress, </strong>and <strong>Done.</strong> Check out how we can receive all of this with a single query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">query KanbanJiraIssues {\n  backlog: jiraissues(where: {status: &quot;Backlog&quot;, project: &quot;Hackers and Slackers&quot;}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  todo: jiraissues(where: {status: &quot;To Do&quot;, project: &quot;Hackers and Slackers&quot;}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  inprogress: jiraissues(where: {status: &quot;In Progress&quot;, project: &quot;Hackers and Slackers&quot;}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n  done: jiraissues(where: {status: &quot;Done&quot;, project: &quot;Hackers and Slackers&quot;}, orderBy: updated_DESC, first: 6){\n    key\n    status\n    summary\n    assignee\n    priority\n    issuetype\n    epic_name\n    updated\n    rank\n    timestamp\n    project\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Unlike our shorthand queries, we begin this query with the syntax <code>query [your_query_name]</code>. You can name your query anything you'd like.</p><p>Within that query, we can perform multiple individual queries which we too give display names. In whole, the structure looks like this:\\</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">query [your_query_name] {\n  [subquery_name]: [model_name]s(where: {[your_criteria]}){\n    [desired_field_name_1]\n    [desired_field_name_2]\n    [desired_field_name_3]\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out the result:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-06-at-11.22.29-PM.png\" class=\"kg-image\"><figcaption>Now THAT's a query.</figcaption></figure><!--kg-card-end: image--><p>This format has helped us accomplish something previously impossible with REST APIs: we've used a single endpoint to give us <em>exactly</em> the information we need while omitting the information we don't.</p><h2 id=\"passing-variables-into-queries\">Passing Variables Into Queries</h2><p>As you can see, queries can get lengthy pretty quick. It would suck if we had to write the entirety of the query above every time we wanted to hit an API. Luckily, we don't don't have to: that's where GraphQL <em>variables </em>come in.</p><p>Variables allow us to use the structure of a GraphQL query repeatedly, while providing different values where we see fit. That means if we have a particularly complicated query structure that we'd like to repurpose, we can pass dynamic values into said query. This is where things start to get really powerful.</p><p>Let's assume that finding JIRA issues by <em>epic link</em> is a common task we'll have to deal with. This is how we'd pass a dynamic value for <strong>epic_link:</strong></p><!--kg-card-begin: code--><pre><code>query JiraIssuesByEpicName($epic_name: String) {\n  jiraissues(where: {epic_name: $epic_name}) {\n    key\n    summary\n    epic_name\n    status\n    updated\n    project\n    priority\n    issuetype\n    timestamp\n  }\n}</code></pre><!--kg-card-end: code--><p><code>$epic_name</code> is the name of our variable, which we set in the object we pass to the query. That object looks like this:</p><!--kg-card-begin: code--><pre><code>{\n  \"epic_name\": \"SEO\"\n}</code></pre><!--kg-card-end: code--><p>So what we're saying on <strong>line 1</strong> is that we're passing a variable named <code>$epic_name</code>, and that variable will be a <code>String</code>. When <code>$epic_name</code> appears again on <strong>line 2</strong>, the variable is interpreted as its value, which is \"SEO\".</p><p>Luckily, our playground has a place specifically for setting variables which get passed to our queries. Here's how it all looks:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/graphql-variables.png\" class=\"kg-image\"><figcaption>Heavy breathing intensifies.</figcaption></figure><!--kg-card-end: image--><h3 id=\"unlimited-power\">Unlimited Power?</h3><p>While GraphQL's syntax looks clean and simple at first glance, it's easy to see how quickly simple queries evolve into complex behemoths. It's no coincidence that all GraphQL services come with a playground. It's hard to imagine how anybody could internalize GraphQL syntax without trial and error, and we're only getting started.</p><p>So far we've only queried existing data; we haven't even begun to touch on mutations yet. Catch us next time when we start modifying data and get ourselves into a whole lot of trouble.</p>","url":"https://hackersandslackers.com/writing-your-first-graphql-queries/","uuid":"4019e61f-0f68-4921-99d4-5085864f9143","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c806baf199621174e904b03"}},{"node":{"id":"Ghost__Post__5c79b0070fa2b110f256e320","title":"Running Jupyter Notebooks on a Ubuntu Server","slug":"running-jupyter-notebooks-on-a-ubuntu-server","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","custom_excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","created_at_pretty":"01 March, 2019","published_at_pretty":"02 March, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-03-01T17:19:51.000-05:00","published_at":"2019-03-01T21:15:40.000-05:00","updated_at":"2019-04-14T12:27:20.000-04:00","meta_title":"Running Jupyter Notebooks on a Ubuntu Server | Hackers and Slackers","meta_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","og_title":"Running Jupyter Notebooks on a Ubuntu Server","twitter_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","twitter_title":"Running Jupyter Notebooks on a Ubuntu Server","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"It dawned on me the other day that for a publication which regularly uses and\ntalks about Jupyter notebooks [https://jupyter.org/], weâve never actually taken\nthe time to explain what they are or how to start using them. No matter where\nyou may have been in your career, first exposure to Jupyter and the IPython\n[https://ipython.org/]  shell is often a confusingly magical experience. Writing\nprograms line-by-line and receiving feedback in real-time feels more like\npainting oil on canvas and programming. I suppose we can finally chalk up a win\nfor dynamically typed languages.\n\nThere are a couple of barriers for practical devs to overcome before using\nJupyter, the most obvious being hardware costs. If youâre utilizing a full \nAnaconda  installation, chances are youâre not the type of person to mess\naround. Real machine learning algorithms take real resources, and real resources\ntake real money. A few vendors have popped up here are offering managed\ncloud-hosted notebooks for this reason. For those of us who bothered to do the\nmath, it turns out most of these services are more expensive than spinning up a\ndedicated VPS.\n\nData scientists with impressive machines have no problem running notebooks\nlocally for most use cases. While thatâs fine and good for scientists, this\nsetup is problematic for those of us with commitments to Python outside of\nnotebooks. Upon installation, Anaconda barges into your systemâs ~/.bash_profile\n, shouts âI am the captain now,â  and crowns itself as your systemâs default\nPython path. Conda and Pip have some trouble getting along, so for those of us\nwho build Python applications and use notebooks, it's best to keep these things\nisolated.\n\nSetting Up a VPS\nWe're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted\nfor DigitalOcean  in my case, both for simplicity and the fact that I'm\nincredibly broke. Depending on how broke you may or may not be, this is where\nyou'll have to make a judgment call for your system resources:\n\nMy kind sir, I would like to order the most exquisite almost-cheapest Droplet on\nthe menuSSH into that bad boy. You know what to do next:\n\n$ sudo apt update\n$ sudo apt upgrade -y\n\n\nWith that out of the way, next we'll grab the latest version of Python:\n\n$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n\n\nFinally, we'll open port 8888 for good measure, since this is the port Jupyter\nruns on:\n\n$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw allow 22\n$ sudo ufw status\n\n\nTo                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n\n\nCreate a New User\nAs always, we should create a Linux user besides root to do just about anything:\n\n$ adduser myuser\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n\n\nThen, add them to the sudoers  group:\n\n$ usermod -aG sudo myuser\n\n\nLog in as the user:\n\n$ su - myuser\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\n\nInstall The Latest Anaconda Distribution\nAnaconda comes with all the fantastic Data Science Python packages we'll need\nfor our notebook. To find the latest distribution, check here: \nhttps://www.anaconda.com/download/. We'll install this to a /tmp  folder:\n\ncd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n\n\nOnce downloaded, begin the installation:\n\n$ sh Anaconda3-2018.12-Linux-x86_64.sh\n\n\nComplete the resulting prompts:\n\nWelcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n>>>\n\n\nGet ready for the wall of text....\n\n===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n\n\nThis kicks off a rather lengthy install process. Afterward, you'll be prompted\nto add Conda to your startup script. Say yes:\n\ninstallation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n\n\nThe final part of the installation will ask if you'd like to install VS Code.\nDecline this offer because Microsoft sucks.\n\nFinally, reload your /.bashrc file to get apply Conda's changes:\n\n$ source ~/.bashrc\n\n\nSetting Up Conda Environments\nConda installations can be isolated to separate environments similarly to how we\nwould Â with Virtualenv. Unlike Virtualenv, however, Conda environments can be\nactivated from anywhere (not just in the directory containing the environment).\nCreate and activate a Conda env:\n\n$ conda create --name myenv python=3\n$ conda activate myenv\n\n\nCongrats, you're now in an active Conda environment!\n\nStarting Up Jupyter\nMake sure you're in a directory you'd like to be running Jupyter in. Entering \njupyter notebook  in this directory should result in the following:\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n\n\nThis next part is tricky. To run our notebook, we need to reconnect to our VPS\nvia an SSH tunnel. Close the terminal and reconnect to your server with the\nfollowing format:\n\nssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nIndeed, localhost  is intended to stay the same, but your_server_ip  is to be\nreplaced with the address of your server.\n\nWith that done, let's try this one more time. Remember to reactivate your Conda\nenvironment first!\n\n$ jupyter notebook\n\n\nThis time around, the links which appear in the terminal should work!\n\nWE DID ITBONUS ROUND: Theme Your Notebooks\nIf ugly interfaces bother you as much as they bother me, I highly recommend\ntaking a look at the jupyter-themes package on Github\n[https://github.com/dunovank/jupyter-themes]. This package allows you to\ncustomize the look and feel of your notebook, either as simple as activating a\nstyle, or as complex as setting your margin width. I highly recommend checking\nout the available themes to spice up your notebook!","html":"<p>It dawned on me the other day that for a publication which regularly uses and talks about <a href=\"https://jupyter.org/\">Jupyter notebooks</a>, weâve never actually taken the time to explain what they are or how to start using them. No matter where you may have been in your career, first exposure to Jupyter and the <a href=\"https://ipython.org/\">IPython</a> shell is often a confusingly magical experience. Writing programs line-by-line and receiving feedback in real-time feels more like painting oil on canvas and programming. I suppose we can finally chalk up a win for dynamically typed languages.</p><p>There are a couple of barriers for practical devs to overcome before using Jupyter, the most obvious being hardware costs. If youâre utilizing a full <strong>Anaconda</strong> installation, chances are youâre not the type of person to mess around. Real machine learning algorithms take real resources, and real resources take real money. A few vendors have popped up here are offering managed cloud-hosted notebooks for this reason. For those of us who bothered to do the math, it turns out most of these services are more expensive than spinning up a dedicated VPS.</p><p>Data scientists with impressive machines have no problem running notebooks locally for most use cases. While thatâs fine and good for scientists, this setup is problematic for those of us with commitments to Python outside of notebooks. Upon installation, Anaconda barges into your systemâs <code>~/.bash_profile</code>, shouts <strong><em>âI am the captain now,â</em></strong> and crowns itself as your systemâs default Python path. Conda and Pip have some trouble getting along, so for those of us who build Python applications and use notebooks, it's best to keep these things isolated.</p><h2 id=\"setting-up-a-vps\">Setting Up a VPS</h2><p>We're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted for <strong>DigitalOcean</strong> in my case, both for simplicity and the fact that I'm incredibly broke. Depending on how broke you may or may not be, this is where you'll have to make a judgment call for your system resources:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/digitaloceanvps.png\" class=\"kg-image\"><figcaption>My kind sir, I would like to order the most exquisite almost-cheapest Droplet on the menu</figcaption></figure><!--kg-card-end: image--><p>SSH into that bad boy. You know what to do next:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt update\n$ sudo apt upgrade -y\n</code></pre>\n<!--kg-card-end: markdown--><p>With that out of the way, next we'll grab the latest version of Python:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we'll open port 8888 for good measure, since this is the port Jupyter runs on:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw allow 22\n$ sudo ufw status\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">To                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-new-user\">Create a New User</h3><p>As always, we should create a Linux user besides root to do just about anything:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ adduser myuser\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n</code></pre>\n<!--kg-card-end: markdown--><p>Then, add them to the <strong>sudoers</strong> group:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ usermod -aG sudo myuser\n</code></pre>\n<!--kg-card-end: markdown--><p>Log in as the user:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ su - myuser\nTo run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.\nSee &quot;man sudo_root&quot; for details.\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"install-the-latest-anaconda-distribution\">Install The Latest Anaconda Distribution</h3><p>Anaconda comes with all the fantastic Data Science Python packages we'll need for our notebook. To find the latest distribution, check here: <a href=\"https://www.anaconda.com/download/\">https://www.anaconda.com/download/</a>. We'll install this to a <code>/tmp</code> folder:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">cd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Once downloaded, begin the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sh Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Complete the resulting prompts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Welcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n&gt;&gt;&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Get ready for the wall of text....</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>This kicks off a rather lengthy install process. Afterward, you'll be prompted to add Conda to your startup script. Say <strong>yes</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">installation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>The final part of the installation will ask if you'd like to install VS Code. Decline this offer because Microsoft sucks.</p><p>Finally, reload your <strong>/.bashrc </strong>file to get apply Conda's changes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ source ~/.bashrc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"setting-up-conda-environments\">Setting Up Conda Environments</h3><p>Conda installations can be isolated to separate environments similarly to how we would Â with Virtualenv. Unlike Virtualenv, however, Conda environments can be activated from anywhere (not just in the directory containing the environment). Create and activate a Conda env:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda create --name myenv python=3\n$ conda activate myenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Congrats, you're now in an active Conda environment!</p><h3 id=\"starting-up-jupyter\">Starting Up Jupyter</h3><p>Make sure you're in a directory you'd like to be running Jupyter in. Entering <code>jupyter notebook</code> in this directory should result in the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n</code></pre>\n<!--kg-card-end: markdown--><p>This next part is tricky. To run our notebook, we need to reconnect to our VPS via an SSH tunnel. Close the terminal and reconnect to your server with the following format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, <code>localhost</code> is intended to stay the same, but <code>your_server_ip</code> is to be replaced with the address of your server.</p><p>With that done, let's try this one more time. Remember to reactivate your Conda environment first!</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter notebook\n</code></pre>\n<!--kg-card-end: markdown--><p>This time around, the links which appear in the terminal should work!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-01-at-7.01.42-PM.png\" class=\"kg-image\"><figcaption>WE DID IT</figcaption></figure><!--kg-card-end: image--><h2 id=\"bonus-round-theme-your-notebooks\">BONUS ROUND: Theme Your Notebooks</h2><p>If ugly interfaces bother you as much as they bother me, I highly recommend taking a look at the <a href=\"https://github.com/dunovank/jupyter-themes\">jupyter-themes package on Github</a>. This package allows you to customize the look and feel of your notebook, either as simple as activating a style, or as complex as setting your margin width. I highly recommend checking out the available themes to spice up your notebook!</p><!--kg-card-begin: gallery--><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/gruvbox-dark-python.png\" width=\"1013\" height=\"903\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/grade3_table.png\" width=\"1293\" height=\"809\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/jtplotDark_reach.png\" width=\"8400\" height=\"3600\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/oceans16_code_headers.png\" width=\"1293\" height=\"808\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/onedork_code_headers.png\" width=\"1293\" height=\"808\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/solarized-dark_iruby.png\" width=\"951\" height=\"498\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/chesterish_code_headers.png\" width=\"1293\" height=\"808\"></div></div></div></figure><!--kg-card-end: gallery-->","url":"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/","uuid":"0cfc9046-2e28-46a2-9f95-8851a9aea770","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c79b0070fa2b110f256e320"}},{"node":{"id":"Ghost__Post__5c17ddd4418434084a873d2a","title":"Drawing Mapbox Route Objects via the Directions API","slug":"mapbox-draw-route-objects","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","excerpt":"Using the Mapbox Directions API to visually draw routes.","custom_excerpt":"Using the Mapbox Directions API to visually draw routes.","created_at_pretty":"17 December, 2018","published_at_pretty":"28 February, 2019","updated_at_pretty":"03 March, 2019","created_at":"2018-12-17T12:33:08.000-05:00","published_at":"2019-02-28T09:15:52.000-05:00","updated_at":"2019-03-03T16:21:52.000-05:00","meta_title":"Draw Route Objects with Mapbox Directions API | Hackers and Slackers","meta_description":"Using the Mapbox Directions API to visually draw routes.","og_description":"Using the Mapbox Directions API to visually draw routes.","og_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","og_title":"Drawing Route Objects with Mapbox Directions API","twitter_description":"Using the Mapbox Directions API to visually draw routes.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","twitter_title":"Drawing Route Objects with Mapbox Directions API","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"If you've been here before, you probably already know our affinity for Mapbox \nand the visualization tools it provides data scientists and analysts. In the\npast, we've covered encoding location data from raw addresses\n[https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/], as well\nas an exploration of Mapbox Studio\n[https://hackersandslackers.com/map-data-visualization-with-mapbox/]  for those\ngetting acquainted with the tool. Today we're going a step further: drawing\ndirections on a map.\n\nIt sounds simple enough: we already know how to geocode addresses, so all we\nneed to do is literally go from point A to point B. That said, things always\ntend to get tricky, and if you've never worked with GeoJSON\n[http://geojson.org/]  before, you're in for a treat.\n\nLoad Up Some Data\nI'm going to assume you have a DataFrame ready containing these columns:\n\n * origin_longitude\n * origin_latitude\n * destination_longitude\n * destination_latitude\n * Name/description of this route \n\nIf you want to play along, there are plenty of free datasets out there to play\nwith - I sourced some information from BigQuery while I was testing things out.\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\nSo far so good- all we've done is load our data, and save our Mapbox token from\nan environment variable.\n\nMapbox Directions Endpoint\nNext, we're going to use Mapbox's Directions API\n[https://docs.mapbox.com/api/navigation/#directions]  to return a route for us.\nThe anatomy of a GET call to receive directions looks like this:\n\nhttps://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n\n\n * method_of_transportation refers to one of the three methods that Mapbox\n   offers for creating routes: driving-traffic, driving, walking, and cycling.\n   Note that there is currently no way to draw route objects which follow public\n   transit: this is perhaps Mapbox's biggest downfall at the moment.\n   Nevertheless, if this is something you need, data can be imported from Google\n   maps to be used with Mapbox.\n * access_token  can be either your public token (visible upon login at\n   mapbox.com) or a generated secret token.\n * geometries  accepts the method by which to draw the object. This can be \n   GeoJSON,  polyline, or polyline6. Let's stick with GeoJSON.\n\nConstructing API Requests\nLet's construct a request per row in our DataFrame. By using Pandas' apply, we\nfire a function per row to do just that:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n\n\nHere's where things get a little tricky. You see, GeoJSON abides by a strict\nformat. It looks something like this:\n\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"coordinates\": [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    \"type\": \"LineString\"\n  },\n  \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n  }],\n  \"weight_name\": \"duration\",\n  \"weight\": 3873.3,\n  \"duration\": 3873.3,\n  \"distance\": 9660.2,\n  \"properties\": {\n    \"name\": \"Empire State\"\n  }\n}\n\n\nFor the sake of being difficult, the Mapbox Directions API doesn't return\nresponses in exactly this format. Instead, their response looks like this:\n\n{\n  \"routes\": [{\n    \"geometry\": {\n      \"coordinates\": [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      \"type\": \"LineString\"\n    },\n    \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n    }],\n    \"weight_name\": \"duration\",\n    \"weight\": 3873.3,\n    \"duration\": 3873.3,\n    \"distance\": 9660.2\n  }],\n  \"waypoints\": [{\n      \"distance\": 34.00158252003884,\n      \"name\": \"West 33rd Street\",\n      \"location\": [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      \"distance\": 6.627227256764976,\n      \"name\": \"\",\n      \"location\": [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  \"code\": \"Ok\",\n  \"uuid\": \"cjsomodyl025642o6f1jsddx6\"\n}\n\n\nThe format isn't too  far off, but it's different enough to not work. \n\nFormatting GeoJSON Correctly\nWe need to write a function to take the response Mapbox has given us and\ntransform it into a usable GeoJSON format:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    \"\"\"Properly formats GeoJson for Mapbox visualization.\"\"\"\n    routes_dict = {\n        \"type\": \"Feature\",\n        \"geometry\": {\n            \"type\": \"LineString\"\n        },\n        \"weight_name\": \"duration\",\n        \"weight\": 718.9,\n        \"duration\": 0,\n        \"distance\": 0,\n        \"properties\": {\n            \"name\": \"\"\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n\n\nIt's not pretty, but it's reliable: we explicitly create the JSON structure we\nneed with routes_dict, and modify it with the API responses coming back from\nMapbox. Of course, we're still doing this one at a time, for every row in our\nDataFrame.\n\nYou'll notice I save each JSON file locally for now. In the future, we'll write\na script to automate the process of uploading our GeoJSON objects and adding\nthem to the proper Tilesets, but right now I just want to see that our work paid\noff!\n\nBy using Mapbox studio, we can see the result of our first route:\n\nA \"Driving\" Route from the Empire State Building to Battery Park.Aha! Would you\nlook at that- Mapbox knew to take the FDR drive. That's some promising stuff.\n\nDrawing Routes En Masse\nNaturally, this is only the tip of the iceberg: of the DataFrame of information\nwe loaded up, we've so far only viewed a single result. If anything in data is\nworth doing, it must be done thousands of times systematically without fail.\nLuckily, Mapbox provides us with the tools to do this: from lending us an S3\nbucket, to modifying datasets via the API, there's nothing to fear.\n\nTune in next time when do more... stuff!","html":"<p>If you've been here before, you probably already know our affinity for <strong>Mapbox</strong> and the visualization tools it provides data scientists and analysts. In the past, we've covered <a href=\"https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/\">encoding location data from raw addresses</a>, as well as an <a href=\"https://hackersandslackers.com/map-data-visualization-with-mapbox/\">exploration of Mapbox Studio</a> for those getting acquainted with the tool. Today we're going a step further: drawing directions on a map.</p><p>It sounds simple enough: we already know how to geocode addresses, so all we need to do is literally go from point A to point B. That said, things always tend to get tricky, and if you've never worked with <a href=\"http://geojson.org/\">GeoJSON</a> before, you're in for a treat.</p><h2 id=\"load-up-some-data\">Load Up Some Data</h2><p>I'm going to assume you have a DataFrame ready containing these columns:</p><ul><li>origin_longitude</li><li>origin_latitude</li><li>destination_longitude</li><li>destination_latitude</li><li>Name/description of this route </li></ul><p>If you want to play along, there are plenty of free datasets out there to play with - I sourced some information from <strong>BigQuery </strong>while I was testing things out.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n</code></pre>\n<!--kg-card-end: markdown--><p>So far so good- all we've done is load our data, and save our Mapbox token from an environment variable.</p><h2 id=\"mapbox-directions-endpoint\">Mapbox Directions Endpoint</h2><p>Next, we're going to use Mapbox's <a href=\"https://docs.mapbox.com/api/navigation/#directions\">Directions API</a> to return a route for us. The anatomy of a GET call to receive directions looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">https://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><strong>method_of_transportation </strong>refers to one of the three methods that Mapbox offers for creating routes: <em>driving-traffic</em>, <em>driving</em>, <em>walking</em>, and <em>cycling</em>. Note that there is currently no way to draw route objects which follow public transit: this is perhaps Mapbox's biggest downfall at the moment. Nevertheless, if this is something you need, data can be imported from Google maps to be used with Mapbox.</li><li><strong>access_token</strong> can be either your public token (visible upon login at mapbox.com) or a generated secret token.</li><li><strong>geometries</strong> accepts the method by which to draw the object. This can be <em>GeoJSON,</em> <em>polyline, </em>or <em>polyline6. </em>Let's stick with GeoJSON.</li></ul><h2 id=\"constructing-api-requests\">Constructing API Requests</h2><p>Let's construct a request per row in our DataFrame. By using Pandas' apply, we fire a function per row to do just that:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's where things get a little tricky. You see, GeoJSON abides by a strict format. It looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;type&quot;: &quot;Feature&quot;,\n  &quot;geometry&quot;: {\n    &quot;coordinates&quot;: [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    &quot;type&quot;: &quot;LineString&quot;\n  },\n  &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n  }],\n  &quot;weight_name&quot;: &quot;duration&quot;,\n  &quot;weight&quot;: 3873.3,\n  &quot;duration&quot;: 3873.3,\n  &quot;distance&quot;: 9660.2,\n  &quot;properties&quot;: {\n    &quot;name&quot;: &quot;Empire State&quot;\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>For the sake of being difficult, the Mapbox Directions API doesn't return responses in exactly this format. Instead, their response looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;routes&quot;: [{\n    &quot;geometry&quot;: {\n      &quot;coordinates&quot;: [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      &quot;type&quot;: &quot;LineString&quot;\n    },\n    &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n    }],\n    &quot;weight_name&quot;: &quot;duration&quot;,\n    &quot;weight&quot;: 3873.3,\n    &quot;duration&quot;: 3873.3,\n    &quot;distance&quot;: 9660.2\n  }],\n  &quot;waypoints&quot;: [{\n      &quot;distance&quot;: 34.00158252003884,\n      &quot;name&quot;: &quot;West 33rd Street&quot;,\n      &quot;location&quot;: [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      &quot;distance&quot;: 6.627227256764976,\n      &quot;name&quot;: &quot;&quot;,\n      &quot;location&quot;: [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  &quot;code&quot;: &quot;Ok&quot;,\n  &quot;uuid&quot;: &quot;cjsomodyl025642o6f1jsddx6&quot;\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>The format isn't <em>too</em> far off, but it's different enough to not work. </p><h2 id=\"formatting-geojson-correctly\">Formatting GeoJSON Correctly</h2><p>We need to write a function to take the response Mapbox has given us and transform it into a usable GeoJSON format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    &quot;&quot;&quot;Properly formats GeoJson for Mapbox visualization.&quot;&quot;&quot;\n    routes_dict = {\n        &quot;type&quot;: &quot;Feature&quot;,\n        &quot;geometry&quot;: {\n            &quot;type&quot;: &quot;LineString&quot;\n        },\n        &quot;weight_name&quot;: &quot;duration&quot;,\n        &quot;weight&quot;: 718.9,\n        &quot;duration&quot;: 0,\n        &quot;distance&quot;: 0,\n        &quot;properties&quot;: {\n            &quot;name&quot;: &quot;&quot;\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>It's not pretty, but it's reliable: we explicitly create the JSON structure we need with <code>routes_dict</code>, and modify it with the API responses coming back from Mapbox. Of course, we're still doing this one at a time, for every row in our DataFrame.</p><p>You'll notice I save each JSON file locally for now. In the future, we'll write a script to automate the process of uploading our GeoJSON objects and adding them to the proper Tilesets, but right now I just want to see that our work paid off!</p><p>By using Mapbox studio, we can see the result of our first route:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2019-02-28-at-8.05.21-AM.png\" class=\"kg-image\"><figcaption>A \"Driving\" Route from the Empire State Building to Battery Park.</figcaption></figure><!--kg-card-end: image--><p>Aha! Would you look at that- Mapbox knew to take the FDR drive. That's some promising stuff.</p><h3 id=\"drawing-routes-en-masse\">Drawing Routes En Masse</h3><p>Naturally, this is only the tip of the iceberg: of the DataFrame of information we loaded up, we've so far only viewed a single result. If anything in data is worth doing, it must be done thousands of times systematically without fail. Luckily, Mapbox provides us with the tools to do this: from lending us an S3 bucket, to modifying datasets via the API, there's nothing to fear.</p><p>Tune in next time when do more... stuff!</p>","url":"https://hackersandslackers.com/mapbox-draw-route-objects/","uuid":"ef0a4639-8818-475b-9a25-6a20b13c1ecf","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c17ddd4418434084a873d2a"}},{"node":{"id":"Ghost__Post__5c654ed3eab17b74dbf2d2b0","title":"Welcome to SQL 3: Building Relations and Combining Data Sets","slug":"welcome-to-sql-3-building-relationships-and-combining-data","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","custom_excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","created_at_pretty":"14 February, 2019","published_at_pretty":"26 February, 2019","updated_at_pretty":"10 April, 2019","created_at":"2019-02-14T06:19:47.000-05:00","published_at":"2019-02-25T20:11:28.000-05:00","updated_at":"2019-04-10T10:16:10.000-04:00","meta_title":"Relationships and Combining Data in SQL | Hackers and Slackers","meta_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","og_title":"Welcome to SQL 3: Building Relationships and Combining Data","twitter_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","twitter_title":"Welcome to SQL 3: Building Relationships and Combining Data","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"If you've felt a bit distance or estranged from SQL so far in the series, never\nfear: we're about to discover the magic of what makes relational databases so...\n relational.  Turn down the lights and put on your favorite Marvin Gaye track;\nwe're about to make connections on a whole other level.\n\nI find that existing attempts to explain Database relations (JOINs in\nparticular) have been an utter failure in illustrating these concepts. The Venn\nDiagrams we're all accustomed to seeing mean nothing to somebody who has never\nseen a JOIN occur, and even then, do they really  describe what's happening? I'd\nlove to toss together some quick animations as an alternative, but chances are\nI'll settle for something mediocre like the rest of us.\n\nRelational Databases in Action\nAs much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no,\nnot that  talk; I meant the obligatory \nexample-of-how-two-tables-might-relate-to-one-another  talk. This talk is a bit\nless awkward, but it definitely won't prepare you for the finer things in life.\nJust kidding, data is  the finer part of life. Or at least it is in mine. Let's\nnot linger on that too long.\n\nLet's look at the most common scenario used to illustrate data relationships:\nthe customers  vs. orders  predicament. Let's say we decided to open up an \nOrganic Vegan Paleo Keto Kale Voltron 5000  health-food marketplace to cater to\na high-end clientele: pretentious rich assholes. It just so happens that the\n\"rich asshole\" market is very receptive to best practices in customer relations,\nso we start a CRM to track our best customers. This record-keeping helps us\npretend to remember the names and personalities of our clientele:\n\nCustomers Table\nid\n first_name\n last_name\n email\n gender\n state\n phone\n 653466635\n Timothea\n Crat\n tcrat0@bandcamp.com\n Female\n Washington\n 206-220-3752\n 418540868\n Kettie\n Fuggle\n kfuggle1@cafepress.com\n Female\n California\n 661-793-1372\n 857532654\n Boonie\n Sommerland\n bsommerland2@soundcloud.com\n Male\n North Carolina\n 919-299-0715\n 563295938-4\n Red\n Seldon\n rseldon3@addthis.com\n Male\n Indiana\n 765-880-7420\n 024844147\n Marika\n Gallatly\n mgallatly4@loc.gov\n Female\n New York\n 718-126-1462\n 900992907\n Sharlene\n McMaster\n smcmaster5@gmpg.org\n Female\n Nevada\n 775-376-0931\n 329211747-X\n Grover\n Okey\n gokey6@weather.com\n Male\n Texas\n 915-913-0625\n 656608031\n Farly\n Pluck\n fpluck7@buzzfeed.com\n Male\n Texas\n 432-670-8809\n 906380018\n Sumner\n Pickerell\n spickerellb@bloglovin.com\n Male\n Colorado\n 719-239-5042\n On the other hand, we need to keep track of inventory and items sold. Since\nwe're already swiping credit cards and getting all this personal customer data,\nwhy not associate purchases to loyal customers? Thus, we have a list of\ntransactions which looks something as such:\n\nOrders Table\nitem_id\n customer_id\n item_purchased\n first_name\n last_name\n amount\n date_purchased\n 82565290-530d-4272-9c8b-38dc0bc7426a\n 653466635\n Creme De Menthe Green\n Timothea\n Crat\n $8.57\n 5/13/18\n 9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0\n 653466635\n Veal Inside - Provimi\n Timothea\n Crat\n $5.77\n 3/3/18\n 5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11\n 656608031\n Arizona - Plum Green Tea\n Grover\n Okey\n $1.72\n 9/6/18\n b4813421-12e8-479b-a3b6-3d1c4c539625\n 656608031\n Beer - Fruli\n Grover\n Okey\n $4.05\n 10/1/18\n 4e7c8548-340f-4e89-a7f1-95173dcc6e53\n 656608031\n Boogies\n Grover\n Okey\n $1.97\n 12/17/18\n 65261e94-494d-48cc-8d5a-642ae6921600\n 656608031\n Cup - 3.5oz; Foam\n Grover\n Okey\n $1.84\n 11/28/18\n 1bfdca0f-d54a-4845-bbf5-982813ab4a65\n 656608031\n Arizona - Green Tea\n Grover\n Gauford\n $0.22\n 5/23/18\n d20d7add-bad4-4559-8896-d4f6d05aa3dd\n 906380018\n Lemonade - Strawberry; 591 Ml\n Sumner\n Tortoishell\n $7.98\n 10/11/18\n 12134510-bc6c-4bd7-b733-b549a61edaa3\n 906380018\n Pasta - Cappellini; Dry\n Sumner\n Wash\n $0.31\n 11/13/18\n 80f1957c-df4d-40dc-b9c4-2c3939dd0865\n 906380018\n Remy Red Berry Infusion\n Sumner\n Pisculli\n $1.25\n 12/31/18\n a75f7593-3312-43e4-a604-43405f02efdd\n 906380018\n Veal - Slab Bacon\n Sumner\n Janaszewski\n $9.80\n 3/9/18\n c6ef1f55-f35d-4618-8de7-36f59ea6653a\n 906380018-5\n Beans - Black Bean; Dry\n Sumner\n Piegrome\n $1.36\n 12/11/18\n c5b87ee3-da94-41b1-973a-ef544a3ffb6f\n 906380018\n Calypso - Strawberry Lemonade\n Sumner\n Piegrome\n $7.71\n 2/21/19\n e383c58b-d8da-40ac-afd6-7ee629dc95c6\n 656608031\n Basil - Primerba; Paste\n Mohammed\n Reed\n $2.77\n 10/21/18\n d88ccd5b-0acb-4144-aceb-c4b4b46d3b17\n 656608031\n Cheese - Fontina\n Mohammed\n Reed\n $4.24\n 7/14/18\n 659df773-719c-447e-a1a9-4577dc9c6885\n 656608031\n Cotton Wet Mop 16 Oz\n Jock\n Skittles\n $8.44\n 1/24/19\n ff52e91e-4a49-4a52-b9a5-ddc0b9316429\n 656608031\n Pastry - Trippleberry Muffin - Mini\n Jock\n Skittles\n $9.77\n 11/17/18\n 86f8ad6a-c04c-4714-8f39-01c28dcbb3cb\n 656608031\n Bread - Olive\n Jock\n Skittles\n $4.51\n 1/10/19\n e7a66b71-86ff-4700-ac57-71291e6997b0\n 656608031\n Wine - White; Riesling; Semi - Dry\n Farly\n Pluck\n $4.23\n 4/15/18\n c448db87-1246-494a-bae4-dceb8ee8a7ae\n 656608031\n Melon - Honey Dew\n Farly\n Pluck\n $1.00\n 9/10/18\n 725c171a-452d-45ef-9f23-73ef20109b90\n 656608031\n Sugar - Invert\n Farly\n Pluck\n $9.04\n 3/24/18\n 849f9140-1469-4e23-a1de-83533af5fb88\n 656608031\n Yokaline\n Farly\n Pluck\n $3.21\n 12/31/18\n 2ea79a6b-bfec-4a08-9457-04128f3b37a9\n 656608031\n Cake - Bande Of Fruit\n Farly\n Pluck\n $1.57\n 5/20/18\n Naturally, customers buy more than one item; they buy a lot. Especially that \nFarly Pluck guy at the bottom- quite the unfortunate auto-generated name.\n\nAs standalone tables, the customers  and orders  tables each serve at least one\nstraightforward purpose on their own. The Customers  table helps us with\nconsumer demographic analysis, whereas the Orders  table makes sure weâre making\nmoney and aren't getting robbed. While important, neither of the functions are\nparticularly revolutionary: this basic level of record keeping has been at the\ncore of nearly every business since the 70s. \n\nThe ability to combine data enables us to gain far more significant insights. We\ncan reward loyal customers, cater to the needs of individuals based on their\npreferences, and perhaps even sell the personal data of where and when Mr. Pluck\nhas been every Tuesday and Thursday for the past 4 months to the highest bidding\ndata broker (hint: he's at our store).\n\nThanks to relational databases, we are neither limited to single monolithic\ntables nor are we shackled by the constraints of the tables we set up front.\nAssociating data is trivial, as long as we have a means by which to associate it\nby. Below is a visualization of matching a foreign key  in our orders table to a\n primary key  in our Customers  table:\n\nAn Order's Foreign Key References a customer's IDThe above illustrates what\nwe've already brushed on a bit: Foreign Key association. Primary and foreign\nkeys are essential to describing relations between the tables, and in performing\nSQL joins. Without further adieu, let's join some data.\n\nJoining Sets of Data\nTo âjoinâ multiple sets of data is to consolidate multiple tables into one. \n\nThe manner of this consolidation is determined by which of the four methods of\njoining tables we use: inner joins, right joins, left joins, and outer joins \n(left and right joins are kind of the same, but whatever). Regardless of the\ntype of join, all joins have the following in common:\n\n * Row comparison: we look for rows where the values of a column in Table A \n   match the values of a column in Table B.\n * Consolidation of columns: The purpose of any join is to come away with a\n   table containing columns from both  tables. \n\nLEFT & RIGHT JOINs\nLEFT  and RIGHT  joins cover a myriad of use cases. With a bit of creativity,\nleft/right joins can help solve problems we may not have expected. The terms \"\nleft\" and \"right\" refer to the table we'd like to join on when reading from\nleft-to-right. When joining tables via LEFT JOIN, the first  table in our query\nwill be the \"left\" table. Alternatively, a RIGHT JOIN  refers to the last \ntable. \n\nWhen we say \"table to join on,\" we're specifying which table's key values will\nbe the \"authority\" for our merge. In a LEFT MERGE, all  of the records in Table\nA will survive the merge:\n\n * For rows which have a match in Table B, these rows will be 'extended' to\n   include the data in Table B. This means the new columns being added to Table\n   A  from  Table B  will contain data for all rows where an association has\n   been made.\n * For rows which exist in Table A  but do NOT have a match in Table B, these\n   rows are unaffected: they will contain the same data as before the join, with\n   values in the new columns left blank.\n * Keys which exist in Table B  but do NOT exist in Table A  will be discarded.\n   The purpose of these joins is to enrich the data of the primary table.\n\nBelow is an example of an actual left join I use to power the Kanban board\nmodule on our \"Projects\" page. The left table is a table of JIRA issues, and the\nright table is a collection of issue-based customizations, such as custom icons\nand colors for issue types. Take a look at how this data is associated, and what\nmakes it into the final table:\n\nKeys on the left table determine which rows stay or go.The structure of a LEFT JOIN  query looks as such:\n\nSELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n\n\nHere's an example with actual values:\n\nSELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n\n\nCompare this to a RIGHT JOIN:\n\nSELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n\n\nINNER JOIN (or CROSS JOIN)\nInner joins are the most conservative method for joining sets of data. Unlike \nLEFT  or RIGHT  joins, there is no authoritative table in an inner join:  only\nrows which contain a match in all  tables will survive the join. All other rows\nwill be ignored:\n\nSELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nBecause inner joins will only act on rows which match in all affected tables, an\ninner join will typically contain the most \"complete\" data set (highest number\nof columns satisfied with values), but will contain the fewest number of rows. \n\nOUTER JOINs\nOuter joins  actually come in a few different flavors. Generally speaking, outer\njoins maximize the amount of data which will survive after the join is\nperformed. \n\nLEFT (OR RIGHT) OUTER JOIN\nAt first glance, you might look at the results of a left/right outer  join and\nmistake them to exactly the same as their pure left/right join counterparts.\nWell, you actually wouldn't be mistaken at all! That's right, I was lying:\nthere's essentially no difference between types of joins (thus our time\nmentioning them has been worthless).\n\nFULL OUTER JOIN\nIn a full outer join, all  columns and rows will be joined into the resulting\noutput, regardless of whether or not the rows matched on our specified key. Why\ndo we specify a key at all, you ask? Matching rows on a key still  combines rows\nwhich are similar to all involved tables (if there are truly no rows with common\nground during a merge, you should ask yourself why you're merging two unrelated\nsets of data in the first place).\n\nThe result is kind of a mess. I'm going to borrow an illustration from the \nPandas  documentation here:\n\nSource: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\nWhile Column B appears to be left somewhat intact, take a look at what's\nhappening around it: columns labeled A_x and A_y  have been generated as a\nresult of the join. The outer join has created a table where every possible\ncombination of values for the keys in column B exists. Thus, the number of rows\nin our new table is effectively length of Table A  *  length of Table B.\n\nI personally rarely use outer joins, but that's just me.\n\nSELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nScenario: Create a New Table from Multiple JOINs\nSo far we've only looked at examples of two tables being joined at once. In\nfact, we can merge as many tables as we want, all at once! Going back to the\nJIRA example, here is the actual query I use to create the final table which\npowers a custom Kanban board:\n\nCREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\n\nIf you're using PostgreSQL, views are a great way to save the results of a join\nwithout adding additional tables. Instead of using CREATE TABLE, try using \nCREATE VIEW:CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\nUnions & Union All\nA good way to think about JOINs is extending our dataset horizontally. A UNION,\nthen, is a way of combining data vertically. Unions  combine data sets with the\nsame structure: they simply create a table with rows from both tables. UNION \noperators can combine the result-set of two or more SELECT statements, as long\nas:\n\n * Each SELECT statement within UNION must have the same number of columns.\n * The columns must also have similar data types.\n * The columns in each SELECT statement must also be in the same order.\n\nUNION\nSELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n\n\nUNION (with WHERE)\nWe can also add logic to unions via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nUNION ALL\nAn interesting distinction is the presence of UNION  versus UNION ALL. Of the\ntwo, UNION  is the more \"intelligent\" operation: if identical rows exist in both\nSELECT queries, a UNION  will know to only give us one row to avoid duplicates.\nOn the other hand, UNION ALL  does  return duplicates: this results in a faster\nquery and could be useful for those who want to know what is in both SELECT \nstatements:\n\nSELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n\n\nUNION ALL (with WHERE)\nJust like UNION, we can add logic to union all via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nMore SQL Ahead\nI hope that visualizing the way which JOINs  and UNIONs  work can help to reduce\nfriction for SQL new-comers. I find it difficult to believe that human beings\ncan fully grasp these concepts without seeing them happen first-hand, which begs\nthe question: why would anybody explore something so poorly explained, without\nknowing the benefits?\n\nIf you find these guides useful, feel welcome to holler at me to keep them\ncoming. We still have more SQL ahead in our series: stay tuned for when we\nexplore aggregate values and more!","html":"<p>If you've felt a bit distance or estranged from SQL so far in the series, never fear: we're about to discover the magic of what makes relational databases so... <em>relational.</em> Turn down the lights and put on your favorite Marvin Gaye track; we're about to make connections on a whole other level.</p><p>I find that existing attempts to explain Database relations (JOINs in particular) have been an utter failure in illustrating these concepts. The Venn Diagrams we're all accustomed to seeing mean nothing to somebody who has never seen a JOIN occur, and even then, do they <em>really</em> describe what's happening? I'd love to toss together some quick animations as an alternative, but chances are I'll settle for something mediocre like the rest of us.</p><h2 id=\"relational-databases-in-action\">Relational Databases in Action</h2><p>As much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no, not <em>that</em> talk; I meant the obligatory <em>example-of-how-two-tables-might-relate-to-one-another</em> talk. This talk is a bit less awkward, but it definitely won't prepare you for the finer things in life. Just kidding, data <em>is</em> the finer part of life. Or at least it is in mine. Let's not linger on that too long.</p><p>Let's look at the most common scenario used to illustrate data relationships: the <strong>customers</strong> vs. <strong>orders</strong> predicament. Let's say we decided to open up an <strong>Organic Vegan Paleo Keto Kale Voltron 5000</strong> health-food marketplace to cater to a high-end clientele: pretentious rich assholes. It just so happens that the \"rich asshole\" market is very receptive to best practices in customer relations, so we start a CRM to track our best customers. This record-keeping helps us pretend to remember the names and personalities of our clientele:</p><h3 id=\"customers-table\">Customers Table</h3><!--kg-card-begin: html--><style>\n    .table1 td {\n        padding: 15px;\n    display: table-cell;\n    text-align: left;\n    vertical-align: middle;\n    font-size: 0.8em;\n    text-align: center;\n    line-height: 1.2;\n    font-size: .75em;\n    }\n    \n    \n    .table1 td:nth-of-type(4) {\n        max-width: 80px;\n    }\n    .table1 td:last-of-type{\n        min-width: 100px;\n    }\n    \n        \n</style>\n\n<div class=\"tableContainer\">\n<table class=\"table1\">\n\t<thead>\n       <tr>\n              <th>id</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>email</th>\n              <th>gender</th>\n              <th>state</th>\n              <th>phone</th>\n          </tr>\n    </thead>\n    <tbody>\n       <tr>\n              <td>653466635</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>tcrat0@bandcamp.com</td>\n              <td>Female</td>\n              <td>Washington</td>\n              <td>206-220-3752</td>\n          </tr>\n       <tr>\n              <td>418540868</td>\n              <td>Kettie</td>\n              <td>Fuggle</td>\n              <td>kfuggle1@cafepress.com</td>\n              <td>Female</td>\n              <td>California</td>\n              <td>661-793-1372</td>\n          </tr>\n       <tr>\n              <td>857532654</td>\n              <td>Boonie</td>\n              <td>Sommerland</td>\n              <td>bsommerland2@soundcloud.com</td>\n              <td>Male</td>\n              <td>North Carolina</td>\n              <td>919-299-0715</td>\n          </tr>\n       <tr>\n              <td>563295938-4</td>\n              <td>Red</td>\n              <td>Seldon</td>\n              <td>rseldon3@addthis.com</td>\n              <td>Male</td>\n              <td>Indiana</td>\n              <td>765-880-7420</td>\n          </tr>\n       <tr>\n              <td>024844147</td>\n              <td>Marika</td>\n              <td>Gallatly</td>\n              <td>mgallatly4@loc.gov</td>\n              <td>Female</td>\n              <td>New York</td>\n              <td>718-126-1462</td>\n          </tr>\n       <tr>\n              <td>900992907</td>\n              <td>Sharlene</td>\n              <td>McMaster</td>\n              <td>smcmaster5@gmpg.org</td>\n              <td>Female</td>\n              <td>Nevada</td>\n              <td>775-376-0931</td>\n          </tr>\n       <tr>\n              <td>329211747-X</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>gokey6@weather.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>915-913-0625</td>\n          </tr>\n       <tr>\n              <td>656608031</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>fpluck7@buzzfeed.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>432-670-8809</td>\n          </tr>\n        <tr>\n              <td>906380018</td>\n              <td>Sumner</td>\n              <td>Pickerell</td>\n              <td>spickerellb@bloglovin.com</td>\n              <td>Male</td>\n              <td>Colorado</td>\n              <td>719-239-5042</td>\n          </tr>\n    </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>On the other hand, we need to keep track of inventory and items sold. Since we're already swiping credit cards and getting all this personal customer data, why not associate purchases to loyal customers? Thus, we have a list of transactions which looks something as such:</p><h3 id=\"orders-table\">Orders Table</h3><!--kg-card-begin: html--><style>\n    .table2 td {\n      padding: 10px 15px;\n    }\n    \n    .table2 tr td:first-of-type {\n        min-width: 100px !important;\n    white-space: nowrap;\n    text-overflow: ellipsis;\n    max-width: 100px;\n    }\n    \n    .table2 tr td:nth-of-type(2) {\n        min-width: 85px !important;\n    }\n</style>\n\n\n<div class=\"tableContainer\">\n   <table class=\"table2\">\n          <thead>\n       <tr>\n              <th>item_id</th>\n              <th>customer_id</th>\n              <th>item_purchased</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>amount</th>\n              <th>date_purchased</th>\n          </tr>\n       </thead>\n       <tbody>\n       <tr>\n              <td>82565290-530d-4272-9c8b-38dc0bc7426a</td>\n              <td>653466635</td>\n              <td>Creme De Menthe Green</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$8.57</td>\n              <td>5/13/18</td>\n          </tr>\n       <tr>\n              <td>9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0</td>\n              <td>653466635</td>\n              <td>Veal Inside - Provimi</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$5.77</td>\n              <td>3/3/18</td>\n          </tr>\n       <tr>\n              <td>5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11</td>\n              <td>656608031</td>\n              <td>Arizona - Plum Green Tea</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.72</td>\n              <td>9/6/18</td>\n          </tr>\n       <tr>\n              <td>b4813421-12e8-479b-a3b6-3d1c4c539625</td>\n              <td>656608031</td>\n              <td>Beer - Fruli</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$4.05</td>\n              <td>10/1/18</td>\n          </tr>\n       <tr>\n              <td>4e7c8548-340f-4e89-a7f1-95173dcc6e53</td>\n              <td>656608031</td>\n              <td>Boogies</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.97</td>\n              <td>12/17/18</td>\n          </tr>\n       <tr>\n              <td>65261e94-494d-48cc-8d5a-642ae6921600</td>\n              <td>656608031</td>\n              <td>Cup - 3.5oz; Foam</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.84</td>\n              <td>11/28/18</td>\n          </tr>\n       <tr>\n              <td>1bfdca0f-d54a-4845-bbf5-982813ab4a65</td>\n              <td>656608031</td>\n              <td>Arizona - Green Tea</td>\n              <td>Grover</td>\n              <td>Gauford</td>\n              <td>$0.22</td>\n              <td>5/23/18</td>\n          </tr>\n       <tr>\n              <td>d20d7add-bad4-4559-8896-d4f6d05aa3dd</td>\n              <td>906380018</td>\n              <td>Lemonade - Strawberry; 591 Ml</td>\n              <td>Sumner</td>\n              <td>Tortoishell</td>\n              <td>$7.98</td>\n              <td>10/11/18</td>\n          </tr>\n       <tr>\n              <td>12134510-bc6c-4bd7-b733-b549a61edaa3</td>\n              <td>906380018</td>\n              <td>Pasta - Cappellini; Dry</td>\n              <td>Sumner</td>\n              <td>Wash</td>\n              <td>$0.31</td>\n              <td>11/13/18</td>\n          </tr>\n       <tr>\n              <td>80f1957c-df4d-40dc-b9c4-2c3939dd0865</td>\n              <td>906380018</td>\n              <td>Remy Red Berry Infusion</td>\n              <td>Sumner</td>\n              <td>Pisculli</td>\n              <td>$1.25</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>a75f7593-3312-43e4-a604-43405f02efdd</td>\n              <td>906380018</td>\n              <td>Veal - Slab Bacon</td>\n              <td>Sumner</td>\n              <td>Janaszewski</td>\n              <td>$9.80</td>\n              <td>3/9/18</td>\n          </tr>\n       <tr>\n              <td>c6ef1f55-f35d-4618-8de7-36f59ea6653a</td>\n              <td>906380018-5</td>\n              <td>Beans - Black Bean; Dry</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$1.36</td>\n              <td>12/11/18</td>\n          </tr>\n       <tr>\n              <td>c5b87ee3-da94-41b1-973a-ef544a3ffb6f</td>\n              <td>906380018</td>\n              <td>Calypso - Strawberry Lemonade</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$7.71</td>\n              <td>2/21/19</td>\n          </tr>\n       <tr>\n              <td>e383c58b-d8da-40ac-afd6-7ee629dc95c6</td>\n              <td>656608031</td>\n              <td>Basil - Primerba; Paste</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$2.77</td>\n              <td>10/21/18</td>\n          </tr>\n       <tr>\n              <td>d88ccd5b-0acb-4144-aceb-c4b4b46d3b17</td>\n              <td>656608031</td>\n              <td>Cheese - Fontina</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$4.24</td>\n              <td>7/14/18</td>\n          </tr>\n       <tr>\n              <td>659df773-719c-447e-a1a9-4577dc9c6885</td>\n              <td>656608031</td>\n              <td>Cotton Wet Mop 16 Oz</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$8.44</td>\n              <td>1/24/19</td>\n          </tr>\n       <tr>\n              <td>ff52e91e-4a49-4a52-b9a5-ddc0b9316429</td>\n              <td>656608031</td>\n              <td>Pastry - Trippleberry Muffin - Mini</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$9.77</td>\n              <td>11/17/18</td>\n          </tr>\n       <tr>\n              <td>86f8ad6a-c04c-4714-8f39-01c28dcbb3cb</td>\n              <td>656608031</td>\n              <td>Bread - Olive</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$4.51</td>\n              <td>1/10/19</td>\n          </tr>\n\t\t\t<tr>\n\n              <td>e7a66b71-86ff-4700-ac57-71291e6997b0</td>\n              <td>656608031</td>\n              <td>Wine - White; Riesling; Semi - Dry</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$4.23</td>\n              <td>4/15/18</td>\n          </tr>\n       <tr>\n              <td>c448db87-1246-494a-bae4-dceb8ee8a7ae</td>\n              <td>656608031</td>\n              <td>Melon - Honey Dew</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.00</td>\n              <td>9/10/18</td>\n          </tr>\n       <tr>\n              <td>725c171a-452d-45ef-9f23-73ef20109b90</td>\n              <td>656608031</td>\n              <td>Sugar - Invert</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$9.04</td>\n              <td>3/24/18</td>\n          </tr>\n       <tr>\n              <td>849f9140-1469-4e23-a1de-83533af5fb88</td>\n              <td>656608031</td>\n              <td>Yokaline</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$3.21</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>2ea79a6b-bfec-4a08-9457-04128f3b37a9</td>\n              <td>656608031</td>\n              <td>Cake - Bande Of Fruit</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.57</td>\n              <td>5/20/18</td>\n          </tr>\n       </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>Naturally, customers buy more than one item; they buy a <em>lot. </em>Especially that <strong>Farly Pluck </strong>guy at the bottom- quite the unfortunate auto-generated name.</p><p>As standalone tables, the <strong>customers</strong> and <strong>orders</strong> tables each serve at least one straightforward purpose on their own. The <strong>Customers</strong> table helps us with consumer demographic analysis, whereas the <strong>Orders</strong> table makes sure weâre making money and aren't getting robbed. While important, neither of the functions are particularly revolutionary: this basic level of record keeping has been at the core of nearly every business since the 70s. </p><p>The ability to combine data enables us to gain far more significant insights. We can reward loyal customers, cater to the needs of individuals based on their preferences, and perhaps even sell the personal data of where and when Mr. Pluck has been every Tuesday and Thursday for the past 4 months to the highest bidding data broker (hint: he's at our store).</p><p>Thanks to relational databases, we are neither limited to single monolithic tables nor are we shackled by the constraints of the tables we set up front. Associating data is trivial, as long as we have a <em>means by which to associate it by</em>. Below is a visualization of matching a <strong>foreign key</strong> in our orders table to a <strong>primary key</strong> in our <strong>Customers</strong> table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/orders4.gif\" class=\"kg-image\"><figcaption>An Order's Foreign Key References a customer's ID</figcaption></figure><!--kg-card-end: image--><p>The above illustrates what we've already brushed on a bit: Foreign Key association. Primary and foreign keys are essential to describing relations between the tables, and in performing SQL joins. Without further adieu, let's join some data.</p><h2 id=\"joining-sets-of-data\">Joining Sets of Data</h2><p>To âjoinâ multiple sets of data is to consolidate multiple tables into one. </p><p>The manner of this consolidation is determined by which of the four methods of joining tables we use: <strong>inner joins</strong>, <strong>right joins</strong>, <strong>left joins</strong>, and <strong>outer joins</strong> (left and right joins are kind of the same, but whatever). Regardless of the type of <em>join</em>, all joins have the following in common:</p><ul><li>Row comparison: we look for rows where the values of a column in <strong>Table A</strong> match the values of a column in <strong>Table B</strong>.</li><li>Consolidation of columns: The purpose of any join is to come away with a table containing columns from <em>both</em> tables. </li></ul><h3 id=\"left-right-joins\">LEFT &amp; RIGHT JOINs</h3><p><code>LEFT</code> and <code>RIGHT</code> joins cover a myriad of use cases. With a bit of creativity, left/right joins can help solve problems we may not have expected. The terms \"<strong>left</strong>\" and \"<strong>right</strong>\" refer to the table we'd like to join on when reading from left-to-right. When joining tables via <code>LEFT JOIN</code>, the <em>first</em> table in our query will be the \"left\" table. Alternatively, a <code>RIGHT JOIN</code> refers to the <em>last</em> table. </p><p>When we say \"table to join on,\" we're specifying which table's key values will be the \"authority\" for our merge. In a <code>LEFT MERGE</code>, <em>all</em> of the records in <strong>Table A </strong>will survive the merge:</p><ul><li>For rows which have a match in <strong>Table B</strong>, these rows will be 'extended' to include the data in <strong>Table B</strong>. This means the new columns being added to <strong>Table A</strong> from<strong> Table B</strong> will contain data for all rows where an association has been made.</li><li>For rows which exist in <strong>Table A</strong> but do NOT have a match in <strong>Table B</strong>, these rows are unaffected: they will contain the same data as before the join, with values in the new columns left blank.</li><li>Keys which exist in <strong>Table B</strong> but do NOT exist in <strong>Table A</strong> will be discarded. The purpose of these joins is to enrich the data of the primary table.</li></ul><p>Below is an example of an actual left join I use to power the Kanban board module on our \"Projects\" page. The left table is a table of JIRA issues, and the right table is a collection of issue-based customizations, such as custom icons and colors for issue types. Take a look at how this data is associated, and what makes it into the final table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/tables15.gif\" class=\"kg-image\"><figcaption>Keys on the left table determine which rows stay or go.</figcaption></figure><!--kg-card-end: image--><p>The structure of a <code>LEFT JOIN</code> query looks as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's an example with actual values:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>Compare this to a <strong>RIGHT JOIN:</strong></p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"inner-join-or-cross-join-\">INNER JOIN (or CROSS JOIN)</h3><p>Inner joins are the most conservative method for joining sets of data. Unlike <code>LEFT</code> or <code>RIGHT</code> joins, there is no authoritative table in an <strong>inner join:</strong> only rows which contain a match in <em>all</em> tables will survive the join. All other rows will be ignored:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Because inner joins will only act on rows which match in all affected tables, an inner join will typically contain the most \"complete\" data set (highest number of columns satisfied with values), but will contain the fewest number of rows. </p><h2 id=\"outer-joins\">OUTER JOINs</h2><p><strong>Outer joins</strong> actually come in a few different flavors. Generally speaking, outer joins maximize the amount of data which will survive after the join is performed. </p><h3 id=\"left-or-right-outer-join\">LEFT (OR RIGHT) OUTER JOIN</h3><p>At first glance, you might look at the results of a left/right <em>outer</em> join and mistake them to exactly the same as their pure left/right join counterparts. Well, you actually wouldn't be mistaken at all! That's right, I was lying: there's essentially no difference between types of joins (thus our time mentioning them has been worthless).</p><h3 id=\"full-outer-join\">FULL OUTER JOIN</h3><p>In a <strong>full outer join</strong>, <em>all</em> columns and rows will be joined into the resulting output, regardless of whether or not the rows matched on our specified key. Why do we specify a key at all, you ask? Matching rows on a key <em>still</em> combines rows which are similar to all involved tables (if there are truly no rows with common ground during a merge, you should ask yourself why you're merging two unrelated sets of data in the first place).</p><p>The result is kind of a mess. I'm going to borrow an illustration from the <strong>Pandas</strong> documentation here:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/merging_merge_on_key_dup.png\" class=\"kg-image\"><figcaption>Source: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html</figcaption></figure><!--kg-card-end: image--><p>While Column B appears to be left somewhat intact, take a look at what's happening around it: columns labeled <strong>A_x </strong>and <strong>A_y</strong> have been generated as a result of the join. The outer join has created a table where every possible combination of values for the keys in column B exists. Thus, the number of rows in our new table is effectively <strong><em>length of Table A</em> </strong>*<strong> <em>length of Table B</em>.</strong></p><p>I personally rarely use <strong>outer joins</strong>, but that's just me.</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"scenario-create-a-new-table-from-multiple-joins\">Scenario: Create a New Table from Multiple JOINs</h2><p>So far we've only looked at examples of two tables being joined at once. In fact, we can merge as many tables as we want, all at once! Going back to the JIRA example, here is the actual query I use to create the final table which powers a custom Kanban board:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">CREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><div class=\"protip\">\n    If you're using PostgreSQL, views are a great way to save the results of a join without adding additional tables. Instead of using <code>CREATE TABLE</code>, try using <code>CREATE VIEW</code>:\n<pre><code>CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;</code></pre>\n</div><!--kg-card-end: markdown--><h2 id=\"unions-union-all\">Unions &amp; Union All</h2><p>A good way to think about <code>JOIN</code>s is extending our dataset <em>horizontally</em>. A <code>UNION</code>, then, is a way of combining data <em>vertically. </em><strong>Unions</strong><em> </em>combine data sets with the same structure: they simply create a table with rows from both tables. <code>UNION</code> operators can combine the result-set of two or more SELECT statements, as long as:</p><ul><li>Each SELECT statement within UNION must have the same number of columns.</li><li>The columns must also have similar data types.</li><li>The columns in each SELECT statement must also be in the same order.</li></ul><h3 id=\"union\">UNION</h3><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-with-where-\">UNION (with WHERE)</h3><p>We can also add logic to <strong>unions </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all\">UNION ALL</h3><p>An interesting distinction is the presence of <code>UNION</code> versus <code>UNION ALL</code>. Of the two, <code>UNION</code> is the more \"intelligent\" operation: if identical rows exist in both SELECT <code>queries</code>, a <code>UNION</code> will know to only give us one row to avoid duplicates. On the other hand, <code>UNION ALL</code> <em>does</em> return duplicates: this results in a faster query and could be useful for those who want to know what is in both <code>SELECT</code> statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all-with-where-\">UNION ALL (with WHERE)</h3><p>Just like <code>UNION</code>, we can add logic to <strong>union all </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"more-sql-ahead\">More SQL Ahead</h2><p>I hope that visualizing the way which <strong>JOINs</strong> and <strong>UNIONs</strong> work can help to reduce friction for SQL new-comers. I find it difficult to believe that human beings can fully grasp these concepts without seeing them happen first-hand, which begs the question: why would anybody explore something so poorly explained, without knowing the benefits?</p><p>If you find these guides useful, feel welcome to holler at me to keep them coming. We still have more SQL ahead in our series: stay tuned for when we explore aggregate values and more!</p>","url":"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/","uuid":"5e222417-19b5-49a7-aa64-fbe042891f00","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654ed3eab17b74dbf2d2b0"}},{"node":{"id":"Ghost__Post__5c654e9aeab17b74dbf2d2a3","title":"Welcome to SQL 2: Working With Data Values","slug":"welcome-to-sql-2-working-with-data-values","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","custom_excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","created_at_pretty":"14 February, 2019","published_at_pretty":"22 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T06:18:50.000-05:00","published_at":"2019-02-21T21:56:50.000-05:00","updated_at":"2019-02-27T22:52:38.000-05:00","meta_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","meta_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","og_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","twitter_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","twitter_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like youâre late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Now that we've gotten the fundamentals of creating databases and tables\n[https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/] \nout of the way, we can start getting into the meat and potatoes of SQL\ninteractions: selecting, updating, and deleting  data.\n\nWe'll start with the basic structure of these queries and then break into the\npowerful operations with enough detail to make you dangerous.\n\nSelecting Data From a Table\nAs mentioned previously, SQL operations have a rather strict order of operations\nwhich clauses have to respect in order to make a valid query. We'll begin by\ndissecting a common SELECT statement:\n\nSELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\";\n\n\nThis is perhaps the most common structure of SELECT queries. First, we list the\nnames of the columns we'd like to select separated by commas. To receive all \ncolumns, we can simply say SELECT *.\n\nThese columns need to come from somewhere, so we specify the table we're\nreferring to next. This either takes a form of FROM table_name \n(non-PostgreSQL), or FROM schema_name.table_name  (PostgreSQL). In theory, a\nsemicolon here would result in a valid query, but we usually want to select rows\nthat meet certain criteria.\n\nThis is where the WHERE  clause comes in: only rows which return \"true\"  for our\n WHERE  conditional will be returned. In the above example, we're validating\nthat a string matches exactly \"Value\". \n\nSelecting only Distinct Values\nSomething that often comes in handy is selecting distinct values in a column. In\nother words, if a value exists in the same column in 100 rows, running DISTINCT \nquery will only show us that value once. This is a good way of seeing the unique\ncontent of a column without yet diving into the distribution of said value. The\neffect is similar to the United States Senate, or the Electoral College: forget\nthe masses, and prop up Wyoming 2020:\n\nSELECT DISTINCT column_name \nFROM table_name;\n\n\nOffsetting and Limiting Results in our Queries\nWhen selecting data, the combination of OFFSET  and LIMIT  are critical at\ntimes. If we're selecting from a database with hundreds of thousands of rows, we\nwould be wasting an obscene amount of system resources to fetch all rows at\nonce; instead, we can have our application or API paginate the results.\n\nLIMIT  is followed by an integer, which in essence says \"return no more than X\nresults.\" \n\nOFFSET  is also followed by an integer, which denotes a numerical starting point\nfor returned results, aka: \"return all results which occur after the Xth\nresult:\"\n\nSELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n\n\nThe above returns the first 50 results. If we wanted to build paginated results\non the application side, we could construct our query like this:\n\nfrom SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n\n\nSuch an application could increment page_number  by 1 each time the user clicks\non to the next page, which would then appropriately modify our query to return\nthe next page of results.\n\nAnother use for OFFSET  could be to pick up where a failed script left off. If\nwe were to write an entire database to a CSV and experience a failure. We could\npick up where the script left off by setting OFFSET  equal to the number of rows\nin the CSV, to avoid running the entire script all over again.\n\nSorting Results\nLast to consider for now is sorting our results by using the ORDER BY  clause.\nWe can sort our results by any specified column, and state whether we'd like the\nresults to be ascending (ASC) or descending (DESC):\n\nSELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\"\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n\n\nSophisticated SELECT Statements\nOf course, we can select rows with WHERE  logic that goes much deeper than an\nexact match. One of the most versatile of these operations is LIKE.\n\nUsing Regex with LIKE\nLIKE  is perhaps the most powerful way to select columns with string values.\nWith LIKE, we can leverage regular expressions to build highly complex logic.\nLet's start with some of my favorites:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE \"%Wade%\";\n\n\nPassing a string to LIKE  with percentage signs on both sides is essentially a \"\ncontains\" statement. %  is equivalent to a wildcard, thus placing %  on either\nside of our string will return true whether the person's first name, middle\nname, or last name is Wade. Check out other useful combinations for %:\n\n * a%: Finds any values that start with \"a\".\n * %a: Finds any values that end with \"a\".\n * %or%: Finds any values that have \"or\" in any position.\n *   _r%: Finds any values that have \"r\" in the second position.\n * a_%_%:  Finds any values that start with \"a\" and are at least 3 characters in\n   length.\n * a%o:  Finds any values that start with \"a\" and ends with \"o\".\n\nFinding Values which are NOT LIKE\nThe opposite of LIKE  is of course NOT LIKE, which runs the same conditional,\nbut returns the opposite true/false value of LIKE:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE \"%Wade%\";\n\n\nConditionals With DateTime Columns\nDateTime columns are extremely useful for selecting data. Unlike plain strings,\nwe can easily extract numerical values for month, day, and year from a DateTime\nby using MONTH(column_name), DAY(column_name), and YEAR(column_name) \nrespectively. For example, using MONTH()  on a column that contains a DateTime\nof 2019-01-26 05:42:34  would return 1, aka January. Because the values come\nback as integers, it is then trivial to find results within a date range:\n\nSELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) < 2018;\n\n\nFinding Rows with NULL Values\nNULL  is a special datatype which essentially denotes the \"absence of\nsomething,\" therefore no conditional will never equal  NULL. Instead, we find\nrows where a value IS NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n\n\nThis should not come as a surprise to anybody familiar with validating\ndatatypes.\n\nThe reverse of this, of course, is NOT NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n\n\nInserting Data\nAn INSERT  query creates a new row, and is rather straightforward: we state the\ncolumns we'd like to insert data into, followed by the values to insert into\nsaid columns:\n\nINSERT INTO table_name (column_1, column_2, column_3)\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nMany things could result in a failed insert. For one, the number of values must\nmatch the number of columns we specify; if we don't we've either provided too\nfew or too many values.\n\nSecond, vales must respect a column's data type. If we try to insert an integer\ninto a DateTime  column, we'll receive an error.\n\nFinally, we must consider the keys and constraints of the table. If keys exist\nthat specify certain columns must not be empty, or must be unique, those keys\nmust too be respected.\n\nAs a shorthand trick, if we're inserting values into all  of a table's columns,\nwe can skip the part where we explicitly list the column names:\n\nINSERT INTO table_name\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nHere's a quick example of an insert query with real data:\n\nINSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n\n\nUPDATE Records: The Basics\nUpdating rows is where things get interesting. There's so much we can do here,\nso let's work our way up:\n\nUPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n\n\nThat's as simple as it gets: the value of a column, in a row that matches our\nconditional. Note that SET  always comes before WHERE. Here's the same query\nwith real data:\n\nUPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n\n\nUPDATE Records: Useful Logic\nJoining Strings Using CONCAT\nYou will find that it's common practice to update rows based on data which\nalready exists in said rows: in other words, sanitizing or modifying data. A\ngreat string operator is CONCAT(). CONCAT(\"string_1\", \"string_2\")  will join all\nthe strings passed to a single string.\n\nBelow is a real-world example of using CONCAT()  in conjunction with NOT LIKE \nto determine which post excerpts don't end in punctuation. If the excerpt does\nnot end with a punctuation mark, we add a period to the end:\n\nUPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n\n\nUsing REPLACE\nREPLACE()  works in SQL as it does in nearly every programming language. We pass\n REPLACE()  three values: \n\n 1. The string to be modified. \n 2. The substring within the string which will be replaced. \n 3. The value of the replacement. \n\nWe can do plenty of clever things with REPLACE(). This is an example that\nchanges the featured image of blog posts to contain the âretina imageâ suffix: \n\nUPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n\n\nScenario: Folder Structure Based on Date\nI across a fun exercise the other day when dealing with a nightmare situation\ninvolving changing CDNs. It touches on everything weâve reviewed thus far and\nserves a great illustration of what can be achieved in SQL alone. \n\nThe challenge in moving hundreds of images for hundreds of posts came in the\nform of a file structure. Ghost likes to save images in a dated folder\nstructure, like 2019/02/image.jpg. Our previous CDN did not abide by this at\nall, so had a dump of all images in a single folder. Not ideal. \n\nThankfully, we can leverage the metadata of our posts to discern this file\nstructure. Because images are added to posts when posts are created, we can use\nthe created_at  column from our posts table to figure out the right dated\nfolder: \n\nUPDATE\n  posts\nSET\n  feature_image = CONCAT(\"https://cdn.example.com/posts/\", \n\tYEAR(created_at),\n\t\"/\", \n\tLPAD(MONTH(created_at), 2, '0'), \n\t\"/\",\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n\n\nLet's break down the contents in our CONCAT:\n\n * https://cdn.example.com/posts/: The base URL of our new CDN.\n * YEAR(created_at): Extracting the year from our post creation date\n   (corresponds to a folder).\n * LPAD(MONTH(created_at), 2, '0'): Using MONTH(created_at)  returns a single\n   digit for early months, but our folder structure wants to always have months\n   a double-digits (ie: 2018/01/ as opposed to 2018/1/). We can use LPAD()  here\n   to 'pad' our dates so that months are always two digits long, and shorter\n   dates will be padded with the number 0.\n * SUBSTRING_INDEX(feature_image, '/', - 1): We're getting the filename of each\n   post's image by finding everything that comes after the last slash in our\n   existing image URL. \n\nThe result for every image will now look like this:\n\nhttps://cdn.example.com/posts/2018/02/image.jpg\n\n\nDELETE Records\nLet's wrap up for today with our last type of query, deleting rows:\n\nDELETE FROM celebs \nWHERE twitter_handle IS NULL;","html":"<p>Now that we've gotten the fundamentals of <a href=\"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/\">creating databases and tables</a> out of the way, we can start getting into the meat and potatoes of SQL interactions: <strong>selecting</strong>, <strong>updating</strong>, and <strong>deleting</strong> data.</p><p>We'll start with the basic structure of these queries and then break into the powerful operations with enough detail to make you dangerous.</p><h2 id=\"selecting-data-from-a-table\">Selecting Data From a Table</h2><p>As mentioned previously, SQL operations have a rather strict order of operations which clauses have to respect in order to make a valid query. We'll begin by dissecting a common SELECT statement:</p><pre><code class=\"language-sql\">SELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;;\n</code></pre>\n<p>This is perhaps the most common structure of SELECT queries. First, we list the names of the columns we'd like to select separated by commas. To receive <em>all</em> columns, we can simply say <code>SELECT *</code>.</p><p>These columns need to come from somewhere, so we specify the table we're referring to next. This either takes a form of <code>FROM table_name</code> (non-PostgreSQL), or <code>FROM schema_name.table_name</code> (PostgreSQL). In theory, a semicolon here would result in a valid query, but we usually want to select rows that meet certain criteria.</p><p>This is where the <code>WHERE</code> clause comes in: only rows which return <strong>\"true\"</strong> for our <code>WHERE</code> conditional will be returned. In the above example, we're validating that a string matches exactly <code>\"Value\"</code>. </p><h3 id=\"selecting-only-distinct-values\">Selecting only Distinct Values</h3><p>Something that often comes in handy is selecting distinct values in a column. In other words, if a value exists in the same column in 100 rows, running <code>DISTINCT</code> query will only show us that value once. This is a good way of seeing the unique content of a column without yet diving into the distribution of said value. The effect is similar to the United States Senate, or the Electoral College: forget the masses, and prop up Wyoming 2020:</p><pre><code class=\"language-sql\">SELECT DISTINCT column_name \nFROM table_name;\n</code></pre>\n<h3 id=\"offsetting-and-limiting-results-in-our-queries\">Offsetting and Limiting Results in our Queries</h3><p>When selecting data, the combination of <code>OFFSET</code> and <code>LIMIT</code> are critical at times. If we're selecting from a database with hundreds of thousands of rows, we would be wasting an obscene amount of system resources to fetch all rows at once; instead, we can have our application or API paginate the results.</p><p><code>LIMIT</code> is followed by an integer, which in essence says \"return no more than X results.\" </p><p><code>OFFSET</code> is also followed by an integer, which denotes a numerical starting point for returned results, aka: \"return all results which occur after the Xth result:\"</p><pre><code class=\"language-sql\">SELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n</code></pre>\n<p>The above returns the first 50 results. If we wanted to build paginated results on the application side, we could construct our query like this:</p><pre><code class=\"language-python\">from SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n</code></pre>\n<p>Such an application could increment <code>page_number</code> by 1 each time the user clicks on to the next page, which would then appropriately modify our query to return the next page of results.</p><p>Another use for <code>OFFSET</code> could be to pick up where a failed script left off. If we were to write an entire database to a CSV and experience a failure. We could pick up where the script left off by setting <code>OFFSET</code> equal to the number of rows in the CSV, to avoid running the entire script all over again.</p><h3 id=\"sorting-results\">Sorting Results</h3><p>Last to consider for now is sorting our results by using the <code>ORDER BY</code> clause. We can sort our results by any specified column, and state whether we'd like the results to be ascending (<code>ASC</code>) or descending (<code>DESC</code>):</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n</code></pre>\n<h2 id=\"sophisticated-select-statements\">Sophisticated SELECT Statements</h2><p>Of course, we can select rows with <code>WHERE</code> logic that goes much deeper than an exact match. One of the most versatile of these operations is <code>LIKE</code>.</p><h3 id=\"using-regex-with-like\">Using Regex with LIKE</h3><p><code>LIKE</code> is perhaps the most powerful way to select columns with string values. With <code>LIKE</code>, we can leverage regular expressions to build highly complex logic. Let's start with some of my favorites:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE &quot;%Wade%&quot;;\n</code></pre>\n<p>Passing a string to <code>LIKE</code> with percentage signs on both sides is essentially a \"<strong>contains</strong>\" statement. <code>%</code> is equivalent to a wildcard, thus placing <code>%</code> on either side of our string will return true whether the person's first name, middle name, or last name is <strong>Wade</strong>. Check out other useful combinations for <code>%</code>:</p><ul><li><code>a%</code>: Finds any values that start with \"a\".</li><li><code>%a</code>: Finds any values that end with \"a\".</li><li><code>%or%</code>: Finds any values that have \"or\" in any position.</li><li> <code>_r%</code>: Finds any values that have \"r\" in the second position.</li><li><code>a_%_%</code><strong>:</strong> Finds any values that start with \"a\" and are at least 3 characters in length.</li><li><code>a%o</code>:<strong> </strong>Finds any values that start with \"a\" and ends with \"o\".</li></ul><h3 id=\"finding-values-which-are-not-like\">Finding Values which are NOT LIKE</h3><p>The opposite of <code>LIKE</code> is of course <code>NOT LIKE</code>, which runs the same conditional, but returns the opposite true/false value of <code>LIKE</code>:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE &quot;%Wade%&quot;;\n</code></pre>\n<h3 id=\"conditionals-with-datetime-columns\">Conditionals With DateTime Columns</h3><p>DateTime columns are extremely useful for selecting data. Unlike plain strings, we can easily extract numerical values for month, day, and year from a DateTime by using <code>MONTH(column_name)</code>, <code>DAY(column_name)</code>, and <code>YEAR(column_name)</code> respectively. For example, using <code>MONTH()</code> on a column that contains a DateTime of <code>2019-01-26 05:42:34</code> would return <code>1</code>, aka January. Because the values come back as integers, it is then trivial to find results within a date range:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) &lt; 2018;\n</code></pre>\n<h3 id=\"finding-rows-with-null-values\">Finding Rows with NULL Values</h3><p><code>NULL</code> is a special datatype which essentially denotes the \"absence of something,\" therefore no conditional will never <em>equal</em> <code>NULL</code>. Instead, we find rows where a value <code>IS NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n</code></pre>\n<p>This should not come as a surprise to anybody familiar with validating datatypes.</p><p>The reverse of this, of course, is <code>NOT NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n</code></pre>\n<h2 id=\"inserting-data\">Inserting Data</h2><p>An <code>INSERT</code> query creates a new row, and is rather straightforward: we state the columns we'd like to insert data into, followed by the values to insert into said columns:</p><pre><code class=\"language-sql\">INSERT INTO table_name (column_1, column_2, column_3)\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Many things could result in a failed insert. For one, the number of values must match the number of columns we specify; if we don't we've either provided too few or too many values.</p><p>Second, vales must respect a column's data type. If we try to insert an integer into a <strong>DateTime</strong> column, we'll receive an error.</p><p>Finally, we must consider the keys and constraints of the table. If keys exist that specify certain columns must not be empty, or must be unique, those keys must too be respected.</p><p>As a shorthand trick, if we're inserting values into <em>all</em> of a table's columns, we can skip the part where we explicitly list the column names:</p><pre><code class=\"language-sql\">INSERT INTO table_name\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Here's a quick example of an insert query with real data:</p><pre><code class=\"language-sql\">INSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n</code></pre>\n<h2 id=\"update-records-the-basics\">UPDATE Records: The Basics</h2><p>Updating rows is where things get interesting. There's so much we can do here, so let's work our way up:</p><pre><code class=\"language-sql\">UPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n</code></pre>\n<p>That's as simple as it gets: the value of a column, in a row that matches our conditional. Note that <code>SET</code> always comes before <code>WHERE</code>. Here's the same query with real data:</p><pre><code class=\"language-sql\">UPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n</code></pre>\n<h2 id=\"update-records-useful-logic\">UPDATE Records: Useful Logic</h2><h3 id=\"joining-strings-using-concat\">Joining Strings Using CONCAT</h3><p>You will find that it's common practice to update rows based on data which already exists in said rows: in other words, sanitizing or modifying data. A great string operator is <code>CONCAT()</code>. <code>CONCAT(\"string_1\", \"string_2\")</code> will join all the strings passed to a single string.</p><p>Below is a real-world example of using <code>CONCAT()</code> in conjunction with <code>NOT LIKE</code> to determine which post excerpts don't end in punctuation. If the excerpt does not end with a punctuation mark, we add a period to the end:</p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n</code></pre>\n<h3 id=\"using-replace\">Using REPLACE</h3><p><code>REPLACE()</code> works in SQL as it does in nearly every programming language. We pass <code>REPLACE()</code> three values: </p><ol><li>The string to be modified. </li><li>The substring within the string which will be replaced. </li><li>The value of the replacement. </li></ol><p>We can do plenty of clever things with <code>REPLACE()</code>. This is an example that changes the featured image of blog posts to contain the âretina imageâ suffix: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n</code></pre>\n<h3 id=\"scenario-folder-structure-based-on-date\">Scenario: Folder Structure Based on Date</h3><p>I across a fun exercise the other day when dealing with a nightmare situation involving changing CDNs. It touches on everything weâve reviewed thus far and serves a great illustration of what can be achieved in SQL alone. </p><p>The challenge in moving hundreds of images for hundreds of posts came in the form of a file structure. Ghost likes to save images in a dated folder structure, like <strong>2019/02/image.jpg</strong>. Our previous CDN did not abide by this at all, so had a dump of all images in a single folder. Not ideal. </p><p>Thankfully, we can leverage the metadata of our posts to discern this file structure. Because images are added to posts when posts are created, we can use the <strong>created_at</strong> column from our posts table to figure out the right dated folder: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = CONCAT(&quot;https://cdn.example.com/posts/&quot;, \n\tYEAR(created_at),\n\t&quot;/&quot;, \n\tLPAD(MONTH(created_at), 2, '0'), \n\t&quot;/&quot;,\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n</code></pre>\n<p>Let's break down the contents in our <code>CONCAT</code>:</p><ul><li><code>https://cdn.example.com/posts/</code>: The base URL of our new CDN.</li><li><code>YEAR(created_at)</code>: Extracting the year from our post creation date (corresponds to a folder).</li><li><code>LPAD(MONTH(created_at), 2, '0')</code>: Using <strong>MONTH(created_at)</strong> returns a single digit for early months, but our folder structure wants to always have months a double-digits (ie: <strong>2018/01/ </strong>as opposed to <strong>2018/1/</strong>). We can use <code>LPAD()</code> here to 'pad' our dates so that months are always two digits long, and shorter dates will be padded with the number 0.</li><li><code>SUBSTRING_INDEX(feature_image, '/', - 1)</code>: We're getting the filename of each post's image by finding everything that comes after the last slash in our existing image URL. </li></ul><p>The result for every image will now look like this:</p><pre><code>https://cdn.example.com/posts/2018/02/image.jpg\n</code></pre>\n<h2 id=\"delete-records\">DELETE Records</h2><p>Let's wrap up for today with our last type of query, deleting rows:</p><pre><code class=\"language-sql\">DELETE FROM celebs \nWHERE twitter_handle IS NULL;\n</code></pre>\n","url":"https://hackersandslackers.com/welcome-to-sql-2-working-with-data-values/","uuid":"e051cdc6-eb17-425f-bb83-2f70a75e85c5","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654e9aeab17b74dbf2d2a3"}}]}},"pageContext":{"slug":"todd","limit":12,"skip":0,"numberOfPages":8,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/author/todd/page/2/"}}