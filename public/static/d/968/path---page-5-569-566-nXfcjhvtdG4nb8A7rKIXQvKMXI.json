{"data":{"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c6639c8c2209e663b5e5aa8","title":"Lynx Roundup, February 25th","slug":"lynx-roundup-february-25th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/146.jpg","excerpt":"Good intro to Jupyter!  Preview of the Walrus (\"assignment\") operator from Python 3.8!  Cortical Connectivity!","custom_excerpt":"Good intro to Jupyter!  Preview of the Walrus (\"assignment\") operator from Python 3.8!  Cortical Connectivity!","created_at_pretty":"15 February, 2019","published_at_pretty":"25 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T23:02:16.000-05:00","published_at":"2019-02-25T15:04:00.000-05:00","updated_at":"2019-02-27T22:49:23.000-05:00","meta_title":"Lynx Roundup, February 25th | Hackers and Slackers","meta_description":"Good intro to Jupyter!  Preview of the Walrus (\"assignment\") operator from Python 3.8!  Cortical Connectivity!","og_description":"Good intro to Jupyter!  Preview of the Walrus (\"assignment\") operator from Python 3.8!  Cortical Connectivity!","og_image":"https://hackersandslackers.com/content/images/2019/02/146.jpg","og_title":"Lynx Roundup, February 25th","twitter_description":"Good intro to Jupyter!  Preview of the Walrus (\"assignment\") operator from Python 3.8!  Cortical Connectivity!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/146.jpg","twitter_title":"Lynx Roundup, February 25th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://medium.com/@jeremy.noring/regrettable-code-episode-1-767dc1b92bd1\n\nhttps://neurosciencenews.com/cortical-connectivity-10654/\n\nhttps://hbr.org/2019/01/the-hard-truth-about-innovative-cultures\n\nhttps://waypoint.vice.com/en_us/article/yw83kg/activision-blizzard-reports-record-revenue-as-they-fuck-over-800-employees\n\nhttps://www.wired.com/story/your-brain-is-memories/\n\nhttps://medium.com/hultner/try-out-walrus-operator-in-python-3-8-d030ce0ce601\n\nhttps://realpython.com/jupyter-notebook-introduction/","html":"<p></p><p><a href=\"https://medium.com/@jeremy.noring/regrettable-code-episode-1-767dc1b92bd1\">https://medium.com/@jeremy.noring/regrettable-code-episode-1-767dc1b92bd1</a></p><p><a href=\"https://neurosciencenews.com/cortical-connectivity-10654/\">https://neurosciencenews.com/cortical-connectivity-10654/</a></p><p><a href=\"https://hbr.org/2019/01/the-hard-truth-about-innovative-cultures\">https://hbr.org/2019/01/the-hard-truth-about-innovative-cultures</a></p><p><a href=\"https://waypoint.vice.com/en_us/article/yw83kg/activision-blizzard-reports-record-revenue-as-they-fuck-over-800-employees\">https://waypoint.vice.com/en_us/article/yw83kg/activision-blizzard-reports-record-revenue-as-they-fuck-over-800-employees</a></p><p><a href=\"https://www.wired.com/story/your-brain-is-memories/\">https://www.wired.com/story/your-brain-is-memories/</a></p><p><a href=\"https://medium.com/hultner/try-out-walrus-operator-in-python-3-8-d030ce0ce601\">https://medium.com/hultner/try-out-walrus-operator-in-python-3-8-d030ce0ce601</a></p><p><a href=\"https://realpython.com/jupyter-notebook-introduction/\">https://realpython.com/jupyter-notebook-introduction/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-25th/","uuid":"45191809-06d8-4c66-a773-eb824e3a5093","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6639c8c2209e663b5e5aa8"}},{"node":{"id":"Ghost__Post__5c6638fec2209e663b5e5a9c","title":"Lynx Roundup, February 24th","slug":"lynx-roundup-february-24th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/141.jpg","excerpt":"Visualizing the Netherlands' train network!  Better PySpark with Quinn!  Things mimicking other things!","custom_excerpt":"Visualizing the Netherlands' train network!  Better PySpark with Quinn!  Things mimicking other things!","created_at_pretty":"15 February, 2019","published_at_pretty":"24 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:58:54.000-05:00","published_at":"2019-02-24T07:00:00.000-05:00","updated_at":"2019-02-27T22:50:27.000-05:00","meta_title":"Lynx Roundup, February 24th | Hackers and Slackers","meta_description":"Visualizing the Netherlands' train network!  Better PySpark with Quinn!  Things mimicking other things!","og_description":"Visualizing the Netherlands' train network!  Better PySpark with Quinn!  Things mimicking other things!","og_image":"https://hackersandslackers.com/content/images/2019/02/141.jpg","og_title":"Lynx Roundup, February 24th","twitter_description":"Visualizing the Netherlands' train network!  Better PySpark with Quinn!  Things mimicking other things!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/141.jpg","twitter_title":"Lynx Roundup, February 24th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://theintercept.com/2019/02/02/shoshana-zuboff-age-of-surveillance-capitalism/\n\nhttp://tulpinteractive.com/on-time-every-time/\n\nhttps://blogs.the451group.com/techdeals/ma/hellman-friedmans-ultimate-hr-deal/\n\nhttps://www.quora.com/If-Alan-Kay-made-an-introductory-technical-course-syllabus-to-teach-real-computer-science-to-undergraduates-what-would-be-on-it/answer/Alan-Kay-11\n\nhttps://github.com/MrPowers/quinn\n\nhttp://csvoss.scripts.mit.edu/thingsmimickingotherthings/\n\nhttps://stackabuse.com/linked-lists-in-detail-with-python-examples-single-linked-lists/","html":"<p></p><p><a href=\"https://theintercept.com/2019/02/02/shoshana-zuboff-age-of-surveillance-capitalism/\">https://theintercept.com/2019/02/02/shoshana-zuboff-age-of-surveillance-capitalism/</a></p><p><a href=\"http://tulpinteractive.com/on-time-every-time/\">http://tulpinteractive.com/on-time-every-time/</a></p><p><a href=\"https://blogs.the451group.com/techdeals/ma/hellman-friedmans-ultimate-hr-deal/\">https://blogs.the451group.com/techdeals/ma/hellman-friedmans-ultimate-hr-deal/</a></p><p><a href=\"https://www.quora.com/If-Alan-Kay-made-an-introductory-technical-course-syllabus-to-teach-real-computer-science-to-undergraduates-what-would-be-on-it/answer/Alan-Kay-11\">https://www.quora.com/If-Alan-Kay-made-an-introductory-technical-course-syllabus-to-teach-real-computer-science-to-undergraduates-what-would-be-on-it/answer/Alan-Kay-11</a></p><p><a href=\"https://github.com/MrPowers/quinn\">https://github.com/MrPowers/quinn</a></p><p><a href=\"http://csvoss.scripts.mit.edu/thingsmimickingotherthings/\">http://csvoss.scripts.mit.edu/thingsmimickingotherthings/</a></p><p><a href=\"https://stackabuse.com/linked-lists-in-detail-with-python-examples-single-linked-lists/\">https://stackabuse.com/linked-lists-in-detail-with-python-examples-single-linked-lists/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-24th/","uuid":"0fbaf7f6-2c45-4283-ae2a-ba39a5673eda","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6638fec2209e663b5e5a9c"}},{"node":{"id":"Ghost__Post__5c663884c2209e663b5e5a91","title":"Lynx Roundup, February 23rd","slug":"lynx-roundup-february-23rd","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/79-7@2x.jpg","excerpt":"Google's cool new composable Automatic Differentiation package!  Likert plots!  Visualizing networks with Dash & Cytoscape!","custom_excerpt":"Google's cool new composable Automatic Differentiation package!  Likert plots!  Visualizing networks with Dash & Cytoscape!","created_at_pretty":"15 February, 2019","published_at_pretty":"23 February, 2019","updated_at_pretty":"27 February, 2019","created_at":"2019-02-14T22:56:52.000-05:00","published_at":"2019-02-23T05:59:00.000-05:00","updated_at":"2019-02-27T03:38:02.000-05:00","meta_title":"Lynx Roundup, February 23rd | Hackers and Slackers","meta_description":"Google's cool new composable Automatic Differentiation package!  Likert plots!  Visualizing networks with Dash & Cytoscape!","og_description":"Google's cool new composable Automatic Differentiation package!  Likert plots!  Visualizing networks with Dash & Cytoscape!","og_image":"https://hackersandslackers.com/content/images/2019/02/79-7@2x.jpg","og_title":"Lynx Roundup, February 23rd","twitter_description":"Google's cool new composable Automatic Differentiation package!  Likert plots!  Visualizing networks with Dash & Cytoscape!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/79-7@2x.jpg","twitter_title":"Lynx Roundup, February 23rd","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://github.com/google/jax\n\nhttps://github.com/sdispater/poetry\n\nhttps://www.sciencealert.com/scientists-just-identified-the-brain-patterns-of-consciousness\n\nhttps://realpython.com/python-for-loop/\n\nhttps://medium.com/@plotlygraphs/introducing-dash-cytoscape-ce96cac824e4\n\nhttps://github.com/srinathh/snippetfu\n\nhttps://dynamicecology.wordpress.com/2019/01/28/comparison-of-ways-of-visualizing-individual-level-likert-data-line-plots-and-heat-maps-and-mosaic-plots-oh-my/","html":"<p></p><p><a href=\"https://github.com/google/jax\">https://github.com/google/jax</a></p><p><a href=\"https://github.com/sdispater/poetry\">https://github.com/sdispater/poetry</a></p><p><a href=\"https://www.sciencealert.com/scientists-just-identified-the-brain-patterns-of-consciousness\">https://www.sciencealert.com/scientists-just-identified-the-brain-patterns-of-consciousness</a></p><p><a href=\"https://realpython.com/python-for-loop/\">https://realpython.com/python-for-loop/</a></p><p><a href=\"https://medium.com/@plotlygraphs/introducing-dash-cytoscape-ce96cac824e4\">https://medium.com/@plotlygraphs/introducing-dash-cytoscape-ce96cac824e4</a></p><p><a href=\"https://github.com/srinathh/snippetfu\">https://github.com/srinathh/snippetfu</a></p><p><a href=\"https://dynamicecology.wordpress.com/2019/01/28/comparison-of-ways-of-visualizing-individual-level-likert-data-line-plots-and-heat-maps-and-mosaic-plots-oh-my/\">https://dynamicecology.wordpress.com/2019/01/28/comparison-of-ways-of-visualizing-individual-level-likert-data-line-plots-and-heat-maps-and-mosaic-plots-oh-my/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-23rd/","uuid":"1a031669-feae-4aca-91e3-f92f7bb2563f","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c663884c2209e663b5e5a91"}},{"node":{"id":"Ghost__Post__5c6637f9c2209e663b5e5a86","title":"Lynx Roundup, February 22nd","slug":"lynx-roundup-february-22nd","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/78-1.jpg","excerpt":"Open source note-taking app!  The lifespans of news stories!  Fighting Climate Change with Data Science!","custom_excerpt":"Open source note-taking app!  The lifespans of news stories!  Fighting Climate Change with Data Science!","created_at_pretty":"15 February, 2019","published_at_pretty":"22 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:54:33.000-05:00","published_at":"2019-02-22T11:50:00.000-05:00","updated_at":"2019-02-27T22:51:25.000-05:00","meta_title":"Lynx Roundup, February 22nd | Hackers and Slackers","meta_description":"Open source note-taking app!  The lifespans of news stories!  Fighting Climate Change with Data Science!","og_description":"Open source note-taking app!  The lifespans of news stories!  Fighting Climate Change with Data Science!","og_image":"https://hackersandslackers.com/content/images/2019/02/78-1.jpg","og_title":"Lynx Roundup, February 22nd","twitter_description":"Open source note-taking app!  The lifespans of news stories!  Fighting Climate Change with Data Science!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/78-1.jpg","twitter_title":"Lynx Roundup, February 22nd","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://www.nature.com/articles/d41586-019-00287-7\n\nhttps://www.reuters.com/investigates/special-report/usa-spying-raven/\n\nhttps://twitter.com/1st1/status/1090325242432630784\n\nhttps://github.com/jddunn/frame/\n\nhttps://www.newslifespan.com/\n\nhttps://www.mattermore.io/\n\nhttp://dataphys.org/list/","html":"<p></p><p><a href=\"https://www.nature.com/articles/d41586-019-00287-7\">https://www.nature.com/articles/d41586-019-00287-7</a></p><p><a href=\"https://www.reuters.com/investigates/special-report/usa-spying-raven/\">https://www.reuters.com/investigates/special-report/usa-spying-raven/</a></p><p><a href=\"https://twitter.com/1st1/status/1090325242432630784\">https://twitter.com/1st1/status/1090325242432630784</a></p><p><a href=\"https://github.com/jddunn/frame/\">https://github.com/jddunn/frame/</a></p><p><a href=\"https://www.newslifespan.com/\">https://www.newslifespan.com/</a></p><p><a href=\"https://www.mattermore.io/\">https://www.mattermore.io/</a></p><p><a href=\"http://dataphys.org/list/\">http://dataphys.org/list/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-22nd/","uuid":"3ce81c72-e539-4e55-b4a1-ce2cfab5ebf9","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6637f9c2209e663b5e5a86"}},{"node":{"id":"Ghost__Post__5c654e9aeab17b74dbf2d2a3","title":"Welcome to SQL 2: Working With Data Values","slug":"welcome-to-sql-2-working-with-data-values","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","custom_excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","created_at_pretty":"14 February, 2019","published_at_pretty":"22 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T06:18:50.000-05:00","published_at":"2019-02-21T21:56:50.000-05:00","updated_at":"2019-02-27T22:52:38.000-05:00","meta_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","meta_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","og_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","twitter_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","twitter_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Now that we've gotten the fundamentals of creating databases and tables\n[https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/] \nout of the way, we can start getting into the meat and potatoes of SQL\ninteractions: selecting, updating, and deleting  data.\n\nWe'll start with the basic structure of these queries and then break into the\npowerful operations with enough detail to make you dangerous.\n\nSelecting Data From a Table\nAs mentioned previously, SQL operations have a rather strict order of operations\nwhich clauses have to respect in order to make a valid query. We'll begin by\ndissecting a common SELECT statement:\n\nSELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\";\n\n\nThis is perhaps the most common structure of SELECT queries. First, we list the\nnames of the columns we'd like to select separated by commas. To receive all \ncolumns, we can simply say SELECT *.\n\nThese columns need to come from somewhere, so we specify the table we're\nreferring to next. This either takes a form of FROM table_name \n(non-PostgreSQL), or FROM schema_name.table_name  (PostgreSQL). In theory, a\nsemicolon here would result in a valid query, but we usually want to select rows\nthat meet certain criteria.\n\nThis is where the WHERE  clause comes in: only rows which return \"true\"  for our\n WHERE  conditional will be returned. In the above example, we're validating\nthat a string matches exactly \"Value\". \n\nSelecting only Distinct Values\nSomething that often comes in handy is selecting distinct values in a column. In\nother words, if a value exists in the same column in 100 rows, running DISTINCT \nquery will only show us that value once. This is a good way of seeing the unique\ncontent of a column without yet diving into the distribution of said value. The\neffect is similar to the United States Senate, or the Electoral College: forget\nthe masses, and prop up Wyoming 2020:\n\nSELECT DISTINCT column_name \nFROM table_name;\n\n\nOffsetting and Limiting Results in our Queries\nWhen selecting data, the combination of OFFSET  and LIMIT  are critical at\ntimes. If we're selecting from a database with hundreds of thousands of rows, we\nwould be wasting an obscene amount of system resources to fetch all rows at\nonce; instead, we can have our application or API paginate the results.\n\nLIMIT  is followed by an integer, which in essence says \"return no more than X\nresults.\" \n\nOFFSET  is also followed by an integer, which denotes a numerical starting point\nfor returned results, aka: \"return all results which occur after the Xth\nresult:\"\n\nSELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n\n\nThe above returns the first 50 results. If we wanted to build paginated results\non the application side, we could construct our query like this:\n\nfrom SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n\n\nSuch an application could increment page_number  by 1 each time the user clicks\non to the next page, which would then appropriately modify our query to return\nthe next page of results.\n\nAnother use for OFFSET  could be to pick up where a failed script left off. If\nwe were to write an entire database to a CSV and experience a failure. We could\npick up where the script left off by setting OFFSET  equal to the number of rows\nin the CSV, to avoid running the entire script all over again.\n\nSorting Results\nLast to consider for now is sorting our results by using the ORDER BY  clause.\nWe can sort our results by any specified column, and state whether we'd like the\nresults to be ascending (ASC) or descending (DESC):\n\nSELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\"\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n\n\nSophisticated SELECT Statements\nOf course, we can select rows with WHERE  logic that goes much deeper than an\nexact match. One of the most versatile of these operations is LIKE.\n\nUsing Regex with LIKE\nLIKE  is perhaps the most powerful way to select columns with string values.\nWith LIKE, we can leverage regular expressions to build highly complex logic.\nLet's start with some of my favorites:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE \"%Wade%\";\n\n\nPassing a string to LIKE  with percentage signs on both sides is essentially a \"\ncontains\" statement. %  is equivalent to a wildcard, thus placing %  on either\nside of our string will return true whether the person's first name, middle\nname, or last name is Wade. Check out other useful combinations for %:\n\n * a%: Finds any values that start with \"a\".\n * %a: Finds any values that end with \"a\".\n * %or%: Finds any values that have \"or\" in any position.\n *   _r%: Finds any values that have \"r\" in the second position.\n * a_%_%:  Finds any values that start with \"a\" and are at least 3 characters in\n   length.\n * a%o:  Finds any values that start with \"a\" and ends with \"o\".\n\nFinding Values which are NOT LIKE\nThe opposite of LIKE  is of course NOT LIKE, which runs the same conditional,\nbut returns the opposite true/false value of LIKE:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE \"%Wade%\";\n\n\nConditionals With DateTime Columns\nDateTime columns are extremely useful for selecting data. Unlike plain strings,\nwe can easily extract numerical values for month, day, and year from a DateTime\nby using MONTH(column_name), DAY(column_name), and YEAR(column_name) \nrespectively. For example, using MONTH()  on a column that contains a DateTime\nof 2019-01-26 05:42:34  would return 1, aka January. Because the values come\nback as integers, it is then trivial to find results within a date range:\n\nSELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) < 2018;\n\n\nFinding Rows with NULL Values\nNULL  is a special datatype which essentially denotes the \"absence of\nsomething,\" therefore no conditional will never equal  NULL. Instead, we find\nrows where a value IS NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n\n\nThis should not come as a surprise to anybody familiar with validating\ndatatypes.\n\nThe reverse of this, of course, is NOT NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n\n\nInserting Data\nAn INSERT  query creates a new row, and is rather straightforward: we state the\ncolumns we'd like to insert data into, followed by the values to insert into\nsaid columns:\n\nINSERT INTO table_name (column_1, column_2, column_3)\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nMany things could result in a failed insert. For one, the number of values must\nmatch the number of columns we specify; if we don't we've either provided too\nfew or too many values.\n\nSecond, vales must respect a column's data type. If we try to insert an integer\ninto a DateTime  column, we'll receive an error.\n\nFinally, we must consider the keys and constraints of the table. If keys exist\nthat specify certain columns must not be empty, or must be unique, those keys\nmust too be respected.\n\nAs a shorthand trick, if we're inserting values into all  of a table's columns,\nwe can skip the part where we explicitly list the column names:\n\nINSERT INTO table_name\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nHere's a quick example of an insert query with real data:\n\nINSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n\n\nUPDATE Records: The Basics\nUpdating rows is where things get interesting. There's so much we can do here,\nso let's work our way up:\n\nUPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n\n\nThat's as simple as it gets: the value of a column, in a row that matches our\nconditional. Note that SET  always comes before WHERE. Here's the same query\nwith real data:\n\nUPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n\n\nUPDATE Records: Useful Logic\nJoining Strings Using CONCAT\nYou will find that it's common practice to update rows based on data which\nalready exists in said rows: in other words, sanitizing or modifying data. A\ngreat string operator is CONCAT(). CONCAT(\"string_1\", \"string_2\")  will join all\nthe strings passed to a single string.\n\nBelow is a real-world example of using CONCAT()  in conjunction with NOT LIKE \nto determine which post excerpts don't end in punctuation. If the excerpt does\nnot end with a punctuation mark, we add a period to the end:\n\nUPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n\n\nUsing REPLACE\nREPLACE()  works in SQL as it does in nearly every programming language. We pass\n REPLACE()  three values: \n\n 1. The string to be modified. \n 2. The substring within the string which will be replaced. \n 3. The value of the replacement. \n\nWe can do plenty of clever things with REPLACE(). This is an example that\nchanges the featured image of blog posts to contain the “retina image” suffix: \n\nUPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n\n\nScenario: Folder Structure Based on Date\nI across a fun exercise the other day when dealing with a nightmare situation\ninvolving changing CDNs. It touches on everything we’ve reviewed thus far and\nserves a great illustration of what can be achieved in SQL alone. \n\nThe challenge in moving hundreds of images for hundreds of posts came in the\nform of a file structure. Ghost likes to save images in a dated folder\nstructure, like 2019/02/image.jpg. Our previous CDN did not abide by this at\nall, so had a dump of all images in a single folder. Not ideal. \n\nThankfully, we can leverage the metadata of our posts to discern this file\nstructure. Because images are added to posts when posts are created, we can use\nthe created_at  column from our posts table to figure out the right dated\nfolder: \n\nUPDATE\n  posts\nSET\n  feature_image = CONCAT(\"https://cdn.example.com/posts/\", \n\tYEAR(created_at),\n\t\"/\", \n\tLPAD(MONTH(created_at), 2, '0'), \n\t\"/\",\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n\n\nLet's break down the contents in our CONCAT:\n\n * https://cdn.example.com/posts/: The base URL of our new CDN.\n * YEAR(created_at): Extracting the year from our post creation date\n   (corresponds to a folder).\n * LPAD(MONTH(created_at), 2, '0'): Using MONTH(created_at)  returns a single\n   digit for early months, but our folder structure wants to always have months\n   a double-digits (ie: 2018/01/ as opposed to 2018/1/). We can use LPAD()  here\n   to 'pad' our dates so that months are always two digits long, and shorter\n   dates will be padded with the number 0.\n * SUBSTRING_INDEX(feature_image, '/', - 1): We're getting the filename of each\n   post's image by finding everything that comes after the last slash in our\n   existing image URL. \n\nThe result for every image will now look like this:\n\nhttps://cdn.example.com/posts/2018/02/image.jpg\n\n\nDELETE Records\nLet's wrap up for today with our last type of query, deleting rows:\n\nDELETE FROM celebs \nWHERE twitter_handle IS NULL;","html":"<p>Now that we've gotten the fundamentals of <a href=\"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/\">creating databases and tables</a> out of the way, we can start getting into the meat and potatoes of SQL interactions: <strong>selecting</strong>, <strong>updating</strong>, and <strong>deleting</strong> data.</p><p>We'll start with the basic structure of these queries and then break into the powerful operations with enough detail to make you dangerous.</p><h2 id=\"selecting-data-from-a-table\">Selecting Data From a Table</h2><p>As mentioned previously, SQL operations have a rather strict order of operations which clauses have to respect in order to make a valid query. We'll begin by dissecting a common SELECT statement:</p><pre><code class=\"language-sql\">SELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;;\n</code></pre>\n<p>This is perhaps the most common structure of SELECT queries. First, we list the names of the columns we'd like to select separated by commas. To receive <em>all</em> columns, we can simply say <code>SELECT *</code>.</p><p>These columns need to come from somewhere, so we specify the table we're referring to next. This either takes a form of <code>FROM table_name</code> (non-PostgreSQL), or <code>FROM schema_name.table_name</code> (PostgreSQL). In theory, a semicolon here would result in a valid query, but we usually want to select rows that meet certain criteria.</p><p>This is where the <code>WHERE</code> clause comes in: only rows which return <strong>\"true\"</strong> for our <code>WHERE</code> conditional will be returned. In the above example, we're validating that a string matches exactly <code>\"Value\"</code>. </p><h3 id=\"selecting-only-distinct-values\">Selecting only Distinct Values</h3><p>Something that often comes in handy is selecting distinct values in a column. In other words, if a value exists in the same column in 100 rows, running <code>DISTINCT</code> query will only show us that value once. This is a good way of seeing the unique content of a column without yet diving into the distribution of said value. The effect is similar to the United States Senate, or the Electoral College: forget the masses, and prop up Wyoming 2020:</p><pre><code class=\"language-sql\">SELECT DISTINCT column_name \nFROM table_name;\n</code></pre>\n<h3 id=\"offsetting-and-limiting-results-in-our-queries\">Offsetting and Limiting Results in our Queries</h3><p>When selecting data, the combination of <code>OFFSET</code> and <code>LIMIT</code> are critical at times. If we're selecting from a database with hundreds of thousands of rows, we would be wasting an obscene amount of system resources to fetch all rows at once; instead, we can have our application or API paginate the results.</p><p><code>LIMIT</code> is followed by an integer, which in essence says \"return no more than X results.\" </p><p><code>OFFSET</code> is also followed by an integer, which denotes a numerical starting point for returned results, aka: \"return all results which occur after the Xth result:\"</p><pre><code class=\"language-sql\">SELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n</code></pre>\n<p>The above returns the first 50 results. If we wanted to build paginated results on the application side, we could construct our query like this:</p><pre><code class=\"language-python\">from SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n</code></pre>\n<p>Such an application could increment <code>page_number</code> by 1 each time the user clicks on to the next page, which would then appropriately modify our query to return the next page of results.</p><p>Another use for <code>OFFSET</code> could be to pick up where a failed script left off. If we were to write an entire database to a CSV and experience a failure. We could pick up where the script left off by setting <code>OFFSET</code> equal to the number of rows in the CSV, to avoid running the entire script all over again.</p><h3 id=\"sorting-results\">Sorting Results</h3><p>Last to consider for now is sorting our results by using the <code>ORDER BY</code> clause. We can sort our results by any specified column, and state whether we'd like the results to be ascending (<code>ASC</code>) or descending (<code>DESC</code>):</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n</code></pre>\n<h2 id=\"sophisticated-select-statements\">Sophisticated SELECT Statements</h2><p>Of course, we can select rows with <code>WHERE</code> logic that goes much deeper than an exact match. One of the most versatile of these operations is <code>LIKE</code>.</p><h3 id=\"using-regex-with-like\">Using Regex with LIKE</h3><p><code>LIKE</code> is perhaps the most powerful way to select columns with string values. With <code>LIKE</code>, we can leverage regular expressions to build highly complex logic. Let's start with some of my favorites:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE &quot;%Wade%&quot;;\n</code></pre>\n<p>Passing a string to <code>LIKE</code> with percentage signs on both sides is essentially a \"<strong>contains</strong>\" statement. <code>%</code> is equivalent to a wildcard, thus placing <code>%</code> on either side of our string will return true whether the person's first name, middle name, or last name is <strong>Wade</strong>. Check out other useful combinations for <code>%</code>:</p><ul><li><code>a%</code>: Finds any values that start with \"a\".</li><li><code>%a</code>: Finds any values that end with \"a\".</li><li><code>%or%</code>: Finds any values that have \"or\" in any position.</li><li> <code>_r%</code>: Finds any values that have \"r\" in the second position.</li><li><code>a_%_%</code><strong>:</strong> Finds any values that start with \"a\" and are at least 3 characters in length.</li><li><code>a%o</code>:<strong> </strong>Finds any values that start with \"a\" and ends with \"o\".</li></ul><h3 id=\"finding-values-which-are-not-like\">Finding Values which are NOT LIKE</h3><p>The opposite of <code>LIKE</code> is of course <code>NOT LIKE</code>, which runs the same conditional, but returns the opposite true/false value of <code>LIKE</code>:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE &quot;%Wade%&quot;;\n</code></pre>\n<h3 id=\"conditionals-with-datetime-columns\">Conditionals With DateTime Columns</h3><p>DateTime columns are extremely useful for selecting data. Unlike plain strings, we can easily extract numerical values for month, day, and year from a DateTime by using <code>MONTH(column_name)</code>, <code>DAY(column_name)</code>, and <code>YEAR(column_name)</code> respectively. For example, using <code>MONTH()</code> on a column that contains a DateTime of <code>2019-01-26 05:42:34</code> would return <code>1</code>, aka January. Because the values come back as integers, it is then trivial to find results within a date range:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) &lt; 2018;\n</code></pre>\n<h3 id=\"finding-rows-with-null-values\">Finding Rows with NULL Values</h3><p><code>NULL</code> is a special datatype which essentially denotes the \"absence of something,\" therefore no conditional will never <em>equal</em> <code>NULL</code>. Instead, we find rows where a value <code>IS NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n</code></pre>\n<p>This should not come as a surprise to anybody familiar with validating datatypes.</p><p>The reverse of this, of course, is <code>NOT NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n</code></pre>\n<h2 id=\"inserting-data\">Inserting Data</h2><p>An <code>INSERT</code> query creates a new row, and is rather straightforward: we state the columns we'd like to insert data into, followed by the values to insert into said columns:</p><pre><code class=\"language-sql\">INSERT INTO table_name (column_1, column_2, column_3)\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Many things could result in a failed insert. For one, the number of values must match the number of columns we specify; if we don't we've either provided too few or too many values.</p><p>Second, vales must respect a column's data type. If we try to insert an integer into a <strong>DateTime</strong> column, we'll receive an error.</p><p>Finally, we must consider the keys and constraints of the table. If keys exist that specify certain columns must not be empty, or must be unique, those keys must too be respected.</p><p>As a shorthand trick, if we're inserting values into <em>all</em> of a table's columns, we can skip the part where we explicitly list the column names:</p><pre><code class=\"language-sql\">INSERT INTO table_name\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Here's a quick example of an insert query with real data:</p><pre><code class=\"language-sql\">INSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n</code></pre>\n<h2 id=\"update-records-the-basics\">UPDATE Records: The Basics</h2><p>Updating rows is where things get interesting. There's so much we can do here, so let's work our way up:</p><pre><code class=\"language-sql\">UPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n</code></pre>\n<p>That's as simple as it gets: the value of a column, in a row that matches our conditional. Note that <code>SET</code> always comes before <code>WHERE</code>. Here's the same query with real data:</p><pre><code class=\"language-sql\">UPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n</code></pre>\n<h2 id=\"update-records-useful-logic\">UPDATE Records: Useful Logic</h2><h3 id=\"joining-strings-using-concat\">Joining Strings Using CONCAT</h3><p>You will find that it's common practice to update rows based on data which already exists in said rows: in other words, sanitizing or modifying data. A great string operator is <code>CONCAT()</code>. <code>CONCAT(\"string_1\", \"string_2\")</code> will join all the strings passed to a single string.</p><p>Below is a real-world example of using <code>CONCAT()</code> in conjunction with <code>NOT LIKE</code> to determine which post excerpts don't end in punctuation. If the excerpt does not end with a punctuation mark, we add a period to the end:</p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n</code></pre>\n<h3 id=\"using-replace\">Using REPLACE</h3><p><code>REPLACE()</code> works in SQL as it does in nearly every programming language. We pass <code>REPLACE()</code> three values: </p><ol><li>The string to be modified. </li><li>The substring within the string which will be replaced. </li><li>The value of the replacement. </li></ol><p>We can do plenty of clever things with <code>REPLACE()</code>. This is an example that changes the featured image of blog posts to contain the “retina image” suffix: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n</code></pre>\n<h3 id=\"scenario-folder-structure-based-on-date\">Scenario: Folder Structure Based on Date</h3><p>I across a fun exercise the other day when dealing with a nightmare situation involving changing CDNs. It touches on everything we’ve reviewed thus far and serves a great illustration of what can be achieved in SQL alone. </p><p>The challenge in moving hundreds of images for hundreds of posts came in the form of a file structure. Ghost likes to save images in a dated folder structure, like <strong>2019/02/image.jpg</strong>. Our previous CDN did not abide by this at all, so had a dump of all images in a single folder. Not ideal. </p><p>Thankfully, we can leverage the metadata of our posts to discern this file structure. Because images are added to posts when posts are created, we can use the <strong>created_at</strong> column from our posts table to figure out the right dated folder: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = CONCAT(&quot;https://cdn.example.com/posts/&quot;, \n\tYEAR(created_at),\n\t&quot;/&quot;, \n\tLPAD(MONTH(created_at), 2, '0'), \n\t&quot;/&quot;,\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n</code></pre>\n<p>Let's break down the contents in our <code>CONCAT</code>:</p><ul><li><code>https://cdn.example.com/posts/</code>: The base URL of our new CDN.</li><li><code>YEAR(created_at)</code>: Extracting the year from our post creation date (corresponds to a folder).</li><li><code>LPAD(MONTH(created_at), 2, '0')</code>: Using <strong>MONTH(created_at)</strong> returns a single digit for early months, but our folder structure wants to always have months a double-digits (ie: <strong>2018/01/ </strong>as opposed to <strong>2018/1/</strong>). We can use <code>LPAD()</code> here to 'pad' our dates so that months are always two digits long, and shorter dates will be padded with the number 0.</li><li><code>SUBSTRING_INDEX(feature_image, '/', - 1)</code>: We're getting the filename of each post's image by finding everything that comes after the last slash in our existing image URL. </li></ul><p>The result for every image will now look like this:</p><pre><code>https://cdn.example.com/posts/2018/02/image.jpg\n</code></pre>\n<h2 id=\"delete-records\">DELETE Records</h2><p>Let's wrap up for today with our last type of query, deleting rows:</p><pre><code class=\"language-sql\">DELETE FROM celebs \nWHERE twitter_handle IS NULL;\n</code></pre>\n","url":"https://hackersandslackers.com/welcome-to-sql-2-working-with-data-values/","uuid":"e051cdc6-eb17-425f-bb83-2f70a75e85c5","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654e9aeab17b74dbf2d2a3"}},{"node":{"id":"Ghost__Post__5c6637acc2209e663b5e5a7c","title":"Lynx Roundup, February 21st","slug":"lynx-roundup-february-21st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/77-1.jpg","excerpt":"Testing Pandas code!  A different take on Python Exceptions!  A blow-by-blow of Join algorithms!","custom_excerpt":"Testing Pandas code!  A different take on Python Exceptions!  A blow-by-blow of Join algorithms!","created_at_pretty":"15 February, 2019","published_at_pretty":"21 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:53:16.000-05:00","published_at":"2019-02-21T07:00:00.000-05:00","updated_at":"2019-02-27T22:53:44.000-05:00","meta_title":"Lynx Roundup, February 21st | Hackers and Slackers","meta_description":"Testing Pandas code!  A different take on Python Exceptions!  A blow-by-blow of Join algorithms!","og_description":"Testing Pandas code!  A different take on Python Exceptions!  A blow-by-blow of Join algorithms!","og_image":"https://hackersandslackers.com/content/images/2019/02/77-1.jpg","og_title":"Lynx Roundup, February 21st","twitter_description":"Testing Pandas code!  A different take on Python Exceptions!  A blow-by-blow of Join algorithms!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/77-1.jpg","twitter_title":"Lynx Roundup, February 21st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://www.slideshare.net/kristw/kristw-datavis-summitfinal\n\nhttps://engineering.hexacta.com/testing-pandas-b65c0ea8a28e\n\nhttps://twitter.com/SwiftOnSecurity/status/975263380369010688\n\nhttps://www.geodose.com/2019/01/realtime-flight-tracking-pandas-bokeh-python.html\n\nhttps://dev.to/wemake-services/python-exceptions-considered-an-anti-pattern-17o9\n\nhttp://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/\n\nhttps://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python","html":"<p></p><p><a href=\"https://www.slideshare.net/kristw/kristw-datavis-summitfinal\">https://www.slideshare.net/kristw/kristw-datavis-summitfinal</a></p><p><a href=\"https://engineering.hexacta.com/testing-pandas-b65c0ea8a28e\">https://engineering.hexacta.com/testing-pandas-b65c0ea8a28e</a></p><p><a href=\"https://twitter.com/SwiftOnSecurity/status/975263380369010688\">https://twitter.com/SwiftOnSecurity/status/975263380369010688</a></p><p><a href=\"https://www.geodose.com/2019/01/realtime-flight-tracking-pandas-bokeh-python.html\">https://www.geodose.com/2019/01/realtime-flight-tracking-pandas-bokeh-python.html</a></p><p><a href=\"https://dev.to/wemake-services/python-exceptions-considered-an-anti-pattern-17o9\">https://dev.to/wemake-services/python-exceptions-considered-an-anti-pattern-17o9</a></p><p><a href=\"http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/\">http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/</a></p><p><a href=\"https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\">https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-21st/","uuid":"4852a9b5-446c-4a0b-a78b-250f53dd7dae","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6637acc2209e663b5e5a7c"}},{"node":{"id":"Ghost__Post__5c66374fc2209e663b5e5a72","title":"Lynx Roundup, February 20th","slug":"lynx-roundup-february-20th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/143@2x.jpg","excerpt":"Automatic code generation & checking at Facebook!  Progress toward a general theory of Neural Networks!  Building a DBMS from scratch!","custom_excerpt":"Automatic code generation & checking at Facebook!  Progress toward a general theory of Neural Networks!  Building a DBMS from scratch!","created_at_pretty":"15 February, 2019","published_at_pretty":"21 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:51:43.000-05:00","published_at":"2019-02-20T21:37:00.000-05:00","updated_at":"2019-02-27T22:54:40.000-05:00","meta_title":"Lynx Roundup, February 20th | Hackers and Slackers","meta_description":"Automatic code generation & checking at Facebook!  Progress toward a general theory of Neural Networks!  Building a DBMS from scratch!","og_description":"Automatic code generation & checking at Facebook!  Progress toward a general theory of Neural Networks!  Building a DBMS from scratch!","og_image":"https://hackersandslackers.com/content/images/2019/02/143@2x.jpg","og_title":"Lynx Roundup, February 20th","twitter_description":"Automatic code generation & checking at Facebook!  Progress toward a general theory of Neural Networks!  Building a DBMS from scratch!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/143@2x.jpg","twitter_title":"Lynx Roundup, February 20th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://www.nature.com/articles/d41586-019-00546-7\n\nhttps://spectrum.ieee.org/tech-talk/computing/software/meet-the-bots-that-review-and-write-snippets-of-facebooks-code\n\nhttps://tigercosmos.xyz/lets-build-dbms/days/1.html\n\nhttps://github.com/vinta/awesome-python\n\nhttps://github.com/tylerwince/pydbg\n\nhttps://www.nature.com/articles/d41586-019-00261-3\n\nhttps://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131","html":"<p></p><p><a href=\"https://www.nature.com/articles/d41586-019-00546-7\">https://www.nature.com/articles/d41586-019-00546-7</a></p><p><a href=\"https://spectrum.ieee.org/tech-talk/computing/software/meet-the-bots-that-review-and-write-snippets-of-facebooks-code\">https://spectrum.ieee.org/tech-talk/computing/software/meet-the-bots-that-review-and-write-snippets-of-facebooks-code</a></p><p><a href=\"https://tigercosmos.xyz/lets-build-dbms/days/1.html\">https://tigercosmos.xyz/lets-build-dbms/days/1.html</a></p><p><a href=\"https://github.com/vinta/awesome-python\">https://github.com/vinta/awesome-python</a></p><p><a href=\"https://github.com/tylerwince/pydbg\">https://github.com/tylerwince/pydbg</a></p><p><a href=\"https://www.nature.com/articles/d41586-019-00261-3\">https://www.nature.com/articles/d41586-019-00261-3</a></p><p><a href=\"https://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131\">https://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-20th/","uuid":"1daf71f7-8430-45be-baf2-e6b1d5707181","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c66374fc2209e663b5e5a72"}},{"node":{"id":"Ghost__Post__5c6dd08fa624d869fba41325","title":"Making API Requests With node-fetch","slug":"making-api-requests-with-nodejs","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","excerpt":"Using the lightweight node-fetch library for REST API requests in NodeJS.","custom_excerpt":"Using the lightweight node-fetch library for REST API requests in NodeJS.","created_at_pretty":"20 February, 2019","published_at_pretty":"21 February, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-02-20T17:11:27.000-05:00","published_at":"2019-02-20T20:22:20.000-05:00","updated_at":"2019-04-08T23:21:34.000-04:00","meta_title":"Making API Requests with node-fetch | Hackers and Slackers","meta_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","og_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","og_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","og_title":"Making API Requests with node-fetch","twitter_description":"Using the lightweight node-fetch library for REST API requests in NodeJS.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/node-fetch.jpg","twitter_title":"Making API Requests with node-fetch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"If you're the type of person to read technical Javascript posts in your free\ntime (you are), you don't need me to tell you that JQuery is dead. JQuery\nthemselves have proclaimed JQuery to be dead. The only cool thing about JQuery\nis who can remove it from their legacy stack the fastest, which begs the\nquestion: why is the third most popular page on this site an old post about\nJQuery?\n\nMaintaining a blog of tutorials has taught me a lot about the gap between\nperception and reality. While we content publishers sling Medium posts from our\nivory towers, we quickly create a perception of what \"everybody\" is doing, but\nit turns out \"everybody\" only includes individuals who are exceptionally\nvisible. That demographic makes up significantly less than 10-20% of the active\nworkforce. I would have assumed any post with the word \"React\" would immediately\nexplode, when in reality people are more interested in using Handlebars with\nExpressJS [https://hackersandslackers.com/handlebars-templating-in-expressjs/] \n(I'm not proud of that post by the way, please don't read it).\n\nI want to provide an alternative to using AJAX calls when interacting with REST\nAPIs to clear my conscious of ever enabling bad behavior in the first place.\nHopefully, those who have lost their way might find something to take from it.\nConsidering how deep I've gone down the GraphQL rabbit hole myself, this may be\nthe last chance to bother writing about REST at all.\n\nLibrary of Choice: node-fetch\nLike everything in Javascript, there are way too many packages doing the same\nthing and solving the same problem. Making API requests is no exception. http\n[https://www.npmjs.com/package/http]  is a bit primitive, request\n[https://www.npmjs.com/package/request]  breaks when building with Webpack, r2\n[https://www.npmjs.com/package/r2]  seems like a pointless clone, and so on.\nDon't get me started with async libraries with 40 different methods for chaining\nrequests. Who is gunslinging API requests to the point where we need this many\noptions to pipe or parallel API requests anyway?\n\nAfter using all of these libraries, node-fetch\n[https://www.npmjs.com/package/node-fetch]  is the weapon of choice for today.\nTo put it simply: it's straightforward, and the only one that actually works out\nof the box with Webpack without absurd configuration nonsense. \n\nThe other request library worth mentioning is isomorphic-fetch\n[https://www.npmjs.com/package/isomorphic-fetch], which is intended to be a\ndrop-in replacement for node-fetch. isometric-fetch  mimics the syntax of \nnode-fetch, but impressively works on both  the client and server-side. When\nused on the client side, isomorphicfetch works by first importing the \nes6-promise  polyfill.\n\nGetting Set Up\nStart a Node project and install node-fetch:\n\nnpm install --save node-fetch\n\n\nIn the JS file we'd like to make a request, we can reference node-fetch  using \nrequire():\n\nconst fetch = require('node-fetch');\n\n\nCreating a node-fetch Request\nWe'll start with the most basic GET request possible:\n\nfetch('https://example.com')\n  .then(response => response.json())\n  .then(data => {\n    console.log(data)\n  })\n  .catch(err => ...)\n\n\nIndeed, that's all it takes a base level. Without specifying a method,\nnode-fetch assumes we're making a GET request. From we generate JSON from the\nrequest body and print the result to the console.\n\nChances are you're not going to get much value out of any request without\npassing headers, parameters, or a body to the target endpoint. Here's how we'd\nmake a more complicated (and realistic) POST call:\n\nvar url ='https://example.com';\nvar headers = {\n  \"Content-Type\": \"application/json\",\n  \"client_id\": \"1001125\",\n  \"client_secret\": \"876JHG76UKFJYGVHf867rFUTFGHCJ8JHV\"\n}\nvar data = {\n  \"name\": \"Wade Wilson\",\n  \"occupation\": \"Murderer\",\n  \"age\": \"30 (forever)\"\n}\nfetch(url, { method: 'POST', headers: headers, body: data})\n  .then((res) => {\n     return res.json()\n})\n.then((json) => {\n  console.log(json);\n  // Do something with the returned data.\n});\n\n\nThat's more like it: now we're passing headers and a JSON body. If needed, the \nfetch()  method also accepts a credentials  parameter for authentication.\n\nNote that we are avoiding callback hell by keeping logic that utilizes the\nresponse JSON in our then()  arrow functions. We can chain together as many of\nthese statements as we want.\n\nProperties of a Response\nThe response object contains much more than just the response body JSON:\n\nfetch('https://example.com')\n.then(res => {\n  res.text()       // response body (=> Promise)\n  res.json()       // parse via JSON (=> Promise)\n  res.status       //=> 200\n  res.statusText   //=> 'OK'\n  res.redirected   //=> false\n  res.ok           //=> true\n  res.url          //=> 'https://example.com'\n  res.type         //=> 'basic'\n                   //   ('cors' 'default' 'error'\n                   //    'opaque' 'opaqueredirect')\n\n  res.headers.get('Content-Type')\n})\n\n\nres.status  is particularly handy when building functionality around catching\nerrors:\n\nfetch('https://example.com')\n  .then(reportStatus)\n  \nfunction checkStatus (res) {\n  if (res.status >= 200 && res.status < 300) {\n    return res\n  } else {\n    let err = new Error(res.statusText)\n    err.response = res\n    throw err\n  }\n}\n\n\nMaking Asynchronous Requests\nChances are that when we make an API request, we're planning to do something\nwith the resulting data. Once we start building logic which depends on the\noutcome of a request, this is when we start running into Callback Hell: perhaps\nthe worst  part of JavaScript. In a nutshell, JavaScript will not wait for a\nrequest to execute the next line of code, therefore making a request and\nreferencing it immediately will result in no data returned. We can get around\nthis by using a combination of async  and await.\n\nasync  is a keyword which denotes that a function is to be executed\nasynchronously (as in async function my_func(){...}). await  can be used when\ncalling async  functions to wait on the result of an async function to be\nreturned (ie: const response = await my_func()).\n\nHere's an example of async/await  in action:\n\nconst fetch = require(\"node-fetch\");\n\nconst url = \"https://example.com\";\n\nconst get_data = async url => {\n  try {\n    const response = await fetch(url);\n    const json = await response.json();\n    console.log(json);\n  } catch (error) {\n    console.log(error);\n  }\n};\n\ngetData(url);","html":"<p>If you're the type of person to read technical Javascript posts in your free time (you are), you don't need me to tell you that JQuery is dead. JQuery themselves have proclaimed JQuery to be dead. The only cool thing about JQuery is who can remove it from their legacy stack the fastest, which begs the question: why is the third most popular page on this site an old post about JQuery?</p><p>Maintaining a blog of tutorials has taught me a lot about the gap between perception and reality. While we content publishers sling Medium posts from our ivory towers, we quickly create a perception of what \"everybody\" is doing, but it turns out \"everybody\" only includes individuals who are exceptionally visible. That demographic makes up significantly less than 10-20% of the active workforce. I would have assumed any post with the word \"React\" would immediately explode, when in reality people are more interested in using <a href=\"https://hackersandslackers.com/handlebars-templating-in-expressjs/\">Handlebars with ExpressJS</a> (I'm not proud of that post by the way, please don't read it).</p><p>I want to provide an alternative to using AJAX calls when interacting with REST APIs to clear my conscious of ever enabling bad behavior in the first place. Hopefully, those who have lost their way might find something to take from it. Considering how deep I've gone down the GraphQL rabbit hole myself, this may be the last chance to bother writing about REST at all.</p><h2 id=\"library-of-choice-node-fetch\">Library of Choice: node-fetch</h2><p>Like everything in Javascript, there are way too many packages doing the same thing and solving the same problem. Making API requests is no exception. <strong><a href=\"https://www.npmjs.com/package/http\">http</a> </strong>is a bit primitive, <strong><a href=\"https://www.npmjs.com/package/request\">request</a></strong> breaks when building with Webpack, <strong><a href=\"https://www.npmjs.com/package/r2\">r2</a></strong> seems like a pointless clone, and so on. Don't get me started with async libraries with 40 different methods for chaining requests. Who is gunslinging API requests to the point where we need this many options to pipe or parallel API requests anyway?</p><p>After using all of these libraries, <strong><a href=\"https://www.npmjs.com/package/node-fetch\">node-fetch</a></strong> is the weapon of choice for today. To put it simply: it's straightforward, and the only one that actually works out of the box with Webpack without absurd configuration nonsense. </p><p>The other request library worth mentioning is <strong><a href=\"https://www.npmjs.com/package/isomorphic-fetch\">isomorphic-fetch</a>, </strong>which is intended to be a drop-in replacement for <em>node-fetch</em>. <strong>isometric-fetch</strong> mimics the syntax of <strong>node-fetch</strong>, but impressively works on <em>both</em> the client and server-side. When used on the client side, <em>isomorphicfetch </em>works by first importing the <code>es6-promise</code> polyfill.</p><h3 id=\"getting-set-up\">Getting Set Up</h3><p>Start a Node project and install node-fetch:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">npm install --save node-fetch\n</code></pre>\n<!--kg-card-end: markdown--><p>In the JS file we'd like to make a request, we can reference <strong>node-fetch</strong> using <strong>require():</strong></p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const fetch = require('node-fetch');\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-a-node-fetch-request\">Creating a node-fetch Request</h2><p>We'll start with the most basic GET request possible:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n  .then(response =&gt; response.json())\n  .then(data =&gt; {\n    console.log(data)\n  })\n  .catch(err =&gt; ...)\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, that's all it takes a base level. Without specifying a method, node-fetch assumes we're making a GET request. From we generate JSON from the request body and print the result to the console.</p><p>Chances are you're not going to get much value out of any request without passing headers, parameters, or a body to the target endpoint. Here's how we'd make a more complicated (and realistic) POST call:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var url ='https://example.com';\nvar headers = {\n  &quot;Content-Type&quot;: &quot;application/json&quot;,\n  &quot;client_id&quot;: &quot;1001125&quot;,\n  &quot;client_secret&quot;: &quot;876JHG76UKFJYGVHf867rFUTFGHCJ8JHV&quot;\n}\nvar data = {\n  &quot;name&quot;: &quot;Wade Wilson&quot;,\n  &quot;occupation&quot;: &quot;Murderer&quot;,\n  &quot;age&quot;: &quot;30 (forever)&quot;\n}\nfetch(url, { method: 'POST', headers: headers, body: data})\n  .then((res) =&gt; {\n     return res.json()\n})\n.then((json) =&gt; {\n  console.log(json);\n  // Do something with the returned data.\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>That's more like it: now we're passing headers and a JSON body. If needed, the <strong>fetch()</strong> method also accepts a <code>credentials</code> parameter for authentication.</p><p>Note that we are avoiding callback hell by keeping logic that utilizes the response JSON in our <strong>then()</strong> arrow functions. We can chain together as many of these statements as we want.</p><h3 id=\"properties-of-a-response\">Properties of a Response</h3><p>The response object contains much more than just the response body JSON:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n.then(res =&gt; {\n  res.text()       // response body (=&gt; Promise)\n  res.json()       // parse via JSON (=&gt; Promise)\n  res.status       //=&gt; 200\n  res.statusText   //=&gt; 'OK'\n  res.redirected   //=&gt; false\n  res.ok           //=&gt; true\n  res.url          //=&gt; 'https://example.com'\n  res.type         //=&gt; 'basic'\n                   //   ('cors' 'default' 'error'\n                   //    'opaque' 'opaqueredirect')\n\n  res.headers.get('Content-Type')\n})\n</code></pre>\n<!--kg-card-end: markdown--><p><code>res.status</code> is particularly handy when building functionality around catching errors:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">fetch('https://example.com')\n  .then(reportStatus)\n  \nfunction checkStatus (res) {\n  if (res.status &gt;= 200 &amp;&amp; res.status &lt; 300) {\n    return res\n  } else {\n    let err = new Error(res.statusText)\n    err.response = res\n    throw err\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"making-asynchronous-requests\">Making Asynchronous Requests</h2><p>Chances are that when we make an API request, we're planning to do something with the resulting data. Once we start building logic which depends on the outcome of a request, this is when we start running into <strong>Callback Hell</strong>: perhaps the <em>worst</em> part of JavaScript. In a nutshell, JavaScript will not wait for a request to execute the next line of code, therefore making a request and referencing it immediately will result in no data returned. We can get around this by using a combination of <strong>async</strong> and <strong>await.</strong></p><p><code>async</code> is a keyword which denotes that a function is to be executed asynchronously (as in <code>async function my_func(){...}</code>). <code>await</code> can be used when calling <code>async</code> functions to wait on the result of an async function to be returned (ie: <code>const response = await my_func()</code>).</p><p>Here's an example of <strong>async/await</strong> in action:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">const fetch = require(&quot;node-fetch&quot;);\n\nconst url = &quot;https://example.com&quot;;\n\nconst get_data = async url =&gt; {\n  try {\n    const response = await fetch(url);\n    const json = await response.json();\n    console.log(json);\n  } catch (error) {\n    console.log(error);\n  }\n};\n\ngetData(url);\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/making-api-requests-with-nodejs/","uuid":"9b46eb2c-0339-44f9-8909-13474dff9377","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6dd08fa624d869fba41325"}},{"node":{"id":"Ghost__Post__5c6636c6c2209e663b5e5a68","title":"Lynx Roundup, February 19th","slug":"lynx-roundup-february-19th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/144.jpg","excerpt":"Window Functions in Pandas and Spark!  Enhancing Jupyter Notebooks!  Better math education!","custom_excerpt":"Window Functions in Pandas and Spark!  Enhancing Jupyter Notebooks!  Better math education!","created_at_pretty":"15 February, 2019","published_at_pretty":"20 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:49:26.000-05:00","published_at":"2019-02-19T21:47:00.000-05:00","updated_at":"2019-02-27T23:01:30.000-05:00","meta_title":"Lynx Roundup, February 19th | Hackers and Slackers","meta_description":"Window Functions in Pandas and Spark!  Enhancing Jupyter Notebooks!  Better math education!","og_description":"Window Functions in Pandas and Spark!  Enhancing Jupyter Notebooks!  Better math education!","og_image":"https://hackersandslackers.com/content/images/2019/02/144.jpg","og_title":"Lynx Roundup, February 19th","twitter_description":"Window Functions in Pandas and Spark!  Enhancing Jupyter Notebooks!  Better math education!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/144.jpg","twitter_title":"Lynx Roundup, February 19th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4\n\nhttps://towardsdatascience.com/power-ups-for-jupyter-notebooks-ebfa6e5e57a\n\nhttps://medium.com/airbnb-engineering/contextualizing-airbnb-by-building-knowledge-graph-b7077e268d5a\n\nhttps://blogs.ams.org/matheducation/2019/02/01/everyone-can-learn-mathematics-to-high-levels-the-evidence-from-neuroscience-that-should-change-our-teaching/\n\nhttps://github.com/pyTaxPrep/taxes-2018\n\nhttps://towardsdatascience.com/expose-endpoints-using-jupyter-kernel-gateway-e55951b0f5ad\n\nhttps://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6","html":"<p></p><p><a href=\"https://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4\">https://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4</a></p><p><a href=\"https://towardsdatascience.com/power-ups-for-jupyter-notebooks-ebfa6e5e57a\">https://towardsdatascience.com/power-ups-for-jupyter-notebooks-ebfa6e5e57a</a></p><p><a href=\"https://medium.com/airbnb-engineering/contextualizing-airbnb-by-building-knowledge-graph-b7077e268d5a\">https://medium.com/airbnb-engineering/contextualizing-airbnb-by-building-knowledge-graph-b7077e268d5a</a></p><p><a href=\"https://blogs.ams.org/matheducation/2019/02/01/everyone-can-learn-mathematics-to-high-levels-the-evidence-from-neuroscience-that-should-change-our-teaching/\">https://blogs.ams.org/matheducation/2019/02/01/everyone-can-learn-mathematics-to-high-levels-the-evidence-from-neuroscience-that-should-change-our-teaching/</a></p><p><a href=\"https://github.com/pyTaxPrep/taxes-2018\">https://github.com/pyTaxPrep/taxes-2018</a></p><p><a href=\"https://towardsdatascience.com/expose-endpoints-using-jupyter-kernel-gateway-e55951b0f5ad\">https://towardsdatascience.com/expose-endpoints-using-jupyter-kernel-gateway-e55951b0f5ad</a></p><p><a href=\"https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\">https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-19th/","uuid":"9db0169e-1185-4eb1-b598-75f887420119","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c6636c6c2209e663b5e5a68"}},{"node":{"id":"Ghost__Post__5c5bb0ec7999ff33f06876e1","title":"Welcome to SQL: Modifying Databases and Tables","slug":"welcome-to-sql-modifying-databases-and-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","custom_excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","created_at_pretty":"07 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-06T23:15:40.000-05:00","published_at":"2019-02-19T18:28:00.000-05:00","updated_at":"2019-02-27T23:16:44.000-05:00","meta_title":"Welcome to SQL: Modifying Databases and Tables | Hackers and Slackers","meta_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","og_title":"Welcome to SQL: Modifying Databases and Tables","twitter_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","twitter_title":"Welcome to SQL: Modifying Databases and Tables","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"SQL: we all pretend to be experts at it, and mostly get away with it thanks to\nStackOverflow. Paired with our vast experience of learning how to code in the\n90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go\nahead and chalk up a win for your resume.\n\nSQL has been around longer than our careers have, so why start a series on it \nnow?  Surely there’s sufficient enough documentation that we can Google the\nspecifics whenever the time comes for us to write a query? That, my friends, is\nprecisely the problem. Regardless of what tools we have at our disposable, some\nskills are better learned and practiced by heart. SQL is one of those skills.\n\nSure, SQLAlchemy or similar ORMs might protect us here-and-there from writing\nraw queries. Considering SQL is just one of many query languages we'll use\nregularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert\nreally that critical? In short, yes: relational databases are not only here to\nstay, but thinking  in queries as a second language solidifies one's\nunderstanding of the fine details of data. Marc Laforet\n[https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032] \n recently published a Medium post which drives home just how important leaning\non SQL is:\n\n> What’s even more interesting is that when these transformation scripts were\napplied to the 6.5 GB dataset, python completely failed. Out of 3 attempts,\npython crashed 2 times and my computer completely froze the 3rd time… while SQL\ntook 226 seconds.\n\n\nKeeping logic out of our apps and pipelines and in SQL results in exponentially\nfaster execution, while also being more readable and universally understood than\nwhatever we’d write in our language of choice. The lower down we can push\napplication logic in our stack, the better. This is why I’d much prefer to see\nthe datasphere saturated with SQL tutorials as opposed to Pandas tutorials.\n\nRelational Database Terminology\nI hate it when informational material kicks off with covering obvious\nterminology definitions. Under normal circumstances, I find this to be cliche,\nunhelpful, and damaging to an author's credibility; but these aren't normal\ncircumstances. In SQL, vocabulary commonly has multiple meanings depending on\ncontext, or even which flavor database you're using. Given this fact, it's\nentirely possible (and common) for individuals to rack up experience with\nrelational databases while completely misinterpreting fundamental concepts.\nLet's make sure that doesn't happen:\n\n * Databases: Every Database instance is separated at the highest level into \n   databases. Yes, a database is a collection of databases - we're already off\n   to a great start.\n * Schemas: In PostgreSQL (and other databases), a schema  is a grouping of\n   tables and other objects, including views, relations, etc. A schema is a way\n   of organizing data. Schemas imply that all the data belonging to it is at\n   some form related, even if only by concept. Note that the term schema  is\n   sometimes used to describe other concepts depending on the context.\n * Tables: The meat and potatos of relational databases. Tables consist of rows\n   and columns which hold our sweet, sweet data. Columns are best thought of as\n   'attributes', whereas rows are entries which consist of values for said\n   attributes. All values in a column must share the same data type. * Keys: Keys are used to help us organize and optimize data, as well as\n      place certain constraints on data coming in (for example, email addresses\n      of user accounts must be unique). Keys can also help us keep count of our\n      entries, ensure automatically unique values, and provide a bridge to link\n      multiple tables of data. * Primary keys: Identification tags for each row of data. The primary key\n         is different for every record in the relational database; values must\n         be provided, and they must be unique between rows.\n       * Foreign keys: Enable data searches and manipulation between the primary\n         database table and other related databases.\n      \n      \n   \n   \n * Objects: A blanket term for anything (including relations) that exist in a\n   schema (somewhat PostgreSQL-specific). * Views (PostgreSQL): Views display data in a fashion similar to tables,\n      with the difference that views do not store  data. Views are a snapshot of\n      data pulled from other tables in the form of a query; a good way to think\n      about views is to consider them to be 'virtual tables.'\n    * Functions (PostgreSQL): Logic for interacting with data saved for the\n      purpose of being reused.\n   \n   \n\nIn MySQL, a schema  is synonymous with a database. These keywords can even be\nswapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using CREATE\nSCHEMA  acheives the same effect as instead of CREATE DATABASE.Navigating and\nCreating Databases\nWe've got to start somewhere, so it might as well be with database management.\nAdmittedly, this will be the most useless of the things we'll cover. The act of\nnavigating databases is best suited for a GUI.\n\nShow Databases\nIf you access your database via command line shell (for some reason), the first\nlogical thing to do is to list the available databases:\n\nSHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\n\nUSE Database\nNow that we've listed the possible databases we can connect to, we can explore\nwhat each of these contains. To do this, we have to specify which database we\nwant to connect to, AKA \"use.\" \n\ndb> USE database_name;\nDatabase changed\n\n\nCreate Database\nCreating databases is straightforward. Be sure to pay attention to the character\nset  when creating a database: this will determine which types of characters\nyour database will be able to accept. For example, if we try to insert special\nencoded characters into a simple UTF-8 database, those characters won’t turn out\nas we’d expect.\n\nCREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n\n\nBonus: here's the shorthand for creating a database and then showing the result:\n\nSHOW CREATE DATABASE database_name;\n\n\nCreating and Modifying Tables\nCreating tables via SQL syntax can be critical when automating data imports.\nWhen creating a table, we also set the column names, types, and keys:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];\n\nWe can specify IF NOT EXISTS  when creating our table if we'd like to include\nvalidation in our query. When present, the table will only be created if a table\nof the specified name does not exist.\n\nWhen creating each of our columns, there are a number of things we can specify\nper-column:\n\n * Data Type (required):  The data which can be saved to cells of this column\n   (such as INTEGER, TEXT, etc).\n * Key Type:  Creates a key for the column.\n * Key Attributes:  Any key-related attributes, such as auto-incrementing.\n * Default:  If rows are created in the table without values passed to the\n   current column, the value specified as DEFAULT  \n * Primary Key:  Allows any of the previous specified columns to be set as the\n   table's primary key.\n\nMySQL tables can have a 'storage engine' specified via ENGINE=[engine_type],\nwhich determines the core logic of how the table will interpret data. Leaving\nthis blank defaults to InnoDB and is almost certainly fine to be left alone. In\ncase you're interested, you can find more about MySQL engines here\n[https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html].\n\nHere's an example of what an actual CREATE TABLE  query would look like:\n\nCREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nManaging Keys for Existing Tables\nIf we don't specify our keys at table creation time, we can always do so after\nthe fact. SQL tables can accept the following key types:\n\n * Primary Key:  One or more fields/columns that uniquely identify a record in\n   the table. It can not accept null, duplicate values.\n * Candidate Key:  Candidate keys are kind of like groups of non-committed\n   Primary Keys; these keys only accept unique values, and could potentially  be\n   used in the place of a Primary Key if need be, but are not actual Primary\n   Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.\n * Alternate Key:  Refers to a single Candidate Key (an alternative which can\n   satisfy the duty of a Primary Key id need be).\n * Composite/Compound Key:  Defined by combing the values of multiple columns;\n   the sum of which will always produce a unique value. There can be multiple\n   Candidate Keys in one table. Each Candidate Key can work as Primary Key.\n * Unique Key:  A set of one or more fields/columns of a table that uniquely\n   identify a record in a database table. Similar to Primary key, but it can\n   accept only one null value, and it can not have duplicate values.\n * Foreign Key: Foreign keys denote fields that serve as another table's \n   Primary key. Foreign keys are useful for building relationships between\n   tables. While a foreign key is required in the parent table where they are\n   primary, foreign keys can be null or empty in the tables intended to relate\n   to the other table.\n\nLet's look at an example query where we add a key to a table and dissect the\npieces:\n\nALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n\n\nALTER TABLE  is used to make any changes to a table's structure, whether that be\nmodifying columns or keys.\n\nIn this example, we ADD  a key which happens to be a FOREIGN KEY. While keys\nalways refer to columns, keys themselves must have names of their own to\ndistinguish the column's data and a key's conceptual logic. We name our key \nforeign_key_name  and specify which column the key will act on with \n(column_name). Because this is a foreign key, we need to specify which table's \nprimary key  we want this to be associated with. REFERENCES\nparent_table(primary_key_column)  is stating that the foreign key in this table\ncorresponds to values held in a column named primary_key_column, in a table\nnamed parent_table.\n\nThe statements ON DELETE  and ON UPDATE  are actions which take place if the\nparent table's primary key is deleted or updated, respectively. ON DELETE\nCASCADE  would result in our tables foreign key being deleted if the\ncorresponding primary key were to disappear.\n\nAdding Columns\nAdding columns follows the same syntax we used when creating tables. An\ninteresting additional feature is the ability to place the new column before or\nafter preexisting columns:\n\nALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n\n\nWhen referencing tables in PostgreSQL databases, we must specify the schema\nbelongs to. Thus, ALTER TABLE table_name  becomes ALTER TABLE\nschema_name.table_name. This applies to any time we reference tables, including\nwhen we create and delete tables.Pop Quiz\nThe below statement uses elements of everything we've learned about modifying\nand creating table structures thus far. Can you discern what is happening here?\n\nCREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n\n\nDropping Data\nDANGER ZONE: this is where we can start to mess things up. Dropping columns or\ntables results in a complete loss of data: whenever you see the word \"drop,\" be\nscared.\n\nIf you're sure you know what you're doing and would like to remove a table\ncolumn, this can be done as such:\n\nALTER TABLE table\nDROP column;\n\n\nDropping a table destroys the table structure as well as all data within it:\n\nDROP TABLE table_name;\n\n\nTruncating a table, on the other hand, will purge the table of data but retain\nthe table itself:\n\nTRUNCATE TABLE table_name;\n\n\nDrop Foreign Key\nLike tables and columns, we can drop keys as well:\n\nALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n\n\nThis can also be handed by dropping CONSTRAINT:\n\nALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n\n\nWorking with Views (Specific to PostgreSQL)\nLastly, let's explore the act of creating views. There are three types of views\nPostgreSQL can handle:\n\n * Simple Views: Virtual tables which represent data of underlying tables.\n   Simple views are automatically updatable: the system will allow INSERT,\n   UPDATE and DELETE statements to be used on the view in the same way as on a\n   regular table.\n * Materialized Views: PostgreSQL extends the view concept to a next level that\n   allows views to store data 'physically', and we call those views are\n   materialized views. A materialized view caches the result of a complex query\n   and then allow you to refresh the result periodically.\n * Recursive Views: Recursive views are a bit difficult to explain without\n   delving deep into the complicated (but cool!) functionality of recursive\n   reporting. I won't get into the details, but these views are able to\n   represent relationships which go multiple layers deep. Here's a quick taste,\n   if you;re curious:\n\nSample RECURSIVE  query:\n\nWITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' > ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n\n\nOutput:\n\n employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North > Megan Berry\n           3 | Michael North > Sarah Berry\n           4 | Michael North > Zoe Black\n           5 | Michael North > Tim James\n           6 | Michael North > Megan Berry > Bella Tucker\n           7 | Michael North > Megan Berry > Ryan Metcalfe\n           8 | Michael North > Megan Berry > Max Mills\n           9 | Michael North > Megan Berry > Benjamin Glover\n          10 | Michael North > Sarah Berry > Carolyn Henderson\n          11 | Michael North > Sarah Berry > Nicola Kelly\n          12 | Michael North > Sarah Berry > Alexandra Climo\n          13 | Michael North > Sarah Berry > Dominic King\n          14 | Michael North > Zoe Black > Leonard Gray\n          15 | Michael North > Zoe Black > Eric Rampling\n          16 | Michael North > Megan Berry > Ryan Metcalfe > Piers Paige\n          17 | Michael North > Megan Berry > Ryan Metcalfe > Ryan Henderson\n          18 | Michael North > Megan Berry > Max Mills > Frank Tucker\n          19 | Michael North > Megan Berry > Max Mills > Nathan Ferguson\n          20 | Michael North > Megan Berry > Max Mills > Kevin Rampling\n(20 rows)\n\n\nCreating a View\nCreating a simple view is as simple as writing a standard query! All that is\nrequired is the addition of CREATE VIEW view_name AS  before the query, and this\nwill create a saved place for us to always come back and reference the results\nof this query:\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n\n\nGet Out There and Start SQLing\nI highly encourage anybody to get in the habit of always writing SQL queries by\nhand. With the right GUI, autocompletion can be your best friend.\n\nExplicitly forcing one's self to write queries instead of copy & pasting\nanything forces us to come to realizations, such as SQL's order of operations.\nIndeed, this query holds the correct syntax...\n\nSELECT *\nFROM table_name\nWHERE column_name = 'Value';\n\n\n...Whereas this one does not:\n\nSELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n\n\nGrasping the subtleties of SQL is the difference between being blazing fast and\nmostly clueless. The good news is, you’ll start to find that these concepts\naren’t nearly as daunting as they may have once seemed, so the track from ‘bad\ndata engineer’ to ‘expert’ is an easy win that would be foolish not to take.\n\nStick around for next time where we actually work with data in SQL: The Sequel,\nrated PG-13.","html":"<p>SQL: we all pretend to be experts at it, and mostly get away with it thanks to StackOverflow. Paired with our vast experience of learning how to code in the 90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go ahead and chalk up a win for your resume.</p><p>SQL has been around longer than our careers have, so why start a series on it <em>now?</em> Surely there’s sufficient enough documentation that we can Google the specifics whenever the time comes for us to write a query? That, my friends, is precisely the problem. Regardless of what tools we have at our disposable, some skills are better learned and practiced by heart. SQL is one of those skills.</p><p>Sure, SQLAlchemy or similar ORMs might protect us here-and-there from writing raw queries. Considering SQL is just one of many query languages we'll use regularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert really that critical? In short, yes: relational databases are not only here to stay, but <em>thinking</em> in queries as a second language solidifies one's understanding of the fine details of data. <a href=\"https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032\">Marc Laforet</a> recently published a Medium post which drives home just how important leaning on SQL is:</p><blockquote>\n<p>What’s even more interesting is that when these transformation scripts were applied to the 6.5 GB dataset, python completely failed. Out of 3 attempts, python crashed 2 times and my computer completely froze the 3rd time… while SQL took 226 seconds.</p>\n</blockquote>\n<p>Keeping logic out of our apps and pipelines and in SQL results in exponentially faster execution, while also being more readable and universally understood than whatever we’d write in our language of choice. The lower down we can push application logic in our stack, the better. This is why I’d much prefer to see the datasphere saturated with SQL tutorials as opposed to Pandas tutorials.</p><h2 id=\"relational-database-terminology\">Relational Database Terminology</h2><p>I hate it when informational material kicks off with covering obvious terminology definitions. Under normal circumstances, I find this to be cliche, unhelpful, and damaging to an author's credibility; but these aren't normal circumstances. In SQL, vocabulary commonly has multiple meanings depending on context, or even which flavor database you're using. Given this fact, it's entirely possible (and common) for individuals to rack up experience with relational databases while completely misinterpreting fundamental concepts. Let's make sure that doesn't happen:</p><ul>\n<li><strong>Databases</strong>: Every Database instance is separated at the highest level into <em>databases</em>. Yes, a database is a collection of databases - we're already off to a great start.</li>\n<li><strong>Schemas</strong>: In PostgreSQL (and other databases), a <em>schema</em> is a grouping of tables and other objects, including views, relations, etc. A schema is a way of organizing data. Schemas imply that all the data belonging to it is at some form related, even if only by concept. Note that the term <em>schema</em> is sometimes used to describe other concepts depending on the context.</li>\n<li><strong>Tables</strong>: The meat and potatos of relational databases. Tables consist of rows and columns which hold our sweet, sweet data. Columns are best thought of as 'attributes', whereas rows are entries which consist of values for said attributes. All values in a column must share the same data type.\n<ul>\n<li><strong>Keys</strong>: Keys are used to help us organize and optimize data, as well as place certain constraints on data coming in (for example, email addresses of user accounts must be <em>unique</em>). Keys can also help us keep count of our entries, ensure automatically unique values, and provide a bridge to link multiple tables of data.\n<ul>\n<li><strong>Primary keys</strong>:  Identification tags for each row of data. The primary key is different for every record in the relational database; values must be provided, and they must be unique between rows.</li>\n<li><strong>Foreign keys</strong>: Enable data searches and manipulation between the primary database table and other related databases.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objects</strong>: A blanket term for anything (including relations) that exist in a schema (somewhat PostgreSQL-specific).\n<ul>\n<li><strong>Views (PostgreSQL)</strong>: Views display data in a fashion similar to tables, with the difference that views do not <em>store</em> data. Views are a snapshot of data pulled from other tables in the form of a query; a good way to think about views is to consider them to be 'virtual tables.'</li>\n<li><strong>Functions  (PostgreSQL)</strong>: Logic for interacting with data saved for the purpose of being reused.</li>\n</ul>\n</li>\n</ul>\n<div class=\"protip\">\nIn MySQL, a <strong>schema</strong> is synonymous with a <strong>database</strong>. These keywords can even be swapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using <code>CREATE SCHEMA</code> acheives the same effect as instead of <code>CREATE DATABASE</code>.   \n</div><h2 id=\"navigating-and-creating-databases\">Navigating and Creating Databases</h2><p>We've got to start somewhere, so it might as well be with database management. Admittedly, this will be the most useless of the things we'll cover. The act of navigating databases is best suited for a GUI.</p><h3 id=\"show-databases\">Show Databases</h3><p>If you access your database via command line shell (for some reason), the first logical thing to do is to list the available databases:</p><pre><code class=\"language-sql\">SHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n</code></pre>\n<h3 id=\"use-database\">USE Database</h3><p>Now that we've listed the possible databases we can connect to, we can explore what each of these contains. To do this, we have to specify which database we want to connect to, AKA \"use.\" </p><pre><code class=\"language-sql\">db&gt; USE database_name;\nDatabase changed\n</code></pre>\n<h3 id=\"create-database\">Create Database</h3><p>Creating databases is straightforward. Be sure to pay attention to the <em>character set</em> when creating a database: this will determine which types of characters your database will be able to accept. For example, if we try to insert special encoded characters into a simple UTF-8 database, those characters won’t turn out as we’d expect.</p><pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n</code></pre>\n<p>Bonus: here's the shorthand for creating a database and then showing the result:</p><pre><code class=\"language-sql\">SHOW CREATE DATABASE database_name;\n</code></pre>\n<h2 id=\"creating-and-modifying-tables\">Creating and Modifying Tables</h2><p>Creating tables via SQL syntax can be critical when automating data imports. When creating a table, we also set the column names, types, and keys:</p><pre><code>CREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];</code></pre><p>We can specify <code>IF NOT EXISTS</code> when creating our table if we'd like to include validation in our query. When present, the table will only be created if a table of the specified name does not exist.</p><p>When creating each of our columns, there are a number of things we can specify per-column:</p><ul><li><strong>Data Type (required):</strong> The data which can be saved to cells of this column (such as INTEGER, TEXT, etc).</li><li><strong>Key Type:</strong> Creates a key for the column.</li><li><strong>Key Attributes:</strong> Any key-related attributes, such as auto-incrementing.</li><li><strong>Default:</strong> If rows are created in the table without values passed to the current column, the value specified as <code>DEFAULT</code> </li><li><strong>Primary Key:</strong> Allows any of the previous specified columns to be set as the table's primary key.</li></ul><p>MySQL tables can have a 'storage engine' specified via <code>ENGINE=[engine_type]</code>, which determines the core logic of how the table will interpret data. Leaving this blank defaults to InnoDB and is almost certainly fine to be left alone. In case you're interested, you can find more about MySQL engines <a href=\"https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html\">here</a>.</p><p>Here's an example of what an actual <code>CREATE TABLE</code> query would look like:</p><pre><code>CREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;</code></pre><h3 id=\"managing-keys-for-existing-tables\">Managing Keys for Existing Tables</h3><p>If we don't specify our keys at table creation time, we can always do so after the fact. SQL tables can accept the following key types:</p><ul><li><strong>Primary Key:</strong> One or more fields/columns that uniquely identify a record in the table. It can not accept null, duplicate values.</li><li><strong>Candidate Key:</strong> Candidate keys are kind of like groups of non-committed Primary Keys; these keys only accept unique values, and <em>could potentially</em> be used in the place of a Primary Key if need be, but are not actual Primary Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.</li><li><strong>Alternate Key:</strong> Refers to a single Candidate Key (an alternative which can satisfy the duty of a Primary Key id need be).</li><li><strong>Composite/Compound Key:</strong> Defined by combing the values of multiple columns; the sum of which will always produce a unique value. There can be multiple Candidate Keys in one table. Each Candidate Key can work as Primary Key.</li><li><strong>Unique Key:</strong> A set of one or more fields/columns of a table that uniquely identify a record in a database table. Similar to Primary key, but it can accept only one null value, and it can not have duplicate values.</li><li><strong>Foreign Key: </strong>Foreign keys denote fields that serve as <em>another table's</em> Primary key. Foreign keys are useful for building relationships between tables. While a foreign key is required in the parent table where they are primary, foreign keys can be null or empty in the tables intended to relate to the other table.</li></ul><p>Let's look at an example query where we add a key to a table and dissect the pieces:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n</code></pre>\n<p><code>ALTER TABLE</code> is used to make any changes to a table's structure, whether that be modifying columns or keys.</p><p>In this example, we <code>ADD</code> a key which happens to be a <code>FOREIGN KEY</code>. While keys always refer to columns, keys themselves must have names of their own to distinguish the column's data and a key's conceptual logic. We name our key <code>foreign_key_name</code> and specify which column the key will act on with <code>(column_name)</code>. Because this is a foreign key, we need to specify which table's <em>primary key</em> we want this to be associated with. <code>REFERENCES parent_table(primary_key_column)</code> is stating that the foreign key in this table corresponds to values held in a column named <code>primary_key_column</code>, in a table named <code>parent_table</code>.</p><p>The statements <code>ON DELETE</code> and <code>ON UPDATE</code> are actions which take place if the parent table's primary key is deleted or updated, respectively. <code>ON DELETE CASCADE</code> would result in our tables foreign key being deleted if the corresponding primary key were to disappear.</p><h3 id=\"adding-columns\">Adding Columns</h3><p>Adding columns follows the same syntax we used when creating tables. An interesting additional feature is the ability to place the new column before or after preexisting columns:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n</code></pre>\n<div class=\"protip\">\nWhen referencing tables in PostgreSQL databases, we must specify the schema belongs to. Thus, <code>ALTER TABLE table_name</code> becomes <code>ALTER TABLE schema_name.table_name</code>. This applies to any time we reference tables, including when we create and delete tables.\n</div><h3 id=\"pop-quiz\">Pop Quiz</h3><p>The below statement uses elements of everything we've learned about modifying and creating table structures thus far. Can you discern what is happening here?</p><pre><code class=\"language-sql\">CREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n</code></pre>\n<h2 id=\"dropping-data\">Dropping Data</h2><p>DANGER ZONE: this is where we can start to mess things up. Dropping columns or tables results in a complete loss of data: whenever you see the word \"drop,\" be scared.</p><p>If you're sure you know what you're doing and would like to remove a table column, this can be done as such:</p><pre><code class=\"language-sql\">ALTER TABLE table\nDROP column;\n</code></pre>\n<p>Dropping a table destroys the table structure as well as all data within it:</p><pre><code class=\"language-sql\">DROP TABLE table_name;\n</code></pre>\n<p>Truncating a table, on the other hand, will purge the table of data but retain the table itself:</p><pre><code class=\"language-sql\">TRUNCATE TABLE table_name;\n</code></pre>\n<h3 id=\"drop-foreign-key\">Drop Foreign Key</h3><p>Like tables and columns, we can drop keys as well:</p><pre><code class=\"language-sql\">ALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n</code></pre>\n<p>This can also be handed by dropping CONSTRAINT:</p><pre><code class=\"language-sql\">ALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n</code></pre>\n<h2 id=\"working-with-views-specific-to-postgresql-\">Working with Views (Specific to PostgreSQL)</h2><p>Lastly, let's explore the act of creating views. There are three types of views PostgreSQL can handle:</p><ul>\n<li><strong>Simple Views</strong>: Virtual tables which represent data of underlying tables. Simple views are automatically updatable: the system will allow INSERT, UPDATE and DELETE statements to be used on the view in the same way as on a regular table.</li>\n<li><strong>Materialized Views</strong>: PostgreSQL extends the view concept to a next level that allows views to store data 'physically', and we call those views are materialized views. A materialized view caches the result of a complex query and then allow you to refresh the result periodically.</li>\n<li><strong>Recursive Views</strong>: Recursive views are a bit difficult to explain without delving deep into the complicated (but cool!) functionality of recursive reporting. I won't get into the details, but these views are able to represent relationships which go multiple layers deep. Here's a quick taste, if you;re curious:</li>\n</ul>\n<p><strong>Sample </strong><code>RECURSIVE</code> <strong>query:</strong></p><pre><code class=\"language-sql\">WITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' &gt; ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n</code></pre>\n<p><strong>Output:</strong></p><pre><code class=\"language-shell\"> employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North &gt; Megan Berry\n           3 | Michael North &gt; Sarah Berry\n           4 | Michael North &gt; Zoe Black\n           5 | Michael North &gt; Tim James\n           6 | Michael North &gt; Megan Berry &gt; Bella Tucker\n           7 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe\n           8 | Michael North &gt; Megan Berry &gt; Max Mills\n           9 | Michael North &gt; Megan Berry &gt; Benjamin Glover\n          10 | Michael North &gt; Sarah Berry &gt; Carolyn Henderson\n          11 | Michael North &gt; Sarah Berry &gt; Nicola Kelly\n          12 | Michael North &gt; Sarah Berry &gt; Alexandra Climo\n          13 | Michael North &gt; Sarah Berry &gt; Dominic King\n          14 | Michael North &gt; Zoe Black &gt; Leonard Gray\n          15 | Michael North &gt; Zoe Black &gt; Eric Rampling\n          16 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Piers Paige\n          17 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Ryan Henderson\n          18 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Frank Tucker\n          19 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Nathan Ferguson\n          20 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Kevin Rampling\n(20 rows)\n</code></pre>\n<h3 id=\"creating-a-view\">Creating a View</h3><p>Creating a simple view is as simple as writing a standard query! All that is required is the addition of <code>CREATE VIEW view_name AS</code> before the query, and this will create a saved place for us to always come back and reference the results of this query:</p><pre><code class=\"language-sql\">CREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n</code></pre>\n<h2 id=\"get-out-there-and-start-sqling\">Get Out There and Start SQLing</h2><p>I highly encourage anybody to get in the habit of <em>always </em>writing SQL queries by hand. With the right GUI, autocompletion can be your best friend.</p><p>Explicitly forcing one's self to write queries instead of copy &amp; pasting anything forces us to come to realizations, such as SQL's order of operations. Indeed, this query holds the correct syntax...</p><pre><code class=\"language-sql\">SELECT *\nFROM table_name\nWHERE column_name = 'Value';\n</code></pre>\n<p>...Whereas this one does not:</p><pre><code class=\"language-sql\">SELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n</code></pre>\n<p>Grasping the subtleties of SQL is the difference between being blazing fast and mostly clueless. The good news is, you’ll start to find that these concepts aren’t nearly as daunting as they may have once seemed, so the track from ‘bad data engineer’ to ‘expert’ is an easy win that would be foolish not to take.</p><p>Stick around for next time where we actually work with data in <strong>SQL: The Sequel</strong>, rated PG-13.</p>","url":"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/","uuid":"fe99e822-f21a-432c-8bbf-4d399e575570","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5bb0ec7999ff33f06876e1"}},{"node":{"id":"Ghost__Post__5c66365ec2209e663b5e5a5e","title":"Lynx Roundup, February 18th","slug":"lynx-roundup-february-18th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/131.jpg","excerpt":"Creating Modules & Packages in Python!  Neat coding tools!  Industrial-grade Python with Zero-Copy and Buffer Protocols!","custom_excerpt":"Creating Modules & Packages in Python!  Neat coding tools!  Industrial-grade Python with Zero-Copy and Buffer Protocols!","created_at_pretty":"15 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T22:47:42.000-05:00","published_at":"2019-02-18T21:09:00.000-05:00","updated_at":"2019-02-27T23:02:12.000-05:00","meta_title":"Lynx Roundup, February 18th | Hackers and Slackers","meta_description":"Creating Modules & Packages in Python!  Neat coding tools!  Industrial-grade Python with Zero-Copy and Buffer Protocols!","og_description":"Creating Modules & Packages in Python!  Neat coding tools!  Industrial-grade Python with Zero-Copy and Buffer Protocols!","og_image":"https://hackersandslackers.com/content/images/2019/02/131.jpg","og_title":"Lynx Roundup, February 18th","twitter_description":"Creating Modules & Packages in Python!  Neat coding tools!  Industrial-grade Python with Zero-Copy and Buffer Protocols!","twitter_image":"https://hackersandslackers.com/content/images/2019/02/131.jpg","twitter_title":"Lynx Roundup, February 18th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"\n\nhttps://arxiv.org/pdf/1902.01046.pdf\n\nhttps://www.internalpointers.com/post/modules-and-packages-create-python-project\n\nhttps://medium.freecodecamp.org/whats-in-a-python-s-name-506262fe61e8\n\nhttps://github.com/csvoss/onelinerizer\n\nhttps://www.labnol.org/internet/useful-tools-for-programmers/29227/\n\nhttps://lispcast.com/tension-between-data-and-entity/\n\nhttps://julien.danjou.info/high-performance-in-python-with-zero-copy-and-the-buffer-protocol/","html":"<p></p><p><a href=\"https://arxiv.org/pdf/1902.01046.pdf\">https://arxiv.org/pdf/1902.01046.pdf</a></p><p><a href=\"https://www.internalpointers.com/post/modules-and-packages-create-python-project\">https://www.internalpointers.com/post/modules-and-packages-create-python-project</a></p><p><a href=\"https://medium.freecodecamp.org/whats-in-a-python-s-name-506262fe61e8\">https://medium.freecodecamp.org/whats-in-a-python-s-name-506262fe61e8</a></p><p><a href=\"https://github.com/csvoss/onelinerizer\">https://github.com/csvoss/onelinerizer</a></p><p><a href=\"https://www.labnol.org/internet/useful-tools-for-programmers/29227/\">https://www.labnol.org/internet/useful-tools-for-programmers/29227/</a></p><p><a href=\"https://lispcast.com/tension-between-data-and-entity/\">https://lispcast.com/tension-between-data-and-entity/</a></p><p><a href=\"https://julien.danjou.info/high-performance-in-python-with-zero-copy-and-the-buffer-protocol/\">https://julien.danjou.info/high-performance-in-python-with-zero-copy-and-the-buffer-protocol/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-february-18th/","uuid":"42cad6f9-3160-46aa-8d29-a4fa947212fc","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c66365ec2209e663b5e5a5e"}},{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}}]}},"pageContext":{"pageNumber":4,"humanPageNumber":5,"skip":48,"limit":12,"numberOfPages":33,"previousPagePath":"/page/4","nextPagePath":"/page/6"}}