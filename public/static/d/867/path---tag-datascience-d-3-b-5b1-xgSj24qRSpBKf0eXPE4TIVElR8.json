{"data":{"ghostTag":{"slug":"datascience","name":"Data Science","visibility":"public","feature_image":null,"description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c79b0070fa2b110f256e320","title":"Running Jupyter Notebooks on a Ubuntu Server","slug":"running-jupyter-notebooks-on-a-ubuntu-server","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","custom_excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","created_at_pretty":"01 March, 2019","published_at_pretty":"02 March, 2019","updated_at_pretty":"24 March, 2019","created_at":"2019-03-01T17:19:51.000-05:00","published_at":"2019-03-01T21:15:40.000-05:00","updated_at":"2019-03-23T22:30:42.000-04:00","meta_title":"Running Jupyter Notebooks on a Ubuntu Server | Hackers and Slackers","meta_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","og_title":"Running Jupyter Notebooks on a Ubuntu Server","twitter_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","twitter_title":"Running Jupyter Notebooks on a Ubuntu Server","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"It dawned on me the other day that for a publication which regularly uses and\ntalks about Jupyter notebooks [https://jupyter.org/], we’ve never actually taken\nthe time to explain what they are or how to start using them. No matter where\nyou may have been in your career, first exposure to Jupyter and the IPython\n[https://ipython.org/]  shell is often a confusingly magical experience. Writing\nprograms line-by-line and receiving feedback in real-time feels more like\npainting oil on canvas and programming. I suppose we can finally chalk up a win\nfor dynamically typed languages.\n\nThere are a couple of barriers for practical devs to overcome before using\nJupyter, the most obvious being hardware costs. If you’re utilizing a full \nAnaconda  installation, chances are you’re not the type of person to mess\naround. Real machine learning algorithms take real resources, and real resources\ntake real money. A few vendors have popped up here are offering managed\ncloud-hosted notebooks for this reason. For those of us who bothered to do the\nmath, it turns out most of these services are more expensive than spinning up a\ndedicated VPS.\n\nData scientists with impressive machines have no problem running notebooks\nlocally for most use cases. While that’s fine and good for scientists, this\nsetup is problematic for those of us with commitments to Python outside of\nnotebooks. Upon installation, Anaconda barges into your system’s ~/.bash_profile\n, shouts “I am the captain now,”  and crowns itself as your system’s default\nPython path. Conda and Pip have some trouble getting along, so for those of us\nwho build Python applications and use notebooks, it's best to keep these things\nisolated.\n\nSetting Up a VPS\nWe're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted\nfor DigitalOcean  in my case, both for simplicity and the fact that I'm\nincredibly broke. Depending on how broke you may or may not be, this is where\nyou'll have to make a judgment call for your system resources:\n\nMy kind sir, I would like to order the most exquisite almost-cheapest Droplet on\nthe menuSSH into that bad boy. You know what to do next:\n\n$ sudo apt update\n$ sudo apt upgrade\n\n\nWith that out of the way, next we'll grab the latest version of Python:\n\n$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n\n\nFinally, we'll open port 8888 for good measure, since this is the port Jupyter\nruns on:\n\n$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw status\n\n\nTo                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n\n\nCreate a New User\nAs always, we should create a Linux user besides root to do just about anything:\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n\n\nThen, add them to the sudoers  group:\n\n$ usermod -aG sudo myuser\n\n\nLog in as the user:\n\n$ su - myuser\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\n\nInstall The Latest Anaconda Distribution\nAnaconda comes with all the fantastic Data Science Python packages we'll need\nfor our notebook. To find the latest distribution, check here: \nhttps://www.anaconda.com/download/. We'll install this to a /tmp  folder:\n\ncd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n\n\nOnce downloaded, begin the installation:\n\n$ sh Anaconda3-2018.12-Linux-x86_64.sh\n\n\nComplete the resulting prompts:\n\nWelcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n>>>\n\n\nGet ready for the wall of text....\n\n===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n\n\nThis kicks off a rather lengthy install process. Afterward, you'll be prompted\nto add Conda to your startup script. Say yes:\n\ninstallation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n\n\nThe final part of the installation will ask if you'd like to install VS Code.\nDecline this offer because Microsoft sucks.\n\nFinally, reload your /.bashrc file to get apply Conda's changes:\n\n$ source ~/.bashrc\n\n\nSetting Up Conda Environments\nConda installations can be isolated to separate environments similarly to the\nway Pipenv might handle this. Create and activate a Conda env:\n\n$ conda create --name myenv python=3\n$ source activate myenv\n\n\nCongrats, you're now in an active Conda environment!\n\nStarting Up Jupyter\nMake sure you're in a directory you'd like to be running Jupyter in. Entering \njupyter notebook  in this directory should result in the following:\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n\n\nThis next part is tricky. To run our notebook, we need to reconnect to our VPS\nvia an SSH tunnel. Close the terminal and reconnect to your server with the\nfollowing format:\n\nssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nIndeed, localhost  is intended to stay the same, but your_server_ip  is to be\nreplaced with the address of your server.\n\nWith that done, let's try this one more time. Remember to reactivate your Conda\nenvironment first!\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n\n\nThis time around, the links which appear in the terminal should work!\n\nWE DID ITBONUS ROUND: Theme Your Notebooks\nIf ugly interfaces bother you as much as they bother me, I highly recommend\ntaking a look at the jupyter-themes package on Github\n[https://github.com/dunovank/jupyter-themes]. This package allows you to\ncustomize the look and feel of your notebook, either as simple as activating a\nstyle, or as complex as setting your margin width. I highly recommend checking\nout the available themes to spice up your notebook!","html":"<p>It dawned on me the other day that for a publication which regularly uses and talks about <a href=\"https://jupyter.org/\">Jupyter notebooks</a>, we’ve never actually taken the time to explain what they are or how to start using them. No matter where you may have been in your career, first exposure to Jupyter and the <a href=\"https://ipython.org/\">IPython</a> shell is often a confusingly magical experience. Writing programs line-by-line and receiving feedback in real-time feels more like painting oil on canvas and programming. I suppose we can finally chalk up a win for dynamically typed languages.</p><p>There are a couple of barriers for practical devs to overcome before using Jupyter, the most obvious being hardware costs. If you’re utilizing a full <strong>Anaconda</strong> installation, chances are you’re not the type of person to mess around. Real machine learning algorithms take real resources, and real resources take real money. A few vendors have popped up here are offering managed cloud-hosted notebooks for this reason. For those of us who bothered to do the math, it turns out most of these services are more expensive than spinning up a dedicated VPS.</p><p>Data scientists with impressive machines have no problem running notebooks locally for most use cases. While that’s fine and good for scientists, this setup is problematic for those of us with commitments to Python outside of notebooks. Upon installation, Anaconda barges into your system’s <code>~/.bash_profile</code>, shouts <strong><em>“I am the captain now,”</em></strong> and crowns itself as your system’s default Python path. Conda and Pip have some trouble getting along, so for those of us who build Python applications and use notebooks, it's best to keep these things isolated.</p><h2 id=\"setting-up-a-vps\">Setting Up a VPS</h2><p>We're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted for <strong>DigitalOcean</strong> in my case, both for simplicity and the fact that I'm incredibly broke. Depending on how broke you may or may not be, this is where you'll have to make a judgment call for your system resources:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/digitaloceanvps.png\" class=\"kg-image\"><figcaption>My kind sir, I would like to order the most exquisite almost-cheapest Droplet on the menu</figcaption></figure><!--kg-card-end: image--><p>SSH into that bad boy. You know what to do next:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt update\n$ sudo apt upgrade\n</code></pre>\n<!--kg-card-end: markdown--><p>With that out of the way, next we'll grab the latest version of Python:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we'll open port 8888 for good measure, since this is the port Jupyter runs on:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw status\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">To                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-new-user\">Create a New User</h3><p>As always, we should create a Linux user besides root to do just about anything:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Adding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n</code></pre>\n<!--kg-card-end: markdown--><p>Then, add them to the <strong>sudoers</strong> group:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ usermod -aG sudo myuser\n</code></pre>\n<!--kg-card-end: markdown--><p>Log in as the user:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ su - myuser\nTo run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.\nSee &quot;man sudo_root&quot; for details.\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"install-the-latest-anaconda-distribution\">Install The Latest Anaconda Distribution</h3><p>Anaconda comes with all the fantastic Data Science Python packages we'll need for our notebook. To find the latest distribution, check here: <a href=\"https://www.anaconda.com/download/\">https://www.anaconda.com/download/</a>. We'll install this to a <code>/tmp</code> folder:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">cd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Once downloaded, begin the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sh Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Complete the resulting prompts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Welcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n&gt;&gt;&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Get ready for the wall of text....</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>This kicks off a rather lengthy install process. Afterward, you'll be prompted to add Conda to your startup script. Say <strong>yes</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">installation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>The final part of the installation will ask if you'd like to install VS Code. Decline this offer because Microsoft sucks.</p><p>Finally, reload your <strong>/.bashrc </strong>file to get apply Conda's changes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ source ~/.bashrc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"setting-up-conda-environments\">Setting Up Conda Environments</h3><p>Conda installations can be isolated to separate environments similarly to the way Pipenv might handle this. Create and activate a Conda env:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda create --name myenv python=3\n$ source activate myenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Congrats, you're now in an active Conda environment!</p><h3 id=\"starting-up-jupyter\">Starting Up Jupyter</h3><p>Make sure you're in a directory you'd like to be running Jupyter in. Entering <code>jupyter notebook</code> in this directory should result in the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n</code></pre>\n<!--kg-card-end: markdown--><p>This next part is tricky. To run our notebook, we need to reconnect to our VPS via an SSH tunnel. Close the terminal and reconnect to your server with the following format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, <code>localhost</code> is intended to stay the same, but <code>your_server_ip</code> is to be replaced with the address of your server.</p><p>With that done, let's try this one more time. Remember to reactivate your Conda environment first!</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n</code></pre>\n<!--kg-card-end: markdown--><p>This time around, the links which appear in the terminal should work!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-01-at-7.01.42-PM.png\" class=\"kg-image\"><figcaption>WE DID IT</figcaption></figure><!--kg-card-end: image--><h2 id=\"bonus-round-theme-your-notebooks\">BONUS ROUND: Theme Your Notebooks</h2><p>If ugly interfaces bother you as much as they bother me, I highly recommend taking a look at the <a href=\"https://github.com/dunovank/jupyter-themes\">jupyter-themes package on Github</a>. This package allows you to customize the look and feel of your notebook, either as simple as activating a style, or as complex as setting your margin width. I highly recommend checking out the available themes to spice up your notebook!</p><!--kg-card-begin: gallery--><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/gruvbox-dark-python.png\" width=\"1013\" height=\"903\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/grade3_table.png\" width=\"1293\" height=\"809\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/jtplotDark_reach.png\" width=\"8400\" height=\"3600\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/oceans16_code_headers.png\" width=\"1293\" height=\"808\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/onedork_code_headers.png\" width=\"1293\" height=\"808\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/solarized-dark_iruby.png\" width=\"951\" height=\"498\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/chesterish_code_headers.png\" width=\"1293\" height=\"808\"></div></div></div></figure><!--kg-card-end: gallery-->","url":"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/","uuid":"0cfc9046-2e28-46a2-9f95-8851a9aea770","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c79b0070fa2b110f256e320"}},{"node":{"id":"Ghost__Post__5c17ddd4418434084a873d2a","title":"Drawing Mapbox Route Objects via the Directions API","slug":"mapbox-draw-route-objects","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","excerpt":"Using the Mapbox Directions API to visually draw routes.","custom_excerpt":"Using the Mapbox Directions API to visually draw routes.","created_at_pretty":"17 December, 2018","published_at_pretty":"28 February, 2019","updated_at_pretty":"03 March, 2019","created_at":"2018-12-17T12:33:08.000-05:00","published_at":"2019-02-28T09:15:52.000-05:00","updated_at":"2019-03-03T16:21:52.000-05:00","meta_title":"Draw Route Objects with Mapbox Directions API | Hackers and Slackers","meta_description":"Using the Mapbox Directions API to visually draw routes.","og_description":"Using the Mapbox Directions API to visually draw routes.","og_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","og_title":"Drawing Route Objects with Mapbox Directions API","twitter_description":"Using the Mapbox Directions API to visually draw routes.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","twitter_title":"Drawing Route Objects with Mapbox Directions API","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"If you've been here before, you probably already know our affinity for Mapbox \nand the visualization tools it provides data scientists and analysts. In the\npast, we've covered encoding location data from raw addresses\n[https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/], as well\nas an exploration of Mapbox Studio\n[https://hackersandslackers.com/map-data-visualization-with-mapbox/]  for those\ngetting acquainted with the tool. Today we're going a step further: drawing\ndirections on a map.\n\nIt sounds simple enough: we already know how to geocode addresses, so all we\nneed to do is literally go from point A to point B. That said, things always\ntend to get tricky, and if you've never worked with GeoJSON\n[http://geojson.org/]  before, you're in for a treat.\n\nLoad Up Some Data\nI'm going to assume you have a DataFrame ready containing these columns:\n\n * origin_longitude\n * origin_latitude\n * destination_longitude\n * destination_latitude\n * Name/description of this route \n\nIf you want to play along, there are plenty of free datasets out there to play\nwith - I sourced some information from BigQuery while I was testing things out.\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\nSo far so good- all we've done is load our data, and save our Mapbox token from\nan environment variable.\n\nMapbox Directions Endpoint\nNext, we're going to use Mapbox's Directions API\n[https://docs.mapbox.com/api/navigation/#directions]  to return a route for us.\nThe anatomy of a GET call to receive directions looks like this:\n\nhttps://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n\n\n * method_of_transportation refers to one of the three methods that Mapbox\n   offers for creating routes: driving-traffic, driving, walking, and cycling.\n   Note that there is currently no way to draw route objects which follow public\n   transit: this is perhaps Mapbox's biggest downfall at the moment.\n   Nevertheless, if this is something you need, data can be imported from Google\n   maps to be used with Mapbox.\n * access_token  can be either your public token (visible upon login at\n   mapbox.com) or a generated secret token.\n * geometries  accepts the method by which to draw the object. This can be \n   GeoJSON,  polyline, or polyline6. Let's stick with GeoJSON.\n\nConstructing API Requests\nLet's construct a request per row in our DataFrame. By using Pandas' apply, we\nfire a function per row to do just that:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n\n\nHere's where things get a little tricky. You see, GeoJSON abides by a strict\nformat. It looks something like this:\n\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"coordinates\": [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    \"type\": \"LineString\"\n  },\n  \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n  }],\n  \"weight_name\": \"duration\",\n  \"weight\": 3873.3,\n  \"duration\": 3873.3,\n  \"distance\": 9660.2,\n  \"properties\": {\n    \"name\": \"Empire State\"\n  }\n}\n\n\nFor the sake of being difficult, the Mapbox Directions API doesn't return\nresponses in exactly this format. Instead, their response looks like this:\n\n{\n  \"routes\": [{\n    \"geometry\": {\n      \"coordinates\": [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      \"type\": \"LineString\"\n    },\n    \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n    }],\n    \"weight_name\": \"duration\",\n    \"weight\": 3873.3,\n    \"duration\": 3873.3,\n    \"distance\": 9660.2\n  }],\n  \"waypoints\": [{\n      \"distance\": 34.00158252003884,\n      \"name\": \"West 33rd Street\",\n      \"location\": [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      \"distance\": 6.627227256764976,\n      \"name\": \"\",\n      \"location\": [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  \"code\": \"Ok\",\n  \"uuid\": \"cjsomodyl025642o6f1jsddx6\"\n}\n\n\nThe format isn't too  far off, but it's different enough to not work. \n\nFormatting GeoJSON Correctly\nWe need to write a function to take the response Mapbox has given us and\ntransform it into a usable GeoJSON format:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    \"\"\"Properly formats GeoJson for Mapbox visualization.\"\"\"\n    routes_dict = {\n        \"type\": \"Feature\",\n        \"geometry\": {\n            \"type\": \"LineString\"\n        },\n        \"weight_name\": \"duration\",\n        \"weight\": 718.9,\n        \"duration\": 0,\n        \"distance\": 0,\n        \"properties\": {\n            \"name\": \"\"\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n\n\nIt's not pretty, but it's reliable: we explicitly create the JSON structure we\nneed with routes_dict, and modify it with the API responses coming back from\nMapbox. Of course, we're still doing this one at a time, for every row in our\nDataFrame.\n\nYou'll notice I save each JSON file locally for now. In the future, we'll write\na script to automate the process of uploading our GeoJSON objects and adding\nthem to the proper Tilesets, but right now I just want to see that our work paid\noff!\n\nBy using Mapbox studio, we can see the result of our first route:\n\nA \"Driving\" Route from the Empire State Building to Battery Park.Aha! Would you\nlook at that- Mapbox knew to take the FDR drive. That's some promising stuff.\n\nDrawing Routes En Masse\nNaturally, this is only the tip of the iceberg: of the DataFrame of information\nwe loaded up, we've so far only viewed a single result. If anything in data is\nworth doing, it must be done thousands of times systematically without fail.\nLuckily, Mapbox provides us with the tools to do this: from lending us an S3\nbucket, to modifying datasets via the API, there's nothing to fear.\n\nTune in next time when do more... stuff!","html":"<p>If you've been here before, you probably already know our affinity for <strong>Mapbox</strong> and the visualization tools it provides data scientists and analysts. In the past, we've covered <a href=\"https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/\">encoding location data from raw addresses</a>, as well as an <a href=\"https://hackersandslackers.com/map-data-visualization-with-mapbox/\">exploration of Mapbox Studio</a> for those getting acquainted with the tool. Today we're going a step further: drawing directions on a map.</p><p>It sounds simple enough: we already know how to geocode addresses, so all we need to do is literally go from point A to point B. That said, things always tend to get tricky, and if you've never worked with <a href=\"http://geojson.org/\">GeoJSON</a> before, you're in for a treat.</p><h2 id=\"load-up-some-data\">Load Up Some Data</h2><p>I'm going to assume you have a DataFrame ready containing these columns:</p><ul><li>origin_longitude</li><li>origin_latitude</li><li>destination_longitude</li><li>destination_latitude</li><li>Name/description of this route </li></ul><p>If you want to play along, there are plenty of free datasets out there to play with - I sourced some information from <strong>BigQuery </strong>while I was testing things out.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n</code></pre>\n<!--kg-card-end: markdown--><p>So far so good- all we've done is load our data, and save our Mapbox token from an environment variable.</p><h2 id=\"mapbox-directions-endpoint\">Mapbox Directions Endpoint</h2><p>Next, we're going to use Mapbox's <a href=\"https://docs.mapbox.com/api/navigation/#directions\">Directions API</a> to return a route for us. The anatomy of a GET call to receive directions looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">https://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><strong>method_of_transportation </strong>refers to one of the three methods that Mapbox offers for creating routes: <em>driving-traffic</em>, <em>driving</em>, <em>walking</em>, and <em>cycling</em>. Note that there is currently no way to draw route objects which follow public transit: this is perhaps Mapbox's biggest downfall at the moment. Nevertheless, if this is something you need, data can be imported from Google maps to be used with Mapbox.</li><li><strong>access_token</strong> can be either your public token (visible upon login at mapbox.com) or a generated secret token.</li><li><strong>geometries</strong> accepts the method by which to draw the object. This can be <em>GeoJSON,</em> <em>polyline, </em>or <em>polyline6. </em>Let's stick with GeoJSON.</li></ul><h2 id=\"constructing-api-requests\">Constructing API Requests</h2><p>Let's construct a request per row in our DataFrame. By using Pandas' apply, we fire a function per row to do just that:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's where things get a little tricky. You see, GeoJSON abides by a strict format. It looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;type&quot;: &quot;Feature&quot;,\n  &quot;geometry&quot;: {\n    &quot;coordinates&quot;: [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    &quot;type&quot;: &quot;LineString&quot;\n  },\n  &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n  }],\n  &quot;weight_name&quot;: &quot;duration&quot;,\n  &quot;weight&quot;: 3873.3,\n  &quot;duration&quot;: 3873.3,\n  &quot;distance&quot;: 9660.2,\n  &quot;properties&quot;: {\n    &quot;name&quot;: &quot;Empire State&quot;\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>For the sake of being difficult, the Mapbox Directions API doesn't return responses in exactly this format. Instead, their response looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;routes&quot;: [{\n    &quot;geometry&quot;: {\n      &quot;coordinates&quot;: [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      &quot;type&quot;: &quot;LineString&quot;\n    },\n    &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n    }],\n    &quot;weight_name&quot;: &quot;duration&quot;,\n    &quot;weight&quot;: 3873.3,\n    &quot;duration&quot;: 3873.3,\n    &quot;distance&quot;: 9660.2\n  }],\n  &quot;waypoints&quot;: [{\n      &quot;distance&quot;: 34.00158252003884,\n      &quot;name&quot;: &quot;West 33rd Street&quot;,\n      &quot;location&quot;: [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      &quot;distance&quot;: 6.627227256764976,\n      &quot;name&quot;: &quot;&quot;,\n      &quot;location&quot;: [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  &quot;code&quot;: &quot;Ok&quot;,\n  &quot;uuid&quot;: &quot;cjsomodyl025642o6f1jsddx6&quot;\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>The format isn't <em>too</em> far off, but it's different enough to not work. </p><h2 id=\"formatting-geojson-correctly\">Formatting GeoJSON Correctly</h2><p>We need to write a function to take the response Mapbox has given us and transform it into a usable GeoJSON format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    &quot;&quot;&quot;Properly formats GeoJson for Mapbox visualization.&quot;&quot;&quot;\n    routes_dict = {\n        &quot;type&quot;: &quot;Feature&quot;,\n        &quot;geometry&quot;: {\n            &quot;type&quot;: &quot;LineString&quot;\n        },\n        &quot;weight_name&quot;: &quot;duration&quot;,\n        &quot;weight&quot;: 718.9,\n        &quot;duration&quot;: 0,\n        &quot;distance&quot;: 0,\n        &quot;properties&quot;: {\n            &quot;name&quot;: &quot;&quot;\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>It's not pretty, but it's reliable: we explicitly create the JSON structure we need with <code>routes_dict</code>, and modify it with the API responses coming back from Mapbox. Of course, we're still doing this one at a time, for every row in our DataFrame.</p><p>You'll notice I save each JSON file locally for now. In the future, we'll write a script to automate the process of uploading our GeoJSON objects and adding them to the proper Tilesets, but right now I just want to see that our work paid off!</p><p>By using Mapbox studio, we can see the result of our first route:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2019-02-28-at-8.05.21-AM.png\" class=\"kg-image\"><figcaption>A \"Driving\" Route from the Empire State Building to Battery Park.</figcaption></figure><!--kg-card-end: image--><p>Aha! Would you look at that- Mapbox knew to take the FDR drive. That's some promising stuff.</p><h3 id=\"drawing-routes-en-masse\">Drawing Routes En Masse</h3><p>Naturally, this is only the tip of the iceberg: of the DataFrame of information we loaded up, we've so far only viewed a single result. If anything in data is worth doing, it must be done thousands of times systematically without fail. Luckily, Mapbox provides us with the tools to do this: from lending us an S3 bucket, to modifying datasets via the API, there's nothing to fear.</p><p>Tune in next time when do more... stuff!</p>","url":"https://hackersandslackers.com/mapbox-draw-route-objects/","uuid":"ef0a4639-8818-475b-9a25-6a20b13c1ecf","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c17ddd4418434084a873d2a"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apache’s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673709","title":"Using Random Forests for Feature Selection with Categorical Features","slug":"random-forests-for-feature-selection","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","custom_excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","created_at_pretty":"24 September, 2018","published_at_pretty":"24 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-23T20:23:32.000-04:00","published_at":"2018-09-24T07:30:00.000-04:00","updated_at":"2019-02-19T03:48:04.000-05:00","meta_title":"Using Random Forests for Feature Selection | Hackers and Slackers","meta_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","og_title":"Using Random Forests for Feature Selection","twitter_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","twitter_title":"Using Random Forests for Feature Selection","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Notebook here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb]\n.  Helper functions here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py]\n.\n\nOne of the best features of Random Forests is that it has built-in Feature\nSelection.  Explicability is one of the things we often lose when we go from\ntraditional statistics to Machine Learning, but Random Forests lets us actually\nget some insight into our dataset instead of just having to treat our model as a\nblack box.\n\nOne problem, though - it doesn't work that well for categorical features.  Since\nyou'll generally have to One-Hot Encode a categorical feature (for instance,\nturn something with 7 categories into 7 variables that are a \"True/False\"),\nyou'll wind up with a bunch of small features.  This gets tough to read,\nespecially if you're dealing with a lot of categories.  It also makes that\nfeature look less important than it is - rather than appearing near the top,\nyou'll maybe have 17 weak-seeming features near the bottom - which gets worse if\nyou're filtering it so that you only see features above a certain threshold.\n\nSoo, here's some helper functions for adding up their importance and displaying\nthem as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my\nmy own One-Hot function, rather than using Scikit's builtin one.\n\nFirst, let's grab a dataset.  I'm using this\n[https://www.kaggle.com/c/avazu-ctr-prediction]  Kaggle dataset because it has a\ngood number of categorical predictors.  I'm also only using the first 500 rows\nbecause the whole dataset is like ~ 1 GB.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)\n\nLet's just use the Categorical variables as our predictors because that's what\nwe're focusing on, but in actual usage you don't have to make them the same.\n\npredVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)\n\nLet's use log_loss  as our metric, because I saw this\n[https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512] \n blog post that used it for this dataset.\n\nfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 \n\nVariable\n Importance\n 0\n device_model\n 0.843122\n 1\n site_category\n 0.083392\n 2\n app_category\n 0.037216\n 3\n device_type\n 0.025057\n 4\n device_conn_type\n 0.011213","html":"<p><em>Notebook <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb\">here</a>.  Helper functions <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py\">here</a>.</em></p><p>One of the best features of Random Forests is that it has built-in Feature Selection.  Explicability is one of the things we often lose when we go from traditional statistics to Machine Learning, but Random Forests lets us actually get some insight into our dataset instead of just having to treat our model as a black box.</p><p>One problem, though - it doesn't work that well for categorical features.  Since you'll generally have to One-Hot Encode a categorical feature (for instance, turn something with 7 categories into 7 variables that are a \"True/False\"), you'll wind up with a bunch of small features.  This gets tough to read, especially if you're dealing with a lot of categories.  It also makes that feature look less important than it is - rather than appearing near the top, you'll maybe have 17 weak-seeming features near the bottom - which gets worse if you're filtering it so that you only see features above a certain threshold.</p><p>Soo, here's some helper functions for adding up their importance and displaying them as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my my own One-Hot function, rather than using Scikit's builtin one.</p><p>First, let's grab a dataset.  I'm using <a href=\"https://www.kaggle.com/c/avazu-ctr-prediction\">this</a> Kaggle dataset because it has a good number of categorical predictors.  I'm also only using the first 500 rows because the whole dataset is like ~ 1 GB.</p><pre><code>import pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)</code></pre><p>Let's just use the Categorical variables as our predictors because that's what we're focusing on, but in actual usage you don't have to make them the same.</p><pre><code>predVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)</code></pre><p>Let's use <code>log_loss</code> as our metric, because I saw <a href=\"https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\">this</a> blog post that used it for this dataset.</p><pre><code>from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 </code></pre><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>device_model</td>\n      <td>0.843122</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>site_category</td>\n      <td>0.083392</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>app_category</td>\n      <td>0.037216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>device_type</td>\n      <td>0.025057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>device_conn_type</td>\n      <td>0.011213</td>\n    </tr>\n  </tbody>\n</table>","url":"https://hackersandslackers.com/random-forests-for-feature-selection/","uuid":"26ebccb3-ab41-44cf-8d57-bf995100b088","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ba82e84a1cf0b13cf2e9886"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673700","title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","slug":"random-forests-hyperparameters-min_samples_leaf","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","custom_excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","created_at_pretty":"17 September, 2018","published_at_pretty":"17 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-16T21:33:48.000-04:00","published_at":"2018-09-17T07:30:00.000-04:00","updated_at":"2019-02-19T03:44:33.000-05:00","meta_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf | Hackers and Slackers","meta_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","og_description":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","og_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","twitter_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","twitter_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Part 1 (n_estimators) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\nPart 2 (max_depth) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/]\nNotebook here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb]\n\n\n--------------------------------------------------------------------------------\n\nAnother parameter, another set of quirks!\n\nmin_samples_leaf  is sort of similar to max_depth.  It helps us avoid\noverfitting.  It's also non-obvious what you should use as your upper and lower\nlimits to search between.  Let's do what we did last week - build a forest with\nno parameters, see what it does, and use the upper and lower limits!\n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\n\nLet's use the handy function from here\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html]  to\ncrawl the number of samples in a tree's leaf nodes: \n\ndef leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n\n\nLast week we made a function to grab them for a whole forest - since this is the\nsecond time we're doing this, and we may do it again, let's make a modular\nlittle function that takes a crawler function as an argument!\n\ndef getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {\"min\": params.min(),\n           \"max\": params.max()}\n\n\nLet's see it in action!\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#> {'max': 199, 'min': 1}\n\n\nAlmost ready to start optimizing!  Since part of what we get out of optimizing \nmin_samples_leaf  is regularization (and because it's just good practice!),\nlet's make a metric with some cross-validation.  Luckily, Scikit  has a builtin \ncross_val_score  function.  We'll just need to do a teensy bit of tweaking to\nmake it use the area under a precision_recall_curve.\n\nfrom sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n\n\nAwesome, now we have a metric that can be fed into our binary search.\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.402102\n 199\n 0.506455\n 1.416349\n 100\n 0.506455\n 1.401090\n 51\n 0.506455\n 1.394548\n 26\n 0.975894\n 1.396503\n 14\n 0.982954\n 1.398522\n 7\n 0.979888\n 1.398929\n 10\n 0.984789\n 1.404815\n 12\n 0.986302\n 1.391171\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.473848\n 0.082938\n 199\n 0.002084\n 1.039718\n 0.000000\n 100\n 0.002084\n 0.433676\n 0.000111\n 51\n 0.002084\n 0.173824\n 0.000396\n 26\n 0.980393\n 0.251484\n 0.154448\n 14\n 0.995105\n 0.331692\n 0.118839\n 7\n 0.988716\n 0.347858\n 0.112585\n 10\n 0.998930\n 0.581632\n 0.067998\n 12\n 1.002084\n 0.039718\n 1.000000\n Looks like the action's between 1 and 51.  More than that, and the score goes\nwhile simultaneously increasing the runtime - the opposite of what we want!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.389387\n 51\n 0.506455\n 1.403807\n 26\n 0.975894\n 1.404517\n 14\n 0.982954\n 1.385420\n 7\n 0.979888\n 1.398840\n 10\n 0.984789\n 1.393863\n 12\n 0.986302\n 1.411774\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n Big drop-off after 26, it seems!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.407957\n 26\n 0.975894\n 1.398042\n 14\n 0.982954\n 1.396782\n 7\n 0.979888\n 1.396096\n 10\n 0.984789\n 1.402322\n 12\n 0.986302\n 1.401080\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.650270\n 1.084306\n 0.040144\n 26\n 0.096077\n 0.248406\n 0.000000\n 14\n 0.774346\n 0.142157\n 0.954016\n 7\n 0.479788\n 0.084306\n 1.000000\n 10\n 0.950677\n 0.609184\n 0.221294\n 12\n 1.096077\n 0.504512\n 0.336668\n One more with 14 as our upper limit!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.401341\n 14\n 0.982954\n 1.400361\n 7\n 0.979888\n 1.402408\n 4\n 0.981121\n 1.401396\n 3\n 0.983580\n 1.401332\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n 3 it is!I suppose when it gets this small we could use a regular Grid Search,\nbut...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs \nGridSearchCV  and/or RandomizedSearchCV.  Who knows what the future holds?","html":"<p>Part 1 (n_estimators) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a><br>Part 2 (max_depth) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/\">here</a><br>Notebook <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb\">here</a></p><hr><p>Another parameter, another set of quirks!</p><p><code>min_samples_leaf</code> is sort of similar to <code>max_depth</code>.  It helps us avoid overfitting.  It's also non-obvious what you should use as your upper and lower limits to search between.  Let's do what we did last week - build a forest with no parameters, see what it does, and use the upper and lower limits!</p><pre><code class=\"language-python\">import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n</code></pre>\n<p>Let's use the handy function from <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">here</a> to crawl the number of samples in a tree's leaf nodes: </p><pre><code class=\"language-python\">def leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n</code></pre>\n<p>Last week we made a function to grab them for a whole forest - since this is the second time we're doing this, and we may do it again, let's make a modular little function that takes a crawler function as an argument!</p><pre><code class=\"language-python\">def getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {&quot;min&quot;: params.min(),\n           &quot;max&quot;: params.max()}\n</code></pre>\n<p>Let's see it in action!</p><pre><code class=\"language-python\">data = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#&gt; {'max': 199, 'min': 1}\n</code></pre>\n<p>Almost ready to start optimizing!  Since part of what we get out of optimizing <code>min_samples_leaf</code> is regularization (and because it's just good practice!), let's make a metric with some cross-validation.  Luckily, <strong>Scikit</strong> has a builtin <code>cross_val_score</code> function.  We'll just need to do a teensy bit of tweaking to make it use the area under a <code>precision_recall_curve</code>.</p><pre><code class=\"language-python\">from sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n</code></pre>\n<p>Awesome, now we have a metric that can be fed into our binary search.</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.402102</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.506455</td>\n      <td>1.416349</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.506455</td>\n      <td>1.401090</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.394548</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.396503</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.398522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398929</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.404815</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.391171</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.473848</td>\n      <td>0.082938</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.002084</td>\n      <td>1.039718</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.002084</td>\n      <td>0.433676</td>\n      <td>0.000111</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.173824</td>\n      <td>0.000396</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.251484</td>\n      <td>0.154448</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.331692</td>\n      <td>0.118839</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.347858</td>\n      <td>0.112585</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.581632</td>\n      <td>0.067998</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>0.039718</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf1.png\" class=\"kg-image\"></figure><p>Looks like the action's between 1 and 51.  More than that, and the score goes while simultaneously increasing the runtime - the opposite of what we want!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.389387</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.403807</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.404517</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.385420</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398840</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.393863</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.411774</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf2.png\" class=\"kg-image\"></figure><p>Big drop-off after 26, it seems!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.407957</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.398042</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.396782</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.396096</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.402322</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.401080</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.650270</td>\n      <td>1.084306</td>\n      <td>0.040144</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.096077</td>\n      <td>0.248406</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.774346</td>\n      <td>0.142157</td>\n      <td>0.954016</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.479788</td>\n      <td>0.084306</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.950677</td>\n      <td>0.609184</td>\n      <td>0.221294</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.096077</td>\n      <td>0.504512</td>\n      <td>0.336668</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf3.png\" class=\"kg-image\"></figure><p>One more with 14 as our upper limit!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.401341</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.400361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.402408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.981121</td>\n      <td>1.401396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.983580</td>\n      <td>1.401332</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf4.png\" class=\"kg-image\"><figcaption>3 it is!</figcaption></figure><p>I suppose when it gets this small we could use a regular Grid Search, but...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs <code>GridSearchCV</code> and/or <code>RandomizedSearchCV</code>.  Who knows what the future holds?</p>","url":"https://hackersandslackers.com/random-forests-hyperparameters-min_samples_leaf/","uuid":"766a3eb8-aacc-47c6-91a9-744b84613626","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b9f047cab64c97c60f7be90"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736fd","title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","slug":"tuning-random-forests-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","custom_excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","created_at_pretty":"09 September, 2018","published_at_pretty":"10 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-09T19:14:32.000-04:00","published_at":"2018-09-10T07:30:00.000-04:00","updated_at":"2019-02-19T03:46:39.000-05:00","meta_title":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","og_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","twitter_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","twitter_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Continued from here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\n\nNotebook for this post is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb]\n\nBinary search code itself is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nmax_depth  is an interesting parameter.  While n_estimators  has a tradeoff\nbetween speed & score, max_depth  has the possibility of improving both.  By\nlimiting the depth of your trees, you can reduce overfitting.\n\nUnfortunately, deciding on upper & lower bounds is less than straightforward.\n It'll depend on your dataset.  Luckily, I found a post on StackOverflow that\nhad a link to a blog post that had a promising methodology\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html].\n\nFirst, we build a tree with default arguments and fit it to our data. \n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\nNow, let's see how deep the trees get when we don't impose any sort of max_depth\n. We'll use the code from that wonderful blog post to crawl our Random Forest,\nand get the height of every tree.\n\n#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#> 2\nnp.hstack(allDepths).max()\n#> 9\n\nWe'll be searching between 2 and 9!\n\nLet's bring back our old make a helper function to easily return scores.\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\nmax_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)\n\nmax_depth\n score\n time\n 2\n 0.987707\n 0.145360\n 9\n 0.987029\n 0.147563\n 6\n 0.986247\n 0.140514\n 4\n 0.968316\n 0.140164\n \nmax_depth\n score\n time\n scoreTimeRatio\n 2\n 1.051571\n 0.837377\n 0.175986\n 9\n 1.016649\n 1.135158\n 0.103478\n 6\n 0.976311\n 0.182516\n 1.000000\n 4\n 0.051571\n 0.135158\n 0.000000\n So, for our purposes, 9 will function as our baseline since that was the\nbiggest depth that it built with default arguments.\n\nLooks like a max_depth  of 2 has a slightly higher score than 9, and is slightly\nfaster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that\nis.","html":"<p>Continued from <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a></p><p>Notebook for this post is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb\">here</a></p><p>Binary search code itself is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p><code>max_depth</code> is an interesting parameter.  While <code>n_estimators</code> has a tradeoff between speed &amp; score, <code>max_depth</code> has the possibility of improving both.  By limiting the depth of your trees, you can reduce overfitting.</p><p>Unfortunately, deciding on upper &amp; lower bounds is less than straightforward.  It'll depend on your dataset.  Luckily, I found a post on StackOverflow that had a link to a blog post that had a promising <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">methodology</a>.  </p><p>First, we build a tree with default arguments and fit it to our data. </p><pre><code>import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)</code></pre><p>Now, let's see how deep the trees get when we don't impose any sort of <code>max_depth</code>. We'll use the code from that wonderful blog post to crawl our Random Forest, and get the height of every tree.</p><pre><code>#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#&gt; 2\nnp.hstack(allDepths).max()\n#&gt; 9</code></pre><p>We'll be searching between 2 and 9!  </p><p>Let's bring back our old make a helper function to easily return scores.</p><pre><code>def getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)</code></pre><pre><code>max_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/09/max_depth.png\" class=\"kg-image\"></figure><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>0.987707</td>\n      <td>0.145360</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.987029</td>\n      <td>0.147563</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.986247</td>\n      <td>0.140514</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.968316</td>\n      <td>0.140164</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>1.051571</td>\n      <td>0.837377</td>\n      <td>0.175986</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.016649</td>\n      <td>1.135158</td>\n      <td>0.103478</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.976311</td>\n      <td>0.182516</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.051571</td>\n      <td>0.135158</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>So, for our purposes, 9 will function as our baseline since that was the biggest depth that it built with default arguments.  </p><p>Looks like a <code>max_depth</code> of 2 has a slightly higher score than 9, and is slightly faster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that is.</p>","url":"https://hackersandslackers.com/tuning-random-forests-hyperparameters-with-binary-search/","uuid":"3c92aed0-61ed-4c1a-b7d5-cc47c709764b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b95a9581fc1fc7d92b5c51f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ec","title":"Tuning Machine Learning Hyperparameters with Binary Search","slug":"tuning-machine-learning-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","custom_excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","created_at_pretty":"30 August, 2018","published_at_pretty":"03 September, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-08-29T21:35:41.000-04:00","published_at":"2018-09-03T07:30:00.000-04:00","updated_at":"2019-02-13T22:50:35.000-05:00","meta_title":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"RandomizedSearchCV goes noticeably faster than a full GridSearchCV but it still takes a while - which can be rough.","og_description":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","og_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","og_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","twitter_description":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python","twitter_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","twitter_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Ah, hyperparameter tuning.  Time & compute-intensive.  Frequently containing\nweird non-linearities in how changing a parameter changes the score and/or the\ntime it takes to train the model.\n\nRandomizedSearchCV  goes noticeably faster than a full GridSearchCV  but it\nstill takes a while - which can be rough, because in my experience you do still\nneed to be iterative with it and experiment with different distributions.  Plus,\nthen you've got hyper-hyperparameters to tune - how many iterations SHOULD you\nrun it for, anyway?\n\nI've been experimenting with using the trusty old Binary Search to tune\nhyperparameters.  I'm finding it has two advantages.\n\n 1. It's blazing fast\n 2. The performance is competitive with a Randomized Search\n 3. It gives you a rough sketch of \"the lay of the land\".  An initial binary\n    search can then provide parameters for future searches, including with Grid\n    or Randomized Searches.\n\nCode is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nNotebook summary is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb]\n\nLet's see it in action!\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nWe'll be using a Random Forest classifier, because, as with all my code posts,\nit's what I've been using recently.\n\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n\nWe'll be using scikit-learn's breast cancer dataset, because I remembered that\nthese packages I'm posting about have built-in demo datasets that I should be\nusing for posts.\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"oob_score\": True}\n\n\nLet's set our random_state  for better reproducibility.\nWe'll set n_jobs=-1  because obviously we want to use all our cores, we are not\npatient people.\n\n\nWe'll have class_weight=\"balanced\"  because that'll compensate for the fact that\nthe breast cancer dataset (like most medical datasets) has unbalanced classes.\nWe'll use oob_score  because we like being lazy, part of the appeal of Random\nForests is the opportunity to be extra lazy (no need to normalize features!),\nand oob  lets us be even lazier  by giving some built-in cross-validation.\n\nNow let's define a function that'll take all this, and spit out a score.  I\nwrote the binary search function to take a function like this as an argument -\nscikit-learn is usually pretty consistent when it comes to the interface it\nprovides you, but sometimes different algorithms need to work a little\ndifferently.  For instance, since we'll be using Area Under \nprecision_recall_curve  as our metric (a good choice for classifiers with\nunbalanced classes!), it takes a teensy bit of extra fiddling to get it to play\nnicely with our oob_decision_function_.\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\n\nWe'll try to optimize the n_estimators  parameter first.  For two reasons:\n\n 1. Finding a good mix between speed and accuracy here will make it easier to\n    tune subsequent parameters.\n 2. It's the most straightforward to decide upper and lower bounds for.  Other\n    ones (like, say, max_depth) require a little work to figure the potential\n    range to search in.\n\nOkay!  So, let's put our lower limit as 32 and our upper limit as 128, because I\nread in a StackOverflow post that there's a paper that says to search within\nthat range.\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nPlotting score, time, and the ratio between them - we're not just optimizing for\nthe best score right now, we're looking for tipping points that give us good\ntradeoffs.  Scores and times are normalized for a more-meaningful ratio between\nthem.\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 32\n 1.073532\n 0.002459\n 1.000000\n 1\n 128\n 1.867858\n 1.002459\n 0.000000\n 2\n 80\n 2.052255\n 0.440060\n 0.006443\n 3\n 56\n 1.605447\n 0.075185\n 0.044843\n 4\n 68\n 1.910411\n 0.107187\n 0.036721\n 5\n 74\n 2.066440\n 0.377136\n 0.008320\n 6\n 77\n 2.066440\n 0.388378\n 0.007955\n 7\n 75\n 2.073532\n 0.457481\n 0.006141\n n_estimators\n score\n time\n 0\n 32\n 0.988663\n 0.180521\n 1\n 128\n 0.989403\n 0.587113\n 2\n 80\n 0.989575\n 0.358446\n 3\n 56\n 0.989159\n 0.210091\n 4\n 68\n 0.989443\n 0.223102\n 5\n 74\n 0.989588\n 0.332861\n 6\n 77\n 0.989588\n 0.337432\n 7\n 75\n 0.989595\n 0.365529\n Hrm, looks like the score starts getting somewhere interesting around 68, and\ntime starts shooting up at about 80.  Let's do another with those as our bounds!\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 68\n 6.390333\n 0.333407\n 0.135692\n 1\n 80\n 7.223667\n 1.064343\n 0.000000\n 2\n 74\n 7.307000\n 0.404471\n 0.123622\n 3\n 71\n 6.307000\n 0.064343\n 1.000000\n 4\n 72\n 6.390333\n 0.175190\n 0.325419\n n_estimators\n score\n time\n 0\n 68\n 0.989443\n 0.344220\n 1\n 80\n 0.989575\n 0.355580\n 2\n 74\n 0.989588\n 0.345324\n 3\n 71\n 0.989430\n 0.340038\n 4\n 72\n 0.989443\n 0.341761\n 71 looks like our winner!  Or close enough for our purposes while we then go\noptimize other things.  And we only had to train our model 13 times - as opposed\nto the 96 we would have with a brute-force grid search.\n\nHopefully this will become a series on using this to tune other RF\nhyperparameters - other ones have some interesting quirks that I'd like to\nexpound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!","html":"<p>Ah, hyperparameter tuning.  Time &amp; compute-intensive.  Frequently containing weird non-linearities in how changing a parameter changes the score and/or the time it takes to train the model.</p><p><code>RandomizedSearchCV</code> goes noticeably faster than a full <code>GridSearchCV</code> but it still takes a while - which can be rough, because in my experience you do still need to be iterative with it and experiment with different distributions.  Plus, then you've got hyper-hyperparameters to tune - how many iterations SHOULD you run it for, anyway?</p><p>I've been experimenting with using the trusty old Binary Search to tune hyperparameters.  I'm finding it has two advantages.</p><ol><li>It's blazing fast</li><li>The performance is competitive with a Randomized Search</li><li>It gives you a rough sketch of \"the lay of the land\".  An initial binary search can then provide parameters for future searches, including with Grid or Randomized Searches.</li></ol><p>Code is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p>Notebook summary is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb\">here</a></p><p>Let's see it in action!</p><pre><code class=\"language-python\">from sklearn.ensemble import RandomForestClassifier\n</code></pre>\n<p>We'll be using a Random Forest classifier, because, as with all my code posts, it's what I've been using recently.</p><pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n</code></pre>\n<p>We'll be using scikit-learn's breast cancer dataset, because I remembered that these packages I'm posting about have built-in demo datasets that I should be using for posts.</p><pre><code class=\"language-python\">rfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;oob_score&quot;: True}\n</code></pre>\n<p>Let's set our <code>random_state</code> for better reproducibility.<br>We'll set <code>n_jobs=-1</code> because obviously we want to use all our cores, we are not patient people.</p><p><br>We'll have <code>class_weight=\"balanced\"</code> because that'll compensate for the fact that the breast cancer dataset (like most medical datasets) has unbalanced classes.<br>We'll use <code>oob_score</code> because we like being lazy, part of the appeal of Random Forests is the opportunity to be extra lazy (no need to normalize features!), and <code>oob</code> lets us be <em>even lazier</em> by giving some built-in cross-validation.</p><p>Now let's define a function that'll take all this, and spit out a score.  I wrote the binary search function to take a function like this as an argument - scikit-learn is usually pretty consistent when it comes to the interface it provides you, but sometimes different algorithms need to work a little differently.  For instance, since we'll be using Area Under <code>precision_recall_curve</code> as our metric (a good choice for classifiers with unbalanced classes!), it takes a teensy bit of extra fiddling to get it to play nicely with our <code>oob_decision_function_</code>.</p><pre><code class=\"language-python\">from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n</code></pre>\n<p>We'll try to optimize the <code>n_estimators</code> parameter first.  For two reasons:</p><ol><li>Finding a good mix between speed and accuracy here will make it easier to tune subsequent parameters.</li><li>It's the most straightforward to decide upper and lower bounds for.  Other ones (like, say, <code>max_depth</code>) require a little work to figure the potential range to search in.</li></ol><p>Okay!  So, let's put our lower limit as 32 and our upper limit as 128, because I read in a StackOverflow post that there's a paper that says to search within that range.</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<p>Plotting score, time, and the ratio between them - we're not just optimizing for the best score right now, we're looking for tipping points that give us good tradeoffs.  Scores and times are normalized for a more-meaningful ratio between them.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--2-.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>1.073532</td>\n      <td>0.002459</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>1.867858</td>\n      <td>1.002459</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>2.052255</td>\n      <td>0.440060</td>\n      <td>0.006443</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1.605447</td>\n      <td>0.075185</td>\n      <td>0.044843</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>1.910411</td>\n      <td>0.107187</td>\n      <td>0.036721</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>2.066440</td>\n      <td>0.377136</td>\n      <td>0.008320</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>2.066440</td>\n      <td>0.388378</td>\n      <td>0.007955</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>2.073532</td>\n      <td>0.457481</td>\n      <td>0.006141</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>0.988663</td>\n      <td>0.180521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>0.989403</td>\n      <td>0.587113</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.358446</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>0.989159</td>\n      <td>0.210091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.223102</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.332861</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>0.989588</td>\n      <td>0.337432</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>0.989595</td>\n      <td>0.365529</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Hrm, looks like the score starts getting somewhere interesting around 68, and time starts shooting up at about 80.  Let's do another with those as our bounds!</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/max_depth.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>6.390333</td>\n      <td>0.333407</td>\n      <td>0.135692</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>7.223667</td>\n      <td>1.064343</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>7.307000</td>\n      <td>0.404471</td>\n      <td>0.123622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>6.307000</td>\n      <td>0.064343</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>6.390333</td>\n      <td>0.175190</td>\n      <td>0.325419</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.344220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.355580</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.345324</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>0.989430</td>\n      <td>0.340038</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>0.989443</td>\n      <td>0.341761</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>71 looks like our winner!  Or close enough for our purposes while we then go optimize other things.  And we only had to train our model 13 times - as opposed to the 96 we would have with a brute-force grid search.</p><p>Hopefully this will become a series on using this to tune other RF hyperparameters - other ones have some interesting quirks that I'd like to expound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!</p>","url":"https://hackersandslackers.com/tuning-machine-learning-hyperparameters-with-binary-search/","uuid":"ca7241c3-52cd-4910-86dc-0bb5474d07af","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b8749ed4b98380b152292ea"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c3","title":"Automagically Turn JSON into Pandas DataFrames","slug":"json-into-pandas-dataframes","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","custom_excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","created_at_pretty":"25 July, 2018","published_at_pretty":"28 July, 2018","updated_at_pretty":"21 February, 2019","created_at":"2018-07-25T10:52:35.000-04:00","published_at":"2018-07-28T08:00:00.000-04:00","updated_at":"2019-02-20T21:35:53.000-05:00","meta_title":"Turn JSON into Pandas DataFrames | Hackers And Slackers","meta_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","og_title":"Automagically Turn JSON into Pandas DataFrames","twitter_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","twitter_title":"Automagically Turn JSON into Pandas DataFrames","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"In his post about extracting data from APIs\n[https://hackersandslackers.com/extracting-massive-datasets-from-apis/], Todd\n[https://hackersandslackers.com/author/todd/]  demonstrated a nice way to\nmassage JSON into a pandas DataFrame. This method works great when our JSON\nresponse is flat, because dict.keys()  only gets the keys on the first \"level\"\nof a dictionary. It gets a little trickier when our JSON starts to become nested\nthough, as I experienced when working with Spotify's API\n[https://developer.spotify.com/documentation/web-api/]  via the Spotipy\n[https://spotipy.readthedocs.io/en/latest/]  library. For example, take a look\nat a response from their https://api.spotify.com/v1/tracks/{id}  endpoint:\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n\ntrack_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n\n\nOutput:\n{\n  \"album\": {\n    \"album_type\": \"album\",\n    \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n        \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n        \"name\": \"Stephen Malkmus\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n        \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n        \"name\": \"The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n      }\n    ],\n    \"available_markets\": [\"AR\",\n      \"BO\",\n      \"BR\",\n      \"CA\",\n      \"...\",\n      \"US\",\n      \"UY\",\n      \"VN\"\n    ],\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n    },\n    \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n    \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n    \"images\": [{\n        \"height\": 640,\n        \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n        \"width\": 640\n      },\n      {\n        \"height\": 300,\n        \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n        \"width\": 300\n      },\n      {\n        \"height\": 64,\n        \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n        \"width\": 64\n      }\n    ],\n    \"name\": \"Real Emotional Trash\",\n    \"release_date\": \"2008-03-04\",\n    \"release_date_precision\": \"day\",\n    \"type\": \"album\",\n    \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n  },\n  \"artists\": [{\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n      \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n      \"name\": \"Stephen Malkmus & The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n      \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n      \"name\": \"Stephen Malkmus\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n      \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n      \"name\": \"The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n    }\n  ],\n  \"available_markets\": [\"AR\",\n    \"BO\",\n    \"BR\",\n    \"CA\",\n    \"...\",\n    \"US\",\n    \"UY\",\n    \"VN\"\n  ],\n  \"disc_number\": 1,\n  \"duration_ms\": 608826,\n  \"explicit\": False,\n  \"external_ids\": {\n    \"isrc\": \"USMTD0877204\"\n  },\n  \"external_urls\": {\n    \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n  },\n  \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n  \"id\": \"0BDYBajZydY54OTgQsH940\",\n  \"is_local\": False,\n  \"name\": \"Real Emotional Trash\",\n  \"popularity\": 21,\n  \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n  \"track_number\": 4,\n  \"type\": \"track\",\n  \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n}\n\n\nIn addition to plenty of information about the track, Spotify also includes\ninformation about the album that contains the track. If we were to just use the \ndict.keys()  method to turn this response into a DataFrame, we'd be missing out\non all that extra album information. Well, it would be there, just not readily\naccessible.\n\ntrack_response.keys()\n\n\nOutput:\ndict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n\n\nSo how do we get around this? Well, we could write our own function, but because\npandas is amazing, it already has a built in tool that takes care of this for\nus.\n\nData Normalization\nMeet json_normalize():\n\nimport pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n\n\nOutput:\nalbum.album_type\n album.artists\n album.available_markets\n album.external_urls.spotify\n album.href\n album.id\n album.images\n album.name\n album.release_date\n album.release_date_precision\n ...\n external_urls.spotify\n href\n id\n is_local\n name\n popularity\n preview_url\n track_number\n type\n uri\n 0\n album\n [{'external_urls': {'spotify': 'https://open.s...\n [AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...\n https://open.spotify.com/album/6pWpb4IdPu9vp9m...\n https://api.spotify.com/v1/albums/6pWpb4IdPu9v...\n 6pWpb4IdPu9vp9mOdh5DjY\n [{'height': 640, 'url': 'https://i.scdn.co/ima...\n Real Emotional Trash\n 2008-03-04\n day\n ...\n https://open.spotify.com/track/0BDYBajZydY54OT...\n https://api.spotify.com/v1/tracks/0BDYBajZydY5...\n 0BDYBajZydY54OTgQsH940\n False\n Real Emotional Trash\n 21\n https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...\n 4\n track\n spotify:track:0BDYBajZydY54OTgQsH940\n Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and\nturns it into a DataFrame.\n\nThis makes our life easier when we're dealing with one record, but it really \ncomes in handy when we're dealing with a response that contains multiple\nrecords.\n\ntracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n\n\nOutput:\n{\n  \"tracks\": [{\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 608826,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877204\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n      \"id\": \"0BDYBajZydY54OTgQsH940\",\n      \"is_local\": False,\n      \"name\": \"Real Emotional Trash\",\n      \"popularity\": 21,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 4,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 222706,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877203\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs\",\n      \"id\": \"7fdUqrzb8oCcIoKvFuzMrs\",\n      \"is_local\": False,\n      \"name\": \"Cold Son\",\n      \"popularity\": 25,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 3,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:7fdUqrzb8oCcIoKvFuzMrs\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 416173,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877202\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj\",\n      \"id\": \"0islTY4Fw6lhYbfqi8Qtdj\",\n      \"is_local\": False,\n      \"name\": \"Hopscotch Willie\",\n      \"popularity\": 24,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 2,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0islTY4Fw6lhYbfqi8Qtdj\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 308146,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877201\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH\",\n      \"id\": \"3jyFLbljUTKjE13nIWXchH\",\n      \"is_local\": False,\n      \"name\": \"Dragonfly Pie\",\n      \"popularity\": 26,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 1,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:3jyFLbljUTKjE13nIWXchH\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        }],\n        \"available_markets\": [\"AR\",\n          \"AU\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"DO\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"JP\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"NZ\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\",\n          \"ZA\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG\",\n        \"id\": \"5DMvSCwRqfNVlMB5LjHOwG\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Sparkle Hard\",\n        \"release_date\": \"2018-05-18\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:5DMvSCwRqfNVlMB5LjHOwG\"\n      },\n      \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      }],\n      \"available_markets\": [\"AR\",\n        \"AU\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"DO\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"JP\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"NZ\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\",\n        \"ZA\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 423275,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD1710380\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs\",\n      \"id\": \"6dNmC2YWtWbVOFOdTuRDQs\",\n      \"is_local\": False,\n      \"name\": \"Difficulties - Let Them Eat Vowels\",\n      \"popularity\": 35,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 11,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:6dNmC2YWtWbVOFOdTuRDQs\"\n    }\n  ]\n}\n\n\n\njson_normalise(tracks_response)\n\n\nOutput:\nalbum.album_typealbum.artistsalbum.available_marketsalbum.external_urls.spotify\nalbum.hrefalbum.idalbum.imagesalbum.namealbum.release_date\nalbum.release_date_precision...external_urls.spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsSeparate Ways (Worlds Apart)\nBy default, json_normalize()  uses periods .  to indicate nested levels of the\nJSON object (which is actually converted to a Python dict  by Spotipy). In our\ncase, the album id is found in track['album']['id'], hence the period between\nalbum and id in the DataFrame. This makes things slightly annoying if we want to\ngrab a Series from our new DataFrame. In pandas, we can grab a Series from a\nDataFrame in many ways. To grab the album.id  column, for example:\n\ntracks_df['album.id']\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\nor\n\ntracks_df.loc[:,'album.id']\n\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\npandas also allows us to use dot notation (i.e. dataframe.column_name) to grab a\ncolumn as a Series, but only if our column name doesn't include a period\nalready. Since json_normalize()  uses a period as a separator by default, this\nruins that method. Never fear though – overriding this behavior is as simple as\noverriding the default argument in the function call:\n\ntracks_df = json_normalize(tracks_response['tracks'],sep=\"_\")\ntracks_df\n\n\nOutput:\nalbum_album_typealbum_artistsalbum_available_marketsalbum_external_urls_spotify\nalbum_hrefalbum_idalbum_imagesalbum_namealbum_release_date\nalbum_release_date_precision...external_urls_spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsNow we can go back to using dot notation to\naccess a column as a Series. This saves us some typing every time we want to\ngrab a column, and it looks a bit nicer (to me, at least). I say worth it.\n\ntracks_df.album_id\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n\n\nI Need That Record\nBy including more parameters when we use json_normlize(), we can really extract\njust the data that we want from our API response.\n\nFrom our responses above, we can see that the artist  property contains a list\nof artists that are associated with a track:\n\ntracks_response['tracks'][0]['artists']\n\n\nOutput:\n[{\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n    \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n    \"name\": \"Stephen Malkmus & The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n    \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n    \"name\": \"Stephen Malkmus\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n    \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n    \"name\": \"The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n  }\n]\n\n\nLet's say I want to load this data into a database later. It would be nice to\nhave a join table that maps each of the artists that are associated with each\ntrack. Luckily, this is possible with json_normalize()'s record_path  and meta \nparameters.\n\nrecord_path  tells json_normalize()  what path of keys leads to each individual\nrecord in the JSON object. In our case, we want to grab every artist id, so our\nfunction call will look like:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],sep=\"_\")\n\n\n\nexternal_urls href id name type uri\n 1 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 1 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 2 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 3 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 4 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 5 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 6 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 7 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 8 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 9 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 10 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 11 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 12 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPeCool – we're\nalmost there. Now we want to use the meta  parameter to specify what data we\nwant to include from the rest of the JSON object. In our case, we want to keep\nthe track id and map it to the artist id. If we look back at our API response,\nthe name of the column that included the track is is called, appropriately, id,\nso our full function call should look like this:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n\nOutput:\n-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    <ipython-input-14-77e00a98c3c0> in <module>()\n    ----> 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --> 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n\n\nUh oh – an error! What's going on? Well, it turns out that both the album id and\ntrack id were given the key id. pandas doesn't like that, and it gives us a\nhelpful error to tell us so: ValueError: Conflicting metadata name id, need\ndistinguishing prefix.\n\nThere are two more parameters we can use to overcome this error: record_prefix \nand meta_prefix. These are strings we'll add to the beginning of our records and\nmetadata to prevent these naming conflicts. Since we're dealing with Spotify\nartist ids for our records and Spotify track ids as the metadata, I'll use \nsp_artist_  and sp_track_  respectively. When that's done, I'll select only the\ncolumns that we're interested in.\n\nartist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=\"_\"\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n\n\nOutput:\nsp_track_id sp_artist_id\n 00BDYBajZydY54OTgQsH940 7wyRA7deGRxozTyBc6QXPe\n 10BDYBajZydY54OTgQsH940 0WISkx0PwT6lYWdPqKUJY8\n 20BDYBajZydY54OTgQsH940 7uStwCeP54Za8gXUFCf5L7\n 37fdUqrzb8oCcIoKvFuzMrs 7wyRA7deGRxozTyBc6QXPe\n 47fdUqrzb8oCcIoKvFuzMrs 0WISkx0PwT6lYWdPqKUJY8\n 57fdUqrzb8oCcIoKvFuzMrs 7uStwCeP54Za8gXUFCf5L7\n 60islTY4Fw6lhYbfqi8Qtdj 7wyRA7deGRxozTyBc6QXPe\n 70islTY4Fw6lhYbfqi8Qtdj 0WISkx0PwT6lYWdPqKUJY8\n 80islTY4Fw6lhYbfqi8Qtdj 7uStwCeP54Za8gXUFCf5L7\n 93jyFLbljUTKjE13nIWXchH 7wyRA7deGRxozTyBc6QXPe\n 103jyFLbljUTKjE13nIWXchH 0WISkx0PwT6lYWdPqKUJY8\n 113jyFLbljUTKjE13nIWXchH 7uStwCeP54Za8gXUFCf5L7\n 126dNmC2YWtWbVOFOdTuRDQs 7wyRA7deGRxozTyBc6QXPeTL;DR\n * Use pd.io.json.json_normalize()  to automagically flatten a nested JSON\n   object into a DataFrame\n * Make your life slightly easier when it comes to selecting columns by\n   overriding the default sep  parameter\n * Specify what data constitutes a record with the record_path  parameter\n * Include data from outside of the record path with the meta  parameter\n * Fix naming conflicts if they arise with the record_prefix  and meta_prefix \n   parameters","html":"<p>In his post about <a href=\"https://hackersandslackers.com/extracting-massive-datasets-from-apis/\">extracting data from APIs</a>, <a href=\"https://hackersandslackers.com/author/todd/\">Todd</a> demonstrated a nice way to massage JSON into a pandas DataFrame. This method works great when our JSON response is flat, because <code>dict.keys()</code> only gets the keys on the first \"level\" of a dictionary. It gets a little trickier when our JSON starts to become nested though, as I experienced when working with <a href=\"https://developer.spotify.com/documentation/web-api/\">Spotify's API</a> via the <a href=\"https://spotipy.readthedocs.io/en/latest/\">Spotipy</a> library. For example, take a look at a response from their <code>https://api.spotify.com/v1/tracks/{id}</code> endpoint:</p><pre><code class=\"language-python\">import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n</code></pre>\n<pre><code class=\"language-python\">track_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n</code></pre>\n<h3 id=\"output-\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;album&quot;: {\n    &quot;album_type&quot;: &quot;album&quot;,\n    &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;name&quot;: &quot;The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n      }\n    ],\n    &quot;available_markets&quot;: [&quot;AR&quot;,\n      &quot;BO&quot;,\n      &quot;BR&quot;,\n      &quot;CA&quot;,\n      &quot;...&quot;,\n      &quot;US&quot;,\n      &quot;UY&quot;,\n      &quot;VN&quot;\n    ],\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;images&quot;: [{\n        &quot;height&quot;: 640,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n        &quot;width&quot;: 640\n      },\n      {\n        &quot;height&quot;: 300,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n        &quot;width&quot;: 300\n      },\n      {\n        &quot;height&quot;: 64,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n        &quot;width&quot;: 64\n      }\n    ],\n    &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n    &quot;release_date&quot;: &quot;2008-03-04&quot;,\n    &quot;release_date_precision&quot;: &quot;day&quot;,\n    &quot;type&quot;: &quot;album&quot;,\n    &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n  },\n  &quot;artists&quot;: [{\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;name&quot;: &quot;The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n    }\n  ],\n  &quot;available_markets&quot;: [&quot;AR&quot;,\n    &quot;BO&quot;,\n    &quot;BR&quot;,\n    &quot;CA&quot;,\n    &quot;...&quot;,\n    &quot;US&quot;,\n    &quot;UY&quot;,\n    &quot;VN&quot;\n  ],\n  &quot;disc_number&quot;: 1,\n  &quot;duration_ms&quot;: 608826,\n  &quot;explicit&quot;: False,\n  &quot;external_ids&quot;: {\n    &quot;isrc&quot;: &quot;USMTD0877204&quot;\n  },\n  &quot;external_urls&quot;: {\n    &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n  },\n  &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n  &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n  &quot;is_local&quot;: False,\n  &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n  &quot;popularity&quot;: 21,\n  &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n  &quot;track_number&quot;: 4,\n  &quot;type&quot;: &quot;track&quot;,\n  &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n}\n</code></pre>\n<p>In addition to plenty of information about the track, Spotify also includes information about the album that contains the track. If we were to just use the <code>dict.keys()</code> method to turn this response into a DataFrame, we'd be missing out on all that extra album information. Well, it would be there, just not readily accessible.</p><pre><code class=\"language-python\">track_response.keys()\n</code></pre>\n<h3 id=\"output--1\">Output:</h3><pre><code class=\"language-python\">dict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n</code></pre>\n<p>So how do we get around this? Well, we could write our own function, but because pandas is amazing, it already has a built in tool that takes care of this for us.</p><h2 id=\"data-normalization\">Data Normalization</h2><p>Meet <code>json_normalize()</code>:</p><pre><code class=\"language-python\">import pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n</code></pre>\n<h3 id=\"output--2\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>album.album_type</th>\n      <th>album.artists</th>\n      <th>album.available_markets</th>\n      <th>album.external_urls.spotify</th>\n      <th>album.href</th>\n      <th>album.id</th>\n      <th>album.images</th>\n      <th>album.name</th>\n      <th>album.release_date</th>\n      <th>album.release_date_precision</th>\n      <th>...</th>\n      <th>external_urls.spotify</th>\n      <th>href</th>\n      <th>id</th>\n      <th>is_local</th>\n      <th>name</th>\n      <th>popularity</th>\n      <th>preview_url</th>\n      <th>track_number</th>\n      <th>type</th>\n      <th>uri</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>album</td>\n      <td>[{'external_urls': {'spotify': 'https://open.s...</td>\n      <td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td>\n      <td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td>\n      <td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td>\n      <td>6pWpb4IdPu9vp9mOdh5DjY</td>\n      <td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td>\n      <td>Real Emotional Trash</td>\n      <td>2008-03-04</td>\n      <td>day</td>\n      <td>...</td>\n      <td>https://open.spotify.com/track/0BDYBajZydY54OT...</td>\n      <td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td>\n      <td>0BDYBajZydY54OTgQsH940</td>\n      <td>False</td>\n      <td>Real Emotional Trash</td>\n      <td>21</td>\n      <td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td>\n      <td>4</td>\n      <td>track</td>\n      <td>spotify:track:0BDYBajZydY54OTgQsH940</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and turns it into a DataFrame.</p><p>This makes our life easier when we're dealing with one record, but it <em>really</em> comes in handy when we're dealing with a response that contains multiple records.</p><pre><code class=\"language-python\">tracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n</code></pre>\n<h3 id=\"output--3\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;tracks&quot;: [{\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 608826,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877204&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n      &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n      &quot;popularity&quot;: 21,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 4,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 222706,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877203&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;id&quot;: &quot;7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Cold Son&quot;,\n      &quot;popularity&quot;: 25,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 3,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:7fdUqrzb8oCcIoKvFuzMrs&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 416173,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877202&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;id&quot;: &quot;0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Hopscotch Willie&quot;,\n      &quot;popularity&quot;: 24,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 2,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0islTY4Fw6lhYbfqi8Qtdj&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 308146,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877201&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;id&quot;: &quot;3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Dragonfly Pie&quot;,\n      &quot;popularity&quot;: 26,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 1,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:3jyFLbljUTKjE13nIWXchH&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        }],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;AU&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;DO&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;JP&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;NZ&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;,\n          &quot;ZA&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;id&quot;: &quot;5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Sparkle Hard&quot;,\n        &quot;release_date&quot;: &quot;2018-05-18&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:5DMvSCwRqfNVlMB5LjHOwG&quot;\n      },\n      &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      }],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;AU&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;DO&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;JP&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;NZ&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;,\n        &quot;ZA&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 423275,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD1710380&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;id&quot;: &quot;6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Difficulties - Let Them Eat Vowels&quot;,\n      &quot;popularity&quot;: 35,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 11,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:6dNmC2YWtWbVOFOdTuRDQs&quot;\n    }\n  ]\n}\n\n</code></pre>\n<pre><code class=\"language-python\">json_normalise(tracks_response)\n</code></pre>\n<h3 id=\"output--4\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album.album_type</th><th>album.artists</th><th>album.available_markets</th><th>album.external_urls.spotify</th><th>album.href</th><th>album.id</th><th>album.images</th><th>album.name</th><th>album.release_date</th><th>album.release_date_precision</th><th>...</th><th>external_urls.spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><h2 id=\"separate-ways-worlds-apart-\">Separate Ways (Worlds Apart)</h2><p>By default, <code>json_normalize()</code> uses periods <code>.</code> to indicate nested levels of the JSON object (which is actually converted to a Python <code>dict</code> by Spotipy). In our case, the album id is found in <code>track['album']['id']</code>, hence the period between album and id in the DataFrame. This makes things slightly annoying if we want to grab a Series from our new DataFrame. In pandas, we can grab a Series from a DataFrame in many ways. To grab the <code>album.id</code> column, for example:</p><pre><code class=\"language-python\">tracks_df['album.id']\n</code></pre>\n<h3 id=\"output--5\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>or</p><pre><code class=\"language-python\">tracks_df.loc[:,'album.id']\n\n</code></pre>\n<h3 id=\"output--6\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>pandas also allows us to use dot notation (i.e. <code>dataframe.column_name</code>) to grab a column as a Series, but only if our column name doesn't include a period already. Since <code>json_normalize()</code> uses a period as a separator by default, this ruins that method. Never fear though – overriding this behavior is as simple as overriding the default argument in the function call:</p><pre><code class=\"language-python\">tracks_df = json_normalize(tracks_response['tracks'],sep=&quot;_&quot;)\ntracks_df\n</code></pre>\n<h3 id=\"output--7\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album_album_type</th><th>album_artists</th><th>album_available_markets</th><th>album_external_urls_spotify</th><th>album_href</th><th>album_id</th><th>album_images</th><th>album_name</th><th>album_release_date</th><th>album_release_date_precision</th><th>...</th><th>external_urls_spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><p>Now we can go back to using dot notation to access a column as a Series. This saves us some typing every time we want to grab a column, and it looks a bit nicer (to me, at least). I say worth it.</p><pre><code class=\"language-python\">tracks_df.album_id\n</code></pre>\n<h3 id=\"output--8\">Output:</h3><pre><code class=\"language-shell\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n</code></pre>\n<h2 id=\"i-need-that-record\">I Need That Record</h2><p>By including more parameters when we use <code>json_normlize()</code>, we can really extract just the data that we want from our API response.</p><p>From our responses above, we can see that the <code>artist</code> property contains a list of artists that are associated with a track:</p><pre><code class=\"language-python\">tracks_response['tracks'][0]['artists']\n</code></pre>\n<h3 id=\"output--9\">Output:</h3><pre><code class=\"language-json\">[{\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;name&quot;: &quot;The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n  }\n]\n</code></pre>\n<p>Let's say I want to load this data into a database later. It would be nice to have a join table that maps each of the artists that are associated with each track. Luckily, this is possible with <code>json_normalize()</code>'s <code>record_path</code> and <code>meta</code> parameters.</p><p><code>record_path</code> tells <code>json_normalize()</code> what path of keys leads to each individual record in the JSON object. In our case, we want to grab every artist id, so our function call will look like:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],sep=&quot;_&quot;)\n</code></pre>\n<h3></h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th> </th><th>external_urls </th><th>href </th><th>id </th><th>name </th><th>type </th><th>uri</th></tr></thead><tbody>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><p>Cool – we're almost there. Now we want to use the <code>meta</code> parameter to specify what data we want to include from the rest of the JSON object. In our case, we want to keep the track id and map it to the artist id. If we look back at our API response, the name of the column that included the track is is called, appropriately, <code>id</code>, so our full function call should look like this:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n</code></pre>\n<h3 id=\"output--10\">Output:</h3><pre><code class=\"language-python\">-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    &lt;ipython-input-14-77e00a98c3c0&gt; in &lt;module&gt;()\n    ----&gt; 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --&gt; 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n</code></pre>\n<p>Uh oh – an error! What's going on? Well, it turns out that both the album id and track id were given the key <code>id</code>. pandas doesn't like that, and it gives us a helpful error to tell us so: <code>ValueError: Conflicting metadata name id, need distinguishing prefix</code>.</p><p>There are two more parameters we can use to overcome this error: <code>record_prefix</code> and <code>meta_prefix</code>. These are strings we'll add to the beginning of our records and metadata to prevent these naming conflicts. Since we're dealing with Spotify artist ids for our records and Spotify track ids as the metadata, I'll use <code>sp_artist_</code> and <code>sp_track_</code> respectively. When that's done, I'll select only the columns that we're interested in.</p><pre><code class=\"language-python\">artist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=&quot;_&quot;\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n</code></pre>\n<h3 id=\"output--11\">Output:</h3><div class=\"tableshadow tableContainer\">\n<table>\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>sp_track_id </th><th>sp_artist_id</th></tr></thead><tbody>\n <tr><td>0</td><td>0BDYBajZydY54OTgQsH940 </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1</td><td>0BDYBajZydY54OTgQsH940 </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2</td><td>0BDYBajZydY54OTgQsH940 </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10</td><td>3jyFLbljUTKjE13nIWXchH </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12</td><td>6dNmC2YWtWbVOFOdTuRDQs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><h2 id=\"tl-dr\">TL;DR</h2><ul><li>Use <code>pd.io.json.json_normalize()</code> to automagically flatten a nested JSON object into a DataFrame</li><li>Make your life slightly easier when it comes to selecting columns by overriding the default <code>sep</code> parameter</li><li>Specify what data constitutes a record with the <code>record_path</code> parameter</li><li>Include data from outside of the record path with the <code>meta</code> parameter</li><li>Fix naming conflicts if they arise with the <code>record_prefix</code> and <code>meta_prefix</code> parameters</li></ul>","url":"https://hackersandslackers.com/json-into-pandas-dataframes/","uuid":"172cef40-4545-4c14-8488-a86a891ef47d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b588eb363c4cc21a000cf51"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ac","title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","slug":"trash-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","excerpt":"(And a way to clean it up with SQLAlchemy).","custom_excerpt":"(And a way to clean it up with SQLAlchemy).","created_at_pretty":"19 July, 2018","published_at_pretty":"23 July, 2018","updated_at_pretty":"17 November, 2018","created_at":"2018-07-18T20:26:25.000-04:00","published_at":"2018-07-23T08:30:00.000-04:00","updated_at":"2018-11-16T20:50:25.000-05:00","meta_title":"(And a way to clean it up with SQLAlchemy) | Hackers And Slackers","meta_description":"Python has an extremely handy little tool called f-strings that make string templating a snap!  ","og_description":"Trash Pandas: Messy, Convenient DB Operations via Pandas","og_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","og_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","twitter_description":"(And a way to clean it up with SQLAlchemy)","twitter_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","twitter_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you were continuing our task from last week\n[https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/]\n: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a\ndatabase.\n\nLet's say you've been given a new CSV that conflicts with some rows you've\nalready entered, and you're told that these rows are the correct values.\n\nWhy Not Use Pandas' Built-in Method?\nPandas' built-in to_sql  DataFrame method won't be useful here.  Remember, it\nwrites as a block - if you set the if_exists  flag to \"replace\", that'll make it\nreplace the entire DB table with a new one based on the DF you're uploading.\n And if you're doing this piecemeal, you presumably don't want that.\n\nLast week, we just made a new DataFrame out of each row and appended it to the\nDB table.  That won't work here - we need an Update.  Writing each update by\nhand would be annoying, though - luckily we can use code to generate more code!\n Python has an extremely handy little tool called f-strings  that make string\ntemplating a snap!\n\ndef updateStr(row):\n    return (f\"UPDATE books \"\n            f\"\"\"SET author = '{x.author}' \"\"\"\n            f\"\"\"WHERE id = {x.id};\"\"\")\n\n\nLet's walk through that.  It takes a row from a Dataframe - note that we're\nusing dot notation here instead of the bracket notation.  The reason we're doing\nthat is because, instead of using iterrows()  like last week, we'll be using \nitertuples  because the docstring for iterrows()  said I should.  One reason for\nthis is that iterrows()  gives a pandas Series, which will store everything as\nthe same datatype, which will be annoying in some cases.  I think it's supposed\nto be faster too?itertuples()  instead gives us Named Tuples, which is kind of\nlike a dictionary, except we have to use dot notation instead of square\nbrackets.\n\nSooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly\njust a convenient way of formatting strings with variables - any code inside\ncurly parentheses will be evaluated.  They're convenient, flexible, and\nsupposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a\nDataFrame df2  that only contains the rows to be updated...\n\ncnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n\n\nSweet!  Now let's actually execute it.  We'll be using the execute()  function\nin Pandas' io.sql  module.  I get the feeling I'm not supposed to, primarily\nbecause it doesn't have an entry in the official Pandas documentation, and I\nonly found it by poking around the module code.  But hey, it works!  (Warning\nfrom last time applies super-duper-extra-double this time!)\n\nfor x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n\n\nAnd now let's see if it worked...\n\npd.io.sql.read_sql_table(\"books\", cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n\n\nSweet!\n\nNow all that's well and good, but surely we're not the first person to try to\nmake SQL statements by calling Python functions!  How about a slightly less\nerror-prone way of doing this?\n\nSQLAlchemy\nI'll level with you - I've never actually used SQLAlchemy for anything but\nconnecting Pandas to databases before via the create_engine()  function.  But\nthat's why blogging's great - gives you an excuse to finally check out that\nthing you knew was gonna be useful!\n\nSQLAlchemy first needs some information about our table, then it'll let us\ncreate statements doing things to said table automagically.  We can either\ndefine it ourselves (maybe in a future post!) or read an existing table.  I\nfound the default way of doing this a little to\n\"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of\nthe command that worked into a little function.  I encourage you all to do the\nsame!\n\ndef loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n\n\nThere we go!  Now, let's load our books  table...\n\nbooks = loadTable(cnx, \"books\")\n\n\nAnd here's the cool part!  Now that we have our table object, it has a bunch of\nbuilt-in methods for doing SQL things!  We can print an example...\n\nstr(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n\n\nIf we call books.update, it'll do exactly that.  It also has a handy string\nrepresentation, for debugging and sanity checks.\n\nSQLAlchemy wants us to have a Connection  in addition to our Engine.  Well,\nalright then.\n\nconn = cnx.connect()\n\n\nFine, happy now?  Good.\n\nSQLAlchemy lets us build SQL statements by chain methods, which is fantastically\nuseful.  Less error-prone, easier to pass collections.  Our basic pattern would\nbe, based on iterating with itertuples...\n\nfor x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n\n\nSuccess!","html":"<p>Let's say you were continuing our task from <em><a href=\"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/\">last week</a></em>: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a database.</p><p>Let's say you've been given a new CSV that conflicts with some rows you've already entered, and you're told that these rows are the correct values.</p><h2 id=\"why-not-use-pandas-built-in-method\">Why Not Use Pandas' Built-in Method?</h2><p>Pandas' built-in <code>to_sql</code> DataFrame method won't be useful here.  Remember, it writes as a block - if you set the <code>if_exists</code> flag to <code>\"replace\"</code>, that'll make it replace the entire DB table with a new one based on the DF you're uploading.  And if you're doing this piecemeal, you presumably don't want that.</p><p>Last week, we just made a new DataFrame out of each row and appended it to the DB table.  That won't work here - we need an Update.  Writing each update by hand would be annoying, though - luckily we can use code to generate more code!  Python has an extremely handy little tool called <code>f-strings</code> that make string templating a snap!  </p><pre><code class=\"language-python\">def updateStr(row):\n    return (f&quot;UPDATE books &quot;\n            f&quot;&quot;&quot;SET author = '{x.author}' &quot;&quot;&quot;\n            f&quot;&quot;&quot;WHERE id = {x.id};&quot;&quot;&quot;)\n</code></pre>\n<p>Let's walk through that.  It takes a row from a Dataframe - note that we're using dot notation here instead of the bracket notation.  The reason we're doing that is because, instead of using <code>iterrows()</code> like last week, we'll be using <code>itertuples</code> because the docstring for <code>iterrows()</code> said I should.  One reason for this is that <code>iterrows()</code> gives a pandas Series, which will store everything as the same datatype, which will be annoying in some cases.  I think it's supposed to be faster too?  <code>itertuples()</code> instead gives us Named Tuples, which is kind of like a dictionary, except we have to use dot notation instead of square brackets.  </p><p>Sooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly just a convenient way of formatting strings with variables - any code inside curly parentheses will be evaluated.  They're convenient, flexible, and supposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a DataFrame <code>df2</code> that only contains the rows to be updated...</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n</code></pre>\n<p>Sweet!  Now let's actually execute it.  We'll be using the <code>execute()</code> function in Pandas' <code>io.sql</code> module.  I get the feeling I'm not supposed to, primarily because it doesn't have an entry in the official Pandas documentation, and I only found it by poking around the module code.  But hey, it works!  (Warning from last time applies super-duper-extra-double this time!)</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n</code></pre>\n<p>And now let's see if it worked...</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n</code></pre>\n<p>Sweet!</p><p>Now all that's well and good, but surely we're not the first person to try to make SQL statements by calling Python functions!  How about a slightly less error-prone way of doing this?</p><h1 id=\"sqlalchemy\">SQLAlchemy</h1><p>I'll level with you - I've never actually used SQLAlchemy for anything but connecting Pandas to databases before via the <code>create_engine()</code> function.  But that's why blogging's great - gives you an excuse to finally check out that thing you knew was gonna be useful!</p><p>SQLAlchemy first needs some information about our table, then it'll let us create statements doing things to said table automagically.  We can either define it ourselves (maybe in a future post!) or read an existing table.  I found the default way of doing this a little to \"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of the command that worked into a little function.  I encourage you all to do the same!</p><pre><code class=\"language-python\">def loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n</code></pre>\n<p>There we go!  Now, let's load our <code>books</code> table...</p><pre><code class=\"language-python\">books = loadTable(cnx, &quot;books&quot;)\n</code></pre>\n<p>And here's the cool part!  Now that we have our table object, it has a bunch of built-in methods for doing SQL things!  We can print an example...</p><pre><code class=\"language-python\">str(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n</code></pre>\n<p>If we call <code>books.update</code>, it'll do exactly that.  It also has a handy string representation, for debugging and sanity checks.</p><p>SQLAlchemy wants us to have a <code>Connection</code> in addition to our <code>Engine</code>.  Well, alright then.</p><pre><code class=\"language-python\">conn = cnx.connect()\n</code></pre>\n<p>Fine, happy now?  Good.</p><p>SQLAlchemy lets us build SQL statements by chain methods, which is fantastically useful.  Less error-prone, easier to pass collections.  Our basic pattern would be, based on iterating with <code>itertuples</code>...</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n</code></pre>\n<p>Success!</p>","url":"https://hackersandslackers.com/trash-pandas/","uuid":"8a789f92-fbde-48b5-8bdf-01206e340cc1","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b4fdab10dda8433e079043f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369b","title":"Data Could Save Humanity if it Weren't for Humanity","slug":"data-could-save-humanity","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","excerpt":"A compelling case for robot overlords.","custom_excerpt":"A compelling case for robot overlords.","created_at_pretty":"03 July, 2018","published_at_pretty":"20 July, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-07-03T03:41:14.000-04:00","published_at":"2018-07-20T00:14:00.000-04:00","updated_at":"2019-02-19T03:42:08.000-05:00","meta_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity | Hackers and Slackers","meta_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","og_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","twitter_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","twitter_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"}],"plaintext":"A decade has passed since I stumbled into technical product development. Looking\nback, I've spent that time almost exclusively in the niche of data-driven\nproducts and engineering. While it seems obvious now, I realized in the 2000s\nthat you could generally create two types of product: you could either build a\n(likely uninspired) UI for existing data, or you could build products which\nproduced new data or interpreted existing data in a new useful way. Betting on\nthe latter seemed like an obvious choice. The late 2000’s felt like building\napps for the sake of apps most of the time. Even today Product Hint is littered\nwith weather apps and rehashed tools, solving problems so insignificant that\nthey almost seem satirical.\n\nYears passed, thus our data-centric tools evolved to fit cultural mind shifts in\nbusinesses which speculated on how these tools could be used. This began to\nbuild a clear yet slowly-growing narrative about how enterprises consider data\nanalysis in their org structures. Unfortunately, I can't say that much about\nthat shift has been positive. There are a number of major problems I believe we\nneed to address:\n\n *   SaaS is created with the goal of selling the product to enterprises. While\n   humanity's understanding of data science reach unprecedented territory, we\n   choose to perfect the  sales pitch  while neglecting education on these\n   tools.\n * As an atrocity to science, individual actors commonly cherry pick information\n   to confirm conclusions for personal benefit, without checks and balances. \n * Data which contradicts knee jerk assumptions made by executives are sometimes\n   taken as personal threats or attacks.\n * Most importantly, data professionals are horribly siloed. Analysts,\n   scientists, and engineers waste far too much time drawing lines between\n   roles: I find it absolutely absurd  to unanimously agree that tool X is for\n   BI  while tool Y is for data cleanup. Considering we all know  these tools\n   are running stacks on Python, R, SQL, etc, there is no reason to succumb to\n   the limitations of proprietary software (such as Tableau). We've turned a\n   blind eye to the possibility of 'data as a service': a chance to overlap\n   responsibilities by building a better  tool to reduce friction, as opposed to\n   increase it in the interest of selling more software.\n\nWhile we might all agree that collective 'data addiction' is reaching a peak,\nmost of us barely know what we mean by that. We conceptually understand that\ndata is important, but our imaginations on how to utilize this power effectively\nleaves a lot to be desired. IBM Watson probably had profound capabilities, but\nits failure lies with the humans tasked to make this technology relevant and\nuseful for humankind.\n\nThe Analytics Honeymoon\nAs I imagine most in the Product Management professionals do, I originally\nconsidered  data analysis to come in the form of web and app analytics. This was\na one-dimensional era; consumer-facing data served the sole purpose of\noptimizing sales and ad revenue, and there were much fewer choices of\nEnterprise-level tools to fall in love with. While the cheaper tools were just\nfine, corporate America had already fallen in love with a fickle mistress known\nas Omniture. \n\nOmniture was in fact in many ways the superior product on the market. As I'm\nsure Sales reps explained in those years, Omniture allowed for a vast level of\nevent tracking customization which was otherwise rare at the time: with the\nproper logic, effort, and willingness, executives could theoretically identify\ngranular issues in their product's conversation flow: issues which came attached\nwith cold hard facts in the form of numbers.\n\nThus, a game of numbers it was: in order to receive the level of granular detail\nexecutives wanted, there came a nominal fee. Well, many fees in fact: the\nproduct out-priced competitors tenfold upfront for the license itself. Since you\njust agreed to spend that much money on proprietary software, it only makes\nsense at that point that you should then hire a certified affiliated consultant\nto implement the custom reports, and then of course pay the lifelong upkeep that\ncomes with tracking events in ever-changing software. Despite all of these\ncosts, companies consistently moved forward with the choice under the\nrealization that the money saved from this data could far outweigh the cost.\n\nSo what happened when we actually collected all that data?\n\nIf You Could Get That Analytics Report, That Would be Great.\nEnterprises and data analysis were made for each other... but in a way that most\nclosely resembles a cliché romcom starring Julia Roberts. This romance follows\nthe beat of a metronome: while executives begin to grasp the impact of\ndata-based decisions, the commitment to actually acknowledge the abundance of\nthis information has its own lifespan. A/B testing, conversation funnels,\nsegmentation, etc: while the foreplay of implementing these buzzwords rolls on\nfor a number of weeks, it gives powerful figures time to reflect on one thing in\nparticular: they've owed their success to a lack of quantifiable accountability,\nand numbers are right around the corner. While this might not be a conscious\nact, it is an entirely real phenomenon.\n\nI've traditionally been a product manager, yet during that period of my career,\nI've found myself nominated to be Gatekeeper of Digital and Financial Data...\nfor whatever reason (it's worth reiterating that I am not nor ever have been an\nanalyst). The phenomenon followed a pattern. Given their expectations,\nexecutives reach their nerves end when the budget they allocated for\nenterprise-level sotftware is still under configuration, and has produced no\nresults. There's a reason why patience is a virtue isn't a phrase you see in\nmany sales pitches: we want what we're being sold, and we want it now. \n\nThats where I'd typically come in. As a product manager, analytics is a valuable\nweapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed\nlike ammunition for change. When a company's problems are become as large as\nthey are obvious, some numerical correlations are nearly common sense..\n\nCue the dashboards, custom reports, event tracking, you name it. Often times\nexecutives would set aside a weekly cadence to review the expensive conclusions\nour software could finally produce. The weekly email newsletters I would produce\nwould be met with a euphoric chain of satisfied stakeholders, time and time\nagain. Finally it seemed, the conclusions were clear and our problems were\nquantifiable. And yet, nothing seemed to change.\n\nHuman Insecurities Versus World Problems\nAs a data enthusiast, I did what we all would've done: I placed analytics\ntracking on our analytics reports themselves. \"Great stuff, groundbreaking work\nhere\" said one CEO, who I'd seen had not bothered to click the link provided. At\na certain point, I began attaching empty Excel spreadsheets and posting dead\nlinks as the content of our beloved reports. Those too were 'groundbreaking',\napparently.\n\nThis is far from an isolated phenomenon in technology. Meeting after meeting,\nclient after client, I took front-row seats to blatant dismissal of numerical\nevidence in favor of  ego-driven decisions. Test A would prove to yield 30%\nhigher conversion rates than Test B, but Test B would prevail thanks to the\nsubjective emotional opinions of talking heads. In retrospect, I can see now how\na grown adult with a household would find the sudden introduction of facts\nthreatening. We have have imposter syndrome, and the twenty-something year old\nanalyst attempting to improve a company will almost always lose to an adult\nprotecting a family.\n\nConsider a recent example uncovered by mistake. While auditing usage for a\nwidely-know project management tool, something seemed off about our volume of\nusage with a product costing us unspeakable dollars. Our department had mostly\nbeen tasked  to upkeep this 'critical' internal system at all costs. As it turns\nout, over 80% of all activity had been out own internal upkeep. That's millions\nof dollars invested in something never used over the course of several years,\nall for the purpose of upholding a guise of value-add.\n\nBI, Data Science, and the Choice to Make That Distinction\nI've spent the last several months working deep under the hood attempting to\ndismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about\nto rip in to a Tableau tangent here, but I promise there's a point.\n\nI was introduced to Tableau as a tool to fill a niche: quick analysis and\none-off extracts on tight timelines. The type of timelines where digging into\nPandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t\nan option. I was pleased with its ability to serve this purpose- such that it\nsparked a spontaneous 1 thousand purchase for a personal license. Tableau\nDesktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the\nrest of my life.\n\nFrom my naïve perspective it seemed logical that Tableau could help assist in\nthe data cleanup and automation I had been handling in scripts previously. This\ncould not be more incorrect. Even with full access to my own Tableau instance,\nit is clear that Tableau has one motive only: to show you your data, and ensure\nyou don't take it elsewhere. Consider this:\n\nCheck out the worksheets and and dashboards you've published to Server.\nConsidering these are equivalent to simple database views, you'd expect the API\ncalls to be exposed in your dev tools... why not? They aren't.\n\nTableau runs on a Postgres database on your personal server. However, no mention\nof \"postgres\" or anything of the sort is searchable to a useful degree. There is\na highly protected Tableau superadmin account which has controls to all tables\nand views in this server, but most research will point users to unlock the\n\"readonly\" user which is essentially a red herring account, or perhaps useful if\nyou're spying on your employee's actions.\n\nAnd then we have the Tableau server API. Ah, what a gift it would be to query\nthose views we created, running on scheduled extracts, so that we might build\nsomething from this information. As it turns out, Tableau's REST API does little\nmore than reveal meta data about files you already knew about. Just in case you\nwere wondering the date it was created, for some weird useless reason.\n\nI'm not just picking on Tableau here (although I'll continue my series about\nhacking them soon enough). This has exposed a massive dichotomy in the way we\nsee and treat data as a profession, or rather, a series of professions. between\nthose who look at data, and those who manipulate, iterate one, and create things\nwith Data. Nobody has ever expressed this realization to me, and many of you\nlikely still don't see what the big deal is. That, to me, is the big deal.\n\nData should be a passion to those looking to improve humanity, without a doubt.\nIf we know personalities are wining the battles against numbers, and feel numb\nto the fact that our proprietary tools prevent us from using data effectively,\nthere's something to be said about the complacency of humanity as we commit to\nconsumption over production. \n\nCompany attitudes towards data are one thing, but individuals are an entirely\ndifferent story. That's a long-winded post for another time.","html":"<p>A decade has passed since I stumbled into technical product development. Looking back, I've spent that time almost exclusively in the niche of data-driven products and engineering. While it seems obvious now, I realized in the 2000s that you could generally create two types of product: you could either build a (likely uninspired) UI for existing data, or you could build products which produced new data or interpreted existing data in a new useful way. Betting on the latter seemed like an obvious choice. The late 2000’s felt like building apps for the sake of apps most of the time. Even today Product Hint is littered with weather apps and rehashed tools, solving problems so insignificant that they almost seem satirical.</p><p>Years passed, thus our data-centric tools evolved to fit cultural mind shifts in businesses which speculated on how these tools could be used. This began to build a clear yet slowly-growing narrative about how enterprises consider data analysis in their org structures. Unfortunately, I can't say that much about that shift has been positive. There are a number of major problems I believe we need to address:</p><ul><li><em> </em>SaaS is created with the goal of <em>selling </em>the product to enterprises. While humanity's understanding of data science reach unprecedented territory, we choose to perfect the<em> sales pitch</em> while neglecting education on these tools.</li><li>As an atrocity to science, individual actors commonly cherry pick information to confirm conclusions for personal benefit, without checks and balances. </li><li>Data which contradicts knee jerk assumptions made by executives are sometimes taken as personal threats or attacks.</li><li>Most importantly, data professionals are <em><strong>horribly siloed. </strong></em>Analysts, scientists, and engineers waste far too much time drawing lines between roles: I find it <em><strong>absolutely absurd</strong></em> to unanimously agree that <strong>tool X is for BI</strong> while <strong>tool Y is for data cleanup</strong>. Considering we <em>all know</em> these tools are running stacks on Python, R, SQL, etc, there is no reason to succumb to the limitations of proprietary software (such as Tableau). We've turned a blind eye to the possibility of 'data as a service': a chance to overlap responsibilities by building a <em>better</em> tool to reduce friction, as opposed to increase it in the interest of selling more software.</li></ul><p>While we might all agree that collective 'data addiction' is reaching a peak, most of us barely know what we mean by that. We conceptually understand that data is important, but our imaginations on how to utilize this power effectively leaves a lot to be desired. IBM Watson probably had profound capabilities, but its failure lies with the humans tasked to make this technology relevant and useful for humankind.</p><h2 id=\"the-analytics-honeymoon\">The Analytics Honeymoon</h2><p>As I imagine most in the Product Management professionals do, I originally considered  data analysis to come in the form of web and app analytics. This was a one-dimensional era; consumer-facing data served the sole purpose of optimizing sales and ad revenue, and there were much fewer choices of Enterprise-level tools to fall in love with. While the cheaper tools were just fine, corporate America had already fallen in love with a fickle mistress known as Omniture. </p><p>Omniture was in fact in many ways the superior product on the market. As I'm sure Sales reps explained in those years, Omniture allowed for a vast level of event tracking customization which was otherwise rare at the time: with the proper logic, effort, and willingness, executives could theoretically identify granular issues in their product's conversation flow: issues which came attached with cold hard facts in the form of numbers.</p><p>Thus, a game of numbers it was: in order to receive the level of granular detail executives wanted, there came a nominal fee. Well, many fees in fact: the product out-priced competitors tenfold upfront for the license itself. Since you just agreed to spend that much money on proprietary software, it only makes sense at that point that you should then hire a certified affiliated consultant to implement the custom reports, and then of course pay the lifelong upkeep that comes with tracking events in ever-changing software. Despite all of these costs, companies consistently moved forward with the choice under the realization that the money saved from this data could far outweigh the cost.</p><p>So what happened when we actually collected all that data?</p><h2 id=\"if-you-could-get-that-analytics-report-that-would-be-great-\">If You Could Get That Analytics Report, That Would be Great.</h2><p>Enterprises and data analysis were made for each other... but in a way that most closely resembles a cliché romcom starring Julia Roberts. This romance follows the beat of a metronome: while executives begin to grasp the impact of data-based decisions, the commitment to actually acknowledge the abundance of this information has its own lifespan. A/B testing, conversation funnels, segmentation, etc: while the foreplay of implementing these buzzwords rolls on for a number of weeks, it gives powerful figures time to reflect on one thing in particular: they've owed their success to a lack of quantifiable accountability, and numbers are right around the corner. While this might not be a conscious act, it is an entirely real phenomenon.</p><p>I've traditionally been a product manager, yet during that period of my career, I've found myself nominated to be Gatekeeper of Digital and Financial Data... for whatever reason (it's worth reiterating that I am not nor ever have been an analyst). The phenomenon followed a pattern. Given their expectations, executives reach their nerves end when the budget they allocated for enterprise-level sotftware is still under configuration, and has produced no results. There's a reason why patience is a virtue isn't a phrase you see in many sales pitches: we want what we're being sold, and we want it now. </p><p>Thats where I'd typically come in. As a product manager, analytics is a valuable weapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed like ammunition for change. When a company's problems are become as large as they are obvious, some numerical correlations are nearly common sense..</p><p>Cue the dashboards, custom reports, event tracking, you name it. Often times executives would set aside a weekly cadence to review the expensive conclusions our software could finally produce. The weekly email newsletters I would produce would be met with a euphoric chain of satisfied stakeholders, time and time again. Finally it seemed, the conclusions were clear and our problems were quantifiable. And yet, nothing seemed to change.</p><h2 id=\"human-insecurities-versus-world-problems\">Human Insecurities Versus World Problems</h2><p>As a data enthusiast, I did what we all would've done: I placed analytics tracking on our analytics reports themselves. \"Great stuff, groundbreaking work here\" said one CEO, who I'd seen had not bothered to click the link provided. At a certain point, I began attaching empty Excel spreadsheets and posting dead links as the content of our beloved reports. Those too were 'groundbreaking', apparently.</p><p>This is far from an isolated phenomenon in technology. Meeting after meeting, client after client, I took front-row seats to blatant dismissal of numerical evidence in favor of  ego-driven decisions. Test A would prove to yield 30% higher conversion rates than Test B, but Test B would prevail thanks to the subjective emotional opinions of talking heads. In retrospect, I can see now how a grown adult with a household would find the sudden introduction of facts threatening. We have have imposter syndrome, and the twenty-something year old analyst attempting to improve a company will almost always lose to an adult protecting a family.</p><p>Consider a recent example uncovered by mistake. While auditing usage for a widely-know project management tool, something seemed off about our volume of usage with a product costing us unspeakable dollars. Our department had mostly been tasked  to upkeep this 'critical' internal system at all costs. As it turns out, over 80% of all activity had been out own internal upkeep. That's millions of dollars invested in something never used over the course of several years, all for the purpose of upholding a guise of value-add.</p><h2 id=\"bi-data-science-and-the-choice-to-make-that-distinction\">BI, Data Science, and the Choice to Make That Distinction</h2><p>I've spent the last several months working deep under the hood attempting to dismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about to rip in to a Tableau tangent here, but I promise there's a point.</p><p>I was introduced to Tableau as a tool to fill a niche: quick analysis and one-off extracts on tight timelines. The type of timelines where digging into Pandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t an option. I was pleased with its ability to serve this purpose- such that it sparked a spontaneous 1 thousand purchase for a personal license. Tableau Desktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the rest of my life.</p><p>From my naïve perspective it seemed logical that Tableau could help assist in the data cleanup and automation I had been handling in scripts previously. This could not be more incorrect. Even with full access to my own Tableau instance, it is clear that Tableau has one motive only: to show you your data, and ensure you don't take it elsewhere. Consider this:</p><p>Check out the worksheets and and dashboards you've published to Server. Considering these are equivalent to simple database views, you'd expect the API calls to be exposed in your dev tools... why not? They aren't.</p><p>Tableau runs on a Postgres database on your personal server. However, no mention of \"postgres\" or anything of the sort is searchable to a useful degree. There is a highly protected Tableau superadmin account which has controls to all tables and views in this server, but most research will point users to unlock the \"readonly\" user which is essentially a red herring account, or perhaps useful if you're spying on your employee's actions.</p><p>And then we have the Tableau server API. Ah, what a gift it would be to query those views we created, running on scheduled extracts, so that we might build something from this information. As it turns out, Tableau's REST API does little more than reveal meta data about files you already knew about. Just in case you were wondering the date it was created, for some weird useless reason.</p><p>I'm not just picking on Tableau here (although I'll continue my series about hacking them soon enough). This has exposed a massive dichotomy in the way we see and treat data as a profession, or rather, a series of professions. between those who look at data, and those who manipulate, iterate one, and create things with Data. Nobody has ever expressed this realization to me, and many of you likely still don't see what the big deal is. That, to me, is the big deal.</p><p>Data should be a passion to those looking to improve humanity, without a doubt. If we know personalities are wining the battles against numbers, and feel numb to the fact that our proprietary tools prevent us from using data effectively, there's something to be said about the complacency of humanity as we commit to consumption over production. </p><p>Company attitudes towards data are one thing, but individuals are an entirely different story. That's a long-winded post for another time.</p>","url":"https://hackersandslackers.com/data-could-save-humanity/","uuid":"01f56f59-a9ad-494a-993d-216716f68a7d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3b289ad0ac8a143588f360"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a7","title":"Lynx Roundup, July 19th","slug":"lynx-roundup-july-19th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","custom_excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","created_at_pretty":"13 July, 2018","published_at_pretty":"19 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:32:02.000-04:00","published_at":"2018-07-19T07:00:00.000-04:00","updated_at":"2018-07-24T22:43:44.000-04:00","meta_title":"Lynx Roundup, July 19th | Hackers and Slackers","meta_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","og_title":"Lynx Roundup, July 19th","twitter_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","twitter_title":"Lynx Roundup, July 19th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\n\n\n\nhttps://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\n\n\n\nhttps://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\n\n\n\nhttps://github.com/Microsoft/dowhy\n\n\n\nhttps://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/","html":"<p></p><p><a href=\"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\">https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428</a></p><p></p><p><a href=\"https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\">https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e</a></p><p></p><p><a href=\"https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\">https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113</a></p><p></p><p><a href=\"https://github.com/Microsoft/dowhy\">https://github.com/Microsoft/dowhy</a></p><p></p><p><a href=\"https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/\">https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-19th/","uuid":"a9739b8c-a429-4948-8b54-9ba44dea7cee","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486382c6a9e951f8a6cc63"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a6","title":"Lynx Roundup, July 18th","slug":"lynx-roundup-july-18th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","excerpt":"Agile Data!  Medical Data!  Python string tricks! ","custom_excerpt":"Agile Data!  Medical Data!  Python string tricks! ","created_at_pretty":"13 July, 2018","published_at_pretty":"18 July, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-07-13T04:30:13.000-04:00","published_at":"2018-07-18T07:00:00.000-04:00","updated_at":"2019-02-28T02:27:37.000-05:00","meta_title":"Lynx Roundup, July 18th | Hackers and Slackers","meta_description":"Agile Data!  Medical Data!  Python string tricks! ","og_description":"Agile Data!  Medical Data!  Python string tricks! ","og_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","og_title":"Lynx Roundup, July 18th","twitter_description":"Agile Data!  Medical Data!  Python string tricks! ","twitter_image":"https://hackersandslackers.com/content/images/2019/02/147.jpg","twitter_title":"Lynx Roundup, July 18th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.locallyoptimistic.com/post/agile-analytics-p1/\n\n\n\nhttps://realpython.com/python-string-formatting/\n\n\n\nhttps://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/\n\n\n\nhttps://css-tricks.com/prototyping-in-the-browser/\n\n\n\nhttps://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13","html":"<p></p><p><a href=\"https://www.locallyoptimistic.com/post/agile-analytics-p1/\">https://www.locallyoptimistic.com/post/agile-analytics-p1/</a></p><p></p><p><a href=\"https://realpython.com/python-string-formatting/\">https://realpython.com/python-string-formatting/</a></p><p></p><p><a href=\"https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/\">https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/</a></p><p></p><p><a href=\"https://css-tricks.com/prototyping-in-the-browser/\">https://css-tricks.com/prototyping-in-the-browser/</a></p><p></p><p><a href=\"https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13\">https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-18th/","uuid":"864fbc9d-d98d-483f-8636-66ffe8ec9987","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b486315c6a9e951f8a6cc5f"}}]}},"pageContext":{"slug":"datascience","limit":12,"skip":0,"numberOfPages":3,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/datascience/page/2/"}}