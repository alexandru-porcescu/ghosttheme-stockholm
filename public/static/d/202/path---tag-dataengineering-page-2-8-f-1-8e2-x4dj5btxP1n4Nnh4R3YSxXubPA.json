{"data":{"ghostTag":{"slug":"dataengineering","name":"Data Engineering","visibility":"public","feature_image":null,"description":"The systematic collection and transformation of data via the creation of tools and pipelines.","meta_title":"Data Engineering | Hackers and Slackers","meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673734","title":"Scraping Data on the Web with BeautifulSoup","slug":"scraping-urls-with-beautifulsoup","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","excerpt":"The honest act of systematically stealing data without permission.","custom_excerpt":"The honest act of systematically stealing data without permission.","created_at_pretty":"11 November, 2018","published_at_pretty":"11 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-11T04:53:44.000-05:00","published_at":"2018-11-11T08:35:09.000-05:00","updated_at":"2019-01-05T13:21:06.000-05:00","meta_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","meta_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","og_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","twitter_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","twitter_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"There are plenty of reliable and open sources of data on the web. Datasets are\nfreely released to the public domain by the likes of Kaggle, Google Cloud, and\nof course local & federal government. Like most things free and open, however,\nfollowing the rules to obtain public data can be a bit... boring. I'm not\nsuggesting we go and blatantly break some grey-area laws by stealing data, but\nthis blog isn't exactly called People Who Play It Safe And Slackers, either. \n\nMy personal Python roots can actually be traced back to an ambitious\nside-project: to aggregate all new music from across the web and deliver it the\nmasses. While that project may have been abandoned (after realizing it already\nexisted), BeautifulSoup  was more-or-less my first ever experience with Python. \n\nThe Tool(s) for the Job(s)\nBefore going any further, we'd be ill-advised to not at least mention Python's\nother web-scraping behemoth, Scrapy [https://scrapy.org/]. BeautifulSoup  and \nScrapy  have two very different agendas. BeautifulSoup is intended to parse or\nextract data one page at a time, with each page being served up via the requests \n library or equivalent. Scrapy,  on the other hand, is for creating crawlers: or\nrather absolute monstrosities unleashed upon the web like a swarm, loosely\nfollowing links and haste-fully grabbing data where data exists to be grabbed.\nTo put this in perspective, Google Cloud functions will not even let you import\nScrapy as a usable library.\n\nThis isn't to say that BeautifulSoup  can't be made into a similar monstrosity\nof its own. For now, we'll focus on a modest task: generating link previews for\nURLs by grabbing their metadata.\n\nStep 1: Stalk Your Prey\nBefore we steal any data, we should take a look at the data we're hoping to\nsteal.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape URLs to generate previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n\n\nThe above is the minimum needed to retrieve the DOM structure of an HTML page. \nBeautifulSoup  accepts the .content  output from a request, from which we can\ninvestigate the contents.\n\nUsing BeauitfulSoup will often result in different results for your scaper than\nyou might see as a human, such as 403 errors or blocked content. An easy way\naround this faking your headers into looking like normal browser agents, as we\ndo here: \nheaders.update({\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101\nFirefox/52.0',\n})`The result of print(soup.prettify())  will predictably output a \"pretty\" printed\nversion of your target DOM structure:\n\n<html class=\"gr__example_com\"><head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta property=\"og:site_name\" content=\"Example dot com\">\n    <meta property=\"og:type\" content=\"website\">\n    <meta property=\"og:title\" content=\"Example\">\n    <meta property=\"og:description\" content=\"An Example website.\">\n    <meta property=\"og:image\" content=\"http://example.com/img/image.jpg\">\n    <meta name=\"twitter:title\" content=\"Hackers and Slackers\">\n    <meta name=\"twitter:description\" content=\"An Example website.\">\n    <meta name=\"twitter:url\" content=\"http://example.com/\">\n    <meta name=\"twitter:image\" content=\"http://example.com/img/image.jpg\">\n</head>\n\n<body data-gr-c-s-loaded=\"true\">\n  <div>\n    <h1>Example Domain</h1>\n      <p>This domain is established to be used for illustrative examples in documents.</p>\n      <p>You may use this domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n  </div>\n</body>\n    \n</html>\n\n\nStep 2: The Extraction\nAfter turning our request content into a BeautifulSoup object, we access items\nin the DOM via dot notation as such:\n\ntitle = soup.title.string\n\n\n.string  gives us the actual content of the tag which is Example Domain, whereas\n soup.title  would return the entirety of the tag as <title>Example\nDomain</title>. \n\nDot notation is fine when pages have predictable hierarchies or structures, but\nbecomes much less useful for extracting patterns we see in the document. soup.a \nwill only return the first instance of a link, and probably isn't what we want.\n\nIf we wanted to extract all  <a>  tags of a page's content while avoiding the\nnoise of nav links etc, we can use CSS selectors to return a list of all\nelements matching the selection. soup.select('body p > a')  retrieves all links\nembedded in paragraph text, limited to the body of the page. \n\nSome other methods of grabbing elements:\n\n * soup.find(id=\"example\"): Useful for when a single element is expected.\n * soup.find_all('a'):  Returns a list of all elements matching the selection\n   after searching the document recursively.\n * .parent and .child: Relative selectors to a currently engaged element.\n\nGet Some Attributes\nChances are we'll almost always want the contents or the attributes of a tag, as\nopposed to the entire <a>  tag's HTML. A common example of going after a tag's\nattributes would be in the cases of img  and a  tags. Chances are we're most\ninterested in the src  and href  attributes of such tags, respectively. \n\nThe .get  method refers specifically to getting the value of attributes on a\ntag. For example, soup.find('.logo').get('href')  would find an element with the\nclass \"logo\", and return the url to that image.\n\nPesky Tags to Deal With\nIn our example of creating link previews, a good first source of information\nwould obviously be the page's meta tags: specifically the og  tags they've\nspecified to openly provide the bite-sized information we're looking for.\nGrabbing these tags are a bit more difficult to deal with:\n\nsoup.find(\"meta\", property=\"og:description\").get('content')\n\n\nOh yeah, now that's some ugly shit right there. Meta tags are especially\ninteresting because they're all uselessly dubbed 'meta', thus we need a second\ndifferentiator in addition to the tag name to specify which meta tag we care\nabout. Only then can we bother to get  the actual content of said tag.\n\nStep 3: Realizing Something Will Always Break\nIf we were to try the above selector on an HTML page which did not contain an \nog:description, our script would break unforgivingly. Not only do we miss this\ndata, but we miss out on everything entirely - this means we always need to\nbuild in a plan B, and at the very least deal with a lack of tag altogether.\n\nIt's best to break out this logic one tag at a time. First, let's look at an\nexample for a base scraper with all the knowledge we have so far:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p > a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n\n\nGreat - there's a base function for snatching all links out of the body of a\npage. Ultimately we'll create a JSON object for each of these links containing\npreview data, link_preview_dict.\n\nTo handle each value of our dict, we have individual functions:\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get a preview image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\nIn case you're wondering:\n\n * getTitle tries to get the <title>  tag, and falls back to the page's first \n   <h1>  tag (surprisingly enough some pages are in fact missing a title).\n * getDescription  looks for the OG description, and falls back to the content\n   of the page's first paragraph.\n * getImage looks for the OG image, and falls back to the page's first image.\n * getSiteName similarly tries to grab the OG attribute, otherwise it does it's\n   best to extract the domain name from the URL string under the assumption that\n   this is the origin's name (look, it ain't perfect).\n\nWhat Did We Just Build?\nBelieve it or not, the above is considered to be enough logic to be a paid\nservice with a monthly fee. Go ahead and Google it; or better yet, just steal my\nsource code entirely:\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p > a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)","html":"<p>There are plenty of reliable and open sources of data on the web. Datasets are freely released to the public domain by the likes of Kaggle, Google Cloud, and of course local &amp; federal government. Like most things free and open, however, following the rules to obtain public data can be a bit... boring. I'm not suggesting we go and blatantly break some grey-area laws by stealing data, but this blog isn't exactly called <strong>People Who Play It Safe And Slackers</strong>, either. </p><p>My personal Python roots can actually be traced back to an ambitious side-project: to aggregate all new music from across the web and deliver it the masses. While that project may have been abandoned (after realizing it already existed), <strong>BeautifulSoup</strong> was more-or-less my first ever experience with Python. </p><h2 id=\"the-tool-s-for-the-job-s-\">The Tool(s) for the Job(s)</h2><p>Before going any further, we'd be ill-advised to not at least mention Python's other web-scraping behemoth, <strong><a href=\"https://scrapy.org/\">Scrapy</a></strong>. <strong>BeautifulSoup</strong> and <strong>Scrapy</strong> have two very different agendas. BeautifulSoup is intended to parse or extract data one page at a time, with each page being served up via the <strong>requests</strong> library or equivalent. <strong>Scrapy,</strong> on the other hand, is for creating crawlers: or rather absolute monstrosities unleashed upon the web like a swarm, loosely following links and haste-fully grabbing data where data exists to be grabbed. To put this in perspective, Google Cloud functions will not even let you import Scrapy as a usable library.</p><p>This isn't to say that <strong>BeautifulSoup</strong> can't be made into a similar monstrosity of its own. For now, we'll focus on a modest task: generating link previews for URLs by grabbing their metadata.</p><h2 id=\"step-1-stalk-your-prey\">Step 1: Stalk Your Prey</h2><p>Before we steal any data, we should take a look at the data we're hoping to steal.</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape URLs to generate previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n</code></pre>\n<p>The above is the minimum needed to retrieve the DOM structure of an HTML page. <strong>BeautifulSoup</strong> accepts the <code>.content</code> output from a request, from which we can investigate the contents.</p><div class=\"protip\">\n    Using BeauitfulSoup will often result in different results for your scaper than you might see as a human, such as 403 errors or blocked content. An easy way around this faking your headers into looking like normal browser agents, as we do here: <br><code>headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })`</code>\n</div><p>The result of <code>print(soup.prettify())</code> will predictably output a \"pretty\" printed version of your target DOM structure:</p><pre><code class=\"language-html\">&lt;html class=&quot;gr__example_com&quot;&gt;&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n    &lt;meta charset=&quot;utf-8&quot;&gt;\n    &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;\n    &lt;meta property=&quot;og:site_name&quot; content=&quot;Example dot com&quot;&gt;\n    &lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n    &lt;meta property=&quot;og:title&quot; content=&quot;Example&quot;&gt;\n    &lt;meta property=&quot;og:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n    &lt;meta name=&quot;twitter:title&quot; content=&quot;Hackers and Slackers&quot;&gt;\n    &lt;meta name=&quot;twitter:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta name=&quot;twitter:url&quot; content=&quot;http://example.com/&quot;&gt;\n    &lt;meta name=&quot;twitter:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n&lt;/head&gt;\n\n&lt;body data-gr-c-s-loaded=&quot;true&quot;&gt;\n  &lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n      &lt;p&gt;This domain is established to be used for illustrative examples in documents.&lt;/p&gt;\n      &lt;p&gt;You may use this domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&quot;http://www.iana.org/domains/example&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n    \n&lt;/html&gt;\n</code></pre>\n<h2 id=\"step-2-the-extraction\">Step 2: The Extraction</h2><p>After turning our request content into a BeautifulSoup object, we access items in the DOM via dot notation as such:</p><pre><code class=\"language-python\">title = soup.title.string\n</code></pre>\n<p><code>.string</code> gives us the actual content of the tag which is <code>Example Domain</code>, whereas <code>soup.title</code> would return the entirety of the tag as <code>&lt;title&gt;Example Domain&lt;/title&gt;</code>. </p><p>Dot notation is fine when pages have predictable hierarchies or structures, but becomes much less useful for extracting patterns we see in the document. <code>soup.a</code> will only return the first instance of a link, and probably isn't what we want.</p><p>If we wanted to extract <em>all</em> <code>&lt;a&gt;</code> tags of a page's content while avoiding the noise of nav links etc, we can use CSS selectors to return a list of all elements matching the selection. <code>soup.select('body p &gt; a')</code> retrieves all links embedded in paragraph text, limited to the body of the page. </p><p>Some other methods of grabbing elements:</p><ul><li><strong>soup.find(id=\"example\")</strong>: Useful for when a single element is expected.</li><li><strong>soup.find_all('a')</strong>:<strong> </strong>Returns a list of all elements matching the selection after searching the document recursively.</li><li><strong>.parent </strong>and <strong>.child</strong>: Relative selectors to a currently engaged element.</li></ul><h3 id=\"get-some-attributes\">Get Some Attributes</h3><p>Chances are we'll almost always want the contents or the attributes of a tag, as opposed to the entire <code>&lt;a&gt;</code> tag's HTML. A common example of going after a tag's attributes would be in the cases of <code>img</code> and <code>a</code> tags. Chances are we're most interested in the <code>src</code> and <code>href</code> attributes of such tags, respectively. </p><p>The <code>.get</code> method refers specifically to getting the value of attributes on a tag. For example, <code>soup.find('.logo').get('href')</code> would find an element with the class \"logo\", and return the url to that image.</p><h3 id=\"pesky-tags-to-deal-with\">Pesky Tags to Deal With</h3><p>In our example of creating link previews, a good first source of information would obviously be the page's meta tags: specifically the <code>og</code> tags they've specified to openly provide the bite-sized information we're looking for. Grabbing these tags are a bit more difficult to deal with:</p><pre><code class=\"language-python\">soup.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n</code></pre>\n<p>Oh yeah, now that's some ugly shit right there. Meta tags are especially interesting because they're all uselessly dubbed 'meta', thus we need a second differentiator in addition to the tag name to specify <em>which </em>meta tag we care about. Only then can we bother to <em>get</em> the actual content of said tag.</p><h2 id=\"step-3-realizing-something-will-always-break\">Step 3: Realizing Something Will Always Break</h2><p>If we were to try the above selector on an HTML page which did not contain an <code>og:description</code>, our script would break unforgivingly. Not only do we miss this data, but we miss out on everything entirely - this means we always need to build in a plan B, and at the very least deal with a lack of tag altogether.</p><p>It's best to break out this logic one tag at a time. First, let's look at an example for a base scraper with all the knowledge we have so far:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p &gt; a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n</code></pre>\n<p>Great - there's a base function for snatching all links out of the body of a page. Ultimately we'll create a JSON object for each of these links containing preview data, <code>link_preview_dict</code>.</p><p>To handle each value of our dict, we have individual functions:</p><pre><code class=\"language-python\">def getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get a preview image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n</code></pre>\n<p>In case you're wondering:</p><ul><li><strong>getTitle </strong>tries to get the <code>&lt;title&gt;</code> tag, and falls back to the page's first <code>&lt;h1&gt;</code> tag (surprisingly enough some pages are in fact missing a title).</li><li><strong>getDescription</strong> looks for the OG description, and falls back to the content of the page's first paragraph.</li><li><strong>getImage </strong>looks for the OG image, and falls back to the page's first image.</li><li><strong>getSiteName </strong>similarly tries to grab the OG attribute, otherwise it does it's best to extract the domain name from the URL string under the assumption that this is the origin's name (look, it ain't perfect).</li></ul><h2 id=\"what-did-we-just-build\">What Did We Just Build?</h2><p>Believe it or not, the above is considered to be enough logic to be a paid service with a monthly fee. Go ahead and Google it; or better yet, just steal my source code entirely:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p &gt; a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)\n</code></pre>\n","url":"https://hackersandslackers.com/scraping-urls-with-beautifulsoup/","uuid":"c933218e-6bbf-44b7-8f01-bfd188c71d89","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be7fc282ec6e0035b4b16bc"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372e","title":"MySQL, Google Cloud, and a REST API that Generates Itself","slug":"mysql-google-cloud-and-a-rest-api-that-autogenerates","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","custom_excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","created_at_pretty":"23 October, 2018","published_at_pretty":"23 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-23T14:57:12.000-04:00","published_at":"2018-10-23T18:47:28.000-04:00","updated_at":"2019-02-02T05:26:16.000-05:00","meta_title":"MySQL, Google Cloud, and a REST API | Hackers and Slackers","meta_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","og_title":"MySQL, Google Cloud, and a REST API that Generates Itself","twitter_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","twitter_title":"MySQL, Google Cloud, and a REST API that Generates Itself","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn’t too long ago that I haphazardly forced us down a journey of exploring\nGoogle Cloud’s cloud SQL service. The focus of this exploration was Google’s\naccompanying REST API for all of its cloud SQL instances. That API turned out to\nbe a relatively disappointing administrative API which did little to extend the\nfeatures you’d expect from the CLI or console.\n\nYou see, I’ve had a dream stuck in my head for a while now. Like most of my\nutopian dreams, this dream is related to data, or more specifically simplifying\nthe manner in which we interact with it. For industry synonymous with AI and\nautomation, many of our very own tools (including ETL tools) involve way too\nmuch manual effort in my opinion. That’s right: I’m talking about the aspiration\nto Slack while we Hack.\n\nThe pitch is this: why do we keep setting up databases, endpoints, and the logic\nto connect them when, 90% of the time, we’re building the same thing over and\nover? Let me guess: there’s a GET endpoint to get records from table X, or a\nPOST endpoint to create users. I know you’ve built this because we all have, but\nwhy do we keep building the same things over and over in isolation? It looks\nlike we might not have to anymore, but first let’s create our database.\n\nCreating a MySQL Instance in GCP \nFull disclosure here: the magical REST API thing is actually independent from\nGoogle Cloud; the service we’ll be using can integrate with any flavor of MySQL\nyou prefer, so go ahead and grab that RDS instance you live so much if you\nreally have to.\n\nFor the rest of us, hit up your GCP console and head into making a new SQL\ninstance. MySQL and Postgres are our only choices here; stick with MySQL.\n\nThere isn’t much to spinning up your instance. Just be sure to create a user and\ndatabase to work from.\n\nOh yeah, and remember to name your instance.Your SQL Firewall and Permissions\nYour instance is set to “public” by default. Oddly, “public” in this case means\n“accessible to everybody on your IP whitelist, which is empty by default,” so\nreally kind of the opposite of public really.\n\nIn fact, if you hypothetically did want to open your instance publicly, Google\nCloud will not allow it. This is good on them, and is actually fairly impressive\nthe depths they go to avoid the IP 0.0.0.0  from ever appearing anywhere in the\ninstance. Go ahead, open the shell and try to add bind address=0.0.0.0 \nyourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s\nversion of MySQL is actually a MariaDB instance)?\n\nThe point is, whitelist your IP address. Simply \"Edit\" your instance and add\nyour address to the authorized networks.\n\nAuthorize that bad boy.The Magic API \nNow, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so\nthis next part is going to feel a bit a bit weird. I’m not sure why, as the\nservice is apparently free, thus I’m clearly not getting paid for any of this.\n\nAnyway, the service is called Apisentris [https://apisentris.com/], and the idea\nis that it will build whatever time-consuming monstrosity of a REST API you were\nplanning to build to access your data for you. Via their own words:\n\nSee, I told you.What does this actually mean? It means if you create a table\ncalled articles  in your database, you will immediately have an endpoint to\nfetch said articles, and it would look like \nhttps://apisentris.com/api/v1/articles. Your client ID and credentials would\nobviously need to be provided to indicate that you're, well, you.\n\nGrabbing entire tables at once would be silly, which is why they also\nautogenerate filters based on the contents of your table:\n\nEndpoints accept query parameters to essentially create a query.Oh yeah, and you\ncan also handle user management via this API as well, if you're building an\nactual app:\n\nPretty easy to hook up into a form or whatever.I'll assume you're sold on the\nidea by now. If a free service that handles the hard parts of backend logic for\nfree isn't your cup of tea, clearly you aren't Slacker material.\n\nSetting it all up\nAs we did before with our own IP, we'll need to whitelist Apisentris' IP the\nsame way in GCP console. Their IP is 104.199.181.125.\n\nCreate a table in your database with some data just to test things out. When\nyou're logged in, you'll be able to see all the endpoints available to you and\nthe associated attributes they have:\n\nNot bad.Any way you slice it, the concept of a self-generating API is very cool\nand yet somehow still not the norm. I'm actually shocked that there are so few\npeople in the Data industry who know \"there must be a better way,\" but then\nagain, data science and software engineering are two very different things. For\nmy fellow Data Engineers out there, take this as a gift and a curse: you have\nthe gift of knowing better from your software background, but are cursed with\nwatching the world not quite realize how pointless half the things they do truly\nare.\n\nOh well. We'll be the ones building the robots anyway.","html":"<p>It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively disappointing administrative API which did little to extend the features you’d expect from the CLI or console.</p><p>You see, I’ve had a dream stuck in my head for a while now. Like most of my utopian dreams, this dream is related to data, or more specifically simplifying the manner in which we interact with it. For industry synonymous with AI and automation, many of our very own tools (including ETL tools) involve way too much manual effort in my opinion. That’s right: I’m talking about the aspiration to Slack while we Hack.</p><p>The pitch is this: why do we keep setting up databases, endpoints, and the logic to connect them when, 90% of the time, we’re building the same thing over and over? Let me guess: there’s a GET endpoint to get records from table X, or a POST endpoint to create users. I know you’ve built this because we all have, but why do we keep building the same things over and over in isolation? It looks like we might not have to anymore, but first let’s create our database.</p><h2 id=\"creating-a-mysql-instance-in-gcp\">Creating a MySQL Instance in GCP </h2><p>Full disclosure here: the magical REST API thing is actually independent from Google Cloud; the service we’ll be using can integrate with any flavor of MySQL you prefer, so go ahead and grab that RDS instance you live so much if you really have to.</p><p>For the rest of us, hit up your GCP console and head into making a new SQL instance. MySQL and Postgres are our only choices here; stick with MySQL.</p><p>There isn’t much to spinning up your instance. Just be sure to create a user and database to work from.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.15.18-PM.png\" class=\"kg-image\"><figcaption>Oh yeah, and remember to name your instance.</figcaption></figure><h3 id=\"your-sql-firewall-and-permissions\">Your SQL Firewall and Permissions</h3><p>Your instance is set to “public” by default. Oddly, “public” in this case means “accessible to everybody on your IP whitelist, which is empty by default,” so really kind of the opposite of public really.</p><p>In fact, if you hypothetically did want to open your instance publicly, Google Cloud will not allow it. This is good on them, and is actually fairly impressive the depths they go to avoid the IP <strong>0.0.0.0</strong> from ever appearing anywhere in the instance. Go ahead, open the shell and try to add <code>bind address=0.0.0.0</code> yourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s version of MySQL is actually a MariaDB instance)?</p><p>The point is, whitelist your IP address. Simply \"Edit\" your instance and add your address to the authorized networks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.12.23-PM.png\" class=\"kg-image\"><figcaption>Authorize that bad boy.</figcaption></figure><h2 id=\"the-magic-api\">The Magic API </h2><p>Now, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so this next part is going to feel a bit a bit weird. I’m not sure why, as the service is apparently free, thus I’m clearly not getting paid for any of this.</p><p>Anyway, the service is called <strong><a href=\"https://apisentris.com/\">Apisentris</a>, </strong>and the idea is that it will build whatever time-consuming monstrosity of a REST API you were planning to build to access your data for you. Via their own words:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.19.16-PM.png\" class=\"kg-image\"><figcaption>See, I told you.</figcaption></figure><p>What does this actually mean? It means if you create a table called <em>articles</em> in your database, you will immediately have an endpoint to fetch said articles, and it would look like <strong>https://apisentris.com/api/v1/articles. </strong>Your client ID and credentials would obviously need to be provided to indicate that you're, well, you.</p><p>Grabbing entire tables at once would be silly, which is why they also autogenerate filters based on the contents of your table:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.25.13-PM.png\" class=\"kg-image\"><figcaption>Endpoints accept query parameters to essentially create a query.</figcaption></figure><p>Oh yeah, and you can also handle user management via this API as well, if you're building an actual app:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.27.43-PM.png\" class=\"kg-image\"><figcaption>Pretty easy to hook up into a form or whatever.</figcaption></figure><p>I'll assume you're sold on the idea by now. If a free service that handles the hard parts of backend logic for free isn't your cup of tea, clearly you aren't Slacker material.</p><h2 id=\"setting-it-all-up\">Setting it all up</h2><p>As we did before with our own IP, we'll need to whitelist Apisentris' IP the same way in GCP console. Their IP is <code>104.199.181.125</code>.</p><p>Create a table in your database with some data just to test things out. When you're logged in, you'll be able to see all the endpoints available to you and the associated attributes they have:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/schema.gif\" class=\"kg-image\"><figcaption>Not bad.</figcaption></figure><p>Any way you slice it, the concept of a self-generating API is very cool and yet somehow still not the norm. I'm actually shocked that there are so few people in the Data industry who know \"there must be a better way,\" but then again, data science and software engineering are two very different things. For my fellow Data Engineers out there, take this as a gift and a curse: you have the gift of knowing better from your software background, but are cursed with watching the world not quite realize how pointless half the things they do truly are.</p><p>Oh well. We'll be the ones building the robots anyway.</p>","url":"https://hackersandslackers.com/mysql-google-cloud-and-a-rest-api-that-autogenerates/","uuid":"c45478bb-54da-4563-89bd-ddd356a234d4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bcf6f08d7ab443ba8b7a5ab"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372d","title":"Create Google Cloud Functions Running Python 3.7","slug":"creating-a-python-google-cloud-function","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","excerpt":"GCP scores another victory by trivializing serverless functions.","custom_excerpt":"GCP scores another victory by trivializing serverless functions.","created_at_pretty":"18 October, 2018","published_at_pretty":"19 October, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-10-18T19:44:02.000-04:00","published_at":"2018-10-18T22:33:07.000-04:00","updated_at":"2019-02-13T23:13:40.000-05:00","meta_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","meta_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","og_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","og_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","og_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","twitter_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","twitter_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"The more I explore Google Cloud's endless catalog of cloud services, the more I\nreally like Google Cloud. This is why before moving forward, I'd like to be\ntransparent that this blog has become little more than thinly veiled Google\npropaganda, where I will henceforth bombard you with persuasive and subtle\nmessaging to sell your soul to Google. Let's be honest; they've probably\nsimulated it anyway.\n\nIt should be safe to assume that you're fairly familiar with AWS Lambda\nFunctions [https://hackersandslackers.com/creating-endpoints-with-lambda/],\nwhich have served as the backbone of what we refer to as \"serverless.\" These\ncode snippets in the cloud have restructured entire IT departments and are\npartially why almost nobody knows enough basic Linux to configure a web server\nor build anything without a vendor. In my opinion, Google Cloud Functions are\nbetter than that, so strap in.\n\nAWS vs GCP Comparison\nFirst off, let's talk about a big one: price. AWS charges based on Lambda usage,\nwhereas Google Cloud Functions are free. The only exception to this is when you\nbreak 2 million invocations/month, at which point you'll be hemorrhaging as\nghastly 40 cents per additional million. That's ridiculous. I think we've just\ndiscovered Google Cloud's lead generation strategy.\n\nWhat about in terms of workflow? AWS holds an architecture philosophy of\nchaining services together, into what inevitably becomes a web of self-contained\nbillable items on your invoice. A fine illustration of this is a fine post on \ncommon AWS patterns\n[https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/]  which\nprovides a decent visual of this complexity, while also revealing how much\npeople apparently love this kind of shit, as though SaaS is the new Legos. To\ninteract with a Lambda function in AWS via HTTP requests, you need to set up an\nAPI Gateway in front. I hate setting up API Gateways: it's a feat more\nconvoluted and difficult than actually coding. Pair this with an inevitable user\npermission struggle just to get the right Lambda roles set up, and you quickly\nhave yourself a nightmare- especially  if you're just trying to get a single\nfunction live.  Eventually you’ll get to write some code or upload a horrendous\nzip file like some sort of neanderthal (friendly reminder: I am entirely\nbiased).\n\nGCP has clearly been taking notes on the sidelines on how to improve this\nprocess by removing red tape around service setup or policy configuration. AWS\nand GCP are tackling opposites approaches; AWS allows you to build a Robust API\ncomplete with staging and testing with the intent that some of these APIs can\neven be sold as standalone products to consumers. GCP takes the opposite\napproach: cloud functions are services intended for developers to develop, which\ncovers the vast majority of use cases in my opinion.\n\nCloud Function Deployment\nTo create our first function to serve as an endpoint, we'll utilize the\nfollowing:\n\n * A new Cloud Function  running Python 3.7\n * Google's Source Repositories: AKA a Github/Bitbucket clone with auto-syncing\n   to your real repos, along with direct access to GCP services (think Heroku's\n   source control).\n * The gcloud  CLI to enable us to work locally.\n\nYou should immediately notice the glaring lack of any mentions of API endpoints,\nmethods, stages, or anything related to handling web requests. It should not be\nunderstated that Cloud Functions are preconfigured with an endpoint, and all\nnonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is\nmissing entirely. These things are handled in the function itself, and because\nGoogle Cloud functions running Python are preconfigured with Flask, all of that\nstuff is really trivially easy.  That's right, we've got Flask, Python,  and GCP \n all in a single post. Typing these words feels like eating cake while Dwyane\nThe Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's\ngreat.\n\nCreate your Function\nOur function will intend to serve as a Python HTTP endpoint:\n\nSingle-page setup. Easy. * Trigger  specifies what will have access to this function. By selecting HTTP,\n   we will immediately receive a URL.\n * Source code  gives us a few options to deploy our code with cloud source\n   repository  being by far the easiest solution, especially when working\n   locally.\n * Runtime  allows you to select NodeJS by accident.\n\nBefore we get to code, let's talk Python libraries.\n\nIncluding Dependencies in your Function\nOur function comes prepared with two files: main.py  and our friend \nrequirements.txt. These files do exactly what you'd expect, as per every project\never:\n\nUnfortunately, ease-of-use ensures that GCP certifications will be in low\ndemand.Our function immediately installs all dependencies in requirements.txt  for use\nupon deployment. Once deployed, we can import these libraries as expected. So,\nlet's deploy something.\n\nGoogle Source Repositories\nGoogle's source repositories can serve as a stand-in replacement for Github\n(unlikely), or auto-sync to any repo on the version control behemoth of your\nchoice. The advantage of this extra layer is mostly to trigger deployments upon\ncommits, which in turn feed into GCP's own CI/CD processes (which remain young\nfor now). Create a repo locally using gcloud:\n\n$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n\n\nNow make that puppy go live with gcloud functions deploy totally-dope-function,\nwhere totally-dope-function  is name of your function, as it should be.\n\nNow let's get to the coding part.\n\nThe Coding Part (ft. Flask)\nHere's perhaps the most basic endpoint you'll ever create:\n\nimport requests\n\ndef endpoint(request):\n    \"\"\"Does the things.\"\"\"\n    if request.method == 'POST':\n    # Allows POST requests from any origin with the Content-Type\n    # header and caches preflight response for an 3600s\n    headers = {\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'POST',\n        'Access-Control-Allow-Headers': 'Content-Type',\n        'Access-Control-Max-Age': '3600'\n    }\n    request_json = request.get_json()\n    if request_json:\n        plaintext = request_json['plain']\n        html = request_json['html']\n        return html\n    else:\n        return 'You didn't pass a JSON body you idiot.'\n\n\nIf you're familiar with Flask (you are, because you're on this blog) you already\nknow what all of this does. Look at that simple copy-paste of headers, as\nopposed to working them into a horrible web interface. Gasp in disbelief as you\nrealize that typing if request.method == 'POST':  would be a 10-minute task in a\nvisual API building tool. We've made it fam.\n\nEase of Logging\nBecause we have a real endpoint to work with, we don't need to waste any time\nsimulating stupid fucking tests where we send fake JSON to our function. We can\nuse postman or anything to immediately interact with our endpoint, and the logs\nare a click away:\n\nEasy. Breezy. Beautiful.There's no point in me droning on at this point because\nyou've surely already ventured into the Google Cloud console in blissful\ndisbelief as a good obedient drone would. If adopting Google Cloud is the last\nshred of hope we have to resist Google's all-knowing algorithms which have\nalready replaced the illusion of free will, I'd gladly take that dystopia over\nsetting up monolithic API Gateways any day.\n\nThe Downsides\nTime for the asterisks to kill that euphoric buzz you might've experienced for a\nbrief moment. My sole purpose as an engineer is to have my dreams crushed\nfull-time; I simply cant resist returning the favor.\n\nFirst notable drawback of Cloud functions is a lack of out-of-the-box custom DNS \n configuration. Firebase has workarounds for this, but Firebase is a beast of\nits own.\n\nWhen it comes to debugging, functions tend to fall short in comparison to their\nLambda rivals. Most Cloud Function debugging involves deploying, testing in dev,\nand sifting through cryptic error logs (they can be quite bad). There's nearly\nno UI mock testing  to speak of. You'd better brush up on PyTest.\n\nMy best advice is to be careful with what services you play around with on GCP.\nLet's not forget this is a platform geared exclusively towards enterprises; the\nfact that we're even playing ball here makes us weirdos in the first place.\nDon't let yourself hemorrhage money like an enterprise.","html":"<p>The more I explore Google Cloud's endless catalog of cloud services, the more I really like Google Cloud. This is why before moving forward, I'd like to be transparent that this blog has become little more than thinly veiled Google propaganda, where I will henceforth bombard you with persuasive and subtle messaging to sell your soul to Google. Let's be honest; they've probably simulated it anyway.</p><p>It should be safe to assume that you're fairly familiar with AWS <a href=\"https://hackersandslackers.com/creating-endpoints-with-lambda/\">Lambda Functions</a>, which have served as the backbone of what we refer to as \"serverless.\" These code snippets in the cloud have restructured entire IT departments and are partially why almost nobody knows enough basic Linux to configure a web server or build anything without a vendor. In my opinion, Google Cloud Functions are better than <em>that</em>, so strap in.</p><h2 id=\"aws-vs-gcp-comparison\">AWS vs GCP Comparison</h2><p>First off, let's talk about a big one: price. AWS charges based on Lambda usage, whereas Google Cloud Functions are <strong>free</strong>. The only exception to this is when you break 2 million invocations/month, at which point you'll be hemorrhaging as ghastly <strong>40 cents per additional million. </strong>That's ridiculous. I think we've just discovered Google Cloud's lead generation strategy.</p><p>What about in terms of workflow? AWS holds an architecture philosophy of chaining services together, into what inevitably becomes a web of self-contained billable items on your invoice. A fine illustration of this is a fine post on <a href=\"https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/\">common AWS patterns</a> which provides a decent visual of this complexity, while also revealing how much people apparently love this kind of shit, as though SaaS is the new Legos. To interact with a Lambda function in AWS via HTTP requests, you need to set up an API Gateway in front. I hate setting up API Gateways: it's a feat more convoluted and difficult than actually coding. Pair this with an inevitable user permission struggle just to get the right Lambda roles set up, and you quickly have yourself a nightmare- <em>especially</em> if you're just trying to get a single function live.<em> </em>Eventually you’ll get to write some code or upload a horrendous zip file like some sort of neanderthal (friendly reminder: I am entirely biased).</p><p>GCP has clearly been taking notes on the sidelines on how to improve this process by removing red tape around service setup or policy configuration. AWS and GCP are tackling opposites approaches; AWS allows you to build a Robust API complete with staging and testing with the intent that some of these APIs can even be sold as standalone products to consumers. GCP takes the opposite approach: cloud functions are services intended for developers to develop, which covers the vast majority of use cases in my opinion.</p><h2 id=\"cloud-function-deployment\">Cloud Function Deployment</h2><p>To create our first function to serve as an endpoint, we'll utilize the following:</p><ul><li>A new <strong>Cloud Function</strong> running Python 3.7</li><li>Google's <strong>Source Repositories: </strong>AKA a Github/Bitbucket clone with auto-syncing to your real repos, along with direct access to GCP services (think Heroku's source control).</li><li>The <strong>gcloud</strong> CLI to enable us to work locally.</li></ul><p>You should immediately notice the glaring lack of any mentions of API endpoints, methods, stages, or anything related to handling web requests. It should not be understated that <em>Cloud Functions are preconfigured with an endpoint</em>, and all nonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely. These things are handled in the function itself, and because Google Cloud functions running Python are preconfigured with <strong>Flask, </strong>all of that stuff is <em>really trivially easy.</em> That's right, we've got <em>Flask</em>, <em>Python</em>,<em> </em>and <em>GCP</em> all in a single post. Typing these words feels like eating cake while Dwyane The Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's great.</p><h3 id=\"create-your-function\">Create your Function</h3><p>Our function will intend to serve as a Python HTTP endpoint:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/func.gif\" class=\"kg-image\"><figcaption>Single-page setup. Easy.</figcaption></figure><ul><li><strong>Trigger</strong> specifies what will have access to this function. By selecting HTTP, we will immediately receive a URL.</li><li><strong>Source code</strong> gives us a few options to deploy our code with <em>cloud source repository</em> being by far the easiest solution, especially when working locally.</li><li><strong>Runtime</strong> allows you to select NodeJS by accident.</li></ul><p>Before we get to code, let's talk Python libraries.</p><h3 id=\"including-dependencies-in-your-function\">Including Dependencies in your Function</h3><p>Our function comes prepared with two files: <code>main.py</code> and our friend <code>requirements.txt</code>. These files do exactly what you'd expect, as per every project ever:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-18-at-9.14.17-PM.png\" class=\"kg-image\"><figcaption>Unfortunately, ease-of-use ensures that GCP certifications will be in low demand.</figcaption></figure><p>Our function immediately installs all dependencies in <code>requirements.txt</code> for use upon deployment. Once deployed, we can import these libraries as expected. So, let's deploy something.</p><h3 id=\"google-source-repositories\">Google Source Repositories</h3><p>Google's source repositories can serve as a stand-in replacement for Github (unlikely), or auto-sync to any repo on the version control behemoth of your choice. The advantage of this extra layer is mostly to trigger deployments upon commits, which in turn feed into GCP's own CI/CD processes (which remain young for now). Create a repo locally using gcloud:</p><pre><code class=\"language-bash\">$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n</code></pre>\n<p>Now make that puppy go live with <code>gcloud functions deploy totally-dope-function</code>, where <em><strong>totally-dope-function</strong> </em>is name of your function, as it should be.</p><p>Now let's get to the coding part.</p><h2 id=\"the-coding-part-ft-flask-\">The Coding Part (ft. Flask)</h2><p>Here's perhaps the most basic endpoint you'll ever create:</p><pre><code class=\"language-python\">import requests\n\ndef endpoint(request):\n    &quot;&quot;&quot;Does the things.&quot;&quot;&quot;\n    if request.method == 'POST':\n    # Allows POST requests from any origin with the Content-Type\n    # header and caches preflight response for an 3600s\n    headers = {\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'POST',\n        'Access-Control-Allow-Headers': 'Content-Type',\n        'Access-Control-Max-Age': '3600'\n    }\n    request_json = request.get_json()\n    if request_json:\n        plaintext = request_json['plain']\n        html = request_json['html']\n        return html\n    else:\n        return 'You didn't pass a JSON body you idiot.'\n</code></pre>\n<p>If you're familiar with Flask (you are, because you're on this blog) you already know what all of this does. Look at that simple copy-paste of headers, as opposed to working them into a horrible web interface. Gasp in disbelief as you realize that typing <code>if request.method == 'POST':</code> would be a 10-minute task in a visual API building tool. We've made it fam.</p><h3 id=\"ease-of-logging\">Ease of Logging</h3><p>Because we have a real endpoint to work with, we don't need to waste any time simulating stupid fucking tests where we send fake JSON to our function. We can use postman or anything to immediately interact with our endpoint, and the logs are a click away:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/logs.gif\" class=\"kg-image\"><figcaption>Easy. Breezy. Beautiful.</figcaption></figure><p>There's no point in me droning on at this point because you've surely already ventured into the Google Cloud console in blissful disbelief as a good obedient drone would. If adopting Google Cloud is the last shred of hope we have to resist Google's all-knowing algorithms which have already replaced the illusion of free will, I'd gladly take that dystopia over setting up monolithic <em>API Gateways </em>any day.</p><h2 id=\"the-downsides\">The Downsides</h2><p>Time for the asterisks to kill that euphoric buzz you might've experienced for a brief moment. My sole purpose as an engineer is to have my dreams crushed full-time; I simply cant resist returning the favor.</p><p>First notable drawback of Cloud functions is a <strong>lack of out-of-the-box custom DNS</strong> configuration. Firebase has workarounds for this, but Firebase is a beast of its own.</p><p>When it comes to debugging, functions tend to fall short in comparison to their Lambda rivals. Most Cloud Function debugging involves deploying, testing in dev, and sifting through cryptic error logs (they can be quite bad). There's nearly no <strong>UI mock testing</strong> to speak of. You'd better brush up on PyTest.</p><p>My best advice is to <em>be careful </em>with what services you play around with on GCP. Let's not forget this is a platform geared exclusively towards enterprises; the fact that we're even playing ball here makes us weirdos in the first place. Don't let yourself hemorrhage money like an enterprise.</p>","url":"https://hackersandslackers.com/creating-a-python-google-cloud-function/","uuid":"ec428cb9-976e-4578-a3de-9120a0dd7352","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bc91ac23d1eab214413b12b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867371b","title":"Extract Nested Data From Complex JSON","slug":"extract-data-from-complex-json-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","excerpt":"Never manually walk through complex JSON objects again by using this function.","custom_excerpt":"Never manually walk through complex JSON objects again by using this function.","created_at_pretty":"10 October, 2018","published_at_pretty":"10 October, 2018","updated_at_pretty":"22 January, 2019","created_at":"2018-10-10T00:15:29.000-04:00","published_at":"2018-10-10T08:00:00.000-04:00","updated_at":"2019-01-22T15:20:23.000-05:00","meta_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","meta_description":"Never manually walk through complex JSON objects again by using this function","og_description":"Never manually walk through complex JSON objects again by using this function","og_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","og_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","twitter_description":"Never manually walk through complex JSON objects again by using this function","twitter_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","twitter_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"We're all data people here, so you already know the scenario: it happens perhaps\nonce a day, perhaps 5, or even more. There's an API you're working with, and\nit's great. It contains all the information you're looking for, but there's just\none problem: the complexity of nested JSON objects is endless, and suddenly the\njob you love needs to be put on hold to painstakingly retrieve the data you\nactually want, and it's 5 levels deep in a nested JSON hell. Nobody feels like\nmuch of a \"scientist\" or an \"engineer\" when half their day becomes dealing with\nkey value errors.\n\nLuckily, we code in Python!  (okay fine, language doesn't make much of a\ndifference here. It felt like a rallying call at the time).\n\nUsing Google Maps API as an Example\nTo visualize the problem, let's take an example somebody might actually want to\nuse.  I think the  Google Maps API is a good candidate to fit the bill here.\n\nWhile Google Maps is actually a collection of APIs, the Google Maps Distance\nMatrix [https://developers.google.com/maps/documentation/distance-matrix/start].\nThe idea is that with a single API call, a user can calculate the distance and\ntime traveled between an origin and an infinite number of destinations. It's a\ngreat full-featured API, but as you might imagine the resulting JSON for\ncalculating commute time between where you stand and every location in the\nconceivable universe  makes an awfully complex JSON structure.\n\nGetting a Taste of JSON Hell\nReal quick, here's an example of the types of parameters this request accepts:\n\nimport requests\nimport API_KEY\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Philadelphia,PA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n\n\nOne origin, one destination. The JSON response for a request this\nstraightforward is quite simple:\n\n{\n    \"destination_addresses\": [\n        \"Philadelphia, PA, USA\"\n    ],\n    \"origin_addresses\": [\n        \"New York, NY, USA\"\n    ],\n    \"rows\": [\n        {\n            \"elements\": [\n                {\n                    \"distance\": {\n                        \"text\": \"94.6 mi\",\n                        \"value\": 152193\n                    },\n                    \"duration\": {\n                        \"text\": \"1 hour 44 mins\",\n                        \"value\": 6227\n                    },\n                    \"status\": \"OK\"\n                }\n            ]\n        }\n    ],\n    \"status\": \"OK\"\n}\n\n\nFor each destination, we're getting two data points: the commute distance, and \nestimated duration. If we hypothetically wanted to extract those values, typing \nresponse['rows'][0]['elements']['distance']['test']  isn't too  crazy. I mean,\nit's somewhat awful and brings on casual thoughts of suicide, but nothing out of\nthe ordinary\n\nNow let's make things interesting by adding a few more stops on our trip:\n\nimport requests \nimport API_KEY\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa County,CA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n\n\nOh fuuucckkkk:\n\n{\n  \"destination_addresses\": [\n    \"Washington, DC, USA\",\n    \"Philadelphia, PA, USA\",\n    \"Santa Barbara, CA, USA\",\n    \"Miami, FL, USA\",\n    \"Austin, TX, USA\",\n    \"Napa County, CA, USA\"\n  ],\n  \"origin_addresses\": [\n    \"New York, NY, USA\"\n  ],\n  \"rows\": [\n    {\n      \"elements\": [\n        {\n          \"distance\": {\n            \"text\": \"227 mi\",\n            \"value\": 365468\n          },\n          \"duration\": {\n            \"text\": \"3 hours 54 mins\",\n            \"value\": 14064\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"94.6 mi\",\n            \"value\": 152193\n          },\n          \"duration\": {\n            \"text\": \"1 hour 44 mins\",\n            \"value\": 6227\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"2,878 mi\",\n            \"value\": 4632197\n          },\n          \"duration\": {\n            \"text\": \"1 day 18 hours\",\n            \"value\": 151772\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"1,286 mi\",\n            \"value\": 2069031\n          },\n          \"duration\": {\n            \"text\": \"18 hours 43 mins\",\n            \"value\": 67405\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"1,742 mi\",\n            \"value\": 2802972\n          },\n          \"duration\": {\n            \"text\": \"1 day 2 hours\",\n            \"value\": 93070\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"2,871 mi\",\n            \"value\": 4620514\n          },\n          \"duration\": {\n            \"text\": \"1 day 18 hours\",\n            \"value\": 152913\n          },\n          \"status\": \"OK\"\n        }\n      ]\n    }\n  ],\n  \"status\": \"OK\"\n}\n\n\nA lot is happening here. There are objects. There are lists. There are lists of\nobjects which are part of an object. The last thing I'd want to deal with is\ntrying to parse this data only to accidentally get a useless key:value pair like\n \"status\": \"OK\".\n\nCode Snippet To The Rescue\nLet's say we only want the human-readable data from this JSON, which is labeled \n\"text\"  for both distance and duration. We've created a function below dubbed \nextract_values()  to help us resolve this very issue. The idea is that \nextract_values()  is flexible and agnostic, therefore can be imported as a\nmodule into any project you might need.\n\n# recursivejson.py\n\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n\nWe need to pass this function two values:\n\n * A JSON object, such as r.json()  from an API request.\n * The name of the key  we're looking to extract values from.\n\nnames = extract_values('myjson.json', 'name')\nprint(names)\n\n\nRegardless of where the key \"text\"  lives in the JSON, this function returns\nevery value for the instance of \"key.\" Here's our function in action:\n\nimport requests\nimport API_KEY\nfrom recursivejson import extract_values\n\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': \"New York City,NY\",\n       'destinations': \"Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa Valley,CA\",\n       'transit_mode': 'car',\n    }\n\n   r = requests.get(endpoint, params=params)\n   travel_values = extract_values(r.json(), 'text')\n   return travel_values\n\n\nRunning this function will result in the following output:\n\n['227 mi', '3 hours 54 mins', '94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n\n\nOh fiddle me timbers! Because the Google API alternates between distance and \ntrip duration, every other value alternates between distance and time (can we\npause to appreciate this horrible design? There are infinitely better ways to\nstructure this response). Never fear, some simple Python can help us split this\nlist into two lists:\n\nmy_values = extract_values(r.json(), 'text')\n\ndurations = my_values[1::2]\ndistances = my_values[2::1]\n\nprint('DURATIONS = ', durations)\nprint('DISTANCES = ', distances)\n\n\nThis will take our one list and split it in to two  lists, alternating between\neven and odd:\n\nDURATIONS = ['3 hours 54 mins', '1 hour 44 mins', '1 day 18 hours', '18 hours 43 mins', '1 day 2 hours', '1 day 18 hours']\nDISTANCES = ['94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n\n\nGetting Creative With Lists\nA common theme I run in to while extracting lists of values from JSON objects\nlike these is that the lists of values I extract are very much related.  In the\nabove example, for every duration  we have an accompanying distance, which is a\none-to-one basis. Imagine if we wanted to associate these values somehow?\n\nTo use a better example, I recently I used this exact_values()  function to\nextract lists of column names and their data types from a database schema. As\nseparate lists, the data looked something like this:\n\ncolumn_names = ['index', 'first_name', 'last_name', 'join_date']\ncolumn_datatypes = ['integer', 'string', 'string', 'date']\n\n\nClearly these two lists are directly related; the latter is describing the\nformer. How can this be useful? By using Python's zip  method!\n\nschema_dict = dict(zip(column_names, column_datatypes))\nprint(schema_dict)\n\n\nI like to think they call it zip  because it's like zipping up a zipper, where\neach side of the zipper is a list. This output a dictionary where list 1 serves\nas the keys, and list 2 serves as values:\n\n{\n'index': 'integer', \n'first_name': 'string', \n'last_name':'string',\n'join_date': 'date'\n}\n\n\nAnd there you have it folks: a free code snippet to copy and secretly pretend\nyou wrote forever. I've thrown the function up on Github Gists\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b], if such\na thing pleases you.\n\nIn the meantime, zip it up and zip it out. Zippity-do-da, buh bye.","html":"<p>We're all data people here, so you already know the scenario: it happens perhaps once a day, perhaps 5, or even more. There's an API you're working with, and it's great. It contains all the information you're looking for, but there's just one problem: the complexity of nested JSON objects is endless, and suddenly the job you love needs to be put on hold to painstakingly retrieve the data you actually want, and it's 5 levels deep in a nested JSON hell. Nobody feels like much of a \"scientist\" or an \"engineer\" when half their day becomes dealing with key value errors.</p><p>Luckily, we code in <strong><em>Python!</em></strong> (okay fine, language doesn't make much of a difference here. It felt like a rallying call at the time).</p><h2 id=\"using-google-maps-api-as-an-example\">Using Google Maps API as an Example</h2><p>To visualize the problem, let's take an example somebody might actually want to use.  I think the<strong> Google Maps API </strong>is a good candidate to fit the bill here.</p><p>While Google Maps is actually a collection of APIs, the <a href=\"https://developers.google.com/maps/documentation/distance-matrix/start\">Google Maps Distance Matrix</a>. The idea is that with a single API call, a user can calculate the distance and time traveled between an origin and an infinite number of destinations. It's a great full-featured API, but as you might imagine the resulting JSON for calculating commute time between where you stand and <em>every location in the conceivable universe</em> makes an awfully complex JSON structure.</p><h3 id=\"getting-a-taste-of-json-hell\">Getting a Taste of JSON Hell</h3><p>Real quick, here's an example of the types of parameters this request accepts:</p><pre><code class=\"language-python\">import requests\nimport API_KEY\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Philadelphia,PA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n</code></pre>\n<p>One origin, one destination. The JSON response for a request this straightforward is quite simple:</p><pre><code class=\"language-json\">{\n    &quot;destination_addresses&quot;: [\n        &quot;Philadelphia, PA, USA&quot;\n    ],\n    &quot;origin_addresses&quot;: [\n        &quot;New York, NY, USA&quot;\n    ],\n    &quot;rows&quot;: [\n        {\n            &quot;elements&quot;: [\n                {\n                    &quot;distance&quot;: {\n                        &quot;text&quot;: &quot;94.6 mi&quot;,\n                        &quot;value&quot;: 152193\n                    },\n                    &quot;duration&quot;: {\n                        &quot;text&quot;: &quot;1 hour 44 mins&quot;,\n                        &quot;value&quot;: 6227\n                    },\n                    &quot;status&quot;: &quot;OK&quot;\n                }\n            ]\n        }\n    ],\n    &quot;status&quot;: &quot;OK&quot;\n}\n</code></pre>\n<p>For each destination, we're getting two data points: the <em>commute distance</em>, and <em>estimated duration</em>. If we hypothetically wanted to extract those values, typing <code>response['rows'][0]['elements']['distance']['test']</code> isn't <em>too</em> crazy. I mean, it's somewhat awful and brings on casual thoughts of suicide, but nothing out of the ordinary</p><p>Now let's make things interesting by adding a few more stops on our trip:</p><pre><code class=\"language-python\">import requests \nimport API_KEY\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa County,CA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n</code></pre>\n<p>Oh fuuucckkkk:</p><pre><code class=\"language-json\">{\n  &quot;destination_addresses&quot;: [\n    &quot;Washington, DC, USA&quot;,\n    &quot;Philadelphia, PA, USA&quot;,\n    &quot;Santa Barbara, CA, USA&quot;,\n    &quot;Miami, FL, USA&quot;,\n    &quot;Austin, TX, USA&quot;,\n    &quot;Napa County, CA, USA&quot;\n  ],\n  &quot;origin_addresses&quot;: [\n    &quot;New York, NY, USA&quot;\n  ],\n  &quot;rows&quot;: [\n    {\n      &quot;elements&quot;: [\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;227 mi&quot;,\n            &quot;value&quot;: 365468\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;3 hours 54 mins&quot;,\n            &quot;value&quot;: 14064\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;94.6 mi&quot;,\n            &quot;value&quot;: 152193\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 hour 44 mins&quot;,\n            &quot;value&quot;: 6227\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;2,878 mi&quot;,\n            &quot;value&quot;: 4632197\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 18 hours&quot;,\n            &quot;value&quot;: 151772\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;1,286 mi&quot;,\n            &quot;value&quot;: 2069031\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;18 hours 43 mins&quot;,\n            &quot;value&quot;: 67405\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;1,742 mi&quot;,\n            &quot;value&quot;: 2802972\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 2 hours&quot;,\n            &quot;value&quot;: 93070\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;2,871 mi&quot;,\n            &quot;value&quot;: 4620514\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 18 hours&quot;,\n            &quot;value&quot;: 152913\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        }\n      ]\n    }\n  ],\n  &quot;status&quot;: &quot;OK&quot;\n}\n</code></pre>\n<p>A lot is happening here. There are objects. There are lists. There are lists of objects which are part of an object. The last thing I'd want to deal with is trying to parse this data only to accidentally get a useless key:value pair like <strong>\"status\": \"OK\".</strong></p><h2 id=\"code-snippet-to-the-rescue\">Code Snippet To The Rescue</h2><p>Let's say we only want the human-readable data from this JSON, which is labeled <em>\"text\"</em> for both distance and duration. We've created a function below dubbed <code>extract_values()</code> to help us resolve this very issue. The idea is that <code>extract_values()</code> is flexible and agnostic, therefore can be imported as a module into any project you might need.</p><pre><code class=\"language-python\"># recursivejson.py\n\ndef extract_values(obj, key):\n    &quot;&quot;&quot;Pull all values of specified key from nested JSON.&quot;&quot;&quot;\n    arr = []\n\n    def extract(obj, arr, key):\n        &quot;&quot;&quot;Recursively search for values of key in JSON tree.&quot;&quot;&quot;\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n</code></pre>\n<p>We need to pass this function two values:</p><ul><li>A JSON object, such as <code>r.json()</code> from an API request.</li><li>The name of the <strong>key</strong> we're looking to extract values from.</li></ul><pre><code class=\"language-python\">names = extract_values('myjson.json', 'name')\nprint(names)\n</code></pre>\n<p>Regardless of where the key <strong>\"text\"</strong> lives in the JSON, this function returns every value for the instance of <strong>\"key.\" </strong>Here's our function in action:</p><pre><code class=\"language-python\">import requests\nimport API_KEY\nfrom recursivejson import extract_values\n\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': &quot;New York City,NY&quot;,\n       'destinations': &quot;Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa Valley,CA&quot;,\n       'transit_mode': 'car',\n    }\n\n   r = requests.get(endpoint, params=params)\n   travel_values = extract_values(r.json(), 'text')\n   return travel_values\n</code></pre>\n<p>Running this function will result in the following output:</p><pre><code class=\"language-python\">['227 mi', '3 hours 54 mins', '94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n</code></pre>\n<p>Oh <em>fiddle me timbers</em>! Because the Google API alternates between <strong>distance </strong>and <strong>trip duration</strong>, every other value alternates between distance and time (can we pause to appreciate this horrible design? There are infinitely better ways to structure this response). Never fear, some simple Python can help us split this list into two lists:</p><pre><code class=\"language-python\">my_values = extract_values(r.json(), 'text')\n\ndurations = my_values[1::2]\ndistances = my_values[2::1]\n\nprint('DURATIONS = ', durations)\nprint('DISTANCES = ', distances)\n</code></pre>\n<p>This will take our one list and split it in to <em>two</em> lists, alternating between even and odd:</p><pre><code class=\"language-python\">DURATIONS = ['3 hours 54 mins', '1 hour 44 mins', '1 day 18 hours', '18 hours 43 mins', '1 day 2 hours', '1 day 18 hours']\nDISTANCES = ['94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n</code></pre>\n<h2 id=\"getting-creative-with-lists\">Getting Creative With Lists</h2><p>A common theme I run in to while extracting lists of values from JSON objects like these is that the lists of values I extract are very much related.  In the above example, for every <em>duration</em> we have an accompanying <em>distance, </em>which is a one-to-one basis. Imagine if we wanted to associate these values somehow?</p><p>To use a better example, I recently I used this <code>exact_values()</code> function to extract lists of column names and their data types from a database schema. As separate lists, the data looked something like this:</p><pre><code class=\"language-python\">column_names = ['index', 'first_name', 'last_name', 'join_date']\ncolumn_datatypes = ['integer', 'string', 'string', 'date']\n</code></pre>\n<p>Clearly these two lists are directly related; the latter is describing the former. How can this be useful? By using Python's <code>zip</code> method!</p><pre><code class=\"language-python\">schema_dict = dict(zip(column_names, column_datatypes))\nprint(schema_dict)\n</code></pre>\n<p>I like to think they call it <em>zip</em> because it's like zipping up a zipper, where each side of the zipper is a list. This output a dictionary where list 1 serves as the keys, and list 2 serves as values:</p><pre><code class=\"language-python\">{\n'index': 'integer', \n'first_name': 'string', \n'last_name':'string',\n'join_date': 'date'\n}\n</code></pre>\n<p>And there you have it folks: a free code snippet to copy and secretly pretend you wrote forever. I've thrown the function up on <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">Github Gists</a>, if such a thing pleases you.</p><p>In the meantime, zip it up and zip it out. Zippity-do-da, buh bye.</p>","url":"https://hackersandslackers.com/extract-data-from-complex-json-python/","uuid":"9a494df4-9e13-45ed-8648-efdda21c55a4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bbd7ce1b936605163ece407"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673718","title":"Reading and Writing to CSVs in Python","slug":"reading-and-writing-to-csvs-in-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","excerpt":"Playing with tabular data the native Python way.","custom_excerpt":"Playing with tabular data the native Python way.","created_at_pretty":"27 September, 2018","published_at_pretty":"27 September, 2018","updated_at_pretty":"05 November, 2018","created_at":"2018-09-27T13:22:47.000-04:00","published_at":"2018-09-27T18:35:00.000-04:00","updated_at":"2018-11-05T07:55:10.000-05:00","meta_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","meta_description":"Using native Python libraries to interact with tabular data. Pandas not included.\n\n\n\n\n\n\n\n\nar \n\n","og_description":"Using native Python libraries to interact with tabular data. Pandas not included.","og_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","og_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","twitter_description":"Using native Python libraries to interact with tabular data. Pandas not included.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","twitter_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Tables. Cells. Two-dimensional data. We here at Hackers & Slackers know how to\ntalk dirty, but there's one word we'll be missing from our vocabulary today:\nPandas.Before the remaining audience closes their browser windows in fury, hear\nme out. We love Pandas; so much so that tend to recklessly gunsling this 30mb\nlibrary to perform simple tasks. This isn't always a wise choice. I get it:\nyou're here for data, not software engineering best practices. We all are, but\nin a landscape where engineers and scientists already produce polarizing code\nquality, we're all just a single bloated lambda function away from looking like\n idiots and taking a hit to our credibility. This is a silly predicament when\nthere are plenty of built-in Python libraries at our disposable which work\nperfectly fine. Python’s built in CSV library can cover quite a bit of data\nmanipulation use cases to achieve the same results of large scientific libraries\njust as easily.\n\nBasic CSV Interaction\nRegardless of whether you're reading or writing to CSVs, there are a couple\nlines of code which will stay mostly the same between the two. \n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n     reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\nBefore accomplishing anything, we've stated some critical things in these two\nlines of code:\n\n * All interactions with our CSV will only be valid as long as they live within\n   the with.open  block (comparable to managing database connections).\n * We'll be interacting with a file in our directory called hackers.csv, for\n   which we only need read (or r) permissions\n * We create a reader  object, which is again comparable to managing database \n   cursors  if you're familiar.\n * We have the ability to set the delimiter of our CSV (a curious feature,\n   considering the meaning of C  in the acronym CSV.\n\nIterating Rows\nAn obvious use case you probably have in mind would be to loop through each row\nto see what sort of values we're dealing with. Your first inclination might be\nto do something like this:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\tfor row in reader.readlines():\n\t\tprint('row = ', row)\n\nThat's fine and all, but row  in this case returns a simple list - this is\nobviously problem if you want to access the values of certain columns by column \nname,  as opposed to numeric index (I bet you do). Well, we've got you covered:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in reader.readlines():\n\t\tprint(row['column_name_1'], row['column_name_2'])\n\nChanging reader  to DictReader  outputs a dictionary  per CSV row, as opposed to\na simple list. Are things starting to feel a little Panda-like yet?\n\nBonus: Printing all Keys and Their Values\nLet's get a little weird just for fun. Since our rows are dict objects now, we\ncan print our entire CSV as a series of dicts like so:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in loc_reader:\n            for (k, v) in row.items():\n\t\t\t\tprint(k, ':', v)\n\nSkipping Headers\nAs we read information from CSVs to be repurposed for, say, API calls, we \nprobably  don't want to iterate over the first row of our CSV: this will output\nour key values alone, which would be useless in this context. Consider this:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r') as myCsvFile:\n\tnext(myCsvFile)\n\tfor row in myCsvFile.readlines():\n\t\tprint(row)\n\nWhoa! A different approach.... but somehow just as simple? In this case, we\nleave out reader  altogether (which still works!) but more importantly, we\nintroduce next(). next(myCsvFile)  immediately skips to the next line in a CSV,\nso in our case, we simply skip line one before going into our For loop. Amazing.\n\nWriting to CSVs\nWriting to CSVs isn't much different than reading from them. In fact, almost all\nthe same principles apply, where instances of the word \"read\" are more or less\nreplaced with\" write. Huh. \n\n# write_csv.py\nimport csv\n\nwith open('hackers.csv', 'w') as myCsvFile:\n    columns = ['column_name_1', 'column_name_2']\n    writer = csv.DictWriter(myCsvFile, fieldnames=columns)\n\n    writer.writeheader()\n    writer.writerow({'column_name_1': 'Mark', 'column_name_2': 'Twain'})\n    writer.writerow({'column_name_1': 'Foo', 'column_name_2: 'Bar'})\n\nWe're writing a brand new CSV here: 'hackers.csv' doesn't technically exist yet,\nbut that doesn't stop Python from not giving a shit. Python knows what you mean.\nPython has your back.\n\nHere, we set our headers as a fixed list set by the column  variable. This is a\nstatic way of creating headers, but the same can be done dynamically by passing\nthe keys  of a dict, or whatever it is you like to do. \n\nwriter.writeheader()  knows what we're saying thanks to the aforementioned \nfieldnames  we passed to our writer earlier. Good for you, writer.\n\nBut how do we write rows, you might ask? Why, with writer.writerow(), of course!\nBecause we use DictWriter  similarly to how we used DictReader  earlier, we can\nmap values to our CSV with simple column references. Easy.","html":"<p>Tables. Cells. Two-dimensional data. We here at Hackers &amp; Slackers know how to talk dirty, but there's one word we'll be missing from our vocabulary today: Pandas.Before the remaining audience closes their browser windows in fury, hear me out. We love Pandas; so much so that tend to recklessly gunsling this 30mb library to perform simple tasks. This isn't always a wise choice. I get it: you're here for data, not software engineering best practices. We all are, but in a landscape where engineers and scientists already produce polarizing code quality, we're all just a single bloated lambda function away from looking like  idiots and taking a hit to our credibility. This is a silly predicament when there are plenty of built-in Python libraries at our disposable which work perfectly fine. Python’s built in CSV library can cover quite a bit of data manipulation use cases to achieve the same results of large scientific libraries just as easily.</p><h2 id=\"basic-csv-interaction\">Basic CSV Interaction</h2><p>Regardless of whether you're reading or writing to CSVs, there are a couple lines of code which will stay mostly the same between the two. </p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n     reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')</code></pre><p>Before accomplishing anything, we've stated some critical things in these two lines of code:</p><ul><li>All interactions with our CSV will only be valid as long as they live within the <code>with.open</code> block (comparable to managing database connections).</li><li>We'll be interacting with a file in our directory called <code>hackers.csv</code>, for which we only need read (or <code>r</code>) permissions</li><li>We create a <code>reader</code> object, which is again comparable to managing database <code>cursors</code> if you're familiar.</li><li>We have the ability to set the delimiter of our CSV (a curious feature, considering the meaning of <strong>C</strong> in the acronym <strong>CSV.</strong></li></ul><h3 id=\"iterating-rows\">Iterating Rows</h3><p>An obvious use case you probably have in mind would be to loop through each row to see what sort of values we're dealing with. Your first inclination might be to do something like this:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\tfor row in reader.readlines():\n\t\tprint('row = ', row)</code></pre><p>That's fine and all, but <code>row</code> in this case returns a simple list - this is obviously problem if you want to access the values of certain columns by column <em>name,</em> as opposed to <em>numeric index </em>(I bet you do). Well, we've got you covered:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in reader.readlines():\n\t\tprint(row['column_name_1'], row['column_name_2'])</code></pre><p>Changing <code>reader</code> to <code>DictReader</code> outputs a <em>dictionary</em> per CSV row, as opposed to a simple list. Are things starting to feel a little Panda-like yet?</p><h4 id=\"bonus-printing-all-keys-and-their-values\">Bonus: Printing all Keys and Their Values</h4><p>Let's get a little weird just for fun. Since our rows are dict objects now, we can print our entire CSV as a series of dicts like so:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in loc_reader:\n            for (k, v) in row.items():\n\t\t\t\tprint(k, ':', v)</code></pre><h3 id=\"skipping-headers\">Skipping Headers</h3><p>As we read information from CSVs to be repurposed for, say, API calls, we <em>probably</em> don't want to iterate over the first row of our CSV: this will output our key values alone, which would be useless in this context. Consider this:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r') as myCsvFile:\n\tnext(myCsvFile)\n\tfor row in myCsvFile.readlines():\n\t\tprint(row)</code></pre><p>Whoa! A different approach.... but somehow just as simple? In this case, we leave out <code>reader</code> altogether (which still works!) but more importantly, we introduce <code>next()</code>. <code>next(myCsvFile)</code> immediately skips to the next line in a CSV, so in our case, we simply skip line one before going into our For loop. Amazing.</p><h2 id=\"writing-to-csvs\">Writing to CSVs</h2><p>Writing to CSVs isn't much different than reading from them. In fact, almost all the same principles apply, where instances of the word \"read\" are more or less replaced with\" write. Huh. </p><pre><code># write_csv.py\nimport csv\n\nwith open('hackers.csv', 'w') as myCsvFile:\n    columns = ['column_name_1', 'column_name_2']\n    writer = csv.DictWriter(myCsvFile, fieldnames=columns)\n\n    writer.writeheader()\n    writer.writerow({'column_name_1': 'Mark', 'column_name_2': 'Twain'})\n    writer.writerow({'column_name_1': 'Foo', 'column_name_2: 'Bar'})</code></pre><p>We're writing a brand new CSV here: 'hackers.csv' doesn't technically exist yet, but that doesn't stop Python from not giving a shit. Python knows what you mean. Python has your back.</p><p>Here, we set our headers as a fixed list set by the <code>column</code> variable. This is a static way of creating headers, but the same can be done dynamically by passing the <code>keys</code> of a dict, or whatever it is you like to do. </p><p><code>writer.writeheader()</code> knows what we're saying thanks to the aforementioned <code>fieldnames</code> we passed to our writer earlier. Good for you, writer.</p><p>But how do we write rows, you might ask? Why, with <code>writer.writerow()</code>, of course! Because we use <code>DictWriter</code> similarly to how we used <code>DictReader</code> earlier, we can map values to our CSV with simple column references. Easy.</p>","url":"https://hackersandslackers.com/reading-and-writing-to-csvs-in-python/","uuid":"eb4f019f-e135-49d2-a568-f03f1e622d62","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bad11e75ee4c83af27dda9e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c5","title":"Hacking Your Tableau Linux Server","slug":"hacking-linux-tableau-server","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/hacktableau@2x.jpg","excerpt":"Cracking Tableau's master Postgres account.","custom_excerpt":"Cracking Tableau's master Postgres account.","created_at_pretty":"26 July, 2018","published_at_pretty":"26 July, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-07-26T03:17:59.000-04:00","published_at":"2018-07-26T05:55:50.000-04:00","updated_at":"2019-02-02T04:23:43.000-05:00","meta_title":"Cracking Tableau's master Postgres account | Hackers And Slackers","meta_description":"BI tools are great for understanding preexisting data, but they don't allow us to go much further. Your data is with them, and it's not going anywhere else.","og_description":"BI tools are great for understanding preexisting data, they don't go much further. Your data is with them, and it's not going anywhere else.","og_image":"https://hackersandslackers.com/content/images/2018/07/hacktableau@2x.jpg","og_title":"Hacking Your Tableau Linux Server","twitter_description":"BI tools are great for understanding preexisting data, they don't go much further. Your data is with them, and it's not going anywhere else.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/hacktableau@2x.jpg","twitter_title":"Hacking Your Tableau Linux Server","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"}],"plaintext":"Let's say you're a Data Scientist. Well, maybe not a data scientist- I mean,\nthose online data analysis courses were definitely worth it, and you'd made it\nthis far without being quizzed on Bayesian linear regression. So maybe you're an\nanalyst or something, but whatever:  you use Tableau, So you must be a\nScientist™.\n\nI've admitted a few times in the past to have purchased a personal Tableau\nServer license in my more ignorant years (aka a few months ago). While BI tools\nare great for understanding preexisting data, they don't allow us to go much\nfurther. This is is entirely by design. Sure, you can clean and slice your data\nand put it into a cute iFrame dashboard, but Tableau explicitly makes one thing\nexplicitly clear in their product design choices: your data is with them, and\nit's not going anywhere else. Today we're going to take a step towards changing\nthat.\n\nProprietary Product Design: Crimes Against Customers\nTableau has explicit hierarchies for information, but let's start with \nworkbooks.  Workbooks are basically spreadsheets, or in other words,  \ncollections of SQL query outputs against a data source (or multiple data\nsources) via a clean UI. The resulting tabular data is referred to as views. An\nExcel user might equate these to \"sheets\", but a SQL user understands that these\nfunction more like a materialized view of sorts. One would think the tables we\ncreate (from our own data) inherently belongs to us, but it doesn't. Not until\nyou get clever.\n\nI realize Tableau maybe be at the top of the market for its niche.... so the\nthings I'm claiming may seem a little farfetched. Why am I so convinced that\nTableau wants to lock your data? Stay with me here, and let me count the ways.\n\nCommon Courtesy API Visibility\nCommon knowledge suggest that visible APIs attracts development talent. The more\nintelligent people are exposed to your product, the more like they are to\ncontribute. What happens if we check out the API response calls in our browser\nwhen viewing  Worksheet on Tableau Server?\n\nLet's just agree this is all useless.While this level of unnecessary paranoia on\nTableau's part is distasteful, let's not forget that we're dealing with a\nproduct archaic enough to preview Windows server support over Linux. The\nnarrative begins to make sense.\n\nPostgres Hide and Seek\nTableau Server is running a Postgres database; really nothing magical happening\nhere. Well, other than the database has been renamed, protected, and obfuscated\nin a way that even the server owner would struggle with. The default commands to\ninteract with PostgreSQL are hidden from server admins altogether.\n\n$ psql postgres -u toddbirchard -p\nThe program 'psql' is currently not installed. To run 'psql' please ask your administrator to install the package 'postgresql-client-common'\n\n-------------------\n\n$ sudo -u postgres psql postgres\nsudo: unknown user: postgres\nsudo: unable to initialize policy plugin\n\n-------------------\n\n$ sudo -u toddbirchard psql postgres\nsudo: psql: command not found\n\n-------------------\n\n$ psql\npsql: could not connect to server: No such file or directory\n        Is the server running locally and accepting\n        connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?\n\n-------------------\n\n$ pgsql\nNo command 'pgsql' found, did you mean:\n Command 'psql' from package 'postgresql-client-common' (main)\npgsql: command not found\n\n\nWhat if we do a search?\n\n$ locate postgresql\n\n/etc/postgresql-common\n/etc/postgresql-common/user_clusters\n/opt/tableau-postgresql-odbc_9.5.3_amd64.deb\n/opt/tableau/tableau_driver/postgresql-odbc\n/opt/tableau/tableau_driver/postgresql-odbc/psqlodbcw.so\n/opt/tableau/tableau_server/packages/bin.20181.18.0510.1418/repo-jars/postgresql-9.4.1209.jar\n/opt/tableau/tableau_server/packages/clientfileservice.20181.18.0510.1418/postgresql-9.4.1208.jar\n/opt/tableau/tableau_server/packages/lib.20181.18.0510.1418/postgresql-9.4.1208.jar\n/opt/tableau/tableau_server/packages/lib.20181.18.0510.1418/postgresql-9.4.1209.jar\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/_int.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/adminpack.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/ascii_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/auth_delay.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/auto_explain.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/autoinc.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/btree_gin.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/btree_gist.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/chkpass.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/citext.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/cube.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/cyrillic_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dblink.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_int.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_snowball.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_xsyn.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/earthdistance.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc2004_sjis2004.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_cn_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_jp_and_sjis.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_kr_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_tw_and_big5.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/file_fdw.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/fuzzystrmatch.so\n\n\n...And so forth. There are over a thousand results. Postgres is definitely up\nand running, Tableau just hates you. Unfortunately for Tableau, this drove me to\nhate them back.\n\nEnter TSM: The Linux Tableau CLI\nOn Linux exclusively, TSM is intended to be your one tool to configure Tableau\nServer. It's a fine tool, but it just so happens to omit critical information\nand capabilities that somebody who owns their data  might want to know. At first\nglance, it seems innocent and helpful:\n\nCommand\n Explanation\n tsm configuration [parameters]\n -- Set customization for Tableau Server.\n tsm customize [parameters] -- Set customization for Tableau Server.\n tsm data-access [parameters]\n -- Category of commands related to data-access.\n tsm help | [category]\n -- Help for tsm commands\n .tsm initialize [parameters]\n -- Initialize Tableau Server\n tsm jobs [parameters]\n -- Category of commands related to async jobs.\n tsm licenses [parameters]\n -- Category of commands related to licensing.\n tsm login [parameters] -- Sign in to the TSM agent\n tsm logout -- Sign out from the TSM agent\n tsm maintenance [parameters]\n -- Category of commands related to maintenance.\n tsm pending-changes [parameters]\n -- Category of commands for pending changes.\n tsm register [parameters]\n -- Register the product\n tsm reset [parameters]\n -- Clears the initial admin user so you can enter a new one. Once reset is\ncompleted you will need to use the tabcmd initialuser command to create a new\ninitial user before remote users can sign in again\n tsm restart [parameters]\n -- Restart Tableau Server\n tsm security [parameters]\n -- Category of commands related to security configuration\n tsm settings [parameters]\n -- Category of commands related to configuration and topology settings\n tsm sites [parameters]\n -- Category of commands related to site import and export\n tsm start [parameters]\n -- Start Tableau Server\n tsm status [parameters]\n -- View Tableau Server status\n tsm stop [parameters]\n -- Stop Tableau Server\n tsm topology [parameters] -- Category of commands related to server topology\n tsm user-identity-store [parameters]\n -- Category of commands related to user-identity-store\n tsm version -- Displays version information.\n The Red Herring\nTableau owns Google results, period. Any search query containing the word\n\"Tableau\" is dominated with pages of content Tableau would prefer  you abide by,\nand of these things is the creation of a readonly  user to access the Postgres\ndatabase. The catch here is that the readonly  user can't read all tables at\nall: there are certain tables reserved specifically for a Postgres tableau\n\"Superuser\", which is utterly and entirely undocumented on Linux.  For all I\nknow, I my be the first to publish an article of this sort, but let's hope not.\n\nFirst, let's see which users exist on Postgres using TSM:\n\n$ tsm data-access repository-access list\n\nUser       Access\nTableau    true\nReadonly   true\n\n\nThere's that Readonly user we talked about: feel free to play around with that\nuser to create meaningless insights if you so please. On the other hand we have\na Tableau  user, which happens to be a Postgres superuser. If you don't feel\ncomfortable accessing Superuser privileges, I suggest you leave now. This is \nHackers And Slackers,  and we don't fuck around; especially when software to the\ntune of 1 thousand dollars hides our data from us.\n\nOperation Shock and Awe\nThere's a little command called tsm configuration  which lets you set some cute\nvariables for your server. The documentation is here\n[https://onlinehelp.tableau.com/current/server-linux/en-us/cli_configuration-set_tsm.htm]\n, but there's just one piece missing, and it's the one we need.\n\nTableau may be our Postgres Superuser, but what would its password possibly be?\nThis isn't documented anywhere. Consider this my gift to you:\n\n$ tsm configuration get -k pgsql.adminpassword\n145v756270d3467bv3140af5f01v5c7e4976bcee\n\n\nCould it be? Did Tableau intentionally prevent users from access PostgreSQL\ndirectly from command line and hide an undocumented password? Yes, it does all\nof those things. It's time to fuck shit up.\n\nClaim Ownership\nWe've made it this far. The bullet is in the chamber. Go ahead and take what is\nrightfully yours.\n\ntsm data-access repository-access enable --repository-username Tableau\n--repository-password 145v756270d3467bv3140af5f01v5c7e4976bcee\n\n\nJust make sure port 8060 is open on your VPC and you're in. Considering that\nthere are zero search results for accomplishing this on Linux, it looks like\nit's just you and me now. One of us may likely go mad with power and turn on one\nanother. That is the way of the Sith. Welcome.\n\nUnspeakable treasures lie within.Moving on Up\nFeel free to cruise the workgroup  database for now and wreck havoc. As fun as\nthis has been, I have another trick up my sleeve. You've spent a lot of time\nbuilding Worksheets and views; what if you could programmatically sync to an\nexternal database and autogenerate a schema for these views, updated on a\nscheduler, to source data for products you're building?\n\nThats sounds a lot like what a useful product would do. Stick around, and next\ntime we'll be beating Tableau down for everything its worth.","html":"<p>Let's say you're a Data Scientist. Well, maybe not a <em>data scientist- </em>I mean, those online data analysis courses were definitely worth it, and you'd made it this far without being quizzed on Bayesian linear regression. So maybe you're an analyst or something, but whatever:  you use Tableau, So you must be a Scientist™.</p><p>I've admitted a few times in the past to have purchased a personal Tableau Server license in my more ignorant years (aka a few months ago). While BI tools are great for understanding preexisting data, they don't allow us to go much further. This is is entirely by design. Sure, you can clean and slice your data and put it into a cute iFrame dashboard, but Tableau explicitly makes one thing explicitly clear in their product design choices: your data is with them, and it's not going anywhere else. Today we're going to take a step towards changing that.</p><h2 id=\"proprietary-product-design-crimes-against-customers\">Proprietary Product Design: Crimes Against Customers</h2><p>Tableau has explicit hierarchies for information, but let's start with <strong>workbooks.</strong> Workbooks are basically spreadsheets, or in other words,<strong> </strong>collections of SQL query outputs against a data source (or multiple data sources) via a clean UI. The resulting tabular data is referred to as <strong>views</strong>. An Excel user might equate these to \"sheets\", but a SQL user understands that these function more like a materialized view of sorts. One would think the tables we create (from our own data) inherently belongs to us, but it doesn't. Not until you get clever.</p><p>I realize Tableau maybe be at the top of the market for its niche.... so the things I'm claiming may seem a little farfetched. Why am I so convinced that Tableau wants to lock your data? Stay with me here, and let me count the ways.</p><h2 id=\"common-courtesy-api-visibility\">Common Courtesy API Visibility</h2><p>Common knowledge suggest that visible APIs attracts development talent. The more intelligent people are exposed to your product, the more like they are to contribute. What happens if we check out the API response calls in our browser when viewing  Worksheet on Tableau Server?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ezgif.com-gif-maker.gif\" class=\"kg-image\"><figcaption>Let's just agree this is all useless.</figcaption></figure><p>While this level of unnecessary paranoia on Tableau's part is distasteful, let's not forget that we're dealing with a product archaic enough to preview Windows server support over Linux. The narrative begins to make sense.</p><h3 id=\"postgres-hide-and-seek\">Postgres Hide and Seek</h3><p>Tableau Server is running a Postgres database; really nothing magical happening here. Well, other than the database has been renamed, protected, and obfuscated in a way that even the server owner would struggle with. The default commands to interact with PostgreSQL are hidden from server admins altogether.</p><pre><code class=\"language-bash\">$ psql postgres -u toddbirchard -p\nThe program 'psql' is currently not installed. To run 'psql' please ask your administrator to install the package 'postgresql-client-common'\n\n-------------------\n\n$ sudo -u postgres psql postgres\nsudo: unknown user: postgres\nsudo: unable to initialize policy plugin\n\n-------------------\n\n$ sudo -u toddbirchard psql postgres\nsudo: psql: command not found\n\n-------------------\n\n$ psql\npsql: could not connect to server: No such file or directory\n        Is the server running locally and accepting\n        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;?\n\n-------------------\n\n$ pgsql\nNo command 'pgsql' found, did you mean:\n Command 'psql' from package 'postgresql-client-common' (main)\npgsql: command not found\n</code></pre>\n<p>What if we do a search?</p><pre><code class=\"language-bash\">$ locate postgresql\n\n/etc/postgresql-common\n/etc/postgresql-common/user_clusters\n/opt/tableau-postgresql-odbc_9.5.3_amd64.deb\n/opt/tableau/tableau_driver/postgresql-odbc\n/opt/tableau/tableau_driver/postgresql-odbc/psqlodbcw.so\n/opt/tableau/tableau_server/packages/bin.20181.18.0510.1418/repo-jars/postgresql-9.4.1209.jar\n/opt/tableau/tableau_server/packages/clientfileservice.20181.18.0510.1418/postgresql-9.4.1208.jar\n/opt/tableau/tableau_server/packages/lib.20181.18.0510.1418/postgresql-9.4.1208.jar\n/opt/tableau/tableau_server/packages/lib.20181.18.0510.1418/postgresql-9.4.1209.jar\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/_int.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/adminpack.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/ascii_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/auth_delay.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/auto_explain.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/autoinc.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/btree_gin.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/btree_gist.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/chkpass.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/citext.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/cube.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/cyrillic_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dblink.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_int.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_snowball.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/dict_xsyn.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/earthdistance.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc2004_sjis2004.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_cn_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_jp_and_sjis.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_kr_and_mic.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/euc_tw_and_big5.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/file_fdw.so\n/opt/tableau/tableau_server/packages/pgsql.20181.18.0510.1418/lib/postgresql/fuzzystrmatch.so\n</code></pre>\n<p>...And so forth. There are over a thousand results. Postgres is definitely up and running, Tableau just hates you. Unfortunately for Tableau, this drove me to hate them back.</p><h2 id=\"enter-tsm-the-linux-tableau-cli\">Enter TSM: The Linux Tableau CLI</h2><p>On Linux exclusively, TSM is intended to be your one tool to configure Tableau Server. It's a fine tool, but it just so happens to omit critical information and capabilities that somebody who <em>owns their data</em> might want to know. At first glance, it seems innocent and helpful:</p><style>\n    td{\n        min-width: 224px;\n            text-align: left;\n    padding: 10px 20px !important;\n            line-height: 1.3;\n        text-align: left !important;\n    }\n    </style>\n\n<table style=\"cellpadding=\" 10px\"=\"\">\n  <thead>\n    <th>Command</th>\n    <th>Explanation</th>\n  </thead>\n  <tbody>\n    <tr>\n      <td>tsm configuration [parameters]</td>\n      <td>-- Set customization for Tableau Server.</td>\n    </tr>\n    <tr>\n      <td>tsm customize [parameters] </td>\n      <td>-- Set customization for Tableau Server.</td>\n    </tr>\n    <tr>\n      <td>tsm data-access [parameters]</td>\n      <td>-- Category of commands related to data-access.</td>\n    </tr>\n    <tr>\n      <td>tsm help  | [category]</td>\n      <td>-- Help for tsm commands</td>\n    </tr>\n    <tr>\n      <td>.tsm initialize [parameters]</td>\n      <td>-- Initialize Tableau Server</td>\n    </tr>\n    <tr>\n      <td>tsm jobs [parameters]</td>\n      <td> -- Category of commands related to async jobs.</td>\n    </tr>\n    <tr>\n      <td>tsm licenses [parameters]</td>\n      <td>-- Category of commands related to licensing.</td>\n    </tr>\n    <tr>\n      <td>tsm login [parameters] </td>\n      <td>-- Sign in to the TSM agent</td>\n    </tr>\n    <tr>\n      <td>tsm logout </td>\n      <td>-- Sign out from the TSM agent</td>\n    </tr>\n    <tr>\n      <td>tsm maintenance  [parameters]</td>\n      <td>-- Category of commands related to maintenance.</td>\n    </tr>\n    <tr>\n      <td>tsm pending-changes [parameters]</td>\n      <td>-- Category of commands for pending changes.</td>\n    </tr>\n    <tr>\n      <td>tsm register [parameters]</td>\n      <td>-- Register the product</td>\n    </tr>\n    <tr>\n      <td>tsm reset [parameters]</td>\n      <td>-- Clears the initial admin user so you can enter a new one. Once reset is completed you will need to use the tabcmd initialuser command to create a new initial user before remote users can sign in again</td>\n    </tr>\n    <tr>\n      <td>tsm restart [parameters]</td>\n      <td>-- Restart Tableau Server</td>\n    </tr>\n    <tr>\n      <td>tsm security [parameters]</td>\n      <td>-- Category of commands related to security configuration</td>\n    </tr>\n    <tr>\n      <td>tsm settings [parameters]</td>\n      <td>-- Category of commands related to configuration and topology settings</td>\n    </tr>\n    <tr>\n      <td>tsm sites [parameters]</td>\n      <td>-- Category of commands related to site import and export</td>\n    </tr>\n    <tr>\n      <td>tsm start [parameters]</td>\n      <td>-- Start Tableau Server</td>\n    </tr>\n    <tr>\n      <td>tsm status [parameters]</td>\n      <td>-- View Tableau Server status</td>\n    </tr>\n    <tr>\n      <td>tsm stop [parameters]</td>\n      <td>-- Stop Tableau Server</td>\n    </tr>\n    <tr>\n      <td>tsm topology  [parameters] </td>\n      <td>-- Category of commands related to server topology</td>\n    </tr>\n    <tr>\n      <td>tsm user-identity-store [parameters]</td>\n      <td>-- Category of commands related to user-identity-store</td>\n    </tr>\n    <tr>\n      <td>tsm version </td>\n      <td>-- Displays version information.</td>\n    </tr>\n  </tbody>\n</table>\n<!-- DivTable.com -->\n<h2 id=\"the-red-herring\">The Red Herring</h2><p>Tableau owns Google results, period. Any search query containing the word \"Tableau\" is dominated with pages of content Tableau would <em>prefer</em> you abide by, and of these things is the creation of a <em>readonly</em> user to access the Postgres database. The catch here is that the <strong>readonly</strong> user can't read all tables at all: there are certain tables reserved specifically for a Postgres tableau \"Superuser\", which is <em>utterly and entirely undocumented on Linux.</em> For all I know, I my be the first to publish an article of this sort, but let's hope not.</p><p>First, let's see which users exist on Postgres using TSM:</p><pre><code class=\"language-bash\">$ tsm data-access repository-access list\n\nUser       Access\nTableau    true\nReadonly   true\n</code></pre>\n<p>There's that Readonly user we talked about: feel free to play around with that user to create meaningless insights if you so please. On the other hand we have a <strong>Tableau</strong> user, which happens to be a Postgres superuser. If you don't feel comfortable accessing Superuser privileges, I suggest you leave now. This is <strong>Hackers And Slackers,</strong> and we don't fuck around; especially when software to the tune of 1 thousand dollars hides our data from us.</p><h2 id=\"operation-shock-and-awe\">Operation Shock and Awe</h2><p>There's a little command called <code>tsm configuration</code> which lets you set some cute variables for your server. The documentation is <a href=\"https://onlinehelp.tableau.com/current/server-linux/en-us/cli_configuration-set_tsm.htm\">here</a>, but there's just one piece missing, and it's the one we need.</p><p><strong>Tableau </strong>may be our Postgres Superuser, but what would its password possibly be? This isn't documented anywhere. Consider this my gift to you:</p><pre><code class=\"language-bash\">$ tsm configuration get -k pgsql.adminpassword\n145v756270d3467bv3140af5f01v5c7e4976bcee\n</code></pre>\n<p>Could it be? Did Tableau intentionally prevent users from access PostgreSQL directly from command line and hide an undocumented password? Yes, it does all of those things. It's time to fuck shit up.</p><h2 id=\"claim-ownership\">Claim Ownership</h2><p>We've made it this far. The bullet is in the chamber. Go ahead and take what is rightfully yours.</p><pre><code class=\"language-bash\">tsm data-access repository-access enable --repository-username Tableau\n--repository-password 145v756270d3467bv3140af5f01v5c7e4976bcee\n</code></pre>\n<p>Just make sure port 8060 is open on your VPC and you're in. Considering that there are zero search results for accomplishing this on Linux, it looks like it's just you and me now. One of us may likely go mad with power and turn on one another. That is the way of the Sith. Welcome.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/winner.png\" class=\"kg-image\"><figcaption>Unspeakable treasures lie within.</figcaption></figure><h2 id=\"moving-on-up\">Moving on Up</h2><p>Feel free to cruise the <strong>workgroup</strong> database for now and wreck havoc. As fun as this has been, I have another trick up my sleeve. You've spent a lot of time building Worksheets and views; what if you could programmatically sync to an external database and autogenerate a schema for these views, updated on a scheduler, to source data for products you're building?</p><p>Thats sounds a lot like what a useful product would do. Stick around, and next time we'll be beating Tableau down for everything its worth. </p>","url":"https://hackersandslackers.com/hacking-linux-tableau-server/","uuid":"4bcb1c4b-bbe2-428c-b7ee-fa7adc751973","page":false,"codeinjection_foot":"<script>\n    hljs.configure({languages:['bash']});\n</script>","codeinjection_head":"","comment_id":"5b5975a75c6b8259b902b66a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673682","title":"Using Pandas with AWS Lambda Functions","slug":"using-pandas-with-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","custom_excerpt":"Forcefully use the Pandas library in your AWS Lambda functions.","created_at_pretty":"20 June, 2018","published_at_pretty":"21 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-20T18:21:31.000-04:00","published_at":"2018-06-21T07:30:00.000-04:00","updated_at":"2019-03-28T08:49:25.000-04:00","meta_title":"Using Pandas with AWS Lambda Functions | Hackers and Slackers","meta_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","og_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-3.jpg","og_title":"Using Pandas with AWS Lambda","twitter_description":"Learn how to forcefully use Python's Pandas library in AWS Lambda functions.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/pandas-lambdas-2-2.jpg","twitter_title":"Using Pandas with AWS Lambda","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In one corner we have Pandas: Python's beloved data analysis library. In the\nother, AWS: the unstoppable cloud provider we're obligated to use for all\neternity. We should have known this day would come.\n\nWhile not the prettiest workflow, uploaded Python package dependencies for usage\nin AWS Lambda is typically straightforward. We install the packages locally to a\nvirtual env, package them with our app logic, and upload a neat CSV to Lambda.\nIn some cases this doesn't always work: some packages result in a cryptic error\nmessage with absolutely no helpful instruction. Pandas is one of those packages.\n\nWhy is this? I can't exactly speak to that, but I can speak to how to fix it.\n\nSpin up an EC2 Instance\nCertain Python packages need to be installed and compiled on an EC2 instance in\norder to work properly with AWS microservices. I wish I could say that this fun\nlittle fact is well-documented somewhere in AWS with a perfectly good\nexplanation. It's not, and it doesn't.  It's probably best not to ask questions.\n\nSpin up a free tier EC2 instance, update your system packages, and make sure\nPython3 is installed. Some people theorize that the Python dependency package\nerrors happen when said dependencies are installed via versions of Python which\ndiffer from the version AWS is running. Those people are wrong.  I've already\nwasted the time to debunk this. They are liars.\n\nWith Python installed,  create a virtual environment inside any empty directory:\n\n$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n\n\nWith the environment active, install pandas via pip3 install pandas. This will\nsave pandas and all its dependencies to the site-packages  folder our\nenvironment is running from, resulting in a URL such as this: \npandasenv/lib/python3.6/site-packages.\n\nPandas is actually 5 packages total. We're going to add each of these libraries\nto a zip file by installing zip, and adding each folder to the zip file\none-by-one. Finally, we'll apply some liberal permissions to the zip file we\njust created so we can grab it via FTP.\n\n$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n\n\nThis should be ready for you to FTP in your instance and grab as a zip file now\n(assuming you want to work locally). Alternatively, we could always copy those\npackages into the directory we'd like to work out of and zip everything once\nwe're done.\n\nUpload Source Code to S3\nAt this point, you should have been able to grab the AWS friendly version of\nPandas which is ready to be included in the final source code which will become\nyour Lambda Function.  You might notice that pandas alone nearly 30Mb: which is\nroughly the file size of countless intelligent people creating their life's\nwork. When Lambda Functions go above this file size, it's best to upload our\nfinal package (with source and dependencies) as a zip file to S3, and link it to\nLambda that way. This is considerably faster than the alternative of uploading\nthe zip to Lambda directly.\n\nBonus Round: Saving Exports\nWhat? You want to save a CSV result of all the cool stuff you're doing in\nPandas? You really are needy.\n\nBecause AWS is invoking the function, any attempt to read_csv()  will be\nworthless to us. To get around this, we can use boto3  to write files to an S3\nbucket instead:\n\nimport pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n\n\nWord of Advice\nThis isn't the prettiest process in the world, but we're somewhat at fault here.\nLambda functions are intended to be small tidbits of logic aimed to serve a\nsingle simple purpose. We just jammed 30Mbs of Python libraries into that simple\npurpose.\n\nThere are alternatives to Pandas that are better suited for usage in Lambda,\nsuch as Toolz  (thanks to Snkia for the heads up). Enjoy your full Pandas\nlibrary for now, but remember to feel bad about what you’ve done for next time.","html":"<p>In one corner we have Pandas: Python's beloved data analysis library. In the other, AWS: the unstoppable cloud provider we're obligated to use for all eternity. We should have known this day would come.</p><p>While not the prettiest workflow, uploaded Python package dependencies for usage in AWS Lambda is typically straightforward. We install the packages locally to a virtual env, package them with our app logic, and upload a neat CSV to Lambda. In some cases this doesn't always work: some packages result in a cryptic error message with absolutely no helpful instruction. Pandas is one of those packages.</p><p>Why is this? I can't exactly speak to that, but I can speak to how to fix it.</p><h2 id=\"spin-up-an-ec2-instance\">Spin up an EC2 Instance</h2><p>Certain Python packages need to be installed and compiled on an EC2 instance in order to work properly with AWS microservices. I wish I could say that this fun little fact is well-documented somewhere in AWS with a perfectly good explanation. It's not, and it doesn't.  It's probably best not to ask questions.</p><p>Spin up a free tier EC2 instance, update your system packages, and make sure Python3 is installed. Some people theorize that the Python dependency package errors happen when said dependencies are installed via versions of Python which differ from the version AWS is running. <em>Those people are wrong.</em> I've already wasted the time to debunk this. They are liars.</p><p>With Python installed,  create a virtual environment inside any empty directory:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt-get install virtualenv\n$ virtualenv pandasenv\n$ source pandasenv/bin/activate\n</code></pre>\n<!--kg-card-end: markdown--><p>With the environment active, install pandas via <code>pip3 install pandas</code>. This will save pandas and all its dependencies to the <em>site-packages</em> folder our environment is running from, resulting in a URL such as this: <code>pandasenv/lib/python3.6/site-packages</code>.</p><p>Pandas is actually 5 packages total. We're going to add each of these libraries to a zip file by installing <code>zip</code>, and adding each folder to the zip file one-by-one. Finally, we'll apply some liberal permissions to the zip file we just created so we can grab it via FTP.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd pandasenv/lib/python3.6/site-packages\n$ apt-get install zip\n$ zip -r pandas_archive.zip pandas\n$ zip -r pandas_archive.zip numpy\n$ zip -r pandas_archive.zip pytz\n$ zip -r pandas_archive.zip six.py\n$ zip -r pandas_archive.zip dateutil\n$ chmod 777 pandas_archive.zip\n</code></pre>\n<!--kg-card-end: markdown--><p>This should be ready for you to FTP in your instance and grab as a zip file now (assuming you want to work locally). Alternatively, we could always copy those packages into the directory we'd like to work out of and zip everything once we're done.</p><h3 id=\"upload-source-code-to-s3\">Upload Source Code to S3</h3><p>At this point, you should have been able to grab the <em>AWS friendly </em>version of Pandas which is ready to be included in the final source code which will become your Lambda Function.  You might notice that pandas alone nearly <em>30Mb</em>: which is roughly the file size of countless intelligent people creating their life's work. When Lambda Functions go above this file size, it's best to upload our final package (with source and dependencies) as a zip file to S3, and link it to Lambda that way. This is considerably faster than the alternative of uploading the zip to Lambda directly.</p><h2 id=\"bonus-round-saving-exports\">Bonus Round: Saving Exports</h2><p>What? You want to save a CSV result of all the cool stuff you're doing in Pandas? You really are needy.</p><p>Because AWS is invoking the function, any attempt to <code>read_csv()</code> will be worthless to us. To get around this, we can use <strong>boto3</strong> to write files to an S3 bucket instead:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\nfrom io import StringIO\nimport boto3\n\ns3 = boto3.client('s3', aws_access_key_id=ACCESSKEY, aws_secret_access_key=SECRETYKEY)\ns3_resource = boto3.resource('s3')\nbucket = 'your_bucket_name'\n\ncsv_buffer = StringIO()\n\nexample_df = pd.DataFrame()\nexample_df.to_csv(csv_buffer)\ns3_resource.Object(bucket, 'export.csv').put(Body=csv_buffer.getvalue())\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"word-of-advice\">Word of Advice</h3><p>This isn't the prettiest process in the world, but we're somewhat at fault here. Lambda functions are intended to be small tidbits of logic aimed to serve a single simple purpose. We just jammed 30Mbs of Python libraries into that simple purpose.</p><p>There are alternatives to Pandas that are better suited for usage in Lambda, such as <em>Toolz</em> (thanks to Snkia for the heads up). Enjoy your full Pandas library for now, but remember to feel bad about what you’ve done for next time.</p>","url":"https://hackersandslackers.com/using-pandas-with-aws-lambda/","uuid":"3d2d6592-5614-4485-b9cd-15d905a28c46","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2ad36cded32f5af8fd674d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673680","title":"Working with XML tree data in Python","slug":"dealing-with-xml-in-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/xml@2x.jpg","excerpt":"Make use of Python's native XML library to walk through and extract data.","custom_excerpt":"Make use of Python's native XML library to walk through and extract data.","created_at_pretty":"19 June, 2018","published_at_pretty":"19 June, 2018","updated_at_pretty":"15 November, 2018","created_at":"2018-06-19T17:38:18.000-04:00","published_at":"2018-06-19T18:54:16.000-04:00","updated_at":"2018-11-15T03:28:07.000-05:00","meta_title":"Dealing with XML in Python | Hackers and Slackers","meta_description":"Make use of Python's native XML library to walk through and extract data.","og_description":"Make use of Python's native XML library to walk through and extract data.","og_image":"https://hackersandslackers.com/content/images/2018/06/xml@2x.jpg","og_title":"Dealing with XML in Python","twitter_description":"Make use of Python's native XML library to walk through and extract data.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/xml@2x.jpg","twitter_title":"Dealing with XML in Python","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Life is filled with things we don't want to do; you're a developer so you\nprobably understand this to a higher degree than most people. Sometimes we waste\nweeks of our lives thanks to an unreasonable and unknowledgeable stakeholder.\nOther times, we need to deal with XML trees.\n\nAt some point or another you're going to need to work with an API that returns\ninformation in XML format. \"Sure,\" we might think, \"I'll just import the\nstandard Python XML package, pick up some syntax nuances, and be on my way.\"\nThat's what I thought too. Today we're going to look at said library, the XML\nElementTree [https://docs.python.org/3/library/xml.etree.elementtree.html] \nlibrary, and see firsthand why this might not be the case.\n\nAs always, the purpose of this is to hopefully save somebody pain. Feel free to\nstash this in your back pocket until XML becomes a problem for you; I'm doing\nthe same.\n\nIt Can't be That Bad\nLet's tackle a few things upfront to save a couple hours of confusion.\n\nFirst off, you know how you've been dot notation to transverse object trees?\nYeah, we can't do that with XML. If we're looking for the child  of a parent, \nparent.child  simply does not work (no, parent['child'] doesn't work either). I\nhope you like looping through trees.\n\nPrint the value of an item in an XML tree doesn’t show you that item's value,\nnor does it show children of that item. It instead prints <Element\n'{http://www.example.com/servicemodel/resources}ItemName' at 0x7fadcf3f83b8>,\nwhich is like a Python equivalent of Javascript's [object Object] in terms of\nusefulness. We can use .text  to see the text value instead of an XML element;.\nGood luck on the other thing, though.\n\nGoing Green\nLet's get this over with and plant some XML trees. I'm going to assume we're\nworking from an API response here.\n\nimport xml.etree.ElementTree as ET\n\ne = ET.fromstring(response.content)\n\n\nIf we were reading an XML file, we'd have to read the file and explicitly search\nfor the root. Even though this doesn’t pertain to us, we should still be aware\nof this inconsistency to avoid future confusion:\n\ne = ET.parse('data.xml')\nroot = e.getroot()\n\n\nWhen we loop through this tree, we 'll need to be mindful of the 3 ways we can\ninteract with XML data. Let's use this tree as an example:\n\n<beer name=\"Bud Light\">\n    <flavor>Water</flavor>\n    <type>Frat</type>\n    <rank>0</rank>\n</beer>\n<beer name=\"PBR\">\n    <flavor>Urine</flavor>\n    <type>Ironic</type>\n    <rank>1</rank>\n</beer>\n<beer name=\"IPA\">\n    <flavor>Pretentious</flavor>\n    <type>Hipster</type>\n    <rank>2</rank>\n</beer>\n\n\n * item.tag  returns the name of the tag. Running this on the first item would\n   return beer, as well as an associated URI.\n * item.attrib()  returns the attributes of the selected item ({'name': 'Bud\n   Light'})\n * item.text  returns the value of text between the open and close tags, if\n   exists.\n\nFinding Stuff\nThere's a few ways to find the data we need in an XML tree, the most obvious of\nwhich would be searching by index. item[0][1]  works, although I have a feeling\nindex-based searching isn't going to be that useful for you.\n\nThe .find and .findall Methods\nOur library has built in .find  and .findall  methods for us to work through a\ntree (returns wither one or all records, as you might have guessed). We search\nby element name as part of a loop:\n\nfor beer in e.findall('beer'):\n    name = beer.get('name') # equivalent to .attrib() in this case\n    flavor = beer.find('flavor').text\n    print(name, \" is \", flavor)\n    \n    \nBud Light is Water    \nPBR is Urine\nIPA is Pretentious\n\n\nThe .iter() Method\nWe can loop through all occurrences of a n element name by using .iter().\n\nfor beertype in e.findall('beer'):\n    print(beertype)\n    \n Frat\n Ironic\n Hipster\n\n\nUsing Some Sort of God-Awful Loop\nIf you're like me you may just skip reading all the documentation altogether,\nget obscenely frustrated, and create some garbage like this:\n\nfor beer in e:\n    for properties in beer:\n          if item.tag == \"{http://www.example.com/beermodel/resources}Type\":\n               print(type)\n                   \nFrat\nIronic\nHipster\n\n\n\nNow that's pretty awful, but I can't tell you had to live your life. You do you.\n\nIn Conclusion\nLook, XML just sucks: don't use it if you don't have to. If you do, save\nyourself some time by coming back to this page.","html":"<p>Life is filled with things we don't want to do; you're a developer so you probably understand this to a higher degree than most people. Sometimes we waste weeks of our lives thanks to an unreasonable and unknowledgeable stakeholder. Other times, we need to deal with XML trees.</p><p>At some point or another you're going to need to work with an API that returns information in XML format. \"Sure,\" we might think, \"I'll just import the standard Python XML package, pick up some syntax nuances, and be on my way.\" That's what I thought too. Today we're going to look at said library, the <a href=\"https://docs.python.org/3/library/xml.etree.elementtree.html\">XML ElementTree</a> library, and see firsthand why this might not be the case.</p><p>As always, the purpose of this is to hopefully save somebody pain. Feel free to stash this in your back pocket until XML becomes a problem for you; I'm doing the same.</p><h2 id=\"it-can-t-be-that-bad\">It Can't be That Bad</h2><p>Let's tackle a few things upfront to save a couple hours of confusion.</p><p>First off, you know how you've been dot notation to transverse object trees? Yeah, we can't do that with XML. If we're looking for the <em>child</em> of a <em>parent</em>, <strong>parent.child</strong> simply does not work (no, parent['child'] doesn't work either). I hope you like looping through trees.</p><p>Print the value of an item in an XML tree doesn’t show you that item's value, nor does it show children of that item. It instead prints <code>&lt;Element '{http://www.example.com/servicemodel/resources}ItemName' at 0x7fadcf3f83b8&gt;</code>, which is like a Python equivalent of Javascript's [object Object] in terms of usefulness. We can use <strong>.text</strong> to see the text value instead of an XML element;. Good luck on the other thing, though.</p><h2 id=\"going-green\">Going Green</h2><p>Let's get this over with and plant some XML trees. I'm going to assume we're working from an API response here.</p><pre><code class=\"language-python\">import xml.etree.ElementTree as ET\n\ne = ET.fromstring(response.content)\n</code></pre>\n<p>If we were reading an XML file, we'd have to read the file and explicitly search for the root. Even though this doesn’t pertain to us, we should still be aware of this inconsistency to avoid future confusion:</p><pre><code class=\"language-python\">e = ET.parse('data.xml')\nroot = e.getroot()\n</code></pre>\n<p>When we loop through this tree, we 'll need to be mindful of the 3 ways we can interact with XML data. Let's use this tree as an example:</p><pre><code class=\"language-xml\">&lt;beer name=&quot;Bud Light&quot;&gt;\n    &lt;flavor&gt;Water&lt;/flavor&gt;\n    &lt;type&gt;Frat&lt;/type&gt;\n    &lt;rank&gt;0&lt;/rank&gt;\n&lt;/beer&gt;\n&lt;beer name=&quot;PBR&quot;&gt;\n    &lt;flavor&gt;Urine&lt;/flavor&gt;\n    &lt;type&gt;Ironic&lt;/type&gt;\n    &lt;rank&gt;1&lt;/rank&gt;\n&lt;/beer&gt;\n&lt;beer name=&quot;IPA&quot;&gt;\n    &lt;flavor&gt;Pretentious&lt;/flavor&gt;\n    &lt;type&gt;Hipster&lt;/type&gt;\n    &lt;rank&gt;2&lt;/rank&gt;\n&lt;/beer&gt;\n</code></pre>\n<ul><li><strong>item.tag</strong> returns the name of the tag. Running this on the first item would return <em>beer</em>, as well as an associated URI.</li><li><strong>item.attrib()</strong> returns the attributes of the selected item (<em>{'name'</em>: <em>'Bud Light'</em>})</li><li><strong>item.text</strong> returns the value of text between the open and close tags, if exists.</li></ul><h2 id=\"finding-stuff\">Finding Stuff</h2><p>There's a few ways to find the data we need in an XML tree, the most obvious of which would be searching by index. <strong>item[0][1]</strong> works, although I have a feeling index-based searching isn't going to be that useful for you.</p><h3 id=\"the-find-and-findall-methods\">The .find and .findall Methods</h3><p>Our library has built in <strong>.find</strong> and <strong>.findall</strong> methods for us to work through a tree (returns wither one or all records, as you might have guessed). We search by element name as part of a loop:</p><pre><code class=\"language-python\">for beer in e.findall('beer'):\n    name = beer.get('name') # equivalent to .attrib() in this case\n    flavor = beer.find('flavor').text\n    print(name, &quot; is &quot;, flavor)\n    \n    \nBud Light is Water    \nPBR is Urine\nIPA is Pretentious\n</code></pre>\n<h3 id=\"the-iter-method\">The .iter() Method</h3><p>We can loop through all occurrences of a n element name by using <strong>.iter()</strong>.</p><pre><code class=\"language-python\">for beertype in e.findall('beer'):\n    print(beertype)\n    \n Frat\n Ironic\n Hipster\n</code></pre>\n<h3 id=\"using-some-sort-of-god-awful-loop\">Using Some Sort of God-Awful Loop</h3><p>If you're like me you may just skip reading all the documentation altogether, get obscenely frustrated, and create some garbage like this:</p><pre><code class=\"language-python\">for beer in e:\n    for properties in beer:\n          if item.tag == &quot;{http://www.example.com/beermodel/resources}Type&quot;:\n               print(type)\n                   \nFrat\nIronic\nHipster\n\n</code></pre>\n<p>Now that's pretty awful, but I can't tell you had to live your life. You do you.</p><h2 id=\"in-conclusion\">In Conclusion</h2><p>Look, XML just sucks: don't use it if you don't have to. If you do, save yourself some time by coming back to this page.</p>","url":"https://hackersandslackers.com/dealing-with-xml-in-python/","uuid":"c28dd935-2162-4d1e-ac2b-4147ee689094","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b2977caded32f5af8fd673e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673678","title":"Using PyMySQL: Python's MySQL Library","slug":"using-pymysql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1.jpg","excerpt":"The lightweight Python library for interacting with MySQL.","custom_excerpt":"The lightweight Python library for interacting with MySQL.","created_at_pretty":"15 June, 2018","published_at_pretty":"15 June, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-06-14T20:32:21.000-04:00","published_at":"2018-06-15T16:48:12.000-04:00","updated_at":"2019-04-10T00:43:08.000-04:00","meta_title":"Using PyMySQL: Python's MySQL Library | Hackers and Slackers","meta_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","og_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","og_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1-2.jpg","og_title":"Using PyMySQL: Python's MySQL Library","twitter_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1-1.jpg","twitter_title":"Using PyMySQL: Python's MySQL Library","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"It's almost Friday night, and the squad at H+S is ready to get cooking. Dim down\nthe lights and slip into something more comfortable as we take you on this 100%\norganic flavor extravaganza. Tonight's menu? A Python MySQL library: PyMySQL\n[https://github.com/PyMySQL/PyMySQL].\n\nPyMySQL is lightweight and perfect for fulfilling MySQL queries. If you want\nbells and whistles, you're probably barking up the wrong tree (and you probably\nshould’ve used a DB other than MySQL in the first place).\n\nWhy write tutorials for technologies we openly trash talk? Out of necessity, of\ncourse! There's nothing wrong with MySQL, most enterprises are married to it in\nsome way. Thus, A great use case for PyMySQL is for usage in AWS lambda when\nworking with large enterprise systems. We'll get to that, but for now let's cook\nup something good.\n\nHeat up the Stove\nTurn on the gas and prep the table to set with your favorite collection of\nplates! That's right, we're talking boilerplate. We knew this was coming; it\nseems like every time you want to do something tangibly cool, we need to get\ninto the business of managing connections and whatnot.\n\nTo ease the pain, I'll share with you a preferred method of handling opening\nconnections with PyMySQL. Here we set a function to separate basic connection\nlogic and error messaging from our app:\n\nimport sys\nimport pymysql\nimport logger\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)    \n    except:\n        logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n        sys.exit()\n\n\n\nNothing fancy here: we set a global variable conn  to serve as our connection,\nand have some basic logic on how to interact with our database. Running \nopenConnection  will attempt to connect to a MySQL db with supplied credentials,\nor throw an error if something goes horribly wrong.\n\nNow we can keep this separate from the rest of our code. Out of sight, out of\nmind.\n\nMeat and Potatoes\nWith the boring stuff out of the way, let's dig into some goodness. We'll start\noff with a basic use case: selecting all rows from a table:\n\ndef getRecords():\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"SELECT * FROM table\"\n            cur.execute(sql)\n            result = cur.fetchall()\n            print(result)\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords() \n\n\nWe split our function into your standard try/except/finally  breakdown. What\nwe're trying is opening a connection using the function we created earlier, and\nrunning queries against it.\n\nThe preferred syntax in PyMySQL is to keep our query in a single string, as seen\nin our variable sql. With our query ready, we need to execute  the query, fetch \nthe resulting records and print the result. We're sure to close the connection\nonce we're done with executing queries... this is critical to ensure db\nconnections don't stay active.\n\nSimple so far, but we're about to kick it up a notch.\n\nSelecting rows\nYou may have noticed we used .fetchall()  to select all records. This is\nimportant to differentiate from .fetchone(), which simply selects the first\nrecord.\n\nWe can iterate over the rows resulting from .fetchall()  with a simple loop, as\nshown in the example below. Beware: attempting to print the result of \n.fetchall()  will simply result in a single integer, which represents the number\nof rows fetched. If there are instances where we know only one record should be\nreturned, .fetchone()  should be used instead.\n\ndef getRecords(table):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"SELECT * FROM %s\"\n            cur.execute(sql, table)\n            result = cur.fetchall()\n            for row in result:\n                record = {\n                        'id': row[0],\n                        'name': row[1],\n                        'email': row[2],\n                        'phone': row[3],\n\n                    }\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords('table_name')       \n\n\nWhoa, what's with the %s? This is how we pass arguments into queries in PyMySQL.\nThe PyMySQL guys were kind enough to realize how obnoxious it is to constantly\nbreak strings to pass in variables - in the case of SQL queries, this beyond\nobnoxious and borderline unworkable. Remember that MySQL requires explicit\nquotations around passing string values, so queries such as these become a\nnonsensical jumble of escaped characters.\n\nPyMySQL supports backquotes: the distant cousin of the single quotation mark,\nAKA the diagonal quote thing above the tilde ~ button on your keyboard. You\nknow: `. If there comes a time to set a string within your query use this\nelusive quotation as such: sql = \"SELECT * FROM %s WHERE column_name =\n`somevalue`\"Updating Rows of Data\nArguments can be passed as anything inside a query: they simply appear in the\norder in which they are passed. In the below example, we pass table as an\nargument, as well values we want updated, and the identifier for target rows:\n\ndef getRecords(table, data):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"UPDATE %s SET date=%s, numsent=%s WHERE email = %s\"\n            cur.execute(sql, (table, data['date_sent'], data['status'], data['email']))\n            conn.commit()\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n\ndata = {\n    'date_sent': '12/01/2018'\n    'email': 'fakeemail@example.com'\n    'status': 'Confirmed'\n}\ngetRecords('table_name', data)       \n\n\nHeads up:  Note the line conn.commit(). Don't forget that - this is what\nactually commits the update to the database. Forgetting this line and wasting\nhours debugging is somewhat of a rite of passage, but let's just skip all that.\n\nFor Dessert: Usage in AWS Lambda\nIt is my treat to share with you my world famous copy & paste recipe for AWS\nLambda. Here we store all of our db credentials in a separate file called \nrdsconfig.py. We also enable logging to take us through what is happening each\nstep of the way:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n#rds settings\nrds_host  = \"rdsName.dfsd834mire.us-west-3.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n    except:\n        logger.error(\"ERROR: Could not connect to MySql instance.\")\n        sys.exit()\n\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n\nWe thank you all for joining us in this adventure of tantalizing treats. May\nyour data be clean and your stomachs full.\n\nBon appétit.","html":"<p>It's almost Friday night, and the squad at H+S is ready to get cooking. Dim down the lights and slip into something more comfortable as we take you on this 100% organic flavor extravaganza. Tonight's menu? A Python MySQL library: <a href=\"https://github.com/PyMySQL/PyMySQL\">PyMySQL</a>.</p><p>PyMySQL is lightweight and perfect for fulfilling MySQL queries. If you want bells and whistles, you're probably barking up the wrong tree (and you probably should’ve used a DB other than MySQL in the first place).</p><p>Why write tutorials for technologies we openly trash talk? Out of necessity, of course! There's nothing wrong with MySQL, most enterprises are married to it in some way. Thus, A great use case for PyMySQL is for usage in AWS lambda when working with large enterprise systems. We'll get to that, but for now let's cook up something good.</p><h2 id=\"heat-up-the-stove\">Heat up the Stove</h2><p>Turn on the gas and prep the table to set with your favorite collection of plates! That's right, we're talking boilerplate. We knew this was coming; it seems like every time you want to do something tangibly cool, we need to get into the business of managing connections and whatnot.</p><p>To ease the pain, I'll share with you a preferred method of handling opening connections with PyMySQL. Here we set a function to separate basic connection logic and error messaging from our app:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport pymysql\nimport logger\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)    \n    except:\n        logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n        sys.exit()\n\n</code></pre>\n<!--kg-card-end: markdown--><p>Nothing fancy here: we set a global variable <em>conn</em> to serve as our connection, and have some basic logic on how to interact with our database. Running <em>openConnection</em> will attempt to connect to a MySQL db with supplied credentials, or throw an error if something goes horribly wrong.</p><p>Now we can keep this separate from the rest of our code. Out of sight, out of mind.</p><h2 id=\"meat-and-potatoes\">Meat and Potatoes</h2><p>With the boring stuff out of the way, let's dig into some goodness. We'll start off with a basic use case: selecting all rows from a table:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords():\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;SELECT * FROM table&quot;\n            cur.execute(sql)\n            result = cur.fetchall()\n            print(result)\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords() \n</code></pre>\n<!--kg-card-end: markdown--><p>We split our function into your standard <em>try/except/finally</em> breakdown. What we're trying is opening a connection using the function we created earlier, and running queries against it.</p><p>The preferred syntax in PyMySQL is to keep our query in a single string, as seen in our variable <em>sql</em>. With our query ready, we need to <em>execute</em> the query, <em>fetch</em> the resulting records and print the result. We're sure to close the connection once we're done with executing queries... this is critical to ensure db connections don't stay active.</p><p>Simple so far, but we're about to kick it up a notch.</p><h3 id=\"selecting-rows\">Selecting rows</h3><p>You may have noticed we used <em>.fetchall()</em> to select all records. This is important to differentiate from <em>.fetchone()</em>, which simply selects the first record.</p><p>We can iterate over the rows resulting from <em>.fetchall()</em> with a simple loop, as shown in the example below. Beware: attempting to print the result of <em>.fetchall()</em> will simply result in a single integer, which represents the number of rows fetched. If there are instances where we know only one record should be returned, <em>.fetchone()</em> should be used instead.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords(table):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;SELECT * FROM %s&quot;\n            cur.execute(sql, table)\n            result = cur.fetchall()\n            for row in result:\n                record = {\n                        'id': row[0],\n                        'name': row[1],\n                        'email': row[2],\n                        'phone': row[3],\n\n                    }\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords('table_name')       \n</code></pre>\n<!--kg-card-end: markdown--><p>Whoa, what's with the <em><strong>%s</strong></em>? This is how we pass arguments into queries in PyMySQL. The PyMySQL guys were kind enough to realize how obnoxious it is to constantly break strings to pass in variables - in the case of SQL queries, this beyond obnoxious and borderline unworkable. Remember that MySQL requires explicit quotations around passing string values, so queries such as these become a nonsensical jumble of escaped characters.</p><!--kg-card-begin: html--><div class=\"protip\">\nPyMySQL supports backquotes: the distant cousin of the single quotation mark, AKA the diagonal quote thing above the tilde ~ button on your keyboard. You know: `. If there comes a time to set a string within your query use this elusive quotation as such: <code>sql = \"SELECT * FROM %s WHERE column_name = `somevalue`\"</code>\n</div><!--kg-card-end: html--><h3 id=\"updating-rows-of-data\">Updating Rows of Data</h3><p>Arguments can be passed as anything inside a query: they simply appear in the order in which they are passed. In the below example, we pass table as an argument, as well values we want updated, and the identifier for target rows:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords(table, data):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;UPDATE %s SET date=%s, numsent=%s WHERE email = %s&quot;\n            cur.execute(sql, (table, data['date_sent'], data['status'], data['email']))\n            conn.commit()\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n\ndata = {\n    'date_sent': '12/01/2018'\n    'email': 'fakeemail@example.com'\n    'status': 'Confirmed'\n}\ngetRecords('table_name', data)       \n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Heads up:</strong> Note the line <em>conn.commit()</em>. Don't forget that - this is what actually commits the update to the database. Forgetting this line and wasting hours debugging is somewhat of a rite of passage, but let's just skip all that.</p><h2 id=\"for-dessert-usage-in-aws-lambda\">For Dessert: Usage in AWS Lambda</h2><p>It is my treat to share with you my world famous copy &amp; paste recipe for AWS Lambda. Here we store all of our db credentials in a separate file called <em>rdsconfig.py</em>. We also enable logging to take us through what is happening each step of the way:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n#rds settings\nrds_host  = &quot;rdsName.dfsd834mire.us-west-3.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n    except:\n        logger.error(&quot;ERROR: Could not connect to MySql instance.&quot;)\n        sys.exit()\n\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>We thank you all for joining us in this adventure of tantalizing treats. May your data be clean and your stomachs full.</p><p>Bon appétit.</p>","url":"https://hackersandslackers.com/using-pymysql/","uuid":"cd6baf62-981c-4034-ba29-b67d257acbeb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b230915f37f772d33bc1eb1"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673641","title":"Accessing Self-Hosted MySQL  Externally","slug":"accessing-mysql-from-external-domains","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","excerpt":"Connecting to MySQL instances hosted on a VPS.","custom_excerpt":"Connecting to MySQL instances hosted on a VPS.","created_at_pretty":"22 April, 2018","published_at_pretty":"22 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-22T16:27:48.000-04:00","published_at":"2018-04-22T17:20:18.000-04:00","updated_at":"2019-03-28T04:54:42.000-04:00","meta_title":"Accessing MySQL Externally | Hackers and Slackers","meta_description":"How to configure a remote instance of MySQL to accept external connections.","og_description":"How to configure a remote instance of MySQL to accept external connections.","og_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","og_title":"Accessing MySQL Externally","twitter_description":"How to configure a remote instance of MySQL to accept external connections.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","twitter_title":"Accessing MySQL Externally","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"In the previous post [https://hackersandslackers.com/set-up-mysql-database/], we\ngot familiar with the basics of creating and navigating MySQL databases. This\nleads us to the next most logical thing to ask: how can I use this in any\nmeaningful way?\n\nMySQL installations default to refusing connections outside of the local\nmachine's IP address, as we should expect. That said, relational databases\naren't usually being used by a single person on a single machine forever (but if\nyou do, we should hang out). It goes without saying that our MySQL instance\nshould be focusing on uptime and accessibility, or in other terms, far away from\nour destructive personalities.\n\nI adore maintaining databases in the command line as much as the next\nself-hating masochist, but we'll need to accomplish work at some point. That\nmeans the remote database we just set up needs to be open-minded enough to allow\na connection from, say, the IP address of our personal local machine, which\nhappens to have a sexy GUI installed for this very purpose.\n\nMaking these kinds of configuration changes to any service or web server is\nalways a bit of fun. You think your day might suck until you cone home and a\npiece of software treats you like a cyber criminal, kicking and screaming while\nwe attempt the most basic out-of-the-box functionality.\n\nThe fine print here is that we wouldn't recommend messing with any of these\nsettings unless you know what you're doing. Then again, if you knew what you\nwere doing you probably wouldn't be reading this. The point is, if you mess up,\nit's your fault because we warned you.\n\nThe first thing we'll need to touch is the MySQL config found here on Ubuntu:\n\nvim /etc/mysql/mysql.conf.d/mysqld.cnf\n\n\nHere you can set various configurations for MySQL, such as the port number,\ndefault user, etc. The line we're interested in is bind-address.\n\n# The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - \"/etc/mysql/my.cnf\" to set global options,\n# - \"~/.my.cnf\" to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain \"#\" chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n\n\nBy default, bind-address is set to your local host. This is basically a\nwhitelist that allows changes only from the domains or IP addresses specified.\nYou can go ahead and add the address of the external domain you'd like to grant\naccess to here.\n\nCommenting out the line completely opens up MySQL to everybody. So there's that.\n\nNow we need to create a user with which to access the DBL:\n\nmysql -u root -p -h localhost -P 3306\n\n\nUse the CREATE USER  command to create a new homie. In the example below,\n'newuser' is the name of the new user, and '%' is from which location the user\nwill be permitted to make changes. This is usually 'localhost', for example. In\nthis case, we added '%' which means everywhere.\n\nmysql> CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n\n\nGrant all privileges to the new user, and always flush privileges  after making\nsuch modifications.\n\nmysql> GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql> FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nWith these changes made, restart MySQL.\n\nservice mysql restart\n\n\nAssuming this was done correctly, your DB should now be able to receive\nread/write queries from an external source, provided the correct username and\npassword are used.","html":"<p>In the <a href=\"https://hackersandslackers.com/set-up-mysql-database/\">previous post</a>, we got familiar with the basics of creating and navigating MySQL databases. This leads us to the next most logical thing to ask: how can I use this in any meaningful way?</p><p>MySQL installations default to refusing connections outside of the local machine's IP address, as we should expect. That said, relational databases aren't usually being used by a single person on a single machine forever (but if you do, we should hang out). It goes without saying that our MySQL instance should be focusing on uptime and accessibility, or in other terms, far away from our destructive personalities.</p><p>I adore maintaining databases in the command line as much as the next self-hating masochist, but we'll need to accomplish work at some point. That means the remote database we just set up needs to be open-minded enough to allow a connection from, say, the IP address of our personal local machine, which happens to have a sexy GUI installed for this very purpose.</p><p>Making these kinds of configuration changes to any service or web server is always a bit of fun. You think your day might suck until you cone home and a piece of software treats you like a cyber criminal, kicking and screaming while we attempt the most basic out-of-the-box functionality.</p><p>The fine print here is that we wouldn't recommend messing with any of these settings unless you know what you're doing. Then again, if you knew what you were doing you probably wouldn't be reading this. The point is, if you mess up, it's your fault because we warned you.</p><p>The first thing we'll need to touch is the MySQL config found here on Ubuntu:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">vim /etc/mysql/mysql.conf.d/mysqld.cnf\n</code></pre>\n<!--kg-card-end: markdown--><p>Here you can set various configurations for MySQL, such as the port number, default user, etc. The line we're interested in is <em>bind-address.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\"># The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - &quot;/etc/mysql/my.cnf&quot; to set global options,\n# - &quot;~/.my.cnf&quot; to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain &quot;#&quot; chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n</code></pre>\n<!--kg-card-end: markdown--><p>By default, bind-address is set to your local host. This is basically a whitelist that allows changes only from the domains or IP addresses specified. You can go ahead and add the address of the external domain you'd like to grant access to here.</p><p>Commenting out the line completely opens up MySQL to everybody. So there's that.</p><p>Now we need to create a user with which to access the DBL:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql -u root -p -h localhost -P 3306\n</code></pre>\n<!--kg-card-end: markdown--><p>Use the <em>CREATE USER</em> command to create a new homie. In the example below, 'newuser' is the name of the new user, and '%' is from which location the user will be permitted to make changes. This is usually 'localhost', for example. In this case, we added '%' which means <em>everywhere.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n</code></pre>\n<!--kg-card-end: markdown--><p>Grant all privileges to the new user, and always <code>flush privileges</code> after making such modifications.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n<!--kg-card-end: markdown--><p>With these changes made, restart MySQL.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">service mysql restart\n</code></pre>\n<!--kg-card-end: markdown--><p>Assuming this was done correctly, your DB should now be able to receive read/write queries from an external source, provided the correct username and password are used.</p>","url":"https://hackersandslackers.com/accessing-mysql-from-external-domains/","uuid":"ca2500b8-b307-4b1c-8ccd-d9cdf4f1e8eb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5adcf04441f6cf7b7a136a4a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867362f","title":"Generating Tree Hierarchies with Treelib","slug":"creating-trees-in-treelib","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","excerpt":"Using Python to visualize file hierarchies as trees.","custom_excerpt":"Using Python to visualize file hierarchies as trees.","created_at_pretty":"17 November, 2017","published_at_pretty":"17 November, 2017","updated_at_pretty":"28 March, 2019","created_at":"2017-11-17T15:45:10.000-05:00","published_at":"2017-11-17T15:56:40.000-05:00","updated_at":"2019-03-28T05:02:39.000-04:00","meta_title":"Tree Hierarchies with Treelib | Hackers and Slackers","meta_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","og_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","og_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","og_title":"Tree Hierarchies with Treelib","twitter_description":"Treelib is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.","twitter_image":"https://hackersandslackers.com/content/images/2017/11/tree7@2x.jpg","twitter_title":"Tree Hierarchies with Treelib","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"The first part of understanding any type of software is taking a glance at its\nfile structure. It may seem like an outlandish and redundant statement to make\nto a generation who grew up on GUIs. GitHub is essentially no more than a GUI\nfor Git, so it’s unsurprisingly that one of the largest company to follow a\nsimilar business model recently bought Github for millions. \n\nAll that said, a question remains: how do we being to understand closed source\napplications? If we can’t see the structure behind an app, I suppose we’ll have\nto build this model ourselves.\n\nTreelib [https://treelib.readthedocs.io/en/latest/]  is a Python library that\nallows you to create a visual tree hierarchy: a simple plaintext representation\nof parent-child relationships.\n\nAside from scraping and mapping the intellectual property of others, Treelib\ncomes in handy in situations where we have access to flat information (like a\ndatabase table) where rows actually relate to one another (such as monolithic\ncontent-heavy site).\n\nTreelib prints results like this: \n\nHarry\n├── Bill\n│   └── n1\n│       ├── n2\n│       └── n3\n└── Jane\n    ├── Diane\n    │   └── Mary\n    └── Mark \n\n\nIt’s is a simple library, and only requires knowledge of a few lines of code in\norder to be used effectively. What’s more, we’re not simply spitting out flat\nuseless data; we're storing these node relationships in memory. If needed, the\ntrees we build can be modified or used for other the future.\n\nWhere da Treez At?\nInstall the Treelib package:\n\npip install treelib\n\n\nIn your project, import Treelib:\n\n# trees.py\nimport from treelib import Node, Tree\n\n\nCreate a Tree with a Parent Node\nThe first step in utilizing Treelib is to create a tree object. We need to give\nour tree a name - this is essentially creating the top-level node that all other\nnodes will stem from. \n\nIn createNode(x, y), X is the value which will be displayed in the node, while Y\nis the unique identifier for that node. Children will be added to this parent\nnode by referencing the unique identifier.\n\nNote that in trees created with TreeLib, unique identifiers may only occur once.\nTherefore it is good to follow a sort of GUI system for identifying nodes.\n\n# tree.py\n\n# Create tree object\ntree = Tree() \n\n# Create the base node\ntree.create_node(\"Confluence\", \"confluence\") \n\n\nCreate Child Nodes\nThe last necessary part of creating a tree is, of course, populating the\nresulting children.\n\nWe will once again use create_node to add additional nodes, but these nodes will\nbe associated with parents via parent=”x”. This will locate existing nodes in\nthe tree by ID and associate these new nodes to that parent. This is why IDs\nmust be unique for each node in the tree.\n\n# tree.py\ntree.create_node(spaceName, id, parent=\"confluence\")\n\n\nView the Tree\nFinally, you'll want to view the fruits of your labor:\n\nprint(tree.show())\n\n\nWay to go Johnny Appleseed, that’s pretty much the gist of it. There are\nadditional features in the way Trees can be parse, and the way that nodes store\nadditional data.\n\nCheck the official documentation [https://treelib.readthedocs.io/en/latest/] \nfor a full list of features.\n\nBonus Round\nIf all you care about is printing the file structure of a current directory with\nzero interest in working with the actual data, you’re in luck (at least on Mac,\nhell if I know anything about Windows).\n\nUnix systems come with a package named tree  which does just what we want. On\nMac OSX, we can install tree  using Homebrew:\n\n$ brew install tree\n\n\nGo ahead and explore the various features of tree, such as writing to files or\neven doing so on a schedule. For now, here's some basic usage:\n\n$ tree -v -L 1 --charset utf-8","html":"<p>The first part of understanding any type of software is taking a glance at its file structure. It may seem like an outlandish and redundant statement to make to a generation who grew up on GUIs. GitHub is essentially no more than a GUI for Git, so it’s unsurprisingly that one of the largest company to follow a similar business model recently bought Github for millions. </p><p>All that said, a question remains: how do we being to understand closed source applications? If we can’t see the structure behind an app, I suppose we’ll have to build this model ourselves.</p><p><strong><a href=\"https://treelib.readthedocs.io/en/latest/\">Treelib</a></strong> is a Python library that allows you to create a visual tree hierarchy: a simple plaintext representation of parent-child relationships.</p><p>Aside from scraping and mapping the intellectual property of others, Treelib comes in handy in situations where we have access to flat information (like a database table) where rows actually relate to one another (such as monolithic content-heavy site).</p><p>Treelib prints results like this: </p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Harry\n├── Bill\n│   └── n1\n│       ├── n2\n│       └── n3\n└── Jane\n    ├── Diane\n    │   └── Mary\n    └── Mark \n</code></pre>\n<!--kg-card-end: markdown--><p>It’s is a simple library, and only requires knowledge of a few lines of code in order to be used effectively. What’s more, we’re not simply spitting out flat useless data; we're storing these node relationships in memory. If needed, the trees we build can be modified or used for other the future.</p><h2 id=\"where-da-treez-at\">Where da Treez At?</h2><p>Install the Treelib package:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">pip install treelib\n</code></pre>\n<!--kg-card-end: markdown--><p>In your project, import Treelib:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># trees.py\nimport from treelib import Node, Tree\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-tree-with-a-parent-node\">Create a Tree with a Parent Node</h3><p>The first step in utilizing Treelib is to create a tree object. We need to give our tree a name - this is essentially creating the top-level node that all other nodes will stem from. </p><p>In createNode(x, y), X is the value which will be displayed in the node, while Y is the unique identifier for that node. Children will be added to this parent node by referencing the unique identifier.</p><p>Note that in trees created with TreeLib, unique identifiers may only occur once. Therefore it is good to follow a sort of GUI system for identifying nodes.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># tree.py\n\n# Create tree object\ntree = Tree() \n\n# Create the base node\ntree.create_node(&quot;Confluence&quot;, &quot;confluence&quot;) \n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"create-child-nodes\">Create Child Nodes</h2><p>The last necessary part of creating a tree is, of course, populating the resulting children.</p><p>We will once again use create_node to add additional nodes, but these nodes will be associated with parents via parent=”x”. This will locate existing nodes in the tree by ID and associate these new nodes to that parent. This is why IDs must be unique for each node in the tree.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># tree.py\ntree.create_node(spaceName, id, parent=&quot;confluence&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"view-the-tree\">View the Tree</h3><p>Finally, you'll want to view the fruits of your labor:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">print(tree.show())\n</code></pre>\n<!--kg-card-end: markdown--><p>Way to go Johnny Appleseed, that’s pretty much the gist of it. There are additional features in the way Trees can be parse, and the way that nodes store additional data.</p><p>Check the <a href=\"https://treelib.readthedocs.io/en/latest/\">official documentation</a> for a full list of features.</p><h2 id=\"bonus-round\">Bonus Round</h2><p>If all you care about is printing the file structure of a current directory with zero interest in working with the actual data, you’re in luck (at least on Mac, hell if I know anything about Windows).</p><p>Unix systems come with a package named <strong>tree</strong> which does just what we want. On Mac OSX, we can install <strong>tree</strong> using Homebrew:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ brew install tree\n</code></pre>\n<!--kg-card-end: markdown--><p>Go ahead and explore the various features of tree, such as writing to files or even doing so on a schedule. For now, here's some basic usage:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ tree -v -L 1 --charset utf-8\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/creating-trees-in-treelib/","uuid":"f0c176ee-c88a-443c-a7b7-b5c5e7c5b9f7","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5a0f4a56e38d612cc826130d"}}]}},"pageContext":{"slug":"dataengineering","limit":12,"skip":12,"numberOfPages":2,"humanPageNumber":2,"prevPageNumber":1,"nextPageNumber":null,"previousPagePath":"/tag/dataengineering/","nextPagePath":null}}