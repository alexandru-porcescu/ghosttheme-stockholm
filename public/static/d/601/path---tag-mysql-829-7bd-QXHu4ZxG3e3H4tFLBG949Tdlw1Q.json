{"data":{"ghostTag":{"slug":"mysql","name":"MySQL","visibility":"public","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c654f34eab17b74dbf2d2c0","title":"Welcome to SQL 4: Aggregate Functions","slug":"welcome-to-sql-4-aggregate-functions","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","custom_excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","created_at_pretty":"14 February, 2019","published_at_pretty":"14 March, 2019","updated_at_pretty":"17 March, 2019","created_at":"2019-02-14T06:21:24.000-05:00","published_at":"2019-03-14T03:10:00.000-04:00","updated_at":"2019-03-17T17:25:34.000-04:00","meta_title":"Welcome to SQL 4: Aggregate Functions | Hackers and Slackers","meta_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","og_title":"Welcome to SQL 4: Aggregate Functions","twitter_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","twitter_title":"Welcome to SQL 4: Aggregate Functions","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Aggregate functions in SQL are super dope. When combining these functions with\nclauses such as GROUP BY  and HAVING, we discover ways to view our data from\ncompletely new perspectives. Instead of looking at the same old endless flat\ntable, we can use these functions to give us entirely new insights; aggregate\nfunctions help us to understand bigger-picture things.  Those things might\ninclude finding outliers in datasets, or simply figuring out which employee with\na family to feed should be terminated, based on some arbitrary metric such as\nsales numbers.\n\nWith the basics of JOINs under our belts, this is when SQL starts feel really,\nreally powerful. Our plain two-dimensional tables suddenly gain this power to be\ncombined, aggregated, folded on to themselves, expand infinitely outward as the\nuniverse itself, and even transcend into the fourth dimension.*\n\n*Needs citationOur Base Aggregation Functions\nFirst up, let's see what we mean by \"aggregate functions\" anyway. These simple\nfunctions provide us with a way to mathematically quantify what exactly is in\nour database. Aggregate functions are performed on table columns to give us the\nmake-up of said column. On their own, they seem quite simple:\n\n * AVG: The average of a set of values in a column.\n * COUNT: Number of rows a column contains in a specified table or view.\n * MIN: The minimum value in a set of values.\n * MAX: The maximum value in a set of values.\n * SUM: The sum of values.\n\nDISTINCT Aggregations\nA particularly useful way of using aggregate functions on their own is when we'd\nlike to know the number of DISTINCT  values. While aggregate values take all\nrecords into account, using DISTINCT  limits the data returned to specifically\nrefer to unique values. COUNT(column_name)  will return the number of all\nrecords in a column, where COUNT(DISTINCT column_name)  will ignore counting\nrecords where the value in the counted column is repeated.\n\nUsing GROUP BY\nThe GROUP BY  statement is often used with aggregate functions (COUNT, MAX, MIN,\nSUM, AVG) to group the result-set by one or more columns.\n\nTo demonstrate how aggregate functions work moving forward, I'll be using a\nfamiliar database: the database which contains all the content for this very\nblog. Let's look at how we can use aggregate functions to find which authors\nhave been posting most frequently:\n\nSELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n\n\nAnd the result:\n\nCount\n author_id\n 102\n 1\n 280\n 5c12c3821345c22dced9f591\n 17\n 5c12c3821345c22dced9f592\n 5\n 5c12c3821345c22dced9f593\n 2\n 5c12c3821345c22dced9f594\n 2\n 5c12c3821345c22dced9f595\n Oh look, a real-life data problem to solve! It seems like authors are\nrepresented in Ghost's posts  table simply by their IDs. This isn't very useful.\nLuckily, we've already learned enough about JOINs\n[https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/] \n to know we can fill in the missing information from the users  table!\n\nSELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n\n\nLet's see the results this time around:\n\nCount\n author_id\n 280\n Matthew Alhonte\n 102\n Todd Birchard\n 17\n Max Mileaf\n 5\n Ryan Rosado\n 2\n Graham Beckley\n 2\n David Aquino\n Now that's more like it! Matt is crushing the game with his Lynx Roundup \nseries, with myself in second place. Max had respectable numbers for a moment\nbut has presumably moved on to other hobbies, such as living his life.\n\nFor the remainder, well, I've got nothing to say other than we're hiring. We\ndon't pay though. In fact, there's probably zero benefits to joining us.\n\nConditional Grouping With \"HAVING\"\nHAVING  is like the WHERE  of aggregations. We can't use WHERE  on aggregate\nvalues, so that's why HAVING  exists. HAVING  can't accept any conditional\nvalue, but instead it must accept a numerical conditional derived from a GROUP\nBY. Perhaps this would be easier to visualize in a query:\n\nSELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n\n\nIn this scenario, we want to see which tags on our blog have the highest number\nof associated posts. The query is very similar to the one we made previously,\nonly this time we have a special guest:\n\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\n\n\nThis usage of HAVING  only gives us tags which have ten posts or more. This\nshould clean up our report by letting Darwinism takes its course. Here's how it\nworked out:\n\ntag\n Count\n Roundup\n 263\n Python\n 80\n Machine Learning\n 29\n DevOps\n 28\n Data Science\n 28\n Software Development\n 27\n Data Engineering\n 23\n Excel\n 19\n SQL\n 18\n Architecture\n 18\n REST APIs\n 16\n #Adventures in Excel\n 16\n Pandas\n 15\n Flask\n 14\n Data Analysis\n 12\n JavaScript\n 12\n AWS\n 11\n MySQL\n 11\n As expected, Matt's roundup posts take the lead (and if we compare this to\nprevious data, we can see Matt has made a total of 17  non-Lynx posts: meaning\nMax and Matt are officially TIED).\n\nIf we hadn't included our HAVING  statement, this list would be much longer,\nfilled with tags nobody cares about. Thanks to explicit omission, now we don't\nneed to experience the dark depression that comes when confronting those sad\npathetic tags. Out of sight, out of mind.\n\nGet Creative\nAggregate functions aren't just about counting values. Especially in Data\nScience, these functions are critical  to drawing any statistical conclusions\nfrom data. That said, attention spans only last so long, and I'm not a\nscientist. Perhaps that can be your job.","html":"<p>Aggregate functions in SQL are super dope. When combining these functions with clauses such as <code>GROUP BY</code> and <code>HAVING</code>, we discover ways to view our data from completely new perspectives. Instead of looking at the same old endless flat table, we can use these functions to give us entirely new insights; aggregate functions help us to understand bigger-picture things.<em> </em>Those things might include finding outliers in datasets, or simply figuring out which employee with a family to feed should be terminated, based on some arbitrary metric such as sales numbers.</p><p>With the basics of <code>JOIN</code>s under our belts, this is when SQL starts feel really, really powerful. Our plain two-dimensional tables suddenly gain this power to be combined, aggregated, folded on to themselves, expand infinitely outward as the universe itself, and even transcend into the fourth dimension.*</p><!--kg-card-begin: html--><div style=\"color:grey; text-align: right; font-style: italic;\">\n    *Needs citation\n</div><!--kg-card-end: html--><h2 id=\"our-base-aggregation-functions\">Our Base Aggregation Functions</h2><p>First up, let's see what we mean by \"aggregate functions\" anyway. These simple functions provide us with a way to mathematically quantify what exactly is in our database. Aggregate functions are performed on table columns to give us the make-up of said column. On their own, they seem quite simple:</p><ul><li><code>AVG</code>: The average of a set of values in a column.</li><li><code>COUNT</code>: Number of rows a column contains in a specified table or view.</li><li><code>MIN</code>: The minimum value in a set of values.</li><li><code>MAX</code>: The maximum value in a set of values.</li><li><code>SUM</code>: The sum of values.</li></ul><h3 id=\"distinct-aggregations\">DISTINCT Aggregations</h3><p>A particularly useful way of using aggregate functions on their own is when we'd like to know the number of <code>DISTINCT</code> values. While aggregate values take all records into account, using <code>DISTINCT</code> limits the data returned to specifically refer to unique values. <code>COUNT(column_name)</code> will return the number of all records in a column, where <code>COUNT(DISTINCT column_name)</code> will ignore counting records where the value in the counted column is repeated.</p><h2 id=\"using-group-by\">Using GROUP BY</h2><p>The <code>GROUP BY</code> statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns.</p><p>To demonstrate how aggregate functions work moving forward, I'll be using a familiar database: the database which contains all the content for this very blog. Let's look at how we can use aggregate functions to find which authors have been posting most frequently:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>And the result:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n  <table>\n    <thead>\n      <tr>\n        <th>Count</th>\n        <th>author_id</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>102</td>\n        <td>1</td>\n      </tr>\n      <tr>\n        <td>280</td>\n        <td>5c12c3821345c22dced9f591</td>\n      </tr>\n      <tr>\n        <td>17</td>\n        <td>5c12c3821345c22dced9f592</td>\n      </tr>\n      <tr>\n        <td>5</td>\n        <td>5c12c3821345c22dced9f593</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f594</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f595</td>\n      </tr>\n    </tbody>\n  </table>\n</div><!--kg-card-end: html--><p>Oh look, a real-life data problem to solve! It seems like authors are represented in Ghost's <strong><em>posts</em></strong> table simply by their IDs. This isn't very useful. Luckily, we've <a href=\"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/\">already learned enough about JOINs</a> to know we can fill in the missing information from the <strong><em>users</em></strong> table!</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see the results this time around:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>Count</th>\n      <th>author_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>280</td>\n      <td>Matthew Alhonte</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>Todd Birchard</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>Max Mileaf</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Ryan Rosado</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Graham Beckley</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>David Aquino</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Now that's more like it! Matt is crushing the game with his <strong>Lynx Roundup</strong> series, with myself in second place. Max had respectable numbers for a moment but has presumably moved on to other hobbies, such as living his life.</p><p>For the remainder, well, I've got nothing to say other than we're hiring. We don't pay though. In fact, there's probably zero benefits to joining us.</p><h3 id=\"conditional-grouping-with-having\">Conditional Grouping With \"HAVING\"</h3><p><code>HAVING</code> is like the <code>WHERE</code> of aggregations. We can't use <code>WHERE</code> on aggregate values, so that's why <code>HAVING</code> exists. <code>HAVING</code> can't accept any conditional value, but instead it <em>must </em>accept a numerical conditional derived from a <code>GROUP BY</code>. Perhaps this would be easier to visualize in a query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>In this scenario, we want to see which tags on our blog have the highest number of associated posts. The query is very similar to the one we made previously, only this time we have a special guest:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">HAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\n</code></pre>\n<!--kg-card-end: markdown--><p>This usage of <code>HAVING</code> only gives us tags which have ten posts or more. This should clean up our report by letting Darwinism takes its course. Here's how it worked out:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>tag</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Roundup</td>\n      <td>263</td>\n    </tr>\n    <tr>\n      <td>Python</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <td>Machine Learning</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <td>DevOps</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Data Science</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Software Development</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <td>Data Engineering</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <td>Excel</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <td>SQL</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>Architecture</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>REST APIs</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>#Adventures in Excel</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>Pandas</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <td>Flask</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <td>Data Analysis</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>JavaScript</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>AWS</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <td>MySQL</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>As expected, Matt's roundup posts take the lead (and if we compare this to previous data, we can see Matt has made a total of <strong>17</strong> non-Lynx posts: meaning Max and Matt are officially TIED).</p><p>If we hadn't included our <code>HAVING</code> statement, this list would be much longer, filled with tags nobody cares about. Thanks to explicit omission, now we don't need to experience the dark depression that comes when confronting those sad pathetic tags. Out of sight, out of mind.</p><h3 id=\"get-creative\">Get Creative</h3><p>Aggregate functions aren't just about counting values. Especially in Data Science, these functions are <em>critical</em> to drawing any statistical conclusions from data. That said, attention spans only last so long, and I'm not a scientist. Perhaps that can be your job.</p>","url":"https://hackersandslackers.com/welcome-to-sql-4-aggregate-functions/","uuid":"f45c0ccc-3efc-4963-a236-a23db74c2e96","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654f34eab17b74dbf2d2c0"}},{"node":{"id":"Ghost__Post__5c654ed3eab17b74dbf2d2b0","title":"Welcome to SQL 3: Building Relations and Combining Data Sets","slug":"welcome-to-sql-3-building-relationships-and-combining-data","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","custom_excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","created_at_pretty":"14 February, 2019","published_at_pretty":"26 February, 2019","updated_at_pretty":"10 April, 2019","created_at":"2019-02-14T06:19:47.000-05:00","published_at":"2019-02-25T20:11:28.000-05:00","updated_at":"2019-04-10T10:16:10.000-04:00","meta_title":"Relationships and Combining Data in SQL | Hackers and Slackers","meta_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","og_title":"Welcome to SQL 3: Building Relationships and Combining Data","twitter_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","twitter_title":"Welcome to SQL 3: Building Relationships and Combining Data","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"If you've felt a bit distance or estranged from SQL so far in the series, never\nfear: we're about to discover the magic of what makes relational databases so...\n relational.  Turn down the lights and put on your favorite Marvin Gaye track;\nwe're about to make connections on a whole other level.\n\nI find that existing attempts to explain Database relations (JOINs in\nparticular) have been an utter failure in illustrating these concepts. The Venn\nDiagrams we're all accustomed to seeing mean nothing to somebody who has never\nseen a JOIN occur, and even then, do they really  describe what's happening? I'd\nlove to toss together some quick animations as an alternative, but chances are\nI'll settle for something mediocre like the rest of us.\n\nRelational Databases in Action\nAs much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no,\nnot that  talk; I meant the obligatory \nexample-of-how-two-tables-might-relate-to-one-another  talk. This talk is a bit\nless awkward, but it definitely won't prepare you for the finer things in life.\nJust kidding, data is  the finer part of life. Or at least it is in mine. Let's\nnot linger on that too long.\n\nLet's look at the most common scenario used to illustrate data relationships:\nthe customers  vs. orders  predicament. Let's say we decided to open up an \nOrganic Vegan Paleo Keto Kale Voltron 5000  health-food marketplace to cater to\na high-end clientele: pretentious rich assholes. It just so happens that the\n\"rich asshole\" market is very receptive to best practices in customer relations,\nso we start a CRM to track our best customers. This record-keeping helps us\npretend to remember the names and personalities of our clientele:\n\nCustomers Table\nid\n first_name\n last_name\n email\n gender\n state\n phone\n 653466635\n Timothea\n Crat\n tcrat0@bandcamp.com\n Female\n Washington\n 206-220-3752\n 418540868\n Kettie\n Fuggle\n kfuggle1@cafepress.com\n Female\n California\n 661-793-1372\n 857532654\n Boonie\n Sommerland\n bsommerland2@soundcloud.com\n Male\n North Carolina\n 919-299-0715\n 563295938-4\n Red\n Seldon\n rseldon3@addthis.com\n Male\n Indiana\n 765-880-7420\n 024844147\n Marika\n Gallatly\n mgallatly4@loc.gov\n Female\n New York\n 718-126-1462\n 900992907\n Sharlene\n McMaster\n smcmaster5@gmpg.org\n Female\n Nevada\n 775-376-0931\n 329211747-X\n Grover\n Okey\n gokey6@weather.com\n Male\n Texas\n 915-913-0625\n 656608031\n Farly\n Pluck\n fpluck7@buzzfeed.com\n Male\n Texas\n 432-670-8809\n 906380018\n Sumner\n Pickerell\n spickerellb@bloglovin.com\n Male\n Colorado\n 719-239-5042\n On the other hand, we need to keep track of inventory and items sold. Since\nwe're already swiping credit cards and getting all this personal customer data,\nwhy not associate purchases to loyal customers? Thus, we have a list of\ntransactions which looks something as such:\n\nOrders Table\nitem_id\n customer_id\n item_purchased\n first_name\n last_name\n amount\n date_purchased\n 82565290-530d-4272-9c8b-38dc0bc7426a\n 653466635\n Creme De Menthe Green\n Timothea\n Crat\n $8.57\n 5/13/18\n 9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0\n 653466635\n Veal Inside - Provimi\n Timothea\n Crat\n $5.77\n 3/3/18\n 5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11\n 656608031\n Arizona - Plum Green Tea\n Grover\n Okey\n $1.72\n 9/6/18\n b4813421-12e8-479b-a3b6-3d1c4c539625\n 656608031\n Beer - Fruli\n Grover\n Okey\n $4.05\n 10/1/18\n 4e7c8548-340f-4e89-a7f1-95173dcc6e53\n 656608031\n Boogies\n Grover\n Okey\n $1.97\n 12/17/18\n 65261e94-494d-48cc-8d5a-642ae6921600\n 656608031\n Cup - 3.5oz; Foam\n Grover\n Okey\n $1.84\n 11/28/18\n 1bfdca0f-d54a-4845-bbf5-982813ab4a65\n 656608031\n Arizona - Green Tea\n Grover\n Gauford\n $0.22\n 5/23/18\n d20d7add-bad4-4559-8896-d4f6d05aa3dd\n 906380018\n Lemonade - Strawberry; 591 Ml\n Sumner\n Tortoishell\n $7.98\n 10/11/18\n 12134510-bc6c-4bd7-b733-b549a61edaa3\n 906380018\n Pasta - Cappellini; Dry\n Sumner\n Wash\n $0.31\n 11/13/18\n 80f1957c-df4d-40dc-b9c4-2c3939dd0865\n 906380018\n Remy Red Berry Infusion\n Sumner\n Pisculli\n $1.25\n 12/31/18\n a75f7593-3312-43e4-a604-43405f02efdd\n 906380018\n Veal - Slab Bacon\n Sumner\n Janaszewski\n $9.80\n 3/9/18\n c6ef1f55-f35d-4618-8de7-36f59ea6653a\n 906380018-5\n Beans - Black Bean; Dry\n Sumner\n Piegrome\n $1.36\n 12/11/18\n c5b87ee3-da94-41b1-973a-ef544a3ffb6f\n 906380018\n Calypso - Strawberry Lemonade\n Sumner\n Piegrome\n $7.71\n 2/21/19\n e383c58b-d8da-40ac-afd6-7ee629dc95c6\n 656608031\n Basil - Primerba; Paste\n Mohammed\n Reed\n $2.77\n 10/21/18\n d88ccd5b-0acb-4144-aceb-c4b4b46d3b17\n 656608031\n Cheese - Fontina\n Mohammed\n Reed\n $4.24\n 7/14/18\n 659df773-719c-447e-a1a9-4577dc9c6885\n 656608031\n Cotton Wet Mop 16 Oz\n Jock\n Skittles\n $8.44\n 1/24/19\n ff52e91e-4a49-4a52-b9a5-ddc0b9316429\n 656608031\n Pastry - Trippleberry Muffin - Mini\n Jock\n Skittles\n $9.77\n 11/17/18\n 86f8ad6a-c04c-4714-8f39-01c28dcbb3cb\n 656608031\n Bread - Olive\n Jock\n Skittles\n $4.51\n 1/10/19\n e7a66b71-86ff-4700-ac57-71291e6997b0\n 656608031\n Wine - White; Riesling; Semi - Dry\n Farly\n Pluck\n $4.23\n 4/15/18\n c448db87-1246-494a-bae4-dceb8ee8a7ae\n 656608031\n Melon - Honey Dew\n Farly\n Pluck\n $1.00\n 9/10/18\n 725c171a-452d-45ef-9f23-73ef20109b90\n 656608031\n Sugar - Invert\n Farly\n Pluck\n $9.04\n 3/24/18\n 849f9140-1469-4e23-a1de-83533af5fb88\n 656608031\n Yokaline\n Farly\n Pluck\n $3.21\n 12/31/18\n 2ea79a6b-bfec-4a08-9457-04128f3b37a9\n 656608031\n Cake - Bande Of Fruit\n Farly\n Pluck\n $1.57\n 5/20/18\n Naturally, customers buy more than one item; they buy a lot. Especially that \nFarly Pluck guy at the bottom- quite the unfortunate auto-generated name.\n\nAs standalone tables, the customers  and orders  tables each serve at least one\nstraightforward purpose on their own. The Customers  table helps us with\nconsumer demographic analysis, whereas the Orders  table makes sure we’re making\nmoney and aren't getting robbed. While important, neither of the functions are\nparticularly revolutionary: this basic level of record keeping has been at the\ncore of nearly every business since the 70s. \n\nThe ability to combine data enables us to gain far more significant insights. We\ncan reward loyal customers, cater to the needs of individuals based on their\npreferences, and perhaps even sell the personal data of where and when Mr. Pluck\nhas been every Tuesday and Thursday for the past 4 months to the highest bidding\ndata broker (hint: he's at our store).\n\nThanks to relational databases, we are neither limited to single monolithic\ntables nor are we shackled by the constraints of the tables we set up front.\nAssociating data is trivial, as long as we have a means by which to associate it\nby. Below is a visualization of matching a foreign key  in our orders table to a\n primary key  in our Customers  table:\n\nAn Order's Foreign Key References a customer's IDThe above illustrates what\nwe've already brushed on a bit: Foreign Key association. Primary and foreign\nkeys are essential to describing relations between the tables, and in performing\nSQL joins. Without further adieu, let's join some data.\n\nJoining Sets of Data\nTo “join” multiple sets of data is to consolidate multiple tables into one. \n\nThe manner of this consolidation is determined by which of the four methods of\njoining tables we use: inner joins, right joins, left joins, and outer joins \n(left and right joins are kind of the same, but whatever). Regardless of the\ntype of join, all joins have the following in common:\n\n * Row comparison: we look for rows where the values of a column in Table A \n   match the values of a column in Table B.\n * Consolidation of columns: The purpose of any join is to come away with a\n   table containing columns from both  tables. \n\nLEFT & RIGHT JOINs\nLEFT  and RIGHT  joins cover a myriad of use cases. With a bit of creativity,\nleft/right joins can help solve problems we may not have expected. The terms \"\nleft\" and \"right\" refer to the table we'd like to join on when reading from\nleft-to-right. When joining tables via LEFT JOIN, the first  table in our query\nwill be the \"left\" table. Alternatively, a RIGHT JOIN  refers to the last \ntable. \n\nWhen we say \"table to join on,\" we're specifying which table's key values will\nbe the \"authority\" for our merge. In a LEFT MERGE, all  of the records in Table\nA will survive the merge:\n\n * For rows which have a match in Table B, these rows will be 'extended' to\n   include the data in Table B. This means the new columns being added to Table\n   A  from  Table B  will contain data for all rows where an association has\n   been made.\n * For rows which exist in Table A  but do NOT have a match in Table B, these\n   rows are unaffected: they will contain the same data as before the join, with\n   values in the new columns left blank.\n * Keys which exist in Table B  but do NOT exist in Table A  will be discarded.\n   The purpose of these joins is to enrich the data of the primary table.\n\nBelow is an example of an actual left join I use to power the Kanban board\nmodule on our \"Projects\" page. The left table is a table of JIRA issues, and the\nright table is a collection of issue-based customizations, such as custom icons\nand colors for issue types. Take a look at how this data is associated, and what\nmakes it into the final table:\n\nKeys on the left table determine which rows stay or go.The structure of a LEFT JOIN  query looks as such:\n\nSELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n\n\nHere's an example with actual values:\n\nSELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n\n\nCompare this to a RIGHT JOIN:\n\nSELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n\n\nINNER JOIN (or CROSS JOIN)\nInner joins are the most conservative method for joining sets of data. Unlike \nLEFT  or RIGHT  joins, there is no authoritative table in an inner join:  only\nrows which contain a match in all  tables will survive the join. All other rows\nwill be ignored:\n\nSELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nBecause inner joins will only act on rows which match in all affected tables, an\ninner join will typically contain the most \"complete\" data set (highest number\nof columns satisfied with values), but will contain the fewest number of rows. \n\nOUTER JOINs\nOuter joins  actually come in a few different flavors. Generally speaking, outer\njoins maximize the amount of data which will survive after the join is\nperformed. \n\nLEFT (OR RIGHT) OUTER JOIN\nAt first glance, you might look at the results of a left/right outer  join and\nmistake them to exactly the same as their pure left/right join counterparts.\nWell, you actually wouldn't be mistaken at all! That's right, I was lying:\nthere's essentially no difference between types of joins (thus our time\nmentioning them has been worthless).\n\nFULL OUTER JOIN\nIn a full outer join, all  columns and rows will be joined into the resulting\noutput, regardless of whether or not the rows matched on our specified key. Why\ndo we specify a key at all, you ask? Matching rows on a key still  combines rows\nwhich are similar to all involved tables (if there are truly no rows with common\nground during a merge, you should ask yourself why you're merging two unrelated\nsets of data in the first place).\n\nThe result is kind of a mess. I'm going to borrow an illustration from the \nPandas  documentation here:\n\nSource: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\nWhile Column B appears to be left somewhat intact, take a look at what's\nhappening around it: columns labeled A_x and A_y  have been generated as a\nresult of the join. The outer join has created a table where every possible\ncombination of values for the keys in column B exists. Thus, the number of rows\nin our new table is effectively length of Table A  *  length of Table B.\n\nI personally rarely use outer joins, but that's just me.\n\nSELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nScenario: Create a New Table from Multiple JOINs\nSo far we've only looked at examples of two tables being joined at once. In\nfact, we can merge as many tables as we want, all at once! Going back to the\nJIRA example, here is the actual query I use to create the final table which\npowers a custom Kanban board:\n\nCREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\n\nIf you're using PostgreSQL, views are a great way to save the results of a join\nwithout adding additional tables. Instead of using CREATE TABLE, try using \nCREATE VIEW:CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\nUnions & Union All\nA good way to think about JOINs is extending our dataset horizontally. A UNION,\nthen, is a way of combining data vertically. Unions  combine data sets with the\nsame structure: they simply create a table with rows from both tables. UNION \noperators can combine the result-set of two or more SELECT statements, as long\nas:\n\n * Each SELECT statement within UNION must have the same number of columns.\n * The columns must also have similar data types.\n * The columns in each SELECT statement must also be in the same order.\n\nUNION\nSELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n\n\nUNION (with WHERE)\nWe can also add logic to unions via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nUNION ALL\nAn interesting distinction is the presence of UNION  versus UNION ALL. Of the\ntwo, UNION  is the more \"intelligent\" operation: if identical rows exist in both\nSELECT queries, a UNION  will know to only give us one row to avoid duplicates.\nOn the other hand, UNION ALL  does  return duplicates: this results in a faster\nquery and could be useful for those who want to know what is in both SELECT \nstatements:\n\nSELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n\n\nUNION ALL (with WHERE)\nJust like UNION, we can add logic to union all via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nMore SQL Ahead\nI hope that visualizing the way which JOINs  and UNIONs  work can help to reduce\nfriction for SQL new-comers. I find it difficult to believe that human beings\ncan fully grasp these concepts without seeing them happen first-hand, which begs\nthe question: why would anybody explore something so poorly explained, without\nknowing the benefits?\n\nIf you find these guides useful, feel welcome to holler at me to keep them\ncoming. We still have more SQL ahead in our series: stay tuned for when we\nexplore aggregate values and more!","html":"<p>If you've felt a bit distance or estranged from SQL so far in the series, never fear: we're about to discover the magic of what makes relational databases so... <em>relational.</em> Turn down the lights and put on your favorite Marvin Gaye track; we're about to make connections on a whole other level.</p><p>I find that existing attempts to explain Database relations (JOINs in particular) have been an utter failure in illustrating these concepts. The Venn Diagrams we're all accustomed to seeing mean nothing to somebody who has never seen a JOIN occur, and even then, do they <em>really</em> describe what's happening? I'd love to toss together some quick animations as an alternative, but chances are I'll settle for something mediocre like the rest of us.</p><h2 id=\"relational-databases-in-action\">Relational Databases in Action</h2><p>As much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no, not <em>that</em> talk; I meant the obligatory <em>example-of-how-two-tables-might-relate-to-one-another</em> talk. This talk is a bit less awkward, but it definitely won't prepare you for the finer things in life. Just kidding, data <em>is</em> the finer part of life. Or at least it is in mine. Let's not linger on that too long.</p><p>Let's look at the most common scenario used to illustrate data relationships: the <strong>customers</strong> vs. <strong>orders</strong> predicament. Let's say we decided to open up an <strong>Organic Vegan Paleo Keto Kale Voltron 5000</strong> health-food marketplace to cater to a high-end clientele: pretentious rich assholes. It just so happens that the \"rich asshole\" market is very receptive to best practices in customer relations, so we start a CRM to track our best customers. This record-keeping helps us pretend to remember the names and personalities of our clientele:</p><h3 id=\"customers-table\">Customers Table</h3><!--kg-card-begin: html--><style>\n    .table1 td {\n        padding: 15px;\n    display: table-cell;\n    text-align: left;\n    vertical-align: middle;\n    font-size: 0.8em;\n    text-align: center;\n    line-height: 1.2;\n    font-size: .75em;\n    }\n    \n    \n    .table1 td:nth-of-type(4) {\n        max-width: 80px;\n    }\n    .table1 td:last-of-type{\n        min-width: 100px;\n    }\n    \n        \n</style>\n\n<div class=\"tableContainer\">\n<table class=\"table1\">\n\t<thead>\n       <tr>\n              <th>id</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>email</th>\n              <th>gender</th>\n              <th>state</th>\n              <th>phone</th>\n          </tr>\n    </thead>\n    <tbody>\n       <tr>\n              <td>653466635</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>tcrat0@bandcamp.com</td>\n              <td>Female</td>\n              <td>Washington</td>\n              <td>206-220-3752</td>\n          </tr>\n       <tr>\n              <td>418540868</td>\n              <td>Kettie</td>\n              <td>Fuggle</td>\n              <td>kfuggle1@cafepress.com</td>\n              <td>Female</td>\n              <td>California</td>\n              <td>661-793-1372</td>\n          </tr>\n       <tr>\n              <td>857532654</td>\n              <td>Boonie</td>\n              <td>Sommerland</td>\n              <td>bsommerland2@soundcloud.com</td>\n              <td>Male</td>\n              <td>North Carolina</td>\n              <td>919-299-0715</td>\n          </tr>\n       <tr>\n              <td>563295938-4</td>\n              <td>Red</td>\n              <td>Seldon</td>\n              <td>rseldon3@addthis.com</td>\n              <td>Male</td>\n              <td>Indiana</td>\n              <td>765-880-7420</td>\n          </tr>\n       <tr>\n              <td>024844147</td>\n              <td>Marika</td>\n              <td>Gallatly</td>\n              <td>mgallatly4@loc.gov</td>\n              <td>Female</td>\n              <td>New York</td>\n              <td>718-126-1462</td>\n          </tr>\n       <tr>\n              <td>900992907</td>\n              <td>Sharlene</td>\n              <td>McMaster</td>\n              <td>smcmaster5@gmpg.org</td>\n              <td>Female</td>\n              <td>Nevada</td>\n              <td>775-376-0931</td>\n          </tr>\n       <tr>\n              <td>329211747-X</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>gokey6@weather.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>915-913-0625</td>\n          </tr>\n       <tr>\n              <td>656608031</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>fpluck7@buzzfeed.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>432-670-8809</td>\n          </tr>\n        <tr>\n              <td>906380018</td>\n              <td>Sumner</td>\n              <td>Pickerell</td>\n              <td>spickerellb@bloglovin.com</td>\n              <td>Male</td>\n              <td>Colorado</td>\n              <td>719-239-5042</td>\n          </tr>\n    </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>On the other hand, we need to keep track of inventory and items sold. Since we're already swiping credit cards and getting all this personal customer data, why not associate purchases to loyal customers? Thus, we have a list of transactions which looks something as such:</p><h3 id=\"orders-table\">Orders Table</h3><!--kg-card-begin: html--><style>\n    .table2 td {\n      padding: 10px 15px;\n    }\n    \n    .table2 tr td:first-of-type {\n        min-width: 100px !important;\n    white-space: nowrap;\n    text-overflow: ellipsis;\n    max-width: 100px;\n    }\n    \n    .table2 tr td:nth-of-type(2) {\n        min-width: 85px !important;\n    }\n</style>\n\n\n<div class=\"tableContainer\">\n   <table class=\"table2\">\n          <thead>\n       <tr>\n              <th>item_id</th>\n              <th>customer_id</th>\n              <th>item_purchased</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>amount</th>\n              <th>date_purchased</th>\n          </tr>\n       </thead>\n       <tbody>\n       <tr>\n              <td>82565290-530d-4272-9c8b-38dc0bc7426a</td>\n              <td>653466635</td>\n              <td>Creme De Menthe Green</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$8.57</td>\n              <td>5/13/18</td>\n          </tr>\n       <tr>\n              <td>9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0</td>\n              <td>653466635</td>\n              <td>Veal Inside - Provimi</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$5.77</td>\n              <td>3/3/18</td>\n          </tr>\n       <tr>\n              <td>5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11</td>\n              <td>656608031</td>\n              <td>Arizona - Plum Green Tea</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.72</td>\n              <td>9/6/18</td>\n          </tr>\n       <tr>\n              <td>b4813421-12e8-479b-a3b6-3d1c4c539625</td>\n              <td>656608031</td>\n              <td>Beer - Fruli</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$4.05</td>\n              <td>10/1/18</td>\n          </tr>\n       <tr>\n              <td>4e7c8548-340f-4e89-a7f1-95173dcc6e53</td>\n              <td>656608031</td>\n              <td>Boogies</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.97</td>\n              <td>12/17/18</td>\n          </tr>\n       <tr>\n              <td>65261e94-494d-48cc-8d5a-642ae6921600</td>\n              <td>656608031</td>\n              <td>Cup - 3.5oz; Foam</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.84</td>\n              <td>11/28/18</td>\n          </tr>\n       <tr>\n              <td>1bfdca0f-d54a-4845-bbf5-982813ab4a65</td>\n              <td>656608031</td>\n              <td>Arizona - Green Tea</td>\n              <td>Grover</td>\n              <td>Gauford</td>\n              <td>$0.22</td>\n              <td>5/23/18</td>\n          </tr>\n       <tr>\n              <td>d20d7add-bad4-4559-8896-d4f6d05aa3dd</td>\n              <td>906380018</td>\n              <td>Lemonade - Strawberry; 591 Ml</td>\n              <td>Sumner</td>\n              <td>Tortoishell</td>\n              <td>$7.98</td>\n              <td>10/11/18</td>\n          </tr>\n       <tr>\n              <td>12134510-bc6c-4bd7-b733-b549a61edaa3</td>\n              <td>906380018</td>\n              <td>Pasta - Cappellini; Dry</td>\n              <td>Sumner</td>\n              <td>Wash</td>\n              <td>$0.31</td>\n              <td>11/13/18</td>\n          </tr>\n       <tr>\n              <td>80f1957c-df4d-40dc-b9c4-2c3939dd0865</td>\n              <td>906380018</td>\n              <td>Remy Red Berry Infusion</td>\n              <td>Sumner</td>\n              <td>Pisculli</td>\n              <td>$1.25</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>a75f7593-3312-43e4-a604-43405f02efdd</td>\n              <td>906380018</td>\n              <td>Veal - Slab Bacon</td>\n              <td>Sumner</td>\n              <td>Janaszewski</td>\n              <td>$9.80</td>\n              <td>3/9/18</td>\n          </tr>\n       <tr>\n              <td>c6ef1f55-f35d-4618-8de7-36f59ea6653a</td>\n              <td>906380018-5</td>\n              <td>Beans - Black Bean; Dry</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$1.36</td>\n              <td>12/11/18</td>\n          </tr>\n       <tr>\n              <td>c5b87ee3-da94-41b1-973a-ef544a3ffb6f</td>\n              <td>906380018</td>\n              <td>Calypso - Strawberry Lemonade</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$7.71</td>\n              <td>2/21/19</td>\n          </tr>\n       <tr>\n              <td>e383c58b-d8da-40ac-afd6-7ee629dc95c6</td>\n              <td>656608031</td>\n              <td>Basil - Primerba; Paste</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$2.77</td>\n              <td>10/21/18</td>\n          </tr>\n       <tr>\n              <td>d88ccd5b-0acb-4144-aceb-c4b4b46d3b17</td>\n              <td>656608031</td>\n              <td>Cheese - Fontina</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$4.24</td>\n              <td>7/14/18</td>\n          </tr>\n       <tr>\n              <td>659df773-719c-447e-a1a9-4577dc9c6885</td>\n              <td>656608031</td>\n              <td>Cotton Wet Mop 16 Oz</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$8.44</td>\n              <td>1/24/19</td>\n          </tr>\n       <tr>\n              <td>ff52e91e-4a49-4a52-b9a5-ddc0b9316429</td>\n              <td>656608031</td>\n              <td>Pastry - Trippleberry Muffin - Mini</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$9.77</td>\n              <td>11/17/18</td>\n          </tr>\n       <tr>\n              <td>86f8ad6a-c04c-4714-8f39-01c28dcbb3cb</td>\n              <td>656608031</td>\n              <td>Bread - Olive</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$4.51</td>\n              <td>1/10/19</td>\n          </tr>\n\t\t\t<tr>\n\n              <td>e7a66b71-86ff-4700-ac57-71291e6997b0</td>\n              <td>656608031</td>\n              <td>Wine - White; Riesling; Semi - Dry</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$4.23</td>\n              <td>4/15/18</td>\n          </tr>\n       <tr>\n              <td>c448db87-1246-494a-bae4-dceb8ee8a7ae</td>\n              <td>656608031</td>\n              <td>Melon - Honey Dew</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.00</td>\n              <td>9/10/18</td>\n          </tr>\n       <tr>\n              <td>725c171a-452d-45ef-9f23-73ef20109b90</td>\n              <td>656608031</td>\n              <td>Sugar - Invert</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$9.04</td>\n              <td>3/24/18</td>\n          </tr>\n       <tr>\n              <td>849f9140-1469-4e23-a1de-83533af5fb88</td>\n              <td>656608031</td>\n              <td>Yokaline</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$3.21</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>2ea79a6b-bfec-4a08-9457-04128f3b37a9</td>\n              <td>656608031</td>\n              <td>Cake - Bande Of Fruit</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.57</td>\n              <td>5/20/18</td>\n          </tr>\n       </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>Naturally, customers buy more than one item; they buy a <em>lot. </em>Especially that <strong>Farly Pluck </strong>guy at the bottom- quite the unfortunate auto-generated name.</p><p>As standalone tables, the <strong>customers</strong> and <strong>orders</strong> tables each serve at least one straightforward purpose on their own. The <strong>Customers</strong> table helps us with consumer demographic analysis, whereas the <strong>Orders</strong> table makes sure we’re making money and aren't getting robbed. While important, neither of the functions are particularly revolutionary: this basic level of record keeping has been at the core of nearly every business since the 70s. </p><p>The ability to combine data enables us to gain far more significant insights. We can reward loyal customers, cater to the needs of individuals based on their preferences, and perhaps even sell the personal data of where and when Mr. Pluck has been every Tuesday and Thursday for the past 4 months to the highest bidding data broker (hint: he's at our store).</p><p>Thanks to relational databases, we are neither limited to single monolithic tables nor are we shackled by the constraints of the tables we set up front. Associating data is trivial, as long as we have a <em>means by which to associate it by</em>. Below is a visualization of matching a <strong>foreign key</strong> in our orders table to a <strong>primary key</strong> in our <strong>Customers</strong> table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/orders4.gif\" class=\"kg-image\"><figcaption>An Order's Foreign Key References a customer's ID</figcaption></figure><!--kg-card-end: image--><p>The above illustrates what we've already brushed on a bit: Foreign Key association. Primary and foreign keys are essential to describing relations between the tables, and in performing SQL joins. Without further adieu, let's join some data.</p><h2 id=\"joining-sets-of-data\">Joining Sets of Data</h2><p>To “join” multiple sets of data is to consolidate multiple tables into one. </p><p>The manner of this consolidation is determined by which of the four methods of joining tables we use: <strong>inner joins</strong>, <strong>right joins</strong>, <strong>left joins</strong>, and <strong>outer joins</strong> (left and right joins are kind of the same, but whatever). Regardless of the type of <em>join</em>, all joins have the following in common:</p><ul><li>Row comparison: we look for rows where the values of a column in <strong>Table A</strong> match the values of a column in <strong>Table B</strong>.</li><li>Consolidation of columns: The purpose of any join is to come away with a table containing columns from <em>both</em> tables. </li></ul><h3 id=\"left-right-joins\">LEFT &amp; RIGHT JOINs</h3><p><code>LEFT</code> and <code>RIGHT</code> joins cover a myriad of use cases. With a bit of creativity, left/right joins can help solve problems we may not have expected. The terms \"<strong>left</strong>\" and \"<strong>right</strong>\" refer to the table we'd like to join on when reading from left-to-right. When joining tables via <code>LEFT JOIN</code>, the <em>first</em> table in our query will be the \"left\" table. Alternatively, a <code>RIGHT JOIN</code> refers to the <em>last</em> table. </p><p>When we say \"table to join on,\" we're specifying which table's key values will be the \"authority\" for our merge. In a <code>LEFT MERGE</code>, <em>all</em> of the records in <strong>Table A </strong>will survive the merge:</p><ul><li>For rows which have a match in <strong>Table B</strong>, these rows will be 'extended' to include the data in <strong>Table B</strong>. This means the new columns being added to <strong>Table A</strong> from<strong> Table B</strong> will contain data for all rows where an association has been made.</li><li>For rows which exist in <strong>Table A</strong> but do NOT have a match in <strong>Table B</strong>, these rows are unaffected: they will contain the same data as before the join, with values in the new columns left blank.</li><li>Keys which exist in <strong>Table B</strong> but do NOT exist in <strong>Table A</strong> will be discarded. The purpose of these joins is to enrich the data of the primary table.</li></ul><p>Below is an example of an actual left join I use to power the Kanban board module on our \"Projects\" page. The left table is a table of JIRA issues, and the right table is a collection of issue-based customizations, such as custom icons and colors for issue types. Take a look at how this data is associated, and what makes it into the final table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/tables15.gif\" class=\"kg-image\"><figcaption>Keys on the left table determine which rows stay or go.</figcaption></figure><!--kg-card-end: image--><p>The structure of a <code>LEFT JOIN</code> query looks as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's an example with actual values:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>Compare this to a <strong>RIGHT JOIN:</strong></p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"inner-join-or-cross-join-\">INNER JOIN (or CROSS JOIN)</h3><p>Inner joins are the most conservative method for joining sets of data. Unlike <code>LEFT</code> or <code>RIGHT</code> joins, there is no authoritative table in an <strong>inner join:</strong> only rows which contain a match in <em>all</em> tables will survive the join. All other rows will be ignored:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Because inner joins will only act on rows which match in all affected tables, an inner join will typically contain the most \"complete\" data set (highest number of columns satisfied with values), but will contain the fewest number of rows. </p><h2 id=\"outer-joins\">OUTER JOINs</h2><p><strong>Outer joins</strong> actually come in a few different flavors. Generally speaking, outer joins maximize the amount of data which will survive after the join is performed. </p><h3 id=\"left-or-right-outer-join\">LEFT (OR RIGHT) OUTER JOIN</h3><p>At first glance, you might look at the results of a left/right <em>outer</em> join and mistake them to exactly the same as their pure left/right join counterparts. Well, you actually wouldn't be mistaken at all! That's right, I was lying: there's essentially no difference between types of joins (thus our time mentioning them has been worthless).</p><h3 id=\"full-outer-join\">FULL OUTER JOIN</h3><p>In a <strong>full outer join</strong>, <em>all</em> columns and rows will be joined into the resulting output, regardless of whether or not the rows matched on our specified key. Why do we specify a key at all, you ask? Matching rows on a key <em>still</em> combines rows which are similar to all involved tables (if there are truly no rows with common ground during a merge, you should ask yourself why you're merging two unrelated sets of data in the first place).</p><p>The result is kind of a mess. I'm going to borrow an illustration from the <strong>Pandas</strong> documentation here:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/merging_merge_on_key_dup.png\" class=\"kg-image\"><figcaption>Source: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html</figcaption></figure><!--kg-card-end: image--><p>While Column B appears to be left somewhat intact, take a look at what's happening around it: columns labeled <strong>A_x </strong>and <strong>A_y</strong> have been generated as a result of the join. The outer join has created a table where every possible combination of values for the keys in column B exists. Thus, the number of rows in our new table is effectively <strong><em>length of Table A</em> </strong>*<strong> <em>length of Table B</em>.</strong></p><p>I personally rarely use <strong>outer joins</strong>, but that's just me.</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"scenario-create-a-new-table-from-multiple-joins\">Scenario: Create a New Table from Multiple JOINs</h2><p>So far we've only looked at examples of two tables being joined at once. In fact, we can merge as many tables as we want, all at once! Going back to the JIRA example, here is the actual query I use to create the final table which powers a custom Kanban board:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">CREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><div class=\"protip\">\n    If you're using PostgreSQL, views are a great way to save the results of a join without adding additional tables. Instead of using <code>CREATE TABLE</code>, try using <code>CREATE VIEW</code>:\n<pre><code>CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;</code></pre>\n</div><!--kg-card-end: markdown--><h2 id=\"unions-union-all\">Unions &amp; Union All</h2><p>A good way to think about <code>JOIN</code>s is extending our dataset <em>horizontally</em>. A <code>UNION</code>, then, is a way of combining data <em>vertically. </em><strong>Unions</strong><em> </em>combine data sets with the same structure: they simply create a table with rows from both tables. <code>UNION</code> operators can combine the result-set of two or more SELECT statements, as long as:</p><ul><li>Each SELECT statement within UNION must have the same number of columns.</li><li>The columns must also have similar data types.</li><li>The columns in each SELECT statement must also be in the same order.</li></ul><h3 id=\"union\">UNION</h3><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-with-where-\">UNION (with WHERE)</h3><p>We can also add logic to <strong>unions </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all\">UNION ALL</h3><p>An interesting distinction is the presence of <code>UNION</code> versus <code>UNION ALL</code>. Of the two, <code>UNION</code> is the more \"intelligent\" operation: if identical rows exist in both SELECT <code>queries</code>, a <code>UNION</code> will know to only give us one row to avoid duplicates. On the other hand, <code>UNION ALL</code> <em>does</em> return duplicates: this results in a faster query and could be useful for those who want to know what is in both <code>SELECT</code> statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all-with-where-\">UNION ALL (with WHERE)</h3><p>Just like <code>UNION</code>, we can add logic to <strong>union all </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"more-sql-ahead\">More SQL Ahead</h2><p>I hope that visualizing the way which <strong>JOINs</strong> and <strong>UNIONs</strong> work can help to reduce friction for SQL new-comers. I find it difficult to believe that human beings can fully grasp these concepts without seeing them happen first-hand, which begs the question: why would anybody explore something so poorly explained, without knowing the benefits?</p><p>If you find these guides useful, feel welcome to holler at me to keep them coming. We still have more SQL ahead in our series: stay tuned for when we explore aggregate values and more!</p>","url":"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/","uuid":"5e222417-19b5-49a7-aa64-fbe042891f00","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654ed3eab17b74dbf2d2b0"}},{"node":{"id":"Ghost__Post__5c654e9aeab17b74dbf2d2a3","title":"Welcome to SQL 2: Working With Data Values","slug":"welcome-to-sql-2-working-with-data-values","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","custom_excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","created_at_pretty":"14 February, 2019","published_at_pretty":"22 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T06:18:50.000-05:00","published_at":"2019-02-21T21:56:50.000-05:00","updated_at":"2019-02-27T22:52:38.000-05:00","meta_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","meta_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","og_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","twitter_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","twitter_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Now that we've gotten the fundamentals of creating databases and tables\n[https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/] \nout of the way, we can start getting into the meat and potatoes of SQL\ninteractions: selecting, updating, and deleting  data.\n\nWe'll start with the basic structure of these queries and then break into the\npowerful operations with enough detail to make you dangerous.\n\nSelecting Data From a Table\nAs mentioned previously, SQL operations have a rather strict order of operations\nwhich clauses have to respect in order to make a valid query. We'll begin by\ndissecting a common SELECT statement:\n\nSELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\";\n\n\nThis is perhaps the most common structure of SELECT queries. First, we list the\nnames of the columns we'd like to select separated by commas. To receive all \ncolumns, we can simply say SELECT *.\n\nThese columns need to come from somewhere, so we specify the table we're\nreferring to next. This either takes a form of FROM table_name \n(non-PostgreSQL), or FROM schema_name.table_name  (PostgreSQL). In theory, a\nsemicolon here would result in a valid query, but we usually want to select rows\nthat meet certain criteria.\n\nThis is where the WHERE  clause comes in: only rows which return \"true\"  for our\n WHERE  conditional will be returned. In the above example, we're validating\nthat a string matches exactly \"Value\". \n\nSelecting only Distinct Values\nSomething that often comes in handy is selecting distinct values in a column. In\nother words, if a value exists in the same column in 100 rows, running DISTINCT \nquery will only show us that value once. This is a good way of seeing the unique\ncontent of a column without yet diving into the distribution of said value. The\neffect is similar to the United States Senate, or the Electoral College: forget\nthe masses, and prop up Wyoming 2020:\n\nSELECT DISTINCT column_name \nFROM table_name;\n\n\nOffsetting and Limiting Results in our Queries\nWhen selecting data, the combination of OFFSET  and LIMIT  are critical at\ntimes. If we're selecting from a database with hundreds of thousands of rows, we\nwould be wasting an obscene amount of system resources to fetch all rows at\nonce; instead, we can have our application or API paginate the results.\n\nLIMIT  is followed by an integer, which in essence says \"return no more than X\nresults.\" \n\nOFFSET  is also followed by an integer, which denotes a numerical starting point\nfor returned results, aka: \"return all results which occur after the Xth\nresult:\"\n\nSELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n\n\nThe above returns the first 50 results. If we wanted to build paginated results\non the application side, we could construct our query like this:\n\nfrom SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n\n\nSuch an application could increment page_number  by 1 each time the user clicks\non to the next page, which would then appropriately modify our query to return\nthe next page of results.\n\nAnother use for OFFSET  could be to pick up where a failed script left off. If\nwe were to write an entire database to a CSV and experience a failure. We could\npick up where the script left off by setting OFFSET  equal to the number of rows\nin the CSV, to avoid running the entire script all over again.\n\nSorting Results\nLast to consider for now is sorting our results by using the ORDER BY  clause.\nWe can sort our results by any specified column, and state whether we'd like the\nresults to be ascending (ASC) or descending (DESC):\n\nSELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\"\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n\n\nSophisticated SELECT Statements\nOf course, we can select rows with WHERE  logic that goes much deeper than an\nexact match. One of the most versatile of these operations is LIKE.\n\nUsing Regex with LIKE\nLIKE  is perhaps the most powerful way to select columns with string values.\nWith LIKE, we can leverage regular expressions to build highly complex logic.\nLet's start with some of my favorites:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE \"%Wade%\";\n\n\nPassing a string to LIKE  with percentage signs on both sides is essentially a \"\ncontains\" statement. %  is equivalent to a wildcard, thus placing %  on either\nside of our string will return true whether the person's first name, middle\nname, or last name is Wade. Check out other useful combinations for %:\n\n * a%: Finds any values that start with \"a\".\n * %a: Finds any values that end with \"a\".\n * %or%: Finds any values that have \"or\" in any position.\n *   _r%: Finds any values that have \"r\" in the second position.\n * a_%_%:  Finds any values that start with \"a\" and are at least 3 characters in\n   length.\n * a%o:  Finds any values that start with \"a\" and ends with \"o\".\n\nFinding Values which are NOT LIKE\nThe opposite of LIKE  is of course NOT LIKE, which runs the same conditional,\nbut returns the opposite true/false value of LIKE:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE \"%Wade%\";\n\n\nConditionals With DateTime Columns\nDateTime columns are extremely useful for selecting data. Unlike plain strings,\nwe can easily extract numerical values for month, day, and year from a DateTime\nby using MONTH(column_name), DAY(column_name), and YEAR(column_name) \nrespectively. For example, using MONTH()  on a column that contains a DateTime\nof 2019-01-26 05:42:34  would return 1, aka January. Because the values come\nback as integers, it is then trivial to find results within a date range:\n\nSELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) < 2018;\n\n\nFinding Rows with NULL Values\nNULL  is a special datatype which essentially denotes the \"absence of\nsomething,\" therefore no conditional will never equal  NULL. Instead, we find\nrows where a value IS NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n\n\nThis should not come as a surprise to anybody familiar with validating\ndatatypes.\n\nThe reverse of this, of course, is NOT NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n\n\nInserting Data\nAn INSERT  query creates a new row, and is rather straightforward: we state the\ncolumns we'd like to insert data into, followed by the values to insert into\nsaid columns:\n\nINSERT INTO table_name (column_1, column_2, column_3)\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nMany things could result in a failed insert. For one, the number of values must\nmatch the number of columns we specify; if we don't we've either provided too\nfew or too many values.\n\nSecond, vales must respect a column's data type. If we try to insert an integer\ninto a DateTime  column, we'll receive an error.\n\nFinally, we must consider the keys and constraints of the table. If keys exist\nthat specify certain columns must not be empty, or must be unique, those keys\nmust too be respected.\n\nAs a shorthand trick, if we're inserting values into all  of a table's columns,\nwe can skip the part where we explicitly list the column names:\n\nINSERT INTO table_name\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nHere's a quick example of an insert query with real data:\n\nINSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n\n\nUPDATE Records: The Basics\nUpdating rows is where things get interesting. There's so much we can do here,\nso let's work our way up:\n\nUPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n\n\nThat's as simple as it gets: the value of a column, in a row that matches our\nconditional. Note that SET  always comes before WHERE. Here's the same query\nwith real data:\n\nUPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n\n\nUPDATE Records: Useful Logic\nJoining Strings Using CONCAT\nYou will find that it's common practice to update rows based on data which\nalready exists in said rows: in other words, sanitizing or modifying data. A\ngreat string operator is CONCAT(). CONCAT(\"string_1\", \"string_2\")  will join all\nthe strings passed to a single string.\n\nBelow is a real-world example of using CONCAT()  in conjunction with NOT LIKE \nto determine which post excerpts don't end in punctuation. If the excerpt does\nnot end with a punctuation mark, we add a period to the end:\n\nUPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n\n\nUsing REPLACE\nREPLACE()  works in SQL as it does in nearly every programming language. We pass\n REPLACE()  three values: \n\n 1. The string to be modified. \n 2. The substring within the string which will be replaced. \n 3. The value of the replacement. \n\nWe can do plenty of clever things with REPLACE(). This is an example that\nchanges the featured image of blog posts to contain the “retina image” suffix: \n\nUPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n\n\nScenario: Folder Structure Based on Date\nI across a fun exercise the other day when dealing with a nightmare situation\ninvolving changing CDNs. It touches on everything we’ve reviewed thus far and\nserves a great illustration of what can be achieved in SQL alone. \n\nThe challenge in moving hundreds of images for hundreds of posts came in the\nform of a file structure. Ghost likes to save images in a dated folder\nstructure, like 2019/02/image.jpg. Our previous CDN did not abide by this at\nall, so had a dump of all images in a single folder. Not ideal. \n\nThankfully, we can leverage the metadata of our posts to discern this file\nstructure. Because images are added to posts when posts are created, we can use\nthe created_at  column from our posts table to figure out the right dated\nfolder: \n\nUPDATE\n  posts\nSET\n  feature_image = CONCAT(\"https://cdn.example.com/posts/\", \n\tYEAR(created_at),\n\t\"/\", \n\tLPAD(MONTH(created_at), 2, '0'), \n\t\"/\",\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n\n\nLet's break down the contents in our CONCAT:\n\n * https://cdn.example.com/posts/: The base URL of our new CDN.\n * YEAR(created_at): Extracting the year from our post creation date\n   (corresponds to a folder).\n * LPAD(MONTH(created_at), 2, '0'): Using MONTH(created_at)  returns a single\n   digit for early months, but our folder structure wants to always have months\n   a double-digits (ie: 2018/01/ as opposed to 2018/1/). We can use LPAD()  here\n   to 'pad' our dates so that months are always two digits long, and shorter\n   dates will be padded with the number 0.\n * SUBSTRING_INDEX(feature_image, '/', - 1): We're getting the filename of each\n   post's image by finding everything that comes after the last slash in our\n   existing image URL. \n\nThe result for every image will now look like this:\n\nhttps://cdn.example.com/posts/2018/02/image.jpg\n\n\nDELETE Records\nLet's wrap up for today with our last type of query, deleting rows:\n\nDELETE FROM celebs \nWHERE twitter_handle IS NULL;","html":"<p>Now that we've gotten the fundamentals of <a href=\"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/\">creating databases and tables</a> out of the way, we can start getting into the meat and potatoes of SQL interactions: <strong>selecting</strong>, <strong>updating</strong>, and <strong>deleting</strong> data.</p><p>We'll start with the basic structure of these queries and then break into the powerful operations with enough detail to make you dangerous.</p><h2 id=\"selecting-data-from-a-table\">Selecting Data From a Table</h2><p>As mentioned previously, SQL operations have a rather strict order of operations which clauses have to respect in order to make a valid query. We'll begin by dissecting a common SELECT statement:</p><pre><code class=\"language-sql\">SELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;;\n</code></pre>\n<p>This is perhaps the most common structure of SELECT queries. First, we list the names of the columns we'd like to select separated by commas. To receive <em>all</em> columns, we can simply say <code>SELECT *</code>.</p><p>These columns need to come from somewhere, so we specify the table we're referring to next. This either takes a form of <code>FROM table_name</code> (non-PostgreSQL), or <code>FROM schema_name.table_name</code> (PostgreSQL). In theory, a semicolon here would result in a valid query, but we usually want to select rows that meet certain criteria.</p><p>This is where the <code>WHERE</code> clause comes in: only rows which return <strong>\"true\"</strong> for our <code>WHERE</code> conditional will be returned. In the above example, we're validating that a string matches exactly <code>\"Value\"</code>. </p><h3 id=\"selecting-only-distinct-values\">Selecting only Distinct Values</h3><p>Something that often comes in handy is selecting distinct values in a column. In other words, if a value exists in the same column in 100 rows, running <code>DISTINCT</code> query will only show us that value once. This is a good way of seeing the unique content of a column without yet diving into the distribution of said value. The effect is similar to the United States Senate, or the Electoral College: forget the masses, and prop up Wyoming 2020:</p><pre><code class=\"language-sql\">SELECT DISTINCT column_name \nFROM table_name;\n</code></pre>\n<h3 id=\"offsetting-and-limiting-results-in-our-queries\">Offsetting and Limiting Results in our Queries</h3><p>When selecting data, the combination of <code>OFFSET</code> and <code>LIMIT</code> are critical at times. If we're selecting from a database with hundreds of thousands of rows, we would be wasting an obscene amount of system resources to fetch all rows at once; instead, we can have our application or API paginate the results.</p><p><code>LIMIT</code> is followed by an integer, which in essence says \"return no more than X results.\" </p><p><code>OFFSET</code> is also followed by an integer, which denotes a numerical starting point for returned results, aka: \"return all results which occur after the Xth result:\"</p><pre><code class=\"language-sql\">SELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n</code></pre>\n<p>The above returns the first 50 results. If we wanted to build paginated results on the application side, we could construct our query like this:</p><pre><code class=\"language-python\">from SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n</code></pre>\n<p>Such an application could increment <code>page_number</code> by 1 each time the user clicks on to the next page, which would then appropriately modify our query to return the next page of results.</p><p>Another use for <code>OFFSET</code> could be to pick up where a failed script left off. If we were to write an entire database to a CSV and experience a failure. We could pick up where the script left off by setting <code>OFFSET</code> equal to the number of rows in the CSV, to avoid running the entire script all over again.</p><h3 id=\"sorting-results\">Sorting Results</h3><p>Last to consider for now is sorting our results by using the <code>ORDER BY</code> clause. We can sort our results by any specified column, and state whether we'd like the results to be ascending (<code>ASC</code>) or descending (<code>DESC</code>):</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n</code></pre>\n<h2 id=\"sophisticated-select-statements\">Sophisticated SELECT Statements</h2><p>Of course, we can select rows with <code>WHERE</code> logic that goes much deeper than an exact match. One of the most versatile of these operations is <code>LIKE</code>.</p><h3 id=\"using-regex-with-like\">Using Regex with LIKE</h3><p><code>LIKE</code> is perhaps the most powerful way to select columns with string values. With <code>LIKE</code>, we can leverage regular expressions to build highly complex logic. Let's start with some of my favorites:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE &quot;%Wade%&quot;;\n</code></pre>\n<p>Passing a string to <code>LIKE</code> with percentage signs on both sides is essentially a \"<strong>contains</strong>\" statement. <code>%</code> is equivalent to a wildcard, thus placing <code>%</code> on either side of our string will return true whether the person's first name, middle name, or last name is <strong>Wade</strong>. Check out other useful combinations for <code>%</code>:</p><ul><li><code>a%</code>: Finds any values that start with \"a\".</li><li><code>%a</code>: Finds any values that end with \"a\".</li><li><code>%or%</code>: Finds any values that have \"or\" in any position.</li><li> <code>_r%</code>: Finds any values that have \"r\" in the second position.</li><li><code>a_%_%</code><strong>:</strong> Finds any values that start with \"a\" and are at least 3 characters in length.</li><li><code>a%o</code>:<strong> </strong>Finds any values that start with \"a\" and ends with \"o\".</li></ul><h3 id=\"finding-values-which-are-not-like\">Finding Values which are NOT LIKE</h3><p>The opposite of <code>LIKE</code> is of course <code>NOT LIKE</code>, which runs the same conditional, but returns the opposite true/false value of <code>LIKE</code>:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE &quot;%Wade%&quot;;\n</code></pre>\n<h3 id=\"conditionals-with-datetime-columns\">Conditionals With DateTime Columns</h3><p>DateTime columns are extremely useful for selecting data. Unlike plain strings, we can easily extract numerical values for month, day, and year from a DateTime by using <code>MONTH(column_name)</code>, <code>DAY(column_name)</code>, and <code>YEAR(column_name)</code> respectively. For example, using <code>MONTH()</code> on a column that contains a DateTime of <code>2019-01-26 05:42:34</code> would return <code>1</code>, aka January. Because the values come back as integers, it is then trivial to find results within a date range:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) &lt; 2018;\n</code></pre>\n<h3 id=\"finding-rows-with-null-values\">Finding Rows with NULL Values</h3><p><code>NULL</code> is a special datatype which essentially denotes the \"absence of something,\" therefore no conditional will never <em>equal</em> <code>NULL</code>. Instead, we find rows where a value <code>IS NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n</code></pre>\n<p>This should not come as a surprise to anybody familiar with validating datatypes.</p><p>The reverse of this, of course, is <code>NOT NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n</code></pre>\n<h2 id=\"inserting-data\">Inserting Data</h2><p>An <code>INSERT</code> query creates a new row, and is rather straightforward: we state the columns we'd like to insert data into, followed by the values to insert into said columns:</p><pre><code class=\"language-sql\">INSERT INTO table_name (column_1, column_2, column_3)\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Many things could result in a failed insert. For one, the number of values must match the number of columns we specify; if we don't we've either provided too few or too many values.</p><p>Second, vales must respect a column's data type. If we try to insert an integer into a <strong>DateTime</strong> column, we'll receive an error.</p><p>Finally, we must consider the keys and constraints of the table. If keys exist that specify certain columns must not be empty, or must be unique, those keys must too be respected.</p><p>As a shorthand trick, if we're inserting values into <em>all</em> of a table's columns, we can skip the part where we explicitly list the column names:</p><pre><code class=\"language-sql\">INSERT INTO table_name\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Here's a quick example of an insert query with real data:</p><pre><code class=\"language-sql\">INSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n</code></pre>\n<h2 id=\"update-records-the-basics\">UPDATE Records: The Basics</h2><p>Updating rows is where things get interesting. There's so much we can do here, so let's work our way up:</p><pre><code class=\"language-sql\">UPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n</code></pre>\n<p>That's as simple as it gets: the value of a column, in a row that matches our conditional. Note that <code>SET</code> always comes before <code>WHERE</code>. Here's the same query with real data:</p><pre><code class=\"language-sql\">UPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n</code></pre>\n<h2 id=\"update-records-useful-logic\">UPDATE Records: Useful Logic</h2><h3 id=\"joining-strings-using-concat\">Joining Strings Using CONCAT</h3><p>You will find that it's common practice to update rows based on data which already exists in said rows: in other words, sanitizing or modifying data. A great string operator is <code>CONCAT()</code>. <code>CONCAT(\"string_1\", \"string_2\")</code> will join all the strings passed to a single string.</p><p>Below is a real-world example of using <code>CONCAT()</code> in conjunction with <code>NOT LIKE</code> to determine which post excerpts don't end in punctuation. If the excerpt does not end with a punctuation mark, we add a period to the end:</p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n</code></pre>\n<h3 id=\"using-replace\">Using REPLACE</h3><p><code>REPLACE()</code> works in SQL as it does in nearly every programming language. We pass <code>REPLACE()</code> three values: </p><ol><li>The string to be modified. </li><li>The substring within the string which will be replaced. </li><li>The value of the replacement. </li></ol><p>We can do plenty of clever things with <code>REPLACE()</code>. This is an example that changes the featured image of blog posts to contain the “retina image” suffix: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n</code></pre>\n<h3 id=\"scenario-folder-structure-based-on-date\">Scenario: Folder Structure Based on Date</h3><p>I across a fun exercise the other day when dealing with a nightmare situation involving changing CDNs. It touches on everything we’ve reviewed thus far and serves a great illustration of what can be achieved in SQL alone. </p><p>The challenge in moving hundreds of images for hundreds of posts came in the form of a file structure. Ghost likes to save images in a dated folder structure, like <strong>2019/02/image.jpg</strong>. Our previous CDN did not abide by this at all, so had a dump of all images in a single folder. Not ideal. </p><p>Thankfully, we can leverage the metadata of our posts to discern this file structure. Because images are added to posts when posts are created, we can use the <strong>created_at</strong> column from our posts table to figure out the right dated folder: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = CONCAT(&quot;https://cdn.example.com/posts/&quot;, \n\tYEAR(created_at),\n\t&quot;/&quot;, \n\tLPAD(MONTH(created_at), 2, '0'), \n\t&quot;/&quot;,\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n</code></pre>\n<p>Let's break down the contents in our <code>CONCAT</code>:</p><ul><li><code>https://cdn.example.com/posts/</code>: The base URL of our new CDN.</li><li><code>YEAR(created_at)</code>: Extracting the year from our post creation date (corresponds to a folder).</li><li><code>LPAD(MONTH(created_at), 2, '0')</code>: Using <strong>MONTH(created_at)</strong> returns a single digit for early months, but our folder structure wants to always have months a double-digits (ie: <strong>2018/01/ </strong>as opposed to <strong>2018/1/</strong>). We can use <code>LPAD()</code> here to 'pad' our dates so that months are always two digits long, and shorter dates will be padded with the number 0.</li><li><code>SUBSTRING_INDEX(feature_image, '/', - 1)</code>: We're getting the filename of each post's image by finding everything that comes after the last slash in our existing image URL. </li></ul><p>The result for every image will now look like this:</p><pre><code>https://cdn.example.com/posts/2018/02/image.jpg\n</code></pre>\n<h2 id=\"delete-records\">DELETE Records</h2><p>Let's wrap up for today with our last type of query, deleting rows:</p><pre><code class=\"language-sql\">DELETE FROM celebs \nWHERE twitter_handle IS NULL;\n</code></pre>\n","url":"https://hackersandslackers.com/welcome-to-sql-2-working-with-data-values/","uuid":"e051cdc6-eb17-425f-bb83-2f70a75e85c5","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654e9aeab17b74dbf2d2a3"}},{"node":{"id":"Ghost__Post__5c5bb0ec7999ff33f06876e1","title":"Welcome to SQL: Modifying Databases and Tables","slug":"welcome-to-sql-modifying-databases-and-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","custom_excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","created_at_pretty":"07 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-06T23:15:40.000-05:00","published_at":"2019-02-19T18:28:00.000-05:00","updated_at":"2019-02-27T23:16:44.000-05:00","meta_title":"Welcome to SQL: Modifying Databases and Tables | Hackers and Slackers","meta_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","og_title":"Welcome to SQL: Modifying Databases and Tables","twitter_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","twitter_title":"Welcome to SQL: Modifying Databases and Tables","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"SQL: we all pretend to be experts at it, and mostly get away with it thanks to\nStackOverflow. Paired with our vast experience of learning how to code in the\n90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go\nahead and chalk up a win for your resume.\n\nSQL has been around longer than our careers have, so why start a series on it \nnow?  Surely there’s sufficient enough documentation that we can Google the\nspecifics whenever the time comes for us to write a query? That, my friends, is\nprecisely the problem. Regardless of what tools we have at our disposable, some\nskills are better learned and practiced by heart. SQL is one of those skills.\n\nSure, SQLAlchemy or similar ORMs might protect us here-and-there from writing\nraw queries. Considering SQL is just one of many query languages we'll use\nregularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert\nreally that critical? In short, yes: relational databases are not only here to\nstay, but thinking  in queries as a second language solidifies one's\nunderstanding of the fine details of data. Marc Laforet\n[https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032] \n recently published a Medium post which drives home just how important leaning\non SQL is:\n\n> What’s even more interesting is that when these transformation scripts were\napplied to the 6.5 GB dataset, python completely failed. Out of 3 attempts,\npython crashed 2 times and my computer completely froze the 3rd time… while SQL\ntook 226 seconds.\n\n\nKeeping logic out of our apps and pipelines and in SQL results in exponentially\nfaster execution, while also being more readable and universally understood than\nwhatever we’d write in our language of choice. The lower down we can push\napplication logic in our stack, the better. This is why I’d much prefer to see\nthe datasphere saturated with SQL tutorials as opposed to Pandas tutorials.\n\nRelational Database Terminology\nI hate it when informational material kicks off with covering obvious\nterminology definitions. Under normal circumstances, I find this to be cliche,\nunhelpful, and damaging to an author's credibility; but these aren't normal\ncircumstances. In SQL, vocabulary commonly has multiple meanings depending on\ncontext, or even which flavor database you're using. Given this fact, it's\nentirely possible (and common) for individuals to rack up experience with\nrelational databases while completely misinterpreting fundamental concepts.\nLet's make sure that doesn't happen:\n\n * Databases: Every Database instance is separated at the highest level into \n   databases. Yes, a database is a collection of databases - we're already off\n   to a great start.\n * Schemas: In PostgreSQL (and other databases), a schema  is a grouping of\n   tables and other objects, including views, relations, etc. A schema is a way\n   of organizing data. Schemas imply that all the data belonging to it is at\n   some form related, even if only by concept. Note that the term schema  is\n   sometimes used to describe other concepts depending on the context.\n * Tables: The meat and potatos of relational databases. Tables consist of rows\n   and columns which hold our sweet, sweet data. Columns are best thought of as\n   'attributes', whereas rows are entries which consist of values for said\n   attributes. All values in a column must share the same data type. * Keys: Keys are used to help us organize and optimize data, as well as\n      place certain constraints on data coming in (for example, email addresses\n      of user accounts must be unique). Keys can also help us keep count of our\n      entries, ensure automatically unique values, and provide a bridge to link\n      multiple tables of data. * Primary keys: Identification tags for each row of data. The primary key\n         is different for every record in the relational database; values must\n         be provided, and they must be unique between rows.\n       * Foreign keys: Enable data searches and manipulation between the primary\n         database table and other related databases.\n      \n      \n   \n   \n * Objects: A blanket term for anything (including relations) that exist in a\n   schema (somewhat PostgreSQL-specific). * Views (PostgreSQL): Views display data in a fashion similar to tables,\n      with the difference that views do not store  data. Views are a snapshot of\n      data pulled from other tables in the form of a query; a good way to think\n      about views is to consider them to be 'virtual tables.'\n    * Functions (PostgreSQL): Logic for interacting with data saved for the\n      purpose of being reused.\n   \n   \n\nIn MySQL, a schema  is synonymous with a database. These keywords can even be\nswapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using CREATE\nSCHEMA  acheives the same effect as instead of CREATE DATABASE.Navigating and\nCreating Databases\nWe've got to start somewhere, so it might as well be with database management.\nAdmittedly, this will be the most useless of the things we'll cover. The act of\nnavigating databases is best suited for a GUI.\n\nShow Databases\nIf you access your database via command line shell (for some reason), the first\nlogical thing to do is to list the available databases:\n\nSHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\n\nUSE Database\nNow that we've listed the possible databases we can connect to, we can explore\nwhat each of these contains. To do this, we have to specify which database we\nwant to connect to, AKA \"use.\" \n\ndb> USE database_name;\nDatabase changed\n\n\nCreate Database\nCreating databases is straightforward. Be sure to pay attention to the character\nset  when creating a database: this will determine which types of characters\nyour database will be able to accept. For example, if we try to insert special\nencoded characters into a simple UTF-8 database, those characters won’t turn out\nas we’d expect.\n\nCREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n\n\nBonus: here's the shorthand for creating a database and then showing the result:\n\nSHOW CREATE DATABASE database_name;\n\n\nCreating and Modifying Tables\nCreating tables via SQL syntax can be critical when automating data imports.\nWhen creating a table, we also set the column names, types, and keys:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];\n\nWe can specify IF NOT EXISTS  when creating our table if we'd like to include\nvalidation in our query. When present, the table will only be created if a table\nof the specified name does not exist.\n\nWhen creating each of our columns, there are a number of things we can specify\nper-column:\n\n * Data Type (required):  The data which can be saved to cells of this column\n   (such as INTEGER, TEXT, etc).\n * Key Type:  Creates a key for the column.\n * Key Attributes:  Any key-related attributes, such as auto-incrementing.\n * Default:  If rows are created in the table without values passed to the\n   current column, the value specified as DEFAULT  \n * Primary Key:  Allows any of the previous specified columns to be set as the\n   table's primary key.\n\nMySQL tables can have a 'storage engine' specified via ENGINE=[engine_type],\nwhich determines the core logic of how the table will interpret data. Leaving\nthis blank defaults to InnoDB and is almost certainly fine to be left alone. In\ncase you're interested, you can find more about MySQL engines here\n[https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html].\n\nHere's an example of what an actual CREATE TABLE  query would look like:\n\nCREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nManaging Keys for Existing Tables\nIf we don't specify our keys at table creation time, we can always do so after\nthe fact. SQL tables can accept the following key types:\n\n * Primary Key:  One or more fields/columns that uniquely identify a record in\n   the table. It can not accept null, duplicate values.\n * Candidate Key:  Candidate keys are kind of like groups of non-committed\n   Primary Keys; these keys only accept unique values, and could potentially  be\n   used in the place of a Primary Key if need be, but are not actual Primary\n   Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.\n * Alternate Key:  Refers to a single Candidate Key (an alternative which can\n   satisfy the duty of a Primary Key id need be).\n * Composite/Compound Key:  Defined by combing the values of multiple columns;\n   the sum of which will always produce a unique value. There can be multiple\n   Candidate Keys in one table. Each Candidate Key can work as Primary Key.\n * Unique Key:  A set of one or more fields/columns of a table that uniquely\n   identify a record in a database table. Similar to Primary key, but it can\n   accept only one null value, and it can not have duplicate values.\n * Foreign Key: Foreign keys denote fields that serve as another table's \n   Primary key. Foreign keys are useful for building relationships between\n   tables. While a foreign key is required in the parent table where they are\n   primary, foreign keys can be null or empty in the tables intended to relate\n   to the other table.\n\nLet's look at an example query where we add a key to a table and dissect the\npieces:\n\nALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n\n\nALTER TABLE  is used to make any changes to a table's structure, whether that be\nmodifying columns or keys.\n\nIn this example, we ADD  a key which happens to be a FOREIGN KEY. While keys\nalways refer to columns, keys themselves must have names of their own to\ndistinguish the column's data and a key's conceptual logic. We name our key \nforeign_key_name  and specify which column the key will act on with \n(column_name). Because this is a foreign key, we need to specify which table's \nprimary key  we want this to be associated with. REFERENCES\nparent_table(primary_key_column)  is stating that the foreign key in this table\ncorresponds to values held in a column named primary_key_column, in a table\nnamed parent_table.\n\nThe statements ON DELETE  and ON UPDATE  are actions which take place if the\nparent table's primary key is deleted or updated, respectively. ON DELETE\nCASCADE  would result in our tables foreign key being deleted if the\ncorresponding primary key were to disappear.\n\nAdding Columns\nAdding columns follows the same syntax we used when creating tables. An\ninteresting additional feature is the ability to place the new column before or\nafter preexisting columns:\n\nALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n\n\nWhen referencing tables in PostgreSQL databases, we must specify the schema\nbelongs to. Thus, ALTER TABLE table_name  becomes ALTER TABLE\nschema_name.table_name. This applies to any time we reference tables, including\nwhen we create and delete tables.Pop Quiz\nThe below statement uses elements of everything we've learned about modifying\nand creating table structures thus far. Can you discern what is happening here?\n\nCREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n\n\nDropping Data\nDANGER ZONE: this is where we can start to mess things up. Dropping columns or\ntables results in a complete loss of data: whenever you see the word \"drop,\" be\nscared.\n\nIf you're sure you know what you're doing and would like to remove a table\ncolumn, this can be done as such:\n\nALTER TABLE table\nDROP column;\n\n\nDropping a table destroys the table structure as well as all data within it:\n\nDROP TABLE table_name;\n\n\nTruncating a table, on the other hand, will purge the table of data but retain\nthe table itself:\n\nTRUNCATE TABLE table_name;\n\n\nDrop Foreign Key\nLike tables and columns, we can drop keys as well:\n\nALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n\n\nThis can also be handed by dropping CONSTRAINT:\n\nALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n\n\nWorking with Views (Specific to PostgreSQL)\nLastly, let's explore the act of creating views. There are three types of views\nPostgreSQL can handle:\n\n * Simple Views: Virtual tables which represent data of underlying tables.\n   Simple views are automatically updatable: the system will allow INSERT,\n   UPDATE and DELETE statements to be used on the view in the same way as on a\n   regular table.\n * Materialized Views: PostgreSQL extends the view concept to a next level that\n   allows views to store data 'physically', and we call those views are\n   materialized views. A materialized view caches the result of a complex query\n   and then allow you to refresh the result periodically.\n * Recursive Views: Recursive views are a bit difficult to explain without\n   delving deep into the complicated (but cool!) functionality of recursive\n   reporting. I won't get into the details, but these views are able to\n   represent relationships which go multiple layers deep. Here's a quick taste,\n   if you;re curious:\n\nSample RECURSIVE  query:\n\nWITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' > ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n\n\nOutput:\n\n employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North > Megan Berry\n           3 | Michael North > Sarah Berry\n           4 | Michael North > Zoe Black\n           5 | Michael North > Tim James\n           6 | Michael North > Megan Berry > Bella Tucker\n           7 | Michael North > Megan Berry > Ryan Metcalfe\n           8 | Michael North > Megan Berry > Max Mills\n           9 | Michael North > Megan Berry > Benjamin Glover\n          10 | Michael North > Sarah Berry > Carolyn Henderson\n          11 | Michael North > Sarah Berry > Nicola Kelly\n          12 | Michael North > Sarah Berry > Alexandra Climo\n          13 | Michael North > Sarah Berry > Dominic King\n          14 | Michael North > Zoe Black > Leonard Gray\n          15 | Michael North > Zoe Black > Eric Rampling\n          16 | Michael North > Megan Berry > Ryan Metcalfe > Piers Paige\n          17 | Michael North > Megan Berry > Ryan Metcalfe > Ryan Henderson\n          18 | Michael North > Megan Berry > Max Mills > Frank Tucker\n          19 | Michael North > Megan Berry > Max Mills > Nathan Ferguson\n          20 | Michael North > Megan Berry > Max Mills > Kevin Rampling\n(20 rows)\n\n\nCreating a View\nCreating a simple view is as simple as writing a standard query! All that is\nrequired is the addition of CREATE VIEW view_name AS  before the query, and this\nwill create a saved place for us to always come back and reference the results\nof this query:\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n\n\nGet Out There and Start SQLing\nI highly encourage anybody to get in the habit of always writing SQL queries by\nhand. With the right GUI, autocompletion can be your best friend.\n\nExplicitly forcing one's self to write queries instead of copy & pasting\nanything forces us to come to realizations, such as SQL's order of operations.\nIndeed, this query holds the correct syntax...\n\nSELECT *\nFROM table_name\nWHERE column_name = 'Value';\n\n\n...Whereas this one does not:\n\nSELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n\n\nGrasping the subtleties of SQL is the difference between being blazing fast and\nmostly clueless. The good news is, you’ll start to find that these concepts\naren’t nearly as daunting as they may have once seemed, so the track from ‘bad\ndata engineer’ to ‘expert’ is an easy win that would be foolish not to take.\n\nStick around for next time where we actually work with data in SQL: The Sequel,\nrated PG-13.","html":"<p>SQL: we all pretend to be experts at it, and mostly get away with it thanks to StackOverflow. Paired with our vast experience of learning how to code in the 90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go ahead and chalk up a win for your resume.</p><p>SQL has been around longer than our careers have, so why start a series on it <em>now?</em> Surely there’s sufficient enough documentation that we can Google the specifics whenever the time comes for us to write a query? That, my friends, is precisely the problem. Regardless of what tools we have at our disposable, some skills are better learned and practiced by heart. SQL is one of those skills.</p><p>Sure, SQLAlchemy or similar ORMs might protect us here-and-there from writing raw queries. Considering SQL is just one of many query languages we'll use regularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert really that critical? In short, yes: relational databases are not only here to stay, but <em>thinking</em> in queries as a second language solidifies one's understanding of the fine details of data. <a href=\"https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032\">Marc Laforet</a> recently published a Medium post which drives home just how important leaning on SQL is:</p><blockquote>\n<p>What’s even more interesting is that when these transformation scripts were applied to the 6.5 GB dataset, python completely failed. Out of 3 attempts, python crashed 2 times and my computer completely froze the 3rd time… while SQL took 226 seconds.</p>\n</blockquote>\n<p>Keeping logic out of our apps and pipelines and in SQL results in exponentially faster execution, while also being more readable and universally understood than whatever we’d write in our language of choice. The lower down we can push application logic in our stack, the better. This is why I’d much prefer to see the datasphere saturated with SQL tutorials as opposed to Pandas tutorials.</p><h2 id=\"relational-database-terminology\">Relational Database Terminology</h2><p>I hate it when informational material kicks off with covering obvious terminology definitions. Under normal circumstances, I find this to be cliche, unhelpful, and damaging to an author's credibility; but these aren't normal circumstances. In SQL, vocabulary commonly has multiple meanings depending on context, or even which flavor database you're using. Given this fact, it's entirely possible (and common) for individuals to rack up experience with relational databases while completely misinterpreting fundamental concepts. Let's make sure that doesn't happen:</p><ul>\n<li><strong>Databases</strong>: Every Database instance is separated at the highest level into <em>databases</em>. Yes, a database is a collection of databases - we're already off to a great start.</li>\n<li><strong>Schemas</strong>: In PostgreSQL (and other databases), a <em>schema</em> is a grouping of tables and other objects, including views, relations, etc. A schema is a way of organizing data. Schemas imply that all the data belonging to it is at some form related, even if only by concept. Note that the term <em>schema</em> is sometimes used to describe other concepts depending on the context.</li>\n<li><strong>Tables</strong>: The meat and potatos of relational databases. Tables consist of rows and columns which hold our sweet, sweet data. Columns are best thought of as 'attributes', whereas rows are entries which consist of values for said attributes. All values in a column must share the same data type.\n<ul>\n<li><strong>Keys</strong>: Keys are used to help us organize and optimize data, as well as place certain constraints on data coming in (for example, email addresses of user accounts must be <em>unique</em>). Keys can also help us keep count of our entries, ensure automatically unique values, and provide a bridge to link multiple tables of data.\n<ul>\n<li><strong>Primary keys</strong>:  Identification tags for each row of data. The primary key is different for every record in the relational database; values must be provided, and they must be unique between rows.</li>\n<li><strong>Foreign keys</strong>: Enable data searches and manipulation between the primary database table and other related databases.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objects</strong>: A blanket term for anything (including relations) that exist in a schema (somewhat PostgreSQL-specific).\n<ul>\n<li><strong>Views (PostgreSQL)</strong>: Views display data in a fashion similar to tables, with the difference that views do not <em>store</em> data. Views are a snapshot of data pulled from other tables in the form of a query; a good way to think about views is to consider them to be 'virtual tables.'</li>\n<li><strong>Functions  (PostgreSQL)</strong>: Logic for interacting with data saved for the purpose of being reused.</li>\n</ul>\n</li>\n</ul>\n<div class=\"protip\">\nIn MySQL, a <strong>schema</strong> is synonymous with a <strong>database</strong>. These keywords can even be swapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using <code>CREATE SCHEMA</code> acheives the same effect as instead of <code>CREATE DATABASE</code>.   \n</div><h2 id=\"navigating-and-creating-databases\">Navigating and Creating Databases</h2><p>We've got to start somewhere, so it might as well be with database management. Admittedly, this will be the most useless of the things we'll cover. The act of navigating databases is best suited for a GUI.</p><h3 id=\"show-databases\">Show Databases</h3><p>If you access your database via command line shell (for some reason), the first logical thing to do is to list the available databases:</p><pre><code class=\"language-sql\">SHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n</code></pre>\n<h3 id=\"use-database\">USE Database</h3><p>Now that we've listed the possible databases we can connect to, we can explore what each of these contains. To do this, we have to specify which database we want to connect to, AKA \"use.\" </p><pre><code class=\"language-sql\">db&gt; USE database_name;\nDatabase changed\n</code></pre>\n<h3 id=\"create-database\">Create Database</h3><p>Creating databases is straightforward. Be sure to pay attention to the <em>character set</em> when creating a database: this will determine which types of characters your database will be able to accept. For example, if we try to insert special encoded characters into a simple UTF-8 database, those characters won’t turn out as we’d expect.</p><pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n</code></pre>\n<p>Bonus: here's the shorthand for creating a database and then showing the result:</p><pre><code class=\"language-sql\">SHOW CREATE DATABASE database_name;\n</code></pre>\n<h2 id=\"creating-and-modifying-tables\">Creating and Modifying Tables</h2><p>Creating tables via SQL syntax can be critical when automating data imports. When creating a table, we also set the column names, types, and keys:</p><pre><code>CREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];</code></pre><p>We can specify <code>IF NOT EXISTS</code> when creating our table if we'd like to include validation in our query. When present, the table will only be created if a table of the specified name does not exist.</p><p>When creating each of our columns, there are a number of things we can specify per-column:</p><ul><li><strong>Data Type (required):</strong> The data which can be saved to cells of this column (such as INTEGER, TEXT, etc).</li><li><strong>Key Type:</strong> Creates a key for the column.</li><li><strong>Key Attributes:</strong> Any key-related attributes, such as auto-incrementing.</li><li><strong>Default:</strong> If rows are created in the table without values passed to the current column, the value specified as <code>DEFAULT</code> </li><li><strong>Primary Key:</strong> Allows any of the previous specified columns to be set as the table's primary key.</li></ul><p>MySQL tables can have a 'storage engine' specified via <code>ENGINE=[engine_type]</code>, which determines the core logic of how the table will interpret data. Leaving this blank defaults to InnoDB and is almost certainly fine to be left alone. In case you're interested, you can find more about MySQL engines <a href=\"https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html\">here</a>.</p><p>Here's an example of what an actual <code>CREATE TABLE</code> query would look like:</p><pre><code>CREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;</code></pre><h3 id=\"managing-keys-for-existing-tables\">Managing Keys for Existing Tables</h3><p>If we don't specify our keys at table creation time, we can always do so after the fact. SQL tables can accept the following key types:</p><ul><li><strong>Primary Key:</strong> One or more fields/columns that uniquely identify a record in the table. It can not accept null, duplicate values.</li><li><strong>Candidate Key:</strong> Candidate keys are kind of like groups of non-committed Primary Keys; these keys only accept unique values, and <em>could potentially</em> be used in the place of a Primary Key if need be, but are not actual Primary Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.</li><li><strong>Alternate Key:</strong> Refers to a single Candidate Key (an alternative which can satisfy the duty of a Primary Key id need be).</li><li><strong>Composite/Compound Key:</strong> Defined by combing the values of multiple columns; the sum of which will always produce a unique value. There can be multiple Candidate Keys in one table. Each Candidate Key can work as Primary Key.</li><li><strong>Unique Key:</strong> A set of one or more fields/columns of a table that uniquely identify a record in a database table. Similar to Primary key, but it can accept only one null value, and it can not have duplicate values.</li><li><strong>Foreign Key: </strong>Foreign keys denote fields that serve as <em>another table's</em> Primary key. Foreign keys are useful for building relationships between tables. While a foreign key is required in the parent table where they are primary, foreign keys can be null or empty in the tables intended to relate to the other table.</li></ul><p>Let's look at an example query where we add a key to a table and dissect the pieces:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n</code></pre>\n<p><code>ALTER TABLE</code> is used to make any changes to a table's structure, whether that be modifying columns or keys.</p><p>In this example, we <code>ADD</code> a key which happens to be a <code>FOREIGN KEY</code>. While keys always refer to columns, keys themselves must have names of their own to distinguish the column's data and a key's conceptual logic. We name our key <code>foreign_key_name</code> and specify which column the key will act on with <code>(column_name)</code>. Because this is a foreign key, we need to specify which table's <em>primary key</em> we want this to be associated with. <code>REFERENCES parent_table(primary_key_column)</code> is stating that the foreign key in this table corresponds to values held in a column named <code>primary_key_column</code>, in a table named <code>parent_table</code>.</p><p>The statements <code>ON DELETE</code> and <code>ON UPDATE</code> are actions which take place if the parent table's primary key is deleted or updated, respectively. <code>ON DELETE CASCADE</code> would result in our tables foreign key being deleted if the corresponding primary key were to disappear.</p><h3 id=\"adding-columns\">Adding Columns</h3><p>Adding columns follows the same syntax we used when creating tables. An interesting additional feature is the ability to place the new column before or after preexisting columns:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n</code></pre>\n<div class=\"protip\">\nWhen referencing tables in PostgreSQL databases, we must specify the schema belongs to. Thus, <code>ALTER TABLE table_name</code> becomes <code>ALTER TABLE schema_name.table_name</code>. This applies to any time we reference tables, including when we create and delete tables.\n</div><h3 id=\"pop-quiz\">Pop Quiz</h3><p>The below statement uses elements of everything we've learned about modifying and creating table structures thus far. Can you discern what is happening here?</p><pre><code class=\"language-sql\">CREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n</code></pre>\n<h2 id=\"dropping-data\">Dropping Data</h2><p>DANGER ZONE: this is where we can start to mess things up. Dropping columns or tables results in a complete loss of data: whenever you see the word \"drop,\" be scared.</p><p>If you're sure you know what you're doing and would like to remove a table column, this can be done as such:</p><pre><code class=\"language-sql\">ALTER TABLE table\nDROP column;\n</code></pre>\n<p>Dropping a table destroys the table structure as well as all data within it:</p><pre><code class=\"language-sql\">DROP TABLE table_name;\n</code></pre>\n<p>Truncating a table, on the other hand, will purge the table of data but retain the table itself:</p><pre><code class=\"language-sql\">TRUNCATE TABLE table_name;\n</code></pre>\n<h3 id=\"drop-foreign-key\">Drop Foreign Key</h3><p>Like tables and columns, we can drop keys as well:</p><pre><code class=\"language-sql\">ALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n</code></pre>\n<p>This can also be handed by dropping CONSTRAINT:</p><pre><code class=\"language-sql\">ALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n</code></pre>\n<h2 id=\"working-with-views-specific-to-postgresql-\">Working with Views (Specific to PostgreSQL)</h2><p>Lastly, let's explore the act of creating views. There are three types of views PostgreSQL can handle:</p><ul>\n<li><strong>Simple Views</strong>: Virtual tables which represent data of underlying tables. Simple views are automatically updatable: the system will allow INSERT, UPDATE and DELETE statements to be used on the view in the same way as on a regular table.</li>\n<li><strong>Materialized Views</strong>: PostgreSQL extends the view concept to a next level that allows views to store data 'physically', and we call those views are materialized views. A materialized view caches the result of a complex query and then allow you to refresh the result periodically.</li>\n<li><strong>Recursive Views</strong>: Recursive views are a bit difficult to explain without delving deep into the complicated (but cool!) functionality of recursive reporting. I won't get into the details, but these views are able to represent relationships which go multiple layers deep. Here's a quick taste, if you;re curious:</li>\n</ul>\n<p><strong>Sample </strong><code>RECURSIVE</code> <strong>query:</strong></p><pre><code class=\"language-sql\">WITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' &gt; ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n</code></pre>\n<p><strong>Output:</strong></p><pre><code class=\"language-shell\"> employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North &gt; Megan Berry\n           3 | Michael North &gt; Sarah Berry\n           4 | Michael North &gt; Zoe Black\n           5 | Michael North &gt; Tim James\n           6 | Michael North &gt; Megan Berry &gt; Bella Tucker\n           7 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe\n           8 | Michael North &gt; Megan Berry &gt; Max Mills\n           9 | Michael North &gt; Megan Berry &gt; Benjamin Glover\n          10 | Michael North &gt; Sarah Berry &gt; Carolyn Henderson\n          11 | Michael North &gt; Sarah Berry &gt; Nicola Kelly\n          12 | Michael North &gt; Sarah Berry &gt; Alexandra Climo\n          13 | Michael North &gt; Sarah Berry &gt; Dominic King\n          14 | Michael North &gt; Zoe Black &gt; Leonard Gray\n          15 | Michael North &gt; Zoe Black &gt; Eric Rampling\n          16 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Piers Paige\n          17 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Ryan Henderson\n          18 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Frank Tucker\n          19 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Nathan Ferguson\n          20 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Kevin Rampling\n(20 rows)\n</code></pre>\n<h3 id=\"creating-a-view\">Creating a View</h3><p>Creating a simple view is as simple as writing a standard query! All that is required is the addition of <code>CREATE VIEW view_name AS</code> before the query, and this will create a saved place for us to always come back and reference the results of this query:</p><pre><code class=\"language-sql\">CREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n</code></pre>\n<h2 id=\"get-out-there-and-start-sqling\">Get Out There and Start SQLing</h2><p>I highly encourage anybody to get in the habit of <em>always </em>writing SQL queries by hand. With the right GUI, autocompletion can be your best friend.</p><p>Explicitly forcing one's self to write queries instead of copy &amp; pasting anything forces us to come to realizations, such as SQL's order of operations. Indeed, this query holds the correct syntax...</p><pre><code class=\"language-sql\">SELECT *\nFROM table_name\nWHERE column_name = 'Value';\n</code></pre>\n<p>...Whereas this one does not:</p><pre><code class=\"language-sql\">SELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n</code></pre>\n<p>Grasping the subtleties of SQL is the difference between being blazing fast and mostly clueless. The good news is, you’ll start to find that these concepts aren’t nearly as daunting as they may have once seemed, so the track from ‘bad data engineer’ to ‘expert’ is an easy win that would be foolish not to take.</p><p>Stick around for next time where we actually work with data in <strong>SQL: The Sequel</strong>, rated PG-13.</p>","url":"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/","uuid":"fe99e822-f21a-432c-8bbf-4d399e575570","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5bb0ec7999ff33f06876e1"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372e","title":"MySQL, Google Cloud, and a REST API that Generates Itself","slug":"mysql-google-cloud-and-a-rest-api-that-autogenerates","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","custom_excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","created_at_pretty":"23 October, 2018","published_at_pretty":"23 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-23T14:57:12.000-04:00","published_at":"2018-10-23T18:47:28.000-04:00","updated_at":"2019-02-02T05:26:16.000-05:00","meta_title":"MySQL, Google Cloud, and a REST API | Hackers and Slackers","meta_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","og_title":"MySQL, Google Cloud, and a REST API that Generates Itself","twitter_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","twitter_title":"MySQL, Google Cloud, and a REST API that Generates Itself","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn’t too long ago that I haphazardly forced us down a journey of exploring\nGoogle Cloud’s cloud SQL service. The focus of this exploration was Google’s\naccompanying REST API for all of its cloud SQL instances. That API turned out to\nbe a relatively disappointing administrative API which did little to extend the\nfeatures you’d expect from the CLI or console.\n\nYou see, I’ve had a dream stuck in my head for a while now. Like most of my\nutopian dreams, this dream is related to data, or more specifically simplifying\nthe manner in which we interact with it. For industry synonymous with AI and\nautomation, many of our very own tools (including ETL tools) involve way too\nmuch manual effort in my opinion. That’s right: I’m talking about the aspiration\nto Slack while we Hack.\n\nThe pitch is this: why do we keep setting up databases, endpoints, and the logic\nto connect them when, 90% of the time, we’re building the same thing over and\nover? Let me guess: there’s a GET endpoint to get records from table X, or a\nPOST endpoint to create users. I know you’ve built this because we all have, but\nwhy do we keep building the same things over and over in isolation? It looks\nlike we might not have to anymore, but first let’s create our database.\n\nCreating a MySQL Instance in GCP \nFull disclosure here: the magical REST API thing is actually independent from\nGoogle Cloud; the service we’ll be using can integrate with any flavor of MySQL\nyou prefer, so go ahead and grab that RDS instance you live so much if you\nreally have to.\n\nFor the rest of us, hit up your GCP console and head into making a new SQL\ninstance. MySQL and Postgres are our only choices here; stick with MySQL.\n\nThere isn’t much to spinning up your instance. Just be sure to create a user and\ndatabase to work from.\n\nOh yeah, and remember to name your instance.Your SQL Firewall and Permissions\nYour instance is set to “public” by default. Oddly, “public” in this case means\n“accessible to everybody on your IP whitelist, which is empty by default,” so\nreally kind of the opposite of public really.\n\nIn fact, if you hypothetically did want to open your instance publicly, Google\nCloud will not allow it. This is good on them, and is actually fairly impressive\nthe depths they go to avoid the IP 0.0.0.0  from ever appearing anywhere in the\ninstance. Go ahead, open the shell and try to add bind address=0.0.0.0 \nyourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s\nversion of MySQL is actually a MariaDB instance)?\n\nThe point is, whitelist your IP address. Simply \"Edit\" your instance and add\nyour address to the authorized networks.\n\nAuthorize that bad boy.The Magic API \nNow, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so\nthis next part is going to feel a bit a bit weird. I’m not sure why, as the\nservice is apparently free, thus I’m clearly not getting paid for any of this.\n\nAnyway, the service is called Apisentris [https://apisentris.com/], and the idea\nis that it will build whatever time-consuming monstrosity of a REST API you were\nplanning to build to access your data for you. Via their own words:\n\nSee, I told you.What does this actually mean? It means if you create a table\ncalled articles  in your database, you will immediately have an endpoint to\nfetch said articles, and it would look like \nhttps://apisentris.com/api/v1/articles. Your client ID and credentials would\nobviously need to be provided to indicate that you're, well, you.\n\nGrabbing entire tables at once would be silly, which is why they also\nautogenerate filters based on the contents of your table:\n\nEndpoints accept query parameters to essentially create a query.Oh yeah, and you\ncan also handle user management via this API as well, if you're building an\nactual app:\n\nPretty easy to hook up into a form or whatever.I'll assume you're sold on the\nidea by now. If a free service that handles the hard parts of backend logic for\nfree isn't your cup of tea, clearly you aren't Slacker material.\n\nSetting it all up\nAs we did before with our own IP, we'll need to whitelist Apisentris' IP the\nsame way in GCP console. Their IP is 104.199.181.125.\n\nCreate a table in your database with some data just to test things out. When\nyou're logged in, you'll be able to see all the endpoints available to you and\nthe associated attributes they have:\n\nNot bad.Any way you slice it, the concept of a self-generating API is very cool\nand yet somehow still not the norm. I'm actually shocked that there are so few\npeople in the Data industry who know \"there must be a better way,\" but then\nagain, data science and software engineering are two very different things. For\nmy fellow Data Engineers out there, take this as a gift and a curse: you have\nthe gift of knowing better from your software background, but are cursed with\nwatching the world not quite realize how pointless half the things they do truly\nare.\n\nOh well. We'll be the ones building the robots anyway.","html":"<p>It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively disappointing administrative API which did little to extend the features you’d expect from the CLI or console.</p><p>You see, I’ve had a dream stuck in my head for a while now. Like most of my utopian dreams, this dream is related to data, or more specifically simplifying the manner in which we interact with it. For industry synonymous with AI and automation, many of our very own tools (including ETL tools) involve way too much manual effort in my opinion. That’s right: I’m talking about the aspiration to Slack while we Hack.</p><p>The pitch is this: why do we keep setting up databases, endpoints, and the logic to connect them when, 90% of the time, we’re building the same thing over and over? Let me guess: there’s a GET endpoint to get records from table X, or a POST endpoint to create users. I know you’ve built this because we all have, but why do we keep building the same things over and over in isolation? It looks like we might not have to anymore, but first let’s create our database.</p><h2 id=\"creating-a-mysql-instance-in-gcp\">Creating a MySQL Instance in GCP </h2><p>Full disclosure here: the magical REST API thing is actually independent from Google Cloud; the service we’ll be using can integrate with any flavor of MySQL you prefer, so go ahead and grab that RDS instance you live so much if you really have to.</p><p>For the rest of us, hit up your GCP console and head into making a new SQL instance. MySQL and Postgres are our only choices here; stick with MySQL.</p><p>There isn’t much to spinning up your instance. Just be sure to create a user and database to work from.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.15.18-PM.png\" class=\"kg-image\"><figcaption>Oh yeah, and remember to name your instance.</figcaption></figure><h3 id=\"your-sql-firewall-and-permissions\">Your SQL Firewall and Permissions</h3><p>Your instance is set to “public” by default. Oddly, “public” in this case means “accessible to everybody on your IP whitelist, which is empty by default,” so really kind of the opposite of public really.</p><p>In fact, if you hypothetically did want to open your instance publicly, Google Cloud will not allow it. This is good on them, and is actually fairly impressive the depths they go to avoid the IP <strong>0.0.0.0</strong> from ever appearing anywhere in the instance. Go ahead, open the shell and try to add <code>bind address=0.0.0.0</code> yourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s version of MySQL is actually a MariaDB instance)?</p><p>The point is, whitelist your IP address. Simply \"Edit\" your instance and add your address to the authorized networks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.12.23-PM.png\" class=\"kg-image\"><figcaption>Authorize that bad boy.</figcaption></figure><h2 id=\"the-magic-api\">The Magic API </h2><p>Now, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so this next part is going to feel a bit a bit weird. I’m not sure why, as the service is apparently free, thus I’m clearly not getting paid for any of this.</p><p>Anyway, the service is called <strong><a href=\"https://apisentris.com/\">Apisentris</a>, </strong>and the idea is that it will build whatever time-consuming monstrosity of a REST API you were planning to build to access your data for you. Via their own words:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.19.16-PM.png\" class=\"kg-image\"><figcaption>See, I told you.</figcaption></figure><p>What does this actually mean? It means if you create a table called <em>articles</em> in your database, you will immediately have an endpoint to fetch said articles, and it would look like <strong>https://apisentris.com/api/v1/articles. </strong>Your client ID and credentials would obviously need to be provided to indicate that you're, well, you.</p><p>Grabbing entire tables at once would be silly, which is why they also autogenerate filters based on the contents of your table:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.25.13-PM.png\" class=\"kg-image\"><figcaption>Endpoints accept query parameters to essentially create a query.</figcaption></figure><p>Oh yeah, and you can also handle user management via this API as well, if you're building an actual app:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.27.43-PM.png\" class=\"kg-image\"><figcaption>Pretty easy to hook up into a form or whatever.</figcaption></figure><p>I'll assume you're sold on the idea by now. If a free service that handles the hard parts of backend logic for free isn't your cup of tea, clearly you aren't Slacker material.</p><h2 id=\"setting-it-all-up\">Setting it all up</h2><p>As we did before with our own IP, we'll need to whitelist Apisentris' IP the same way in GCP console. Their IP is <code>104.199.181.125</code>.</p><p>Create a table in your database with some data just to test things out. When you're logged in, you'll be able to see all the endpoints available to you and the associated attributes they have:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/schema.gif\" class=\"kg-image\"><figcaption>Not bad.</figcaption></figure><p>Any way you slice it, the concept of a self-generating API is very cool and yet somehow still not the norm. I'm actually shocked that there are so few people in the Data industry who know \"there must be a better way,\" but then again, data science and software engineering are two very different things. For my fellow Data Engineers out there, take this as a gift and a curse: you have the gift of knowing better from your software background, but are cursed with watching the world not quite realize how pointless half the things they do truly are.</p><p>Oh well. We'll be the ones building the robots anyway.</p>","url":"https://hackersandslackers.com/mysql-google-cloud-and-a-rest-api-that-autogenerates/","uuid":"c45478bb-54da-4563-89bd-ddd356a234d4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bcf6f08d7ab443ba8b7a5ab"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867367c","title":"Lynx Roundup, June 21st","slug":"lynx-roundup-june-21st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 6/21/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 6/21/2018.","created_at_pretty":"19 June, 2018","published_at_pretty":"21 June, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-06-19T15:48:26.000-04:00","published_at":"2018-06-21T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:03.000-04:00","meta_title":"Lynx Roundup, June 21st | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","og_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","og_title":"Lynx Roundup, June 21st","twitter_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","twitter_title":"Lynx Roundup, June 21st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Science News","slug":"science-news","description":"Breakthroughs in general science.","feature_image":null,"meta_description":"Breakthroughs in general science.","meta_title":"Science News | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/\n\nhttps://github.com/o1lab/xmysql\n\nhttps://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html\n\nKickin' rad MOOC that I'm taking right now, from a kickin' rad organization that\ndoes kickin' rad research. \nhttps://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells\n\nhttp://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html","html":"<p><a href=\"https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/\">https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/</a></p>\n<p><a href=\"https://github.com/o1lab/xmysql\">https://github.com/o1lab/xmysql</a></p>\n<p><a href=\"https://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html\">https://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html</a></p>\n<p>Kickin' rad MOOC that I'm taking right now, from a kickin' rad organization that does kickin' rad research.  <a href=\"https://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells\">https://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells</a></p>\n<p><a href=\"http://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html\">http://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html</a></p>\n","url":"https://hackersandslackers.com/lynx-roundup-june-21st/","uuid":"cbf4f4ce-d4b0-4d70-acce-215fc8ce6740","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b295e0aded32f5af8fd6723"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673678","title":"Using PyMySQL: Python's MySQL Library","slug":"using-pymysql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1.jpg","excerpt":"The lightweight Python library for interacting with MySQL.","custom_excerpt":"The lightweight Python library for interacting with MySQL.","created_at_pretty":"15 June, 2018","published_at_pretty":"15 June, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-06-14T20:32:21.000-04:00","published_at":"2018-06-15T16:48:12.000-04:00","updated_at":"2019-04-10T00:43:08.000-04:00","meta_title":"Using PyMySQL: Python's MySQL Library | Hackers and Slackers","meta_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","og_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","og_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1-2.jpg","og_title":"Using PyMySQL: Python's MySQL Library","twitter_description":"Learn to work with PyMySQL: the lightweight Python library for interacting with MySQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/pymysql-1-1.jpg","twitter_title":"Using PyMySQL: Python's MySQL Library","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"It's almost Friday night, and the squad at H+S is ready to get cooking. Dim down\nthe lights and slip into something more comfortable as we take you on this 100%\norganic flavor extravaganza. Tonight's menu? A Python MySQL library: PyMySQL\n[https://github.com/PyMySQL/PyMySQL].\n\nPyMySQL is lightweight and perfect for fulfilling MySQL queries. If you want\nbells and whistles, you're probably barking up the wrong tree (and you probably\nshould’ve used a DB other than MySQL in the first place).\n\nWhy write tutorials for technologies we openly trash talk? Out of necessity, of\ncourse! There's nothing wrong with MySQL, most enterprises are married to it in\nsome way. Thus, A great use case for PyMySQL is for usage in AWS lambda when\nworking with large enterprise systems. We'll get to that, but for now let's cook\nup something good.\n\nHeat up the Stove\nTurn on the gas and prep the table to set with your favorite collection of\nplates! That's right, we're talking boilerplate. We knew this was coming; it\nseems like every time you want to do something tangibly cool, we need to get\ninto the business of managing connections and whatnot.\n\nTo ease the pain, I'll share with you a preferred method of handling opening\nconnections with PyMySQL. Here we set a function to separate basic connection\nlogic and error messaging from our app:\n\nimport sys\nimport pymysql\nimport logger\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)    \n    except:\n        logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n        sys.exit()\n\n\n\nNothing fancy here: we set a global variable conn  to serve as our connection,\nand have some basic logic on how to interact with our database. Running \nopenConnection  will attempt to connect to a MySQL db with supplied credentials,\nor throw an error if something goes horribly wrong.\n\nNow we can keep this separate from the rest of our code. Out of sight, out of\nmind.\n\nMeat and Potatoes\nWith the boring stuff out of the way, let's dig into some goodness. We'll start\noff with a basic use case: selecting all rows from a table:\n\ndef getRecords():\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"SELECT * FROM table\"\n            cur.execute(sql)\n            result = cur.fetchall()\n            print(result)\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords() \n\n\nWe split our function into your standard try/except/finally  breakdown. What\nwe're trying is opening a connection using the function we created earlier, and\nrunning queries against it.\n\nThe preferred syntax in PyMySQL is to keep our query in a single string, as seen\nin our variable sql. With our query ready, we need to execute  the query, fetch \nthe resulting records and print the result. We're sure to close the connection\nonce we're done with executing queries... this is critical to ensure db\nconnections don't stay active.\n\nSimple so far, but we're about to kick it up a notch.\n\nSelecting rows\nYou may have noticed we used .fetchall()  to select all records. This is\nimportant to differentiate from .fetchone(), which simply selects the first\nrecord.\n\nWe can iterate over the rows resulting from .fetchall()  with a simple loop, as\nshown in the example below. Beware: attempting to print the result of \n.fetchall()  will simply result in a single integer, which represents the number\nof rows fetched. If there are instances where we know only one record should be\nreturned, .fetchone()  should be used instead.\n\ndef getRecords(table):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"SELECT * FROM %s\"\n            cur.execute(sql, table)\n            result = cur.fetchall()\n            for row in result:\n                record = {\n                        'id': row[0],\n                        'name': row[1],\n                        'email': row[2],\n                        'phone': row[3],\n\n                    }\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords('table_name')       \n\n\nWhoa, what's with the %s? This is how we pass arguments into queries in PyMySQL.\nThe PyMySQL guys were kind enough to realize how obnoxious it is to constantly\nbreak strings to pass in variables - in the case of SQL queries, this beyond\nobnoxious and borderline unworkable. Remember that MySQL requires explicit\nquotations around passing string values, so queries such as these become a\nnonsensical jumble of escaped characters.\n\nPyMySQL supports backquotes: the distant cousin of the single quotation mark,\nAKA the diagonal quote thing above the tilde ~ button on your keyboard. You\nknow: `. If there comes a time to set a string within your query use this\nelusive quotation as such: sql = \"SELECT * FROM %s WHERE column_name =\n`somevalue`\"Updating Rows of Data\nArguments can be passed as anything inside a query: they simply appear in the\norder in which they are passed. In the below example, we pass table as an\nargument, as well values we want updated, and the identifier for target rows:\n\ndef getRecords(table, data):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = \"UPDATE %s SET date=%s, numsent=%s WHERE email = %s\"\n            cur.execute(sql, (table, data['date_sent'], data['status'], data['email']))\n            conn.commit()\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n\ndata = {\n    'date_sent': '12/01/2018'\n    'email': 'fakeemail@example.com'\n    'status': 'Confirmed'\n}\ngetRecords('table_name', data)       \n\n\nHeads up:  Note the line conn.commit(). Don't forget that - this is what\nactually commits the update to the database. Forgetting this line and wasting\nhours debugging is somewhat of a rite of passage, but let's just skip all that.\n\nFor Dessert: Usage in AWS Lambda\nIt is my treat to share with you my world famous copy & paste recipe for AWS\nLambda. Here we store all of our db credentials in a separate file called \nrdsconfig.py. We also enable logging to take us through what is happening each\nstep of the way:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n#rds settings\nrds_host  = \"rdsName.dfsd834mire.us-west-3.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n    except:\n        logger.error(\"ERROR: Could not connect to MySql instance.\")\n        sys.exit()\n\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n\nWe thank you all for joining us in this adventure of tantalizing treats. May\nyour data be clean and your stomachs full.\n\nBon appétit.","html":"<p>It's almost Friday night, and the squad at H+S is ready to get cooking. Dim down the lights and slip into something more comfortable as we take you on this 100% organic flavor extravaganza. Tonight's menu? A Python MySQL library: <a href=\"https://github.com/PyMySQL/PyMySQL\">PyMySQL</a>.</p><p>PyMySQL is lightweight and perfect for fulfilling MySQL queries. If you want bells and whistles, you're probably barking up the wrong tree (and you probably should’ve used a DB other than MySQL in the first place).</p><p>Why write tutorials for technologies we openly trash talk? Out of necessity, of course! There's nothing wrong with MySQL, most enterprises are married to it in some way. Thus, A great use case for PyMySQL is for usage in AWS lambda when working with large enterprise systems. We'll get to that, but for now let's cook up something good.</p><h2 id=\"heat-up-the-stove\">Heat up the Stove</h2><p>Turn on the gas and prep the table to set with your favorite collection of plates! That's right, we're talking boilerplate. We knew this was coming; it seems like every time you want to do something tangibly cool, we need to get into the business of managing connections and whatnot.</p><p>To ease the pain, I'll share with you a preferred method of handling opening connections with PyMySQL. Here we set a function to separate basic connection logic and error messaging from our app:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport pymysql\nimport logger\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)    \n    except:\n        logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n        sys.exit()\n\n</code></pre>\n<!--kg-card-end: markdown--><p>Nothing fancy here: we set a global variable <em>conn</em> to serve as our connection, and have some basic logic on how to interact with our database. Running <em>openConnection</em> will attempt to connect to a MySQL db with supplied credentials, or throw an error if something goes horribly wrong.</p><p>Now we can keep this separate from the rest of our code. Out of sight, out of mind.</p><h2 id=\"meat-and-potatoes\">Meat and Potatoes</h2><p>With the boring stuff out of the way, let's dig into some goodness. We'll start off with a basic use case: selecting all rows from a table:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords():\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;SELECT * FROM table&quot;\n            cur.execute(sql)\n            result = cur.fetchall()\n            print(result)\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords() \n</code></pre>\n<!--kg-card-end: markdown--><p>We split our function into your standard <em>try/except/finally</em> breakdown. What we're trying is opening a connection using the function we created earlier, and running queries against it.</p><p>The preferred syntax in PyMySQL is to keep our query in a single string, as seen in our variable <em>sql</em>. With our query ready, we need to <em>execute</em> the query, <em>fetch</em> the resulting records and print the result. We're sure to close the connection once we're done with executing queries... this is critical to ensure db connections don't stay active.</p><p>Simple so far, but we're about to kick it up a notch.</p><h3 id=\"selecting-rows\">Selecting rows</h3><p>You may have noticed we used <em>.fetchall()</em> to select all records. This is important to differentiate from <em>.fetchone()</em>, which simply selects the first record.</p><p>We can iterate over the rows resulting from <em>.fetchall()</em> with a simple loop, as shown in the example below. Beware: attempting to print the result of <em>.fetchall()</em> will simply result in a single integer, which represents the number of rows fetched. If there are instances where we know only one record should be returned, <em>.fetchone()</em> should be used instead.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords(table):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;SELECT * FROM %s&quot;\n            cur.execute(sql, table)\n            result = cur.fetchall()\n            for row in result:\n                record = {\n                        'id': row[0],\n                        'name': row[1],\n                        'email': row[2],\n                        'phone': row[3],\n\n                    }\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n        \ngetRecords('table_name')       \n</code></pre>\n<!--kg-card-end: markdown--><p>Whoa, what's with the <em><strong>%s</strong></em>? This is how we pass arguments into queries in PyMySQL. The PyMySQL guys were kind enough to realize how obnoxious it is to constantly break strings to pass in variables - in the case of SQL queries, this beyond obnoxious and borderline unworkable. Remember that MySQL requires explicit quotations around passing string values, so queries such as these become a nonsensical jumble of escaped characters.</p><!--kg-card-begin: html--><div class=\"protip\">\nPyMySQL supports backquotes: the distant cousin of the single quotation mark, AKA the diagonal quote thing above the tilde ~ button on your keyboard. You know: `. If there comes a time to set a string within your query use this elusive quotation as such: <code>sql = \"SELECT * FROM %s WHERE column_name = `somevalue`\"</code>\n</div><!--kg-card-end: html--><h3 id=\"updating-rows-of-data\">Updating Rows of Data</h3><p>Arguments can be passed as anything inside a query: they simply appear in the order in which they are passed. In the below example, we pass table as an argument, as well values we want updated, and the identifier for target rows:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def getRecords(table, data):\n    try:\n        openConnection()\n        with conn.cursor() as cur:\n            sql = &quot;UPDATE %s SET date=%s, numsent=%s WHERE email = %s&quot;\n            cur.execute(sql, (table, data['date_sent'], data['status'], data['email']))\n            conn.commit()\n            cur.close()\n            conn.close()\n    except Exception as e:\n        print(e)\n    finally:\n        print('Query Successful')\n\ndata = {\n    'date_sent': '12/01/2018'\n    'email': 'fakeemail@example.com'\n    'status': 'Confirmed'\n}\ngetRecords('table_name', data)       \n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Heads up:</strong> Note the line <em>conn.commit()</em>. Don't forget that - this is what actually commits the update to the database. Forgetting this line and wasting hours debugging is somewhat of a rite of passage, but let's just skip all that.</p><h2 id=\"for-dessert-usage-in-aws-lambda\">For Dessert: Usage in AWS Lambda</h2><p>It is my treat to share with you my world famous copy &amp; paste recipe for AWS Lambda. Here we store all of our db credentials in a separate file called <em>rdsconfig.py</em>. We also enable logging to take us through what is happening each step of the way:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n#rds settings\nrds_host  = &quot;rdsName.dfsd834mire.us-west-3.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nconn = None\n\ndef openConnection():\n    global conn\n    try:\n        if(conn is None):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n        elif (not conn.open):\n            conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=10)\n    except:\n        logger.error(&quot;ERROR: Could not connect to MySql instance.&quot;)\n        sys.exit()\n\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>We thank you all for joining us in this adventure of tantalizing treats. May your data be clean and your stomachs full.</p><p>Bon appétit.</p>","url":"https://hackersandslackers.com/using-pymysql/","uuid":"cd6baf62-981c-4034-ba29-b67d257acbeb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b230915f37f772d33bc1eb1"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673648","title":"MySQL on the Cloud with AWS RDS","slug":"setting-up-mysql-on-aws","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","excerpt":"Spinning up a standalone MySQL Database with Amazon.","custom_excerpt":"Spinning up a standalone MySQL Database with Amazon.","created_at_pretty":"30 April, 2018","published_at_pretty":"01 May, 2018","updated_at_pretty":"27 November, 2018","created_at":"2018-04-29T23:12:26.000-04:00","published_at":"2018-04-30T20:14:57.000-04:00","updated_at":"2018-11-27T03:54:21.000-05:00","meta_title":"Setting up MySQL on AWS | Hackers and Slackers","meta_description":"Spinning up a standalone MySQL Database with Amazon","og_description":"Spinning up a standalone MySQL Database with Amazon","og_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","og_title":"Setting up MySQL on AWS","twitter_description":"Spinning up a standalone MySQL Database with Amazon","twitter_image":"https://hackersandslackers.com/content/images/2018/04/aws_mysql@2x.jpg","twitter_title":"Setting up MySQL on AWS","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"Last time we became familiar with the handiwork of setting up MySQL locally,\nnavigating databases via command line, and exposing your database to external\naccess. While badass, it has come to my attention that most people don't bother\ndoing things this way. Unless you're getting deep into some heavy architecture,\nmost people opt to use cloud services such as AWS to set up databases which are\nintended to be interacted with by multiple services.\n\nA perfect example is one we ran into over the weekend while working on this very\nblog. We're running a Ghost instance, which is respectably complex\nproduction-ready app. For a bunch of guys just looking to make some stupid blog\nwidgets, it became obvious that reverse engineering the undocumented inner\nworkings of an open source node app was a rabbit hole of complexity.\n\nHosting on AWS\nIn our case, AWS is useful for enforcing separation of concerns. Instead of\nbuilding new logic into a live app, we can build that logic elsewhere in a way\nthat's reusable across multiple apps.\n\nThe end goal here is simply to read/write to a database. That said, there's\nstill a fair amount of complexity involved. We'll need to leverage the following\nAWS services:\n\n * RDS (Relational Database Service): A cloud hosted database\n * API Gateway: An interface for building APIs\n * Lambda: The necessary serverless connector between RDS and Gateway\n * IAM: Amazon's god-awful user and policy manager\n\nFor now, all we're going to worry about is RDS.\n\nData is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs\nHead to the AWS console and create a new RDS instance. Once prompted, go with\nMySQL:\n\nAs though some of these are real options. Please.Stick with MySQL Production  on the next screen.\n\nDo everything in production. AlwaysConfiguration Settings\nThis is where we set our configurations. You'll notice immediately how\nconvoluted AWS tends to be with their naming conventions. I personally hate how\nintentionally unintuitive all of AWS tends to be (what the hell is a \ndb.t2.medium)? This sort of absurdity is just something we need to deal with\nforever. Amazon is technically outside the realm of enforceable Monopoly laws,\nand there's no reason to believe their reign of mediocre products and talking\nrobots is ever going to end.\n\n * License: Select general-public-license\n * Version: Choose whichever, just don't do an old one\n * Instance class: Most of these instances are huge and unnecessary. Go with\n   something small: I would also advise looking at the pricing plan.\n * Multi AZ: Create a replica.\n * Storage type: General.\n * Allocated storage: Feel free to allocate more for latency.\n * Publicly Accessible: True.\n\nGod I love configuring esoteric shit.Once configuration is complete, it takes a\ngood amount of time for the database to be created. While we wait, let's move on\nto creating to a user to access this. We can do this with IAM: another AWS\nproduct with an even more terrible interface.\n\nAccess\nFair warning: user roles and permissions are the worst part of AWS. I could\nwrite an entire series on how deep this mess of a scheme goes, but quite\nhonestly I still barely understand what I'm doing most of the time.\n\nCreating a User\nCreate a new user that will access the database. Go to the Users panel  and\ncreate a user:\n\nModifying permission policies\nPermissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS\nhas some default policies that we can leverage for our purposes, so this should\nluckily be somewhat straightforward.\n\nPolicies can also be combined so that users have multiple policies across AWS\nproducts.\n\nNative Client\nOnce your DB pops up in AWS, we're going to need to get you a GUI to modify your\nDB. Don't even try to be a hotshot by setting up all your tables via command\nline. It sucks, it's slower, and nobody is impressed. Don't bother downloading\nthe AWS CLI either. Do not pass GO. Do not collect 500 dollars.\n\nIn case you need to install MySQL locally, an OSX download can be found here.\nCome to think of it, that step was probably unnecessary. I'm not sure why I did\nthat.\n\nI settled on Sequel Pro [https://www.sequelpro.com/]  as a client. It's good\nenough, and their logo looks like pancakes. That's really the only metric I\nneeded tbh.\n\nTo connect to your database, you'll need to retrieve the endpoint and port\nnumber from your RDS console:\n\nConnect to that ish:\n\nHopefully everything went well! If not, I'm sure the problem will be a quick and\neasy fix. Surely it won't involve mindlessly swapping permissions for an entire\nday. You defintely won't somehow end up corrupting your .bash_profile, making\nPython invisible to your OS, and effectively destroying your computer. Only an\nidiot would do something like that. Yesterday evening.\n\nGo ahead and get accustomed to the UI of Sequel Pro - it's pretty\nstraightforward, and ten thousand million times less effort than creating tables\nvia terminal. Create columns under the \"structure\" tab - the terminology should\nimmediately seem familiar if you've been following the series until this point.\n\nProtip: Issues with Security Groups\nIf you're running into an issue connecting to your DB externally, I happened to\nrun in to a nice little issue the other day with security groups. RDS instances\nlimit what kinds of connections they accept via \"security groups.\" This is yet\nanother layer of AWS security hassle where you'll need to specify which hosts\nare permitted to access your DB, by type of connection, port range, etc.\n\nIf you'd like to get this over with as soon as possible, this configuration will\nopen you up to the entire world:\n\nHappy Trails\nNext time we're going to sink deeper into this rabbit hole by exploring the\nwonderful world of serverless functions. Setting up AWS Lambda will allow us to\nconfigure endpoints which will allow us to read and write data to our brand new\ntable in the sky.\n\nWe'll still need to get into API Gateway after that, but let's not think about\nthat just yet. Let's not address the absurd amount of time and effort we're\nabout to spend just to make a god damn widget that shows Github commits.","html":"<p>Last time we became familiar with the handiwork of setting up MySQL locally, navigating databases via command line, and exposing your database to external access. While badass, it has come to my attention that most people don't bother doing things this way. Unless you're getting deep into some heavy architecture, most people opt to use cloud services such as AWS to set up databases which are intended to be interacted with by multiple services.</p><p>A perfect example is one we ran into over the weekend while working on this very blog. We're running a Ghost instance, which is respectably complex production-ready app. For a bunch of guys just looking to make some stupid blog widgets, it became obvious that reverse engineering the undocumented inner workings of an open source node app was a rabbit hole of complexity.</p><h2 id=\"hosting-on-aws\">Hosting on AWS</h2><p>In our case, AWS is useful for enforcing separation of concerns. Instead of building new logic into a live app, we can build that logic elsewhere in a way that's reusable across multiple apps.</p><p>The end goal here is simply to read/write to a database. That said, there's still a fair amount of complexity involved. We'll need to leverage the following AWS services:</p><ul><li>RDS (Relational Database Service): A cloud hosted database</li><li>API Gateway: An interface for building APIs</li><li>Lambda: The necessary serverless connector between RDS and Gateway</li><li>IAM: Amazon's god-awful user and policy manager</li></ul><p>For now, all we're going to worry about is RDS.</p><h2 id=\"data-is-the-new-kale-is-the-new-money-is-the-new-bitcoin-oil-gold-etfs\">Data is the New Kale is the New Money is the new Bitcoin Oil Gold ETFs</h2><p>Head to the AWS console and create a new RDS instance. Once prompted, go with MySQL:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.13.png\" class=\"kg-image\" alt=\"Database Type\"><figcaption>As though some of these are real options. Please.</figcaption></figure><p>Stick with <strong>MySQL Production</strong> on the next screen.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.22.png\" class=\"kg-image\" alt=\"Use case\"><figcaption>Do everything in production. Always</figcaption></figure><h3 id=\"configuration-settings\">Configuration Settings</h3><p>This is where we set our configurations. You'll notice immediately how convoluted AWS tends to be with their naming conventions. I personally hate how intentionally unintuitive all of AWS tends to be (what the hell is a <em>db.t2.medium</em>)? This sort of absurdity is just something we need to deal with forever. Amazon is technically outside the realm of enforceable Monopoly laws, and there's no reason to believe their reign of mediocre products and talking robots is ever going to end.</p><ul><li><strong>License</strong>: Select <em>general-public-license</em></li><li><strong>Version</strong>: Choose whichever, just don't do an old one</li><li><strong>Instance class</strong>: Most of these instances are huge and unnecessary. Go with something small: I would also advise looking at the pricing plan.</li><li><strong>Multi AZ</strong>: Create a replica.</li><li><strong>Storage type</strong>: General.</li><li><strong>Allocated storage</strong>: Feel free to allocate more for latency.</li><li><strong>Publicly Accessible</strong>: True.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-04-30-19.59.46.png\" class=\"kg-image\" alt=\"Configuration\"><figcaption>God I love configuring esoteric shit.</figcaption></figure><p>Once configuration is complete, it takes a good amount of time for the database to be created. While we wait, let's move on to creating to a user to access this. We can do this with IAM: another AWS product with an even more terrible interface.</p><h2 id=\"access\">Access</h2><p>Fair warning: user roles and permissions are the worst part of AWS. I could write an entire series on how deep this mess of a scheme goes, but quite honestly I still barely understand what I'm doing most of the time.</p><h3 id=\"creating-a-user\">Creating a User</h3><p>Create a new user that will access the database. Go to the <a href=\"https://console.aws.amazon.com/iam/home?region=us-east-1#/users\">Users panel</a> and create a user:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.37.20.png\" class=\"kg-image\" alt=\"Users\"></figure><h3 id=\"modifying-permission-policies\">Modifying permission policies</h3><p>Permissions works by \"attaching\" existing \"policies\" to users, groups, etc. AWS has some default policies that we can leverage for our purposes, so this should luckily be somewhat straightforward.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.39.15.png\" class=\"kg-image\" alt=\"Permissions\"></figure><p>Policies can also be combined so that users have multiple policies across AWS products.</p><h2 id=\"native-client\">Native Client</h2><p>Once your DB pops up in AWS, we're going to need to get you a GUI to modify your DB. Don't even try to be a hotshot by setting up all your tables via command line. It sucks, it's slower, and nobody is impressed. Don't bother downloading the AWS CLI either. Do not pass GO. Do not collect 500 dollars.</p><p>In case you need to install MySQL locally, an OSX download can be found <a href=\"https://dev.mysql.com/downloads/mysql/5.5.html#macosx-dmg\">here</a>. Come to think of it, that step was probably unnecessary. I'm not sure why I did that.</p><p>I settled on <a href=\"https://www.sequelpro.com/\">Sequel Pro</a> as a client. It's good enough, and their logo looks like pancakes. That's really the only metric I needed tbh.</p><p>To connect to your database, you'll need to retrieve the endpoint and port number from your RDS console:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.44.png\" class=\"kg-image\" alt=\"Endpoint\"></figure><p>Connect to that ish:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/04/Screenshot-2018-04-30-19.51.28.png\" class=\"kg-image\" alt=\"Sequel Pro\"></figure><p>Hopefully everything went well! If not, I'm sure the problem will be a quick and easy fix. Surely it won't involve mindlessly swapping permissions for an entire day. You defintely won't somehow end up corrupting your .bash_profile, making Python invisible to your OS, and effectively destroying your computer. Only an idiot would do something like that. Yesterday evening.</p><p>Go ahead and get accustomed to the UI of Sequel Pro - it's pretty straightforward, and ten thousand million times less effort than creating tables via terminal. Create columns under the \"structure\" tab - the terminology should immediately seem familiar if you've been following the series until this point.</p><h2 id=\"protip-issues-with-security-groups\">Protip: Issues with Security Groups</h2><p>If you're running into an issue connecting to your DB externally, I happened to run in to a nice little issue the other day with security groups. RDS instances limit what kinds of connections they accept via \"security groups.\" This is yet another layer of AWS security hassle where you'll need to specify which hosts are permitted to access your DB, by type of connection, port range, etc.</p><p>If you'd like to get this over with as soon as possible, this configuration will open you up to the entire world:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/05/Screenshot-2018-05-06-07.15.26.png\" class=\"kg-image\" alt=\"Security Groups\"></figure><h2 id=\"happy-trails\">Happy Trails</h2><p>Next time we're going to sink deeper into this rabbit hole by exploring the wonderful world of serverless functions. Setting up AWS Lambda will allow us to configure endpoints which will allow us to read and write data to our brand new table in the sky.</p><p>We'll still need to get into API Gateway after that, but let's not think about that just yet. Let's not address the absurd amount of time and effort we're about to spend just to make a god damn widget that shows Github commits.</p>","url":"https://hackersandslackers.com/setting-up-mysql-on-aws/","uuid":"bf9a9804-206a-4556-ade4-b7cbdd896ecc","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5ae6899aed09bd1cb7110e51"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673641","title":"Accessing Self-Hosted MySQL  Externally","slug":"accessing-mysql-from-external-domains","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","excerpt":"Connecting to MySQL instances hosted on a VPS.","custom_excerpt":"Connecting to MySQL instances hosted on a VPS.","created_at_pretty":"22 April, 2018","published_at_pretty":"22 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-22T16:27:48.000-04:00","published_at":"2018-04-22T17:20:18.000-04:00","updated_at":"2019-03-28T04:54:42.000-04:00","meta_title":"Accessing MySQL Externally | Hackers and Slackers","meta_description":"How to configure a remote instance of MySQL to accept external connections.","og_description":"How to configure a remote instance of MySQL to accept external connections.","og_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","og_title":"Accessing MySQL Externally","twitter_description":"How to configure a remote instance of MySQL to accept external connections.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/mysql2-2.jpg","twitter_title":"Accessing MySQL Externally","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"In the previous post [https://hackersandslackers.com/set-up-mysql-database/], we\ngot familiar with the basics of creating and navigating MySQL databases. This\nleads us to the next most logical thing to ask: how can I use this in any\nmeaningful way?\n\nMySQL installations default to refusing connections outside of the local\nmachine's IP address, as we should expect. That said, relational databases\naren't usually being used by a single person on a single machine forever (but if\nyou do, we should hang out). It goes without saying that our MySQL instance\nshould be focusing on uptime and accessibility, or in other terms, far away from\nour destructive personalities.\n\nI adore maintaining databases in the command line as much as the next\nself-hating masochist, but we'll need to accomplish work at some point. That\nmeans the remote database we just set up needs to be open-minded enough to allow\na connection from, say, the IP address of our personal local machine, which\nhappens to have a sexy GUI installed for this very purpose.\n\nMaking these kinds of configuration changes to any service or web server is\nalways a bit of fun. You think your day might suck until you cone home and a\npiece of software treats you like a cyber criminal, kicking and screaming while\nwe attempt the most basic out-of-the-box functionality.\n\nThe fine print here is that we wouldn't recommend messing with any of these\nsettings unless you know what you're doing. Then again, if you knew what you\nwere doing you probably wouldn't be reading this. The point is, if you mess up,\nit's your fault because we warned you.\n\nThe first thing we'll need to touch is the MySQL config found here on Ubuntu:\n\nvim /etc/mysql/mysql.conf.d/mysqld.cnf\n\n\nHere you can set various configurations for MySQL, such as the port number,\ndefault user, etc. The line we're interested in is bind-address.\n\n# The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - \"/etc/mysql/my.cnf\" to set global options,\n# - \"~/.my.cnf\" to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain \"#\" chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n\n\nBy default, bind-address is set to your local host. This is basically a\nwhitelist that allows changes only from the domains or IP addresses specified.\nYou can go ahead and add the address of the external domain you'd like to grant\naccess to here.\n\nCommenting out the line completely opens up MySQL to everybody. So there's that.\n\nNow we need to create a user with which to access the DBL:\n\nmysql -u root -p -h localhost -P 3306\n\n\nUse the CREATE USER  command to create a new homie. In the example below,\n'newuser' is the name of the new user, and '%' is from which location the user\nwill be permitted to make changes. This is usually 'localhost', for example. In\nthis case, we added '%' which means everywhere.\n\nmysql> CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n\n\nGrant all privileges to the new user, and always flush privileges  after making\nsuch modifications.\n\nmysql> GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql> FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n\n\nWith these changes made, restart MySQL.\n\nservice mysql restart\n\n\nAssuming this was done correctly, your DB should now be able to receive\nread/write queries from an external source, provided the correct username and\npassword are used.","html":"<p>In the <a href=\"https://hackersandslackers.com/set-up-mysql-database/\">previous post</a>, we got familiar with the basics of creating and navigating MySQL databases. This leads us to the next most logical thing to ask: how can I use this in any meaningful way?</p><p>MySQL installations default to refusing connections outside of the local machine's IP address, as we should expect. That said, relational databases aren't usually being used by a single person on a single machine forever (but if you do, we should hang out). It goes without saying that our MySQL instance should be focusing on uptime and accessibility, or in other terms, far away from our destructive personalities.</p><p>I adore maintaining databases in the command line as much as the next self-hating masochist, but we'll need to accomplish work at some point. That means the remote database we just set up needs to be open-minded enough to allow a connection from, say, the IP address of our personal local machine, which happens to have a sexy GUI installed for this very purpose.</p><p>Making these kinds of configuration changes to any service or web server is always a bit of fun. You think your day might suck until you cone home and a piece of software treats you like a cyber criminal, kicking and screaming while we attempt the most basic out-of-the-box functionality.</p><p>The fine print here is that we wouldn't recommend messing with any of these settings unless you know what you're doing. Then again, if you knew what you were doing you probably wouldn't be reading this. The point is, if you mess up, it's your fault because we warned you.</p><p>The first thing we'll need to touch is the MySQL config found here on Ubuntu:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">vim /etc/mysql/mysql.conf.d/mysqld.cnf\n</code></pre>\n<!--kg-card-end: markdown--><p>Here you can set various configurations for MySQL, such as the port number, default user, etc. The line we're interested in is <em>bind-address.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\"># The MySQL database server configuration file.\n#\n# You can copy this to one of:\n# - &quot;/etc/mysql/my.cnf&quot; to set global options,\n# - &quot;~/.my.cnf&quot; to set user-specific options.\n# \n# One can use all long options that the program supports.\n# Run program with --help to get a list of available options and with\n# --print-defaults to see which it would actually understand and use.\n#\n# For explanations see\n# http://dev.mysql.com/doc/mysql/en/server-system-variables.html\n\n# This will be passed to all mysql clients\n# It has been reported that passwords should be enclosed with ticks/quotes\n# escpecially if they contain &quot;#&quot; chars...\n# Remember to edit /etc/mysql/debian.cnf when changing the socket location.\n\n# Here is entries for some specific programs\n# The following values assume you have at least 32M ram\n\n[mysqld_safe]\nsocket          = /var/run/mysqld/mysqld.sock\nnice            = 0\n\n[mysqld]\n#\n# * Basic Settings\n#\nuser            = mysql\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\nport            = 3306\nbasedir         = /usr\ndatadir         = /var/lib/mysql\ntmpdir          = /tmp\nlc-messages-dir = /usr/share/mysql\nskip-external-locking\n#\n# Instead of skip-networking the default is now to listen only on\n# localhost which is more compatible and is not less secure.\nbind-address           = 127.0.0.1\n</code></pre>\n<!--kg-card-end: markdown--><p>By default, bind-address is set to your local host. This is basically a whitelist that allows changes only from the domains or IP addresses specified. You can go ahead and add the address of the external domain you'd like to grant access to here.</p><p>Commenting out the line completely opens up MySQL to everybody. So there's that.</p><p>Now we need to create a user with which to access the DBL:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql -u root -p -h localhost -P 3306\n</code></pre>\n<!--kg-card-end: markdown--><p>Use the <em>CREATE USER</em> command to create a new homie. In the example below, 'newuser' is the name of the new user, and '%' is from which location the user will be permitted to make changes. This is usually 'localhost', for example. In this case, we added '%' which means <em>everywhere.</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; CREATE USER ‘newuser’@‘%' IDENTIFIED BY ‘password123’;\n</code></pre>\n<!--kg-card-end: markdown--><p>Grant all privileges to the new user, and always <code>flush privileges</code> after making such modifications.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; GRANT ALL ON *.* to newuser@'%' IDENTIFIED BY 'password123';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n<!--kg-card-end: markdown--><p>With these changes made, restart MySQL.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">service mysql restart\n</code></pre>\n<!--kg-card-end: markdown--><p>Assuming this was done correctly, your DB should now be able to receive read/write queries from an external source, provided the correct username and password are used.</p>","url":"https://hackersandslackers.com/accessing-mysql-from-external-domains/","uuid":"ca2500b8-b307-4b1c-8ccd-d9cdf4f1e8eb","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5adcf04441f6cf7b7a136a4a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867363b","title":"Setting up a MySQL Database on Ubuntu","slug":"set-up-mysql-database","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","excerpt":"Setting up MySQL the old-fashioned way: on a Linux server.","custom_excerpt":"Setting up MySQL the old-fashioned way: on a Linux server.","created_at_pretty":"17 April, 2018","published_at_pretty":"18 April, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-04-16T23:53:52.000-04:00","published_at":"2018-04-17T22:58:58.000-04:00","updated_at":"2019-03-28T04:52:03.000-04:00","meta_title":"Setting up a MySQL Database on Ubuntu | Hackers and Slackers","meta_description":"Setting up MySQL the old-fashioned way: on a linux server","og_description":"Setting up MySQL the old-fashioned way: on a Linux server","og_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","og_title":"Setting up a MySQL Database on Ubuntu","twitter_description":"Setting up MySQL the old-fashioned way: on a Linux server","twitter_image":"https://hackersandslackers.com/content/images/2018/04/mysql1@2x.jpg","twitter_title":"Setting up a MySQL Database on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"}],"plaintext":"As frameworks and services evolve to remove us further away from boilerplate\ncode, the first casualty of saved time is the fundamental understanding of what\nwe're actually doing sometimes. This has good reason; one can only learn so much\nfrom repetitive command-line interactions with databases, thus making any\nservice's one-click-deploy  button all the more sensible.  If I  had to imagine\nthe least sexy title for a post in software development, it would be something\nalong the lines of How to Configure MySQL on a VPS, as opposed to like, a\ncloud-based solution, or Even a Docker Container, as Though we Live in the God\nDamn 90s or Something.\" And that's more or less the gist of this post.\n\nI'm not exactly crushing it in the MySQL shell every day- chances are a lot of\nus aren't considering we have plenty of tools to protect us from ever thinking\nabout doing so. That said, this is very much a real use-case for pretty much any\nself-hosted application running a database natively.\n\nSo here it goes: a crash course in MySQL, by An Idiot.\n\nInstallation\nInstalling MySQL server on Ubuntu is simple:\n\nsudo apt-get install mysql-server\n\n\nConfigure MySQL via the Shell\nCreating databases, users, and permissions all happens within the MySQL shell.\nThis can be accessed via:\n\nmysql -u root -p\n\n\nThis will log you in to MySQL as the root  user. In the future, the shell can be\naccessed as any other MySQL user you may create in the future.\n\nExplore your Databases\nSee which MySQL databases exit:\n\nmysql> SHOW DATABASES;\n\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| ghost_prod         |\n| hackers_prod       |\n| ind_prod           |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n\n\nNice databases bro. Notice the mysql  database. As you might imagine, there's\nprobably a lot of cool important shit in there that makes everything work. Let's\ncheck it out.\n\nGet in There\nTo access and start messing with your db, use the USE  query:\n\nmysql> USE mysql;\n\nDatabase changed\n\n\nLet's see which tables are chillin in here.\n\nmysql> SHOW tables;\n\n+---------------------------+\n| Tables_in_mysql           |\n+---------------------------+\n| columns_priv              |\n| db                        |\n| engine_cost               |\n| event                     |\n| func                      |\n| general_log               |\n| gtid_executed             |\n| help_category             |\n| help_keyword              |\n| help_relation             |\n| help_topic                |\n| innodb_index_stats        |\n| innodb_table_stats        |\n| ndb_binlog_index          |\n| plugin                    |\n| proc                      |\n| procs_priv                |\n| proxies_priv              |\n| server_cost               |\n| servers                   |\n| slave_master_info         |\n| slave_relay_log_info      |\n| slave_worker_info         |\n| slow_log                  |\n| tables_priv               |\n| time_zone                 |\n| time_zone_leap_second     |\n| time_zone_name            |\n| time_zone_transition      |\n| time_zone_transition_type |\n| user                      |\n+---------------------------+\n\n\nOh wow yeah, that looks pretty important. This is where configurations such as\nuser information exists. When we create users and grant them permissions, we'll\nbe doing so in mysql. We'll worry about that later, but let's see who's in there\nanyway for the hell of it:\n\nmysql> select user from user;\n\n+------------------+\n| user             |\n+------------------+\n| debian-sys-maint |\n| mysql.session    |\n| mysql.sys        |\n| root             |\n+------------------+\n\n\nSick. Without knowing much SQL at all, we can already see our databases, their\ntables, and get the values of whichever columns they might have. Now let's start\ndoing stuff.\n\nCreate a Database\nGo ahead and create a new database. In my case, I want to create a database\nwhich lists my Github repositories, so I'll create a db named github_repos:\n\nCREATE DATABASE github_repos;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> USE github_repos;\nDatabase changed\n\n\nCreating a table\nNow it's getting good: we're going to create a table in our database: to do\nthis, we're going to need to define our columns upfront, including the type of\ndata each column can accept as well as the restrictions on that column. I'm\nkeeping it simple and storing values of text for now.\n\nmysql> CREATE TABLE githubrepos (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, \n    -> full_name VARCHAR(100),\n    -> description VARCHAR(300),\n    -> name VARCHAR(200),\n    -> url VARCHAR(150));\n\n\nid  is a standard column which indicates the numerical index of each row. Here,\nwe're stating that our rows will count themselves.\n\nEach following line creates a column by [name]  [type of data]****[limit]. In\nthis example we're creating columns which accept alphanumeric characters, up to\na maximum of the the number specified.\n\nNOTE: you should really overestimate the number of characters each field can\naccept. I didn't. Its a waste of time and might drive you crazy down the line:\njust accept a large number and be done with it.\n\nFruits of your labor\nGo ahead and check out what you've done:\n\nmysql> SHOW tables;\n\n+------------------------+\n| Tables_in_github_repos |\n+------------------------+\n| githubrepos            |\n+------------------------+\n\n\nDamn dude, you did it. Let's take a look just to make sure:\n\n+-------------+--------------+------+-----+---------+----------------+\n| Field       | Type         | Null | Key | Default | Extra          |\n+-------------+--------------+------+-----+---------+----------------+\n| id          | int(11)      | NO   | PRI | NULL    | auto_increment |\n| full_name   | varchar(200) | YES  |     | NULL    |                |\n| description | varchar(300) | YES  |     | NULL    |                |\n| name        | varchar(200) | YES  |     | NULL    |                |\n| url         | varchar(200) | YES  |     | NULL    |                |\n+-------------+--------------+------+-----+---------+----------------+\n5 rows in set (0.01 sec)\n\n\nHoly shit it's literally a table.\n\nMaking Changes\nLet's make some changes to our table after the fact. We can use ALTER TABLE to\nadd, modify, or remove columns.\n\nmysql> ALTER TABLE githubrepos ADD homepage VARCHAR(255);\nQuery OK, 0 rows affected (0.09 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\n\nThis added column homepage  which accepts alphanumeric characters.\n\nNow that we've created our own database with our own defined structure, the\npossibilities are endless. The next step is to actually fill it with data, but\nlet's save that for next time.\n\nmysql> \\q\nBye","html":"<p>As frameworks and services evolve to remove us further away from boilerplate code, the first casualty of saved time is the fundamental understanding of what we're actually doing sometimes. This has good reason; one can only learn so much from repetitive command-line interactions with databases, thus making any service's <em>one-click-deploy</em> button all the more sensible.  If I  had to imagine the least sexy title for a post in software development, it would be something along the lines of <em><strong>How to Configure MySQL on a VPS, as opposed to like, a cloud-based solution, or Even a Docker Container, as Though we Live in the God Damn 90s or Something.\" </strong> </em>And that's more or less the gist of this post.</p><p>I'm not exactly crushing it in the MySQL shell every day- chances are a lot of us aren't considering we have plenty of tools to protect us from ever thinking about doing so. That said, this is very much a real use-case for pretty much any self-hosted application running a database natively.</p><p>So here it goes: a crash course in MySQL, by An Idiot.</p><h3 id=\"installation\">Installation</h3><p>Installing MySQL server on Ubuntu is simple:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">sudo apt-get install mysql-server\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"configure-mysql-via-the-shell\">Configure MySQL via the Shell</h3><p>Creating databases, users, and permissions all happens within the MySQL shell. This can be accessed via:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql -u root -p\n</code></pre>\n<!--kg-card-end: markdown--><p>This will log you in to MySQL as the <strong>root</strong> user. In the future, the shell can be accessed as any other MySQL user you may create in the future.</p><h3 id=\"explore-your-databases\">Explore your Databases</h3><p>See which MySQL databases exit:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW DATABASES;\n\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| ghost_prod         |\n| hackers_prod       |\n| ind_prod           |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice databases bro. Notice the <em>mysql</em> database. As you might imagine, there's probably a lot of cool important shit in there that makes everything work. Let's check it out.</p><h3 id=\"get-in-there\">Get in There</h3><p>To access and start messing with your db, use the <strong>USE</strong> query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; USE mysql;\n\nDatabase changed\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see which tables are chillin in here.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW tables;\n\n+---------------------------+\n| Tables_in_mysql           |\n+---------------------------+\n| columns_priv              |\n| db                        |\n| engine_cost               |\n| event                     |\n| func                      |\n| general_log               |\n| gtid_executed             |\n| help_category             |\n| help_keyword              |\n| help_relation             |\n| help_topic                |\n| innodb_index_stats        |\n| innodb_table_stats        |\n| ndb_binlog_index          |\n| plugin                    |\n| proc                      |\n| procs_priv                |\n| proxies_priv              |\n| server_cost               |\n| servers                   |\n| slave_master_info         |\n| slave_relay_log_info      |\n| slave_worker_info         |\n| slow_log                  |\n| tables_priv               |\n| time_zone                 |\n| time_zone_leap_second     |\n| time_zone_name            |\n| time_zone_transition      |\n| time_zone_transition_type |\n| user                      |\n+---------------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Oh wow yeah, that looks pretty important. This is where configurations such as user information exists. When we create users and grant them permissions, we'll be doing so in <em>mysql</em>. We'll worry about that later, but let's see who's in there anyway for the hell of it:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; select user from user;\n\n+------------------+\n| user             |\n+------------------+\n| debian-sys-maint |\n| mysql.session    |\n| mysql.sys        |\n| root             |\n+------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Sick. Without knowing much SQL at all, we can already see our databases, their tables, and get the values of whichever columns they might have. Now let's start doing stuff.</p><h3 id=\"create-a-database\">Create a Database</h3><p>Go ahead and create a new database. In my case, I want to create a database which lists my Github repositories, so I'll create a db named <em>github_repos</em>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">CREATE DATABASE github_repos;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql&gt; USE github_repos;\nDatabase changed\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"creating-a-table\">Creating a table</h3><p>Now it's getting good: we're going to create a table in our database: to do this, we're going to need to define our columns upfront, including the type of data each column can accept as well as the restrictions on that column. I'm keeping it simple and storing values of text for now.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; CREATE TABLE githubrepos (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, \n    -&gt; full_name VARCHAR(100),\n    -&gt; description VARCHAR(300),\n    -&gt; name VARCHAR(200),\n    -&gt; url VARCHAR(150));\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>id</strong> is a standard column which indicates the numerical index of each row. Here, we're stating that our rows will count themselves.</p><p>Each following line creates a column by <strong>[name]</strong> <strong>[type of data]****[limit]</strong>. In this example we're creating columns which accept alphanumeric characters, up to a maximum of the the number specified.</p><p><em><strong>NOTE: you should really overestimate the number of characters each field can accept. I didn't. Its a waste of time and might drive you crazy down the line: just accept a large number and be done with it.</strong></em></p><h3 id=\"fruits-of-your-labor\">Fruits of your labor</h3><p>Go ahead and check out what you've done:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; SHOW tables;\n\n+------------------------+\n| Tables_in_github_repos |\n+------------------------+\n| githubrepos            |\n+------------------------+\n</code></pre>\n<!--kg-card-end: markdown--><p>Damn dude, you did it. Let's take a look just to make sure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">+-------------+--------------+------+-----+---------+----------------+\n| Field       | Type         | Null | Key | Default | Extra          |\n+-------------+--------------+------+-----+---------+----------------+\n| id          | int(11)      | NO   | PRI | NULL    | auto_increment |\n| full_name   | varchar(200) | YES  |     | NULL    |                |\n| description | varchar(300) | YES  |     | NULL    |                |\n| name        | varchar(200) | YES  |     | NULL    |                |\n| url         | varchar(200) | YES  |     | NULL    |                |\n+-------------+--------------+------+-----+---------+----------------+\n5 rows in set (0.01 sec)\n</code></pre>\n<!--kg-card-end: markdown--><p>Holy shit it's literally a table.</p><h3 id=\"making-changes\">Making Changes</h3><p>Let's make some changes to our table after the fact. We can use ALTER TABLE to add, modify, or remove columns.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; ALTER TABLE githubrepos ADD homepage VARCHAR(255);\nQuery OK, 0 rows affected (0.09 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n</code></pre>\n<!--kg-card-end: markdown--><p>This added column <em>homepage</em> which accepts alphanumeric characters.</p><p>Now that we've created our own database with our own defined structure, the possibilities are endless. The next step is to actually fill it with data, but let's save that for next time.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">mysql&gt; \\q\nBye\n</code></pre>\n<!--kg-card-end: markdown-->","url":"https://hackersandslackers.com/set-up-mysql-database/","uuid":"697bc755-a833-43fe-b807-5668b8c284f4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ad56fd09bfe350c74e8a8cb"}}]}},"pageContext":{"slug":"mysql","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}