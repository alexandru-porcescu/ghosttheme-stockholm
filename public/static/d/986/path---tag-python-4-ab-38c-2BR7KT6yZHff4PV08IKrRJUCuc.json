{"data":{"ghostTag":{"slug":"python","name":"Python","visibility":"public","feature_image":null,"description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold"},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5cacc6440a88d417374aaf43","title":"Upgrading to JupyterLab on Ubuntu","slug":"upgrading-to-jupyter-lab-on-ubuntu","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-1.jpg","excerpt":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","custom_excerpt":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","created_at_pretty":"09 April, 2019","published_at_pretty":"14 April, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-04-09T12:20:20.000-04:00","published_at":"2019-04-14T13:25:33.000-04:00","updated_at":"2019-04-14T13:25:33.000-04:00","meta_title":"Upgrading to Jupyter Lab on Ubuntu | Hackers and Slackers","meta_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","og_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","og_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-3.jpg","og_title":"Upgrading to Jupyter Lab on Ubuntu","twitter_description":"Improve your Jupyter workflow with JupyterLab's powerful interface & customizations.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/jupyterlab-1-2.jpg","twitter_title":"Upgrading to Jupyter Lab on Ubuntu","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"Last we chatted, we walked through the process of getting started with Jupyter\nnotebooks on a Ubuntu server\n[https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/].\nThe classic Jupyter notebook interface is certainly well-suited to the job done.\nThat said, it only takes some time of getting lost in the interface to start\nthinking we can do better. That's where Jupyter Lab comes in.\n\nWhy JupyterLab?\nJupyterLab is sometimes referred to as \"the next generation of Jupyter\nnotebooks,\" which is a statement I can attest to. JupyterLab offers an improved\ninterface for Jupyter notebooks which is both available out of the box, as well\nas highly customizable for your workflow.\n\nOut of the box, the traditional Jupyter interface is extended to include a tree\nfile manager: similar to what you might expect from an IDE. This allows you to\neasily browse all  available notebooks on your server. In addition, the notebook\ninterface has been simplified to reduce noise brought on by (mostly useless)\ntoolbars and excessive buttons. Take a look at the interface prior to any\ncustomization:\n\nEven more appealing than an updated interface is JupyterLab's openness to \ncustomization. JupyterLab has a strongly growing ecosystem for extension\ndevelopment: we'll be getting a taste of some of those goodies in just a moment.\n\nGetting Started\nIf you're starting from scratch, go ahead and follow the same steps in the \nJupyter Notebook setup tutorial\n[https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/] \nup until Conda is set up and activated. \n\nWith a Ubuntu server prepped and ready, install Jupyter Lab with the following\ncommand:\n\n$ conda install -c conda-forge jupyterlab\n\n\nBefore we go any further, make sure you're tunneled into your server - we need\nto do this in order to launch notebooks, remember?:\n\n$ ssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nStart up the Jupyter Lab environment like this:\n\n$ conda activate your_env\n$ jupyter lab\n\n\nIf everything went well, you should be greeted with a fancy Jupyter Lab loading\nscreen and then thrown into the Jupyer Lab environment.\n\nHome sweet home.Things are looking good right off the bat. Without any added\nlibraries, we've already beefed up our Jupyter Notebook workspace. The tree view\nis available, we can launch terminals, and don't forget: we can split screen by\ndragging snapping windows where we see fit. \n\nCustomizing Your Workspace\nJupyterLab uses NodeJS to enable some of its cooler functionality and\nextensions. Â Go ahead and install Node:\n\n$ cd /tmp\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n\n\nNice! Now we have all we need to go nuts with extensions. Here are the ones I\nrecommend:\n\nStatus Bar\n\n--------------------------------------------------------------------------------\n\nKeep an eye on your environment's vitals.Repository found here [https://github.com/jupyterlab/jupyterlab-statusbar].\nInstallation steps:\n\n$ pip install nbresuse\n$ jupyter serverextension enable --py nbresuse\n$ jupyter labextension install @jupyterlab/statusbar\n\n\nTable of Contents\nAuto-generate table of contents to help navigate and organize your notebooks.\nRepository found here [https://github.com/jupyterlab/jupyterlab-toc].\nInstallation steps:\n\n$ jupyter labextension install @jupyterlab/toc\n\n\nVariable Inspector\n\n--------------------------------------------------------------------------------\n\nKeep tabs on every variable used in your notebook and their respective values.\nRepository found here [https://github.com/lckr/jupyterlab-variableInspector].\nInstallation steps:\n\n$ jupyter labextension install @lckr/jupyterlab_variableinspector\n\n\nGit Integration\n\n--------------------------------------------------------------------------------\n\nA visual approach to version control.Repository found here [https://github.com/jupyterlab/jupyterlab-git].\nInstallation steps:\n\n$ jupyter labextension install @jupyterlab/git\n$ pip install jupyterlab-git\n$ jupyter serverextension enable --py jupyterlab_git\n\n\nDraw.io\n\n--------------------------------------------------------------------------------\n\nCreate Draw.io diagrams right from your notebook.Repository found here [https://github.com/QuantStack/jupyterlab-drawio].\nInstallation steps:\n\n$ jupyter labextension install jupyterlab-drawio\n\n\nAdditional Resources\nYou have everything you need to go nuts from here forward. INSTALL ALL THE\nEXTENSIONS!\n\nIf you're looking for more extension goodness, I'd start with the Awesome\nJupyter [https://github.com/markusschanta/awesome-jupyter#jupyterlab-extensions] \n repo on Github- there's a section specifically for JupyterLab.\n\nIf you're totally into JupyterLab now and want to join a gang, the community\npage can be found here [https://github.com/topics/jupyterlab-extension].\n\nLastly, if you've gone totally off the deep end and already want to start\ncreating extensions of your own, check out the extension documentation\n[https://jupyterlab.readthedocs.io/en/stable/user/extensions.html]. That's all\nfolks!","html":"<p>Last we chatted, we walked through the process of getting started with <a href=\"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/\">Jupyter notebooks on a Ubuntu server</a>. The classic Jupyter notebook interface is certainly well-suited to the job done. That said, it only takes some time of getting lost in the interface to start thinking we can do better. That's where Jupyter Lab comes in.</p><h2 id=\"why-jupyterlab\">Why JupyterLab?</h2><p>JupyterLab is sometimes referred to as \"the next generation of Jupyter notebooks,\" which is a statement I can attest to. JupyterLab offers an improved interface for Jupyter notebooks which is both available out of the box, as well as highly customizable for your workflow.</p><p>Out of the box, the traditional Jupyter interface is extended to include a tree file manager: similar to what you might expect from an IDE. This allows you to easily browse <em>all</em> available notebooks on your server. In addition, the notebook interface has been simplified to reduce noise brought on by (mostly useless) toolbars and excessive buttons. Take a look at the interface prior to any customization:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-no-customization.png\" class=\"kg-image\"></figure><!--kg-card-end: image--><p>Even more appealing than an updated interface is JupyterLab's openness to <em>customization</em>. JupyterLab has a strongly growing ecosystem for extension development: we'll be getting a taste of some of those goodies in just a moment.</p><h2 id=\"getting-started\">Getting Started</h2><p>If you're starting from scratch, go ahead and follow the same steps in the <a href=\"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/\">Jupyter Notebook setup tutorial</a> up until Conda is set up and activated. </p><p>With a Ubuntu server prepped and ready, install Jupyter Lab with the following command:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda install -c conda-forge jupyterlab\n</code></pre>\n<!--kg-card-end: markdown--><p>Before we go any further, make sure you're tunneled into your server - we need to do this in order to launch notebooks, remember?:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Start up the Jupyter Lab environment like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda activate your_env\n$ jupyter lab\n</code></pre>\n<!--kg-card-end: markdown--><p>If everything went well, you should be greeted with a fancy Jupyter Lab loading screen and then thrown into the Jupyer Lab environment.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyter-lab-home.png\" class=\"kg-image\"><figcaption>Home sweet home.</figcaption></figure><!--kg-card-end: image--><p>Things are looking good right off the bat. Without any added libraries, we've already beefed up our Jupyter Notebook workspace. The tree view is available, we can launch terminals, and don't forget: we can split screen by dragging snapping windows where we see fit. </p><h2 id=\"customizing-your-workspace\">Customizing Your Workspace</h2><p>JupyterLab uses NodeJS to enable some of its cooler functionality and extensions. Â Go ahead and install Node:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd /tmp\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n</code></pre>\n<!--kg-card-end: markdown--><p>Nice! Now we have all we need to go nuts with extensions. Here are the ones I recommend:</p><h3 id=\"status-bar\">Status Bar</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-statusbar.gif\" class=\"kg-image\"><figcaption>Keep an eye on your environment's vitals.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-statusbar\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ pip install nbresuse\n$ jupyter serverextension enable --py nbresuse\n$ jupyter labextension install @jupyterlab/statusbar\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"table-of-contents\">Table of Contents</h3><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-toc.gif\" class=\"kg-image\"><figcaption>Auto-generate table of contents to help navigate and organize your notebooks.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-toc\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @jupyterlab/toc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"variable-inspector\">Variable Inspector</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyterlab-vars.gif\" class=\"kg-image\"><figcaption>Keep tabs on every variable used in your notebook and their respective values.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/lckr/jupyterlab-variableInspector\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @lckr/jupyterlab_variableinspector\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"git-integration\">Git Integration</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/jupyter-git.gif\" class=\"kg-image\"><figcaption>A visual approach to version control.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/jupyterlab/jupyterlab-git\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install @jupyterlab/git\n$ pip install jupyterlab-git\n$ jupyter serverextension enable --py jupyterlab_git\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"draw-io\">Draw.io</h3><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/drawio.gif\" class=\"kg-image\"><figcaption>Create Draw.io diagrams right from your notebook.</figcaption></figure><!--kg-card-end: image--><p>Repository found <a href=\"https://github.com/QuantStack/jupyterlab-drawio\">here</a>. Installation steps:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter labextension install jupyterlab-drawio\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"additional-resources\">Additional Resources</h2><p>You have everything you need to go nuts from here forward. INSTALL ALL THE EXTENSIONS!</p><p>If you're looking for more extension goodness, I'd start with the <a href=\"https://github.com/markusschanta/awesome-jupyter#jupyterlab-extensions\">Awesome Jupyter</a> repo on Github- there's a section specifically for JupyterLab.</p><p>If you're totally into JupyterLab now and want to join a gang, the community page can be found <a href=\"https://github.com/topics/jupyterlab-extension\">here</a>.</p><p>Lastly, if you've gone totally off the deep end and already want to start creating extensions of your own, check out the <a href=\"https://jupyterlab.readthedocs.io/en/stable/user/extensions.html\">extension documentation</a>. That's all folks!</p><p></p>","url":"https://hackersandslackers.com/upgrading-to-jupyter-lab-on-ubuntu/","uuid":"58669f89-0b58-4689-bc6d-03f93d843919","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5cacc6440a88d417374aaf43"}},{"node":{"id":"Ghost__Post__5c779ffbc380a221de39c7cf","title":"Using Flask-Login to Handle User Accounts","slug":"authenticating-users-with-flask-login","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin.jpg","excerpt":"Add user authentication to your Flask app with Flask-Login","custom_excerpt":"Add user authentication to your Flask app with Flask-Login","created_at_pretty":"28 February, 2019","published_at_pretty":"04 April, 2019","updated_at_pretty":"11 April, 2019","created_at":"2019-02-28T03:46:51.000-05:00","published_at":"2019-04-04T18:36:51.000-04:00","updated_at":"2019-04-10T22:01:31.000-04:00","meta_title":"Authenticating Users With Flask-Login | Hackers and Slackers","meta_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","og_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","og_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin-2.jpg","og_title":"Authenticating Users With Flask-Login","twitter_description":"Adding user authentication to your Flask app with the Flask-Login Python library. Manage user creation, log-ins, signups, and application security.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/flasklogin-1.jpg","twitter_title":"Authenticating Users With Flask-Login","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Pythonâs fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Pythonâs fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"Weâve covered a lot of Flask goodness in this series thus far. We fully\nunderstand how to structure a sensible application; we can serve up complex\npage\ntemplates\n[https://hackersandslackers.com/powerful-page-templates-in-flask-with-jinja/],\nand have dived into interacting with databases using Flask-SQLAlchemy\n[https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/].\nFor our next challenge, weâre going to need all  of the knowledge we've acquired\nthus far and much, much more. Welcome to the Super Bowl of Flask development.\nThis. Is. Flask-Login.\n\nFlask-Login [https://flask-login.readthedocs.io/en/latest/]  is a dope library\nwhich handles all aspects of user management, including vital nuances you might\nnot expect. Some noteworthy features include securing parts of our app behind\nlogin walls, encrypting passwords, and handling sessions. Moreover, It plays\nnicely with other Flask libraries weâre already familiar with: Flask-SQLAlchemy\n[https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/] \nto create and fetch accounts, and Flask-WTForms\n[https://hackersandslackers.com/guide-to-building-forms-in-flask/]  for handling\nintelligent sign-up & log-in forms. This tutorial assumes you have some working\nknowledge of these things.\n\nFlask-Login is shockingly quite easy to use after the initial learning curve...\nbut therein lies the catch. Perhaps Iâm not the only one to have noticed, but\nmost Flask-related documentation tends to be, well, God-awful. The community is\nriddled with helplessly outdated information; if you ever come across flask.ext \nin a tutorial, it is inherently worthless to anybody developing in 2019. To make\nmatters worse, official Flask-Login documentation contains some artifacts which\nare just plain wrong. The documentation contradicts itself (Iâll show you what I\nmean), and offers little to no code examples to speak of. My only hope is that I\nmight save somebody the endless headaches Iâve experienced myself.\n\nStructuring Our Application\nLetâs start with installing dependencies. This should give you an idea of what\nyouâre in for:\n\n$ pip3 install flask flask-login flask-sqlalchemy psycopg2-binary python-dotenv\n\n\nSweet. Letâs take this one step at a time, starting with our project file\nstructure:\n\nflasklogin-tutorial\nâââ /login_tutorial\nâ   âââ __init__.py\nâ   âââ auth.py\nâ   âââ forms.py\nâ   âââ models.py\nâ   âââ routes.py\nâ   âââ /static\nâ   â   âââ /dist\nâ   â   â   âââ /css\nâ   â   â   â   âââ account.css\nâ   â   â   â   âââ dashboard.css\nâ   â   â   âââ /js\nâ   â   â       âââ main.min.js\nâ   â   âââ /src\nâ   â       âââ /js\nâ   â       â   âââ main.js\nâ   â       âââ /less\nâ   â           âââ account.less\nâ   â           âââ dashboard.less\nâ   â           âââ vars.less\nâ   âââ /templates\nâ       âââ dashboard.html\nâ       âââ layout.html\nâ       âââ login.html\nâ       âââ meta.html\nâ       âââ scripts.html\nâ       âââ signup.html\nâââ config.py\nâââ requirements.txt\nâââ setup.py\nâââ start.sh\nâââ wsgi.py\n\n\nOf course, I wouldn't be a gentleman unless I revealed my config.py  as well:\n\nimport os\n\n\nclass Config:\n    \"\"\"Set Flask configuration vars from .env file.\"\"\"\n\n    # General Config\n    SECRET_KEY = os.environ.get('SECRET_KEY')\n    FLASK_APP = os.environ.get('FLASK_APP')\n    FLASK_ENV = os.environ.get('FLASK_ENV')\n    FLASK_DEBUG = os.environ.get('FLASK_DEBUG')\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get('SQLALCHEMY_TRACK_MODIFICATIONS')\n\n\nThe configuration values live in a .env  file, a practice I highly encourage. Of\nthese configuration variables, SECRET_KEY  is where we should turn our\nattention. SECRET_KEY is the equivalent of a password used to secure our app; it\nshould be as long, nonsensical, and impossible-to-remember as humanly possible.\nSeriously: having your secret key compromised is the equivalent of feeding\ngremlins after midnight.\n\nInitializing Flask-Login\nWith a standard \"application factory\" app, setting up Flask-Login is no\ndifferent from other Flask plugins (or whatever they're called now). This makes\nsetting up easy; all we need to do is make sure Flask-Login  is initialized in \n__init__.py  along with the rest of our plugins:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\n\n\ndb = SQLAlchemy()\nlogin_manager = LoginManager()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n\n    # Application Configuration\n    app.config.from_object('config.Config')\n\n    # Initialize Plugins\n    db.init_app(app)\n    login_manager.init_app(app)\n\n    with app.app_context():\n        # Import parts of our application\n        from . import routes\n        from . import login\n        app.register_blueprint(routes.main_bp)\n        app.register_blueprint(login.login_bp)\n\n        # Initialize Global db\n        db.create_all()\n\n        return app\n\n\nIn the above example, we're using the minimal number of plug-ins to get logins\nworking: Flask-SQLAlchemy  and Flask-Login.\n\nTo keep our sanity, we're going to separate our login routes from our main\napplication routes and logic. This is why we register a Blueprint called auth_bp\n, imported from a file called auth.py. Our âmainâ application (AKA anything that\nisnât logging in) will instead live in routes.py, in a Blueprint called main_bp.\nWe'll come back to these in a moment\n\nCreating a User Model\nWe'll save our User  model in models.py. There are a few things to keep in mind\nwhen creating models compatible with Flask-Login- the most important being the\nutilization of UserMixin  from the flask_login  library. When we inherit our\nclass from UserMixin,  our model is immediately extended to include all the\nmethods necessary for Flask-Login to work. This is by far the easiest way of\ncreating a User model. I won't bother getting into details of what these methods\ndo, because if you simply begin your class with class User(UserMixin, db.Model):\n, you genuinely don't need to understand any of it:\n\nfrom . import db\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n\nclass User(UserMixin, db.Model):\n    \"\"\"Model for user accounts.\"\"\"\n\n    __tablename__ = 'flasklogin-users'\n\n    id = db.Column(db.Integer,\n                   primary_key=True,\n                   )\n    name = db.Column(db.String,\n                     nullable=False,\n                     unique=False)\n    email = db.Column(db.String(40),\n                      unique=True,\n                      nullable=False\n                      )\n    password = db.Column(db.String(200),\n                         primary_key=False,\n                         unique=False,\n                         nullable=False\n                         )\n    website = db.Column(db.String(60),\n                        index=False,\n                        unique=False,\n                        nullable=True\n                        )\n    created_on = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n    last_login = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n\n    def set_password(self, password):\n        \"\"\"Create hashed password.\"\"\"\n        self.password = generate_password_hash(password, method='sha256')\n\n    def check_password(self, password):\n        \"\"\"Check hashed password.\"\"\"\n        return check_password_hash(self.password, password)\n\n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n\nThe set_password  and check_password  methods don't necessarily need to live\ninside our User model, but it's nice to keep related logic bundled together and\nout of our routes.\n\nYou may notice that our password field explicitly allows 200 characters: this is\nbecause our database will be storing hashed passwords. Thus, even if a user's\npassword is 8 characters long, the string in our database will look much\ndifferent.\n\nCreating Log-in and Sign-up Forms\nIf you're well versed in WTForms, our form logic in forms.py  probably looks as\nyou'd expect. Of course, the constraints we set here are to handle front-end\nvalidation only:\n\nfrom wtforms import Form, StringField, PasswordField, validators, SubmitField\nfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo, Length, Optional\n\n\nclass SignupForm(Form):\n    \"\"\"User Signup Form.\"\"\"\n\n    name = StringField('Name',\n                        validators=[DataRequired(message=('Enter a fake name or something.'))])\n    email = StringField('Email',\n                        validators=[Length(min=6, message=('Please enter a valid email address.')),\n                                    Email(message=('Please enter a valid email address.')),\n                                    DataRequired(message=('Please enter a valid email address.'))])\n    password = PasswordField('Password',\n                             validators=[DataRequired(message='Please enter a password.'),\n                                         Length(min=6, message=('Please select a stronger password.')),\n                                         EqualTo('confirm', message='Passwords must match')])\n    confirm = PasswordField('Confirm Your Password',)\n    website = StringField('Website',\n                          validators=[Optional()])\n    submit = SubmitField('Register')\n\n\nclass LoginForm(Form):\n    \"\"\"User Login Form.\"\"\"\n\n    email = StringField('Email', validators=[DataRequired('Please enter a valid email address.'),\n                                             Email('Please enter a valid email address.')])\n    password = PasswordField('Password', validators=[DataRequired('Uhh, your password tho?')])\n    submit = SubmitField('Log In')\n\n\nWith those out of the way, let's look at how we implement these on the Jinja\nside.\n\nsignup.html\n{% extends \"layout.html\" %}\n\n{% block pagestyles %}\n    <link href=\"{{ url_for('static', filename='dist/css/account.css') }}\" rel=\"stylesheet\" type=\"text/css\">\n{% endblock %}\n\n{% block content %}\n  <div class=\"formwrapper\">\n    <form method=post>\n      <div class=\"logo\">\n        <img src=\"{{ url_for('static', filename='dist/img/logo.png') }}\">\n      </div>\n      {% for message in get_flashed_messages() %}\n        <div class=\"alert alert-warning\">\n            <button type=\"button\" class=\"close\" data-dismiss=\"alert\">&times;</button>\n            {{ message }}\n        </div>\n      {% endfor %}\n      <h1>Sign Up</h1>\n      <div class=\"name\">\n        {{ form.name.label }}\n        {{ form.name(placeholder='John Smith') }}\n        {% if form.name.errors %}\n          <ul class=\"errors\">\n            {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"email\">\n        {{ form.email.label }}\n        {{ form.email(placeholder='youremail@example.com') }}\n        {% if form.email.errors %}\n          <ul class=\"errors\">\n            {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"password\">\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.password.errors %}\n          <ul class=\"errors\">\n            {% for error in form.password.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"confirm\">\n        {{ form.confirm.label }}\n        {{ form.confirm }}\n        {% if form.confirm.errors %}\n          <ul class=\"errors\">\n            {% for error in form.confirm.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"website\">\n        {{ form.website.label }}\n        {{ form.website(placeholder='http://example.com') }}\n      </div>\n      <div class=\"submitbutton\">\n        <input id=\"submit\" type=\"submit\" value=\"Submit\">\n      </div>\n    </form>\n    <div class=\"loginsignup\">\n      <span>Already have an account? <a href=\"{{ url_for('auth_bp.login_page') }}\">Log in.</a><span>\n    </div>\n  </div>\n{% endblock %}\n\n\n\nlogin.py\n{% extends \"layout.html\" %}\n\n{% block pagestyles %}\n  <link href=\"{{ url_for('static', filename='dist/css/account.css') }}\" rel=\"stylesheet\" type=\"text/css\">\n{% endblock %}\n\n{% block content %}\n  <div class=\"formwrapper\">\n    <form method=post>\n      <div class=\"logo\">\n        <img src=\"{{ url_for('static', filename='dist/img/logo.png') }}\">\n      </div>\n      {% for message in get_flashed_messages() %}\n        <div class=\"alert alert-warning\">\n            <button type=\"button\" class=\"close\" data-dismiss=\"alert\">&times;</button>\n            {{ message }}\n        </div>\n      {% endfor %}\n      <h1>Log In</h1>\n      <div class=\"email\">\n         {{ form.email.label }}\n         {{ form.email(placeholder='youremail@example.com') }}\n         {% if form.email.errors %}\n           <ul class=\"errors\">\n             {% for error in form.email.errors %}<li>{{ error }}</li>{% endfor %}\n           </ul>\n         {% endif %}\n      </div>\n      <div class=\"password\">\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.email.errors %}\n          <ul class=\"errors\">\n            {% for error in form.password.errors %}<li>{{ error }}</li>{% endfor %}\n          </ul>\n        {% endif %}\n      </div>\n      <div class=\"submitbutton\">\n        <input id=\"submit\" type=\"submit\" value=\"Submit\">\n      </div>\n      <div class=\"loginsignup\">\n        <span>Don't have an account? <a href=\"{{ url_for('auth_bp.signup_page') }}\">Sign up.</a><span>\n        </div>\n    </form>\n  </div>\n{% endblock %}\n\n\n\nExcellent: the stage is set to start kicking some ass.\n\nCreating Our Login Routes\nLet us turn our attention to the heart of the logic we'll be writing in auth.py:\n\nimport os\nfrom flask import redirect, render_template, flash, Blueprint, request, session, url_for\nfrom flask_login import login_required, logout_user, current_user, login_user\nfrom flask import current_app as app\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom .forms import LoginForm, SignupForm\nfrom .models import db, User\nfrom . import login_manager\n\n\n# Blueprint Configuration\nauth_bp = Blueprint('auth_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    \"\"\"User login page.\"\"\"\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        ...\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=\"Log in with your User account.\")\n\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    \"\"\"User sign-up page.\"\"\"\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        ...\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=\"Sign up for a user account.\")\n\n\nHere we find two separate skeleton routes for Sign up  and Log in. Without the\nauthentication logic added quite yet, these routes look almost identical thus\nfar.\n\nEach time a user visits a page in your app, the corresponding route is sent a \nrequest  object. This object contains contextual information about the request\nmade by the user, such as the type of request (GET or POST), any form data which\nwas submitted, etc. We leverage this to see whether the user is just arriving at\nthe page for the first time (a GET request), or if they're attempting to sign in\n(a POST request). The fairly clever takeaway here is that our login pages verify\nusers by making POST requests to themselves: this allows us to keep all logic\nrelated to logging in or signing up in a single route.\n\nSigning Up\nWe're able to validate the submitted form by importing the SignupForm  class and\npassing request.form  as the form in question. if signup_form.validate()  checks\nthe information submitted by the user against all the form's validators. If any\nof the validators are not met, the user is redirected back to the signup form\nwith error messages present.\n\nAssuming that our user isn't inept, we can move on with our logic. First, we\nneed to make sure a user with the provided email doesn't already exist:\n\n...\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    \"\"\"User sign-up page.\"\"\"\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        if signup_form.validate():\n            # Get Form Fields\n            name = request.form.get('name')\n            email = request.form.get('email')\n            password = request.form.get('password')\n            website = request.form.get('website')\n            existing_user = User.query.filter_by(email=email).first()\n            if existing_user is None:\n                user = User(name=name,\n                            email=email,\n                            password=generate_password_hash(password, method='sha256'),\n                            website=website)\n                db.session.add(user)\n                db.session.commit()\n                login_user(user)\n                return redirect(url_for('main_bp.dashboard'))\n            flash('A user already exists with that email address.')\n            return redirect(url_for('auth_bp.signup_page'))\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=\"Sign up for a user account.\")\n\n\nIf existing_user is None, we're all clear to actually clear to create a new user\nrecord. We create an instance of our User model via user = User(...). We then\nadd the user via standard SQLAlchemy syntax and finally use the imported method \nlogin_user()  to log the user in.\n\nIf everything goes well, the user will finally be redirected to the main\napplication, which is handled by return redirect(url_for('main_bp.dashboard')):\n\nA successful user log in.And here's what will happen if we log out and try to\nsign up with the same information:\n\nAttempting to sign up with an existing emailLogging In\nMoving on to our login route:\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    \"\"\"User login page.\"\"\"\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        if login_form.validate():\n            # Get Form Fields\n            email = request.form.get('email')\n            password = request.form.get('password')\n            # Validate Login Attempt\n            user = User.query.filter_by(email=email).first()\n            if user:\n                if user.check_password(password=password):\n                    login_user(user)\n                    next = request.args.get('next')\n                    return redirect(next or url_for('main_bp.dashboard'))\n        flash('Invalid username/password combination')\n        return redirect(url_for('auth_bp.login_page'))\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=\"Log in with your User account.\")\n\n\nThis should mostly look the same! our logic is identical up until the point\nwhere we check to see if the user exists. This time, a match results in success\nas opposed to a failure. Continuing, we then use user.check_password()  to check\nthe hashed password we created earlier with user.generate_password_hash(). Both\nof these methods handle the encrypting and decrypting of passwords on their own\n(based on that SECRET_KEY we created earlier) to ensure that nobody (not even\nus) has any business looking at user passwords.\n\nAs with last time, a successful login ends in login_user(user). Our redirect\nlogic is little more sophisticated this time around: instead of always sending\nthe user back to the dashboard, we check for next, which is a parameter stored\nin the query string of the current user. If the user attempted to access our app\nbefore logging in, next  would equal the page they had attempted to reach: this\nallows us wall-off our app from unauthorized users, and then drop users off at\nthe page they attempted to reach before they logged in:\n\nA successful log inIMPORTANT: Login Helpers\nBefore your app can work like the above, we need to finish auth.py  by providing\na few more routes:\n\n@auth_bp.route(\"/logout\")\n@login_required\ndef logout_page():\n    \"\"\"User log-out logic.\"\"\"\n    logout_user()\n    return redirect(url_for('auth_bp.login_page'))\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    \"\"\"Check if user is logged-in on every page load.\"\"\"\n    if user_id is not None:\n        return User.query.get(user_id)\n    return None\n\n\n@login_manager.unauthorized_handler\ndef unauthorized():\n    \"\"\"Redirect unauthorized users to Login page.\"\"\"\n    flash('You must be logged in to view that page.')\n    return redirect(url_for('auth_bp.login_page'))\n\n\nOur first route, logout_page, handles the logic of users logging out. This will\nsimply end the user's session and redirect them to the login screen.\n\nload_user  is critical for making our app work: before every page load, our app\nmust verify whether or not the user is logged in (or still  logged in after time\nhas elapsed). user_loader  loads users by their unique ID. If a user is\nreturned, this signifies a logged-out user. Otherwise, when None  is returned,\nthe user is logged out.\n\nLastly, we have the unauthorized  route, which uses the unauthorized_handler \ndecorator for dealing with unauthorized users. Any time a user attempts to hit\nour app and is unauthorized, this route will fire.\n\nThe Last Piece: routes.py\nThe last thing we'll cover is how to protect parts of our app from unauthorized\nusers. Here's what we have in routes.py:\n\nimport os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask_login import current_user\nfrom flask import current_app as app\nfrom .models import User\nfrom flask_login import login_required\n\n\n# Blueprint Configuration\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@main_bp.route('/', methods=['GET'])\n@login_required\ndef dashboard():\n    \"\"\"Serve logged in Dashboard.\"\"\"\n    return render_template('dashboard.html',\n                           title='Flask-Login Tutorial.',\n                           template='dashboard-template',\n                           current_user=current_user,\n                           body=\"You are now logged in!\")\n\n\nThe magic here is all contained within the @login_required  decorator. When this\ndecorator is present on a route, the following things happen:\n\n * The @login_manager.user_loader  route we created determines whether or not\n   the user is authorized to view the page (logged in). If the user is logged\n   in, they'll be permitted to view the page.\n * If the user is not logged in, the user will be redirected as per the logic in\n   the route decorated with @login_manager.unauthorized_handler.\n * The name of the route the user attempted to access will be stored in the URL\n   as ?url=[name-of-route]. This what allows next  to work.\n\nThere You Have It\nIf you've made it this far, I commend you for your courage. To reward your\naccomplishments, I've published the source code for this tutorial on Github\n[https://github.com/toddbirchard/flasklogin-tutorial]  for your reference.\nGodspeed, brave adventurer.","html":"<p>Weâve covered a lot of Flask goodness in this series thus far. We fully understand how to structure a sensible application; we can serve up <a href=\"https://hackersandslackers.com/powerful-page-templates-in-flask-with-jinja/\"><strong>complex page templates</strong></a>, and have dived into <a href=\"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/\"><strong>interacting with databases using Flask-SQLAlchemy</strong></a>. For our next challenge, weâre going to need <em>all</em> of the knowledge we've acquired thus far and much, much more. Welcome to the Super Bowl of Flask development. This. Is. Flask-Login.</p><p><a href=\"https://flask-login.readthedocs.io/en/latest/\"><strong>Flask-Login</strong></a> is a dope library which handles all aspects of user management, including vital nuances you might not expect. Some noteworthy features include securing parts of our app behind login walls, encrypting passwords, and handling sessions. Moreover, It plays nicely with other Flask libraries weâre already familiar with: <a href=\"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/\"><strong>Flask-SQLAlchemy</strong></a> to create and fetch accounts, and <a href=\"https://hackersandslackers.com/guide-to-building-forms-in-flask/\"><strong>Flask-WTForms</strong></a> for handling intelligent sign-up &amp; log-in forms. This tutorial assumes you have <em>some </em>working knowledge of these things.</p><p>Flask-Login is shockingly quite easy to use after the initial learning curve... but therein lies the catch. Perhaps Iâm not the only one to have noticed, but most Flask-related documentation tends to be, well, God-awful. The community is riddled with helplessly outdated information; if you ever come across <code>flask.ext</code> in a tutorial, it is inherently worthless to anybody developing in 2019. To make matters worse, official Flask-Login documentation contains some artifacts which are just plain wrong. The documentation contradicts itself (Iâll show you what I mean), and offers little to no code examples to speak of. My only hope is that I might save somebody the endless headaches Iâve experienced myself.</p><h2 id=\"structuring-our-application\">Structuring Our Application</h2><p>Letâs start with installing dependencies. This should give you an idea of what youâre in for:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ pip3 install flask flask-login flask-sqlalchemy psycopg2-binary python-dotenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Sweet. Letâs take this one step at a time, starting with our project file structure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">flasklogin-tutorial\nâââ /login_tutorial\nâ   âââ __init__.py\nâ   âââ auth.py\nâ   âââ forms.py\nâ   âââ models.py\nâ   âââ routes.py\nâ   âââ /static\nâ   â   âââ /dist\nâ   â   â   âââ /css\nâ   â   â   â   âââ account.css\nâ   â   â   â   âââ dashboard.css\nâ   â   â   âââ /js\nâ   â   â       âââ main.min.js\nâ   â   âââ /src\nâ   â       âââ /js\nâ   â       â   âââ main.js\nâ   â       âââ /less\nâ   â           âââ account.less\nâ   â           âââ dashboard.less\nâ   â           âââ vars.less\nâ   âââ /templates\nâ       âââ dashboard.html\nâ       âââ layout.html\nâ       âââ login.html\nâ       âââ meta.html\nâ       âââ scripts.html\nâ       âââ signup.html\nâââ config.py\nâââ requirements.txt\nâââ setup.py\nâââ start.sh\nâââ wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><p>Of course, I wouldn't be a gentleman unless I revealed my <strong>config.py</strong> as well:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\n\n\nclass Config:\n    &quot;&quot;&quot;Set Flask configuration vars from .env file.&quot;&quot;&quot;\n\n    # General Config\n    SECRET_KEY = os.environ.get('SECRET_KEY')\n    FLASK_APP = os.environ.get('FLASK_APP')\n    FLASK_ENV = os.environ.get('FLASK_ENV')\n    FLASK_DEBUG = os.environ.get('FLASK_DEBUG')\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get('SQLALCHEMY_TRACK_MODIFICATIONS')\n</code></pre>\n<!--kg-card-end: markdown--><p>The configuration values live in a <code>.env</code> file, a practice I highly encourage. Of these configuration variables, <strong>SECRET_KEY</strong> is where we should turn our attention. SECRET_KEY is the equivalent of a password used to secure our app; it should be as long, nonsensical, and impossible-to-remember as humanly possible. Seriously: having your secret key compromised is the equivalent of feeding gremlins after midnight.</p><h2 id=\"initializing-flask-login\">Initializing Flask-Login</h2><p>With a standard \"application factory\" app, setting up Flask-Login is no different from other Flask plugins (or whatever they're called now). This makes setting up easy; all we need to do is make sure <strong>Flask-Login</strong> is initialized in <code>__init__.py</code> along with the rest of our plugins:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager\n\n\ndb = SQLAlchemy()\nlogin_manager = LoginManager()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n\n    # Application Configuration\n    app.config.from_object('config.Config')\n\n    # Initialize Plugins\n    db.init_app(app)\n    login_manager.init_app(app)\n\n    with app.app_context():\n        # Import parts of our application\n        from . import routes\n        from . import login\n        app.register_blueprint(routes.main_bp)\n        app.register_blueprint(login.login_bp)\n\n        # Initialize Global db\n        db.create_all()\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>In the above example, we're using the minimal number of plug-ins to get logins working: <strong>Flask-SQLAlchemy</strong> and <strong>Flask-Login</strong>.</p><p>To keep our sanity, we're going to separate our login routes from our main application routes and logic. This is why we register a Blueprint called <strong>auth_bp</strong>, imported from a file called <code>auth.py</code>. Our âmainâ application (AKA anything that isnât logging in) will instead live in <code>routes.py</code>, in a Blueprint called <strong>main_bp</strong>. We'll come back to these in a moment</p><h2 id=\"creating-a-user-model\">Creating a User Model</h2><p>We'll save our <strong>User</strong> model in <code>models.py</code>. There are a few things to keep in mind when creating models compatible with Flask-Login- the most important being the utilization of <code>UserMixin</code> from the <code>flask_login</code> library. When we inherit our class from <strong>UserMixin,</strong> our model is immediately extended to include all the methods necessary for Flask-Login to work. This is by far the easiest way of creating a User model. I won't bother getting into details of what these methods do, because if you simply begin your class with <code>class User(UserMixin, db.Model):</code>, you genuinely don't need to understand any of it:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from . import db\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n\nclass User(UserMixin, db.Model):\n    &quot;&quot;&quot;Model for user accounts.&quot;&quot;&quot;\n\n    __tablename__ = 'flasklogin-users'\n\n    id = db.Column(db.Integer,\n                   primary_key=True,\n                   )\n    name = db.Column(db.String,\n                     nullable=False,\n                     unique=False)\n    email = db.Column(db.String(40),\n                      unique=True,\n                      nullable=False\n                      )\n    password = db.Column(db.String(200),\n                         primary_key=False,\n                         unique=False,\n                         nullable=False\n                         )\n    website = db.Column(db.String(60),\n                        index=False,\n                        unique=False,\n                        nullable=True\n                        )\n    created_on = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n    last_login = db.Column(db.DateTime,\n                           index=False,\n                           unique=False,\n                           nullable=True\n                           )\n\n    def set_password(self, password):\n        &quot;&quot;&quot;Create hashed password.&quot;&quot;&quot;\n        self.password = generate_password_hash(password, method='sha256')\n\n    def check_password(self, password):\n        &quot;&quot;&quot;Check hashed password.&quot;&quot;&quot;\n        return check_password_hash(self.password, password)\n\n    def __repr__(self):\n        return '&lt;User {}&gt;'.format(self.username)\n</code></pre>\n<!--kg-card-end: markdown--><p>The <code>set_password</code> and <code>check_password</code> methods don't necessarily need to live inside our User model, but it's nice to keep related logic bundled together and out of our routes.</p><p>You may notice that our password field explicitly allows 200 characters: this is because our database will be storing hashed passwords. Thus, even if a user's password is 8 characters long, the string in our database will look much different.</p><h2 id=\"creating-log-in-and-sign-up-forms\">Creating Log-in and Sign-up Forms</h2><p>If you're well versed in <strong>WTForms</strong>, our form logic in <strong>forms.py</strong> probably looks as you'd expect. Of course, the constraints we set here are to handle front-end validation only:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from wtforms import Form, StringField, PasswordField, validators, SubmitField\nfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo, Length, Optional\n\n\nclass SignupForm(Form):\n    &quot;&quot;&quot;User Signup Form.&quot;&quot;&quot;\n\n    name = StringField('Name',\n                        validators=[DataRequired(message=('Enter a fake name or something.'))])\n    email = StringField('Email',\n                        validators=[Length(min=6, message=('Please enter a valid email address.')),\n                                    Email(message=('Please enter a valid email address.')),\n                                    DataRequired(message=('Please enter a valid email address.'))])\n    password = PasswordField('Password',\n                             validators=[DataRequired(message='Please enter a password.'),\n                                         Length(min=6, message=('Please select a stronger password.')),\n                                         EqualTo('confirm', message='Passwords must match')])\n    confirm = PasswordField('Confirm Your Password',)\n    website = StringField('Website',\n                          validators=[Optional()])\n    submit = SubmitField('Register')\n\n\nclass LoginForm(Form):\n    &quot;&quot;&quot;User Login Form.&quot;&quot;&quot;\n\n    email = StringField('Email', validators=[DataRequired('Please enter a valid email address.'),\n                                             Email('Please enter a valid email address.')])\n    password = PasswordField('Password', validators=[DataRequired('Uhh, your password tho?')])\n    submit = SubmitField('Log In')\n</code></pre>\n<!--kg-card-end: markdown--><p>With those out of the way, let's look at how we implement these on the Jinja side.</p><h3 id=\"signup-html\">signup.html</h3><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block pagestyles %}\n    &lt;link href=&quot;{{ url_for('static', filename='dist/css/account.css') }}&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;div class=&quot;formwrapper&quot;&gt;\n    &lt;form method=post&gt;\n      &lt;div class=&quot;logo&quot;&gt;\n        &lt;img src=&quot;{{ url_for('static', filename='dist/img/logo.png') }}&quot;&gt;\n      &lt;/div&gt;\n      {% for message in get_flashed_messages() %}\n        &lt;div class=&quot;alert alert-warning&quot;&gt;\n            &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot;&gt;&amp;times;&lt;/button&gt;\n            {{ message }}\n        &lt;/div&gt;\n      {% endfor %}\n      &lt;h1&gt;Sign Up&lt;/h1&gt;\n      &lt;div class=&quot;name&quot;&gt;\n        {{ form.name.label }}\n        {{ form.name(placeholder='John Smith') }}\n        {% if form.name.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;email&quot;&gt;\n        {{ form.email.label }}\n        {{ form.email(placeholder='youremail@example.com') }}\n        {% if form.email.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;password&quot;&gt;\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.password.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.password.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;confirm&quot;&gt;\n        {{ form.confirm.label }}\n        {{ form.confirm }}\n        {% if form.confirm.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.confirm.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;website&quot;&gt;\n        {{ form.website.label }}\n        {{ form.website(placeholder='http://example.com') }}\n      &lt;/div&gt;\n      &lt;div class=&quot;submitbutton&quot;&gt;\n        &lt;input id=&quot;submit&quot; type=&quot;submit&quot; value=&quot;Submit&quot;&gt;\n      &lt;/div&gt;\n    &lt;/form&gt;\n    &lt;div class=&quot;loginsignup&quot;&gt;\n      &lt;span&gt;Already have an account? &lt;a href=&quot;{{ url_for('auth_bp.login_page') }}&quot;&gt;Log in.&lt;/a&gt;&lt;span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"login-py\">login.py</h3><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block pagestyles %}\n  &lt;link href=&quot;{{ url_for('static', filename='dist/css/account.css') }}&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;div class=&quot;formwrapper&quot;&gt;\n    &lt;form method=post&gt;\n      &lt;div class=&quot;logo&quot;&gt;\n        &lt;img src=&quot;{{ url_for('static', filename='dist/img/logo.png') }}&quot;&gt;\n      &lt;/div&gt;\n      {% for message in get_flashed_messages() %}\n        &lt;div class=&quot;alert alert-warning&quot;&gt;\n            &lt;button type=&quot;button&quot; class=&quot;close&quot; data-dismiss=&quot;alert&quot;&gt;&amp;times;&lt;/button&gt;\n            {{ message }}\n        &lt;/div&gt;\n      {% endfor %}\n      &lt;h1&gt;Log In&lt;/h1&gt;\n      &lt;div class=&quot;email&quot;&gt;\n         {{ form.email.label }}\n         {{ form.email(placeholder='youremail@example.com') }}\n         {% if form.email.errors %}\n           &lt;ul class=&quot;errors&quot;&gt;\n             {% for error in form.email.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n           &lt;/ul&gt;\n         {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;password&quot;&gt;\n        {{ form.password.label }}\n        {{ form.password }}\n        {% if form.email.errors %}\n          &lt;ul class=&quot;errors&quot;&gt;\n            {% for error in form.password.errors %}&lt;li&gt;{{ error }}&lt;/li&gt;{% endfor %}\n          &lt;/ul&gt;\n        {% endif %}\n      &lt;/div&gt;\n      &lt;div class=&quot;submitbutton&quot;&gt;\n        &lt;input id=&quot;submit&quot; type=&quot;submit&quot; value=&quot;Submit&quot;&gt;\n      &lt;/div&gt;\n      &lt;div class=&quot;loginsignup&quot;&gt;\n        &lt;span&gt;Don't have an account? &lt;a href=&quot;{{ url_for('auth_bp.signup_page') }}&quot;&gt;Sign up.&lt;/a&gt;&lt;span&gt;\n        &lt;/div&gt;\n    &lt;/form&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n</code></pre>\n<!--kg-card-end: markdown--><p>Excellent: the stage is set to start kicking some ass.</p><h2 id=\"creating-our-login-routes\">Creating Our Login Routes</h2><p>Let us turn our attention to the heart of the logic we'll be writing in <strong>auth.py</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nfrom flask import redirect, render_template, flash, Blueprint, request, session, url_for\nfrom flask_login import login_required, logout_user, current_user, login_user\nfrom flask import current_app as app\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom .forms import LoginForm, SignupForm\nfrom .models import db, User\nfrom . import login_manager\n\n\n# Blueprint Configuration\nauth_bp = Blueprint('auth_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    &quot;&quot;&quot;User login page.&quot;&quot;&quot;\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        ...\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=&quot;Log in with your User account.&quot;)\n\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    &quot;&quot;&quot;User sign-up page.&quot;&quot;&quot;\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        ...\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=&quot;Sign up for a user account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Here we find two separate skeleton routes for <strong>Sign up</strong> and <strong>Log in</strong>. Without the authentication logic added quite yet, these routes look almost identical thus far.</p><p>Each time a user visits a page in your app, the corresponding route is sent a <code>request</code> object. This object contains contextual information about the request made by the user, such as the type of request (GET or POST), any form data which was submitted, etc. We leverage this to see whether the user is just arriving at the page for the first time (a GET request), or if they're attempting to sign in (a POST request). The fairly clever takeaway here is that our login pages verify users by making POST requests to themselves: this allows us to keep all logic related to logging in or signing up in a single route.</p><h3 id=\"signing-up\">Signing Up</h3><p>We're able to validate the submitted form by importing the <code>SignupForm</code> class and passing <code>request.form</code> as the form in question. <code>if signup_form.validate()</code> checks the information submitted by the user against all the form's validators. If any of the validators are not met, the user is redirected back to the signup form with error messages present.</p><p>Assuming that our user isn't inept, we can move on with our logic. First, we need to make sure a user with the provided email doesn't already exist:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">...\n\n@auth_bp.route('/signup', methods=['GET', 'POST'])\ndef signup_page():\n    &quot;&quot;&quot;User sign-up page.&quot;&quot;&quot;\n    signup_form = SignupForm(request.form)\n    # POST: Sign user in\n    if request.method == 'POST':\n        if signup_form.validate():\n            # Get Form Fields\n            name = request.form.get('name')\n            email = request.form.get('email')\n            password = request.form.get('password')\n            website = request.form.get('website')\n            existing_user = User.query.filter_by(email=email).first()\n            if existing_user is None:\n                user = User(name=name,\n                            email=email,\n                            password=generate_password_hash(password, method='sha256'),\n                            website=website)\n                db.session.add(user)\n                db.session.commit()\n                login_user(user)\n                return redirect(url_for('main_bp.dashboard'))\n            flash('A user already exists with that email address.')\n            return redirect(url_for('auth_bp.signup_page'))\n    # GET: Serve Sign-up page\n    return render_template('/signup.html',\n                           title='Create an Account | Flask-Login Tutorial.',\n                           form=SignupForm(),\n                           template='signup-page',\n                           body=&quot;Sign up for a user account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>If <code>existing_user is None</code>, we're all clear to actually clear to create a new user record. We create an instance of our User model via <code>user = User(...)</code>. We then add the user via standard SQLAlchemy syntax and finally use the imported method <code>login_user()</code> to log the user in.</p><p>If everything goes well, the user will finally be redirected to the main application, which is handled by <code>return redirect(url_for('main_bp.dashboard'))</code>:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/signup.gif\" class=\"kg-image\"><figcaption>A successful user log in.</figcaption></figure><!--kg-card-end: image--><p>And here's what will happen if we log out and try to sign up with the same information:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/signupfailed.gif\" class=\"kg-image\"><figcaption>Attempting to sign up with an existing email</figcaption></figure><!--kg-card-end: image--><h3 id=\"logging-in\">Logging In</h3><p>Moving on to our login route:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">@auth_bp.route('/login', methods=['GET', 'POST'])\ndef login_page():\n    &quot;&quot;&quot;User login page.&quot;&quot;&quot;\n    # Bypass Login screen if user is logged in\n    if current_user.is_authenticated:\n        return redirect(url_for('main_bp.dashboard'))\n    login_form = LoginForm(request.form)\n    # POST: Create user and redirect them to the app\n    if request.method == 'POST':\n        if login_form.validate():\n            # Get Form Fields\n            email = request.form.get('email')\n            password = request.form.get('password')\n            # Validate Login Attempt\n            user = User.query.filter_by(email=email).first()\n            if user:\n                if user.check_password(password=password):\n                    login_user(user)\n                    next = request.args.get('next')\n                    return redirect(next or url_for('main_bp.dashboard'))\n        flash('Invalid username/password combination')\n        return redirect(url_for('auth_bp.login_page'))\n    # GET: Serve Log-in page\n    return render_template('login.html',\n                           form=LoginForm(),\n                           title='Log in | Flask-Login Tutorial.',\n                           template='login-page',\n                           body=&quot;Log in with your User account.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>This should mostly look the same! our logic is identical up until the point where we check to see if the user exists. This time, a match results in success as opposed to a failure. Continuing, we then use <code>user.check_password()</code> to check the hashed password we created earlier with <code>user.generate_password_hash()</code>. Both of these methods handle the encrypting and decrypting of passwords on their own (based on that SECRET_KEY we created earlier) to ensure that nobody (not even us) has any business looking at user passwords.</p><p>As with last time, a successful login ends in <code>login_user(user)</code>. Our redirect logic is little more sophisticated this time around: instead of always sending the user back to the dashboard, we check for <code>next</code>, which is a parameter stored in the query string of the current user. If the user attempted to access our app before logging in, <code>next</code> would equal the page they had attempted to reach: this allows us wall-off our app from unauthorized users, and then drop users off at the page they attempted to reach before they logged in:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/login.gif\" class=\"kg-image\"><figcaption>A successful log in</figcaption></figure><!--kg-card-end: image--><h3 id=\"important-login-helpers\">IMPORTANT: Login Helpers</h3><p>Before your app can work like the above, we need to finish <strong>auth.py</strong> by providing a few more routes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">@auth_bp.route(&quot;/logout&quot;)\n@login_required\ndef logout_page():\n    &quot;&quot;&quot;User log-out logic.&quot;&quot;&quot;\n    logout_user()\n    return redirect(url_for('auth_bp.login_page'))\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    &quot;&quot;&quot;Check if user is logged-in on every page load.&quot;&quot;&quot;\n    if user_id is not None:\n        return User.query.get(user_id)\n    return None\n\n\n@login_manager.unauthorized_handler\ndef unauthorized():\n    &quot;&quot;&quot;Redirect unauthorized users to Login page.&quot;&quot;&quot;\n    flash('You must be logged in to view that page.')\n    return redirect(url_for('auth_bp.login_page'))\n</code></pre>\n<!--kg-card-end: markdown--><p>Our first route, <code>logout_page</code>, handles the logic of users logging out. This will simply end the user's session and redirect them to the login screen.</p><p><code>load_user</code> is critical for making our app work: before every page load, our app must verify whether or not the user is logged in (or <em>still</em> logged in after time has elapsed). <code>user_loader</code> loads users by their unique ID. If a user is returned, this signifies a logged-out user. Otherwise, when <code>None</code> is returned, the user is logged out.</p><p>Lastly, we have the <code>unauthorized</code> route, which uses the <code>unauthorized_handler</code> decorator for dealing with unauthorized users. Any time a user attempts to hit our app and is unauthorized, this route will fire.</p><h2 id=\"the-last-piece-routes-py\">The Last Piece: routes.py</h2><p>The last thing we'll cover is how to protect parts of our app from unauthorized users. Here's what we have in <strong>routes.py</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask_login import current_user\nfrom flask import current_app as app\nfrom .models import User\nfrom flask_login import login_required\n\n\n# Blueprint Configuration\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\n\n\n@main_bp.route('/', methods=['GET'])\n@login_required\ndef dashboard():\n    &quot;&quot;&quot;Serve logged in Dashboard.&quot;&quot;&quot;\n    return render_template('dashboard.html',\n                           title='Flask-Login Tutorial.',\n                           template='dashboard-template',\n                           current_user=current_user,\n                           body=&quot;You are now logged in!&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>The magic here is all contained within the <code>@login_required</code> decorator. When this decorator is present on a route, the following things happen:</p><ul><li>The <code>@login_manager.user_loader</code> route we created determines whether or not the user is authorized to view the page (logged in). If the user is logged in, they'll be permitted to view the page.</li><li>If the user is not logged in, the user will be redirected as per the logic in the route decorated with <code>@login_manager.unauthorized_handler</code>.</li><li>The name of the route the user attempted to access will be stored in the URL as <code>?url=[name-of-route]</code>. This what allows <code>next</code> to work.</li></ul><h3 id=\"there-you-have-it\">There You Have It</h3><p>If you've made it this far, I commend you for your courage. To reward your accomplishments, I've published the <a href=\"https://github.com/toddbirchard/flasklogin-tutorial\">source code for this tutorial on Github</a> for your reference. Godspeed, brave adventurer.</p>","url":"https://hackersandslackers.com/authenticating-users-with-flask-login/","uuid":"23a82e0a-31e7-49ea-8cc1-fecdd466bcfd","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c779ffbc380a221de39c7cf"}},{"node":{"id":"Ghost__Post__5c95e08ef654036aa06c6a02","title":"Building an ETL Pipeline: From JIRA to SQL","slug":"building-an-etl-pipeline-from-jira-to-postgresql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-3.jpg","excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","custom_excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","created_at_pretty":"23 March, 2019","published_at_pretty":"28 March, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-03-23T03:30:22.000-04:00","published_at":"2019-03-28T04:15:00.000-04:00","updated_at":"2019-04-08T23:34:47.000-04:00","meta_title":"Building an ETL Pipeline: From JIRA to SQL | Hackers and Slackers","meta_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-2.jpg","og_title":"Building an ETL Pipeline: From JIRA to SQL","twitter_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-1.jpg","twitter_title":"Building an ETL Pipeline: From JIRA to SQL","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Atlassian","slug":"atlassian","description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","feature_image":null,"meta_description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","meta_title":"Atlassian Development for JIRA and Confluence. | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we haven't done just yet on this site is walking through the humble\nprocess of creating data pipelines: the art of taking a bunch of data, changing\nsaid data, and putting it somewhere else. It's kind of a weird thing to be into,\nhence why the MoMA has been rejecting my submissions of Github repositories.\nDon't worry; I'll keep at it.\n\nSomething you don't see every day are people sharing their pipelines, which is\nunderstandable. Presumably, the other people who do this kind of stuff do it for\nwork; nobody is happily building stupid pipelines in their free time begging to\nbe open sourced. Except me.\n\nWe've recently revamped our projects [https://hackersandslackers.com/projects/] \npage to include a public-facing Kanban board using GraphQL. To achieve this, we\nneed to extract JIRA data from the JIRA Cloud REST API and place it securely in\nour database.\n\nStructuring our Pipeline\nAn ETL pipeline which is considered 'well-structured' is in the eyes of the\nbeholder. There are a million different ways to pull and mess with data, so\nthere isn't a \"template\" for building these things out. In my case, the\nstructure of my script just so happened to end up as three modules: one for \nextracting, one for loading, and one for transforming. This was unplanned, but\nit's a good sign when our app matches our mindset. Here's the breakdown:\n\njira-database-etl\nâââ __main__.py\nâââ jira_etl\nâ   âââ __init__.py\nâ   âââ fetch.py\nâ   âââ data.py\nâ   âââ db.py\nâââ LICENSE\nâââ MANIFEST.in\nâââ Pipfile\nâââ Pipfile.lock\nâââ README.md\nâââ requirements.txt\nâââ setup.cfg\nâââ setup.py\n\n\nmain.py  is our application entry point. The logic of our pipeline is stored in\nthree parts under the jira_etl  directory:\n\n * fetch.py  grabs the data from the source (JIRA Cloud's REST API) and handles\n   fetching all JIRA issues.\n * data.py  transforms the data we've fetched and constructs a neat DataFrame\n   containing only the information we're after.\n * db.py  finally loads the data into a SQL database.\n\nDon't look into it too much, but here's our entry point:\n\nfrom jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    \"\"\"Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    \"\"\"\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n\n\nWithout further adieu, let's dig into the logic!\n\nExtracting Our Data\nBefore doing anything, it's essential we become familiar with the data we're\nabout to pull. Firstly, JIRA's REST API returns paginated results which max out\nat 100 results per page. This means we'll have to loop through the pages\nrecursively until all results are loaded.\n\nNext, let's look at an example of a single  JIRA issue JSON object returned by\nthe API:\n\n{\n    \"expand\": \"names,schema\",\n    \"startAt\": 0,\n    \"maxResults\": 1,\n    \"total\": 888,\n    \"issues\": [\n        {\n            \"expand\": \"operations,versionedRepresentations,editmeta,changelog,renderedFields\",\n            \"id\": \"11718\",\n            \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/11718\",\n            \"key\": \"HACK-756\",\n            \"fields\": {\n                \"issuetype\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014\",\n                    \"id\": \"10014\",\n                    \"description\": \"Placeholder item for \\\"holy shit this is going to be a lot of work\\\"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&avatarId=10311&avatarType=issuetype\",\n                    \"name\": \"Major Functionality\",\n                    \"subtask\": false,\n                    \"avatarId\": 10311\n                },\n                \"customfield_10070\": null,\n                \"customfield_10071\": null,\n                \"customfield_10073\": null,\n                \"customfield_10074\": null,\n                \"customfield_10075\": null,\n                \"project\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/project/10015\",\n                    \"id\": \"10015\",\n                    \"key\": \"HACK\",\n                    \"name\": \"Hackers and Slackers\",\n                    \"projectTypeKey\": \"software\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&avatarId=10535\",\n                        \"24x24\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&pid=10015&avatarId=10535\",\n                        \"16x16\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&pid=10015&avatarId=10535\",\n                        \"32x32\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&pid=10015&avatarId=10535\"\n                    }\n                },\n                \"fixVersions\": [],\n                \"resolution\": null,\n                \"resolutiondate\": null,\n                \"workratio\": -1,\n                \"lastViewed\": \"2019-03-24T02:01:31.355-0400\",\n                \"watches\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers\",\n                    \"watchCount\": 1,\n                    \"isWatching\": true\n                },\n                \"created\": \"2019-02-03T00:47:36.677-0500\",\n                \"customfield_10062\": null,\n                \"customfield_10063\": null,\n                \"customfield_10064\": null,\n                \"customfield_10065\": null,\n                \"customfield_10066\": null,\n                \"priority\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/priority/2\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg\",\n                    \"name\": \"High\",\n                    \"id\": \"2\"\n                },\n                \"customfield_10067\": null,\n                \"customfield_10068\": null,\n                \"customfield_10069\": [],\n                \"labels\": [],\n                \"versions\": [],\n                \"issuelinks\": [],\n                \"assignee\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"updated\": \"2019-03-24T02:01:30.724-0400\",\n                \"status\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/status/10004\",\n                    \"description\": \"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/\",\n                    \"name\": \"To Do\",\n                    \"id\": \"10004\",\n                    \"statusCategory\": {\n                        \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2\",\n                        \"id\": 2,\n                        \"key\": \"new\",\n                        \"colorName\": \"blue-gray\",\n                        \"name\": \"To Do\"\n                    }\n                },\n                \"components\": [],\n                \"description\": {\n                    \"version\": 1,\n                    \"type\": \"doc\",\n                    \"content\": [\n                        {\n                            \"type\": \"paragraph\",\n                            \"content\": [\n                                {\n                                    \"type\": \"text\",\n                                    \"text\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\",\n                                    \"marks\": [\n                                        {\n                                            \"type\": \"link\",\n                                            \"attrs\": {\n                                                \"href\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\"\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                \"customfield_10010\": null,\n                \"customfield_10011\": \"0|i0064j:i\",\n                \"customfield_10012\": null,\n                \"customfield_10013\": null,\n                \"security\": null,\n                \"customfield_10008\": \"HACK-143\",\n                \"customfield_10009\": {\n                    \"hasEpicLinkFieldDependency\": false,\n                    \"showField\": false,\n                    \"nonEditableReason\": {\n                        \"reason\": \"PLUGIN_LICENSE_ERROR\",\n                        \"message\": \"Portfolio for Jira must be licensed for the Parent Link to be available.\"\n                    }\n                },\n                \"summary\": \"Automate newsletter\",\n                \"creator\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"subtasks\": [],\n                \"reporter\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"customfield_10000\": \"{}\",\n                \"customfield_10001\": null,\n                \"customfield_10004\": null,\n                \"environment\": null,\n                \"duedate\": null,\n                \"votes\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes\",\n                    \"votes\": 0,\n                    \"hasVoted\": false\n                }\n            }\n        }\n    ]\n}\n\n\nWhoa, mama! That's a ton of BS for a single issue. You can see now why we'd want\nto transform this data before importing ten million fields into any database.\nMake note of these important fields:\n\n * startAt: An integer which tells us which issue number the paginated results\n   start at.\n * maxResults: Denotes the maximum number of results page - maxes out at 100\n   issues.\n * total: The total number of issues across all pages.\n * issues: A list of objects which contain the information for exactly one JIRA\n   issue per object\n\nGreat. So the purpose of fetch.py will essentially consist of creating a list of\nall 888  issues (in my case), and passing that off for transformation. Check it\nthe source I came up with:\n\nimport os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    \"\"\"Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     \"\"\"\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        \"Accept\": \"application/json\"\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        \"\"\"Gets the total number of results.\"\"\"\n        params = {\n            \"jql\": cls.jql,\n            \"maxResults\": 0,\n            \"startAt\": 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        \"\"\"Recursively retrieve all pages of JIRA issues.\"\"\"\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            \"\"\"Fetch one page of results, and determine if another page exists.\"\"\"\n            params = {\n                \"jql\": cls.jql,\n                \"maxResults\": cls.results_per_page,\n                \"startAt\": len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n\n\nYep, I'm using classes. This class has two methods:\n\n * get_total_number_of_issues: All this does is essentially pull the number of\n   issues (888) from the REST API. We'll use this number in our next function to\n   check if additional pages exist.\n * fetch_all_results: This is where things start getting fun. fetch_all_results \n   is a @classmethod  which contains a function within itself. fetch_all_results \n    gets the total number of JIRA issues and then calls upon child function \n   fetch_single_page to pull JIRA issue JSON objects and dump them into a list\n   called issue_arr  until all issues are accounted for.\n\nBecause we have 888 issues and can retrieve 100 issues  at a time, our function\nfetch_single_page  should run 9 times. And it does!\n\nTransforming Our Data\nSo now we have a list of 888 messy JIRA issues. The scope of data.py  should be\nto pull out only the data we want, and make sure that data is clean:\n\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    \"\"\"Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    \"\"\"\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        \"\"\"Make DataFrame out of data received from JIRA API.\"\"\"\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        \"\"\"Convert dict to JSON to string.\"\"\"\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        \"\"\"Create a JSON body for each ticket.\"\"\"\n        updated_date = datetime.strptime(issue['fields']['updated'], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n\n\nAgain, let's see the methods at work:\n\n * construct_dataframe: The main function we invoke to build our DataFrame\n   (mostly just calls other methods). Once all transformations are completed,\n   creates a DataFrame called jira_df  by using the Pandas json_normalize() \n   method.\n * make_issue_body: Creates a new dictionary per singular JIRA issue. Extracts \n   only  the fields we want to be imported into our database. Converts each\n   field into either a string or an int as a lazy way of avoiding null values\n   (for example, if issue['fields']['priority']['name']  contained a null value,\n   the script would error out. Wrapping this in str() is a dirty way of\n   converting null  to an empty string).\n * dict_to_json_string  Takes each issue dictionary and converts it to a JSON\n   object, which is then turned into a string (this is done for Pandas).\n\nLoading Our Data\nAnd now for the final step! Thanks to the joyful marriage of Pandas and\nSQLAlchemy, turning DataFrames into SQL tables is super simple. We never make\nthings simple, though.\n\nimport os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    \"\"\"Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    \"\"\"\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=\"hackers$prod\")\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        \"\"\"Clear table of data.\"\"\"\n        sql = text('TRUNCATE TABLE \"hackers$prod\".\"JiraIssue\"')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        \"\"\"Merge epic metadata from existing SQL table.\"\"\"\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        \"\"\"Upload JIRA DataFrame to PostgreSQL database.\"\"\"\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={\"assignee\": String(30),\n                                     \"assignee_url\": Text,\n                                     \"epic_link\": String(50),\n                                     \"issuetype_name\": String(50),\n                                     \"issuetype_icon\": Text,\n                                     \"key\": String(10),\n                                     \"priority_name\": String(30),\n                                     \"priority_rank\": Integer,\n                                     \"priority_url\": Text,\n                                     \"project\": String(50),\n                                     \"status\": String(30),\n                                     \"summary\": Text,\n                                     \"updated\": Integer,\n                                     \"updatedAt\": TIMESTAMP,\n                                     \"createdAt\": TIMESTAMP,\n                                     \"epic_color\": String(20),\n                                     \"epic_name\": String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n\n\n * merge_epic_metadata: Due to the nature of the JIRA REST API, some metadata is\n   missing per issue. If you're interested, the data missing revolves around \n   Epics: JIRA's REST API does not include the Epic Name  or Epic Color  fields\n   of linked epics.\n * upload_dataframe: Uses Panda's to_sql()  method to upload our DataFrame into\n   a SQL table (our target happens to be PostgreSQL, so we pass schema  here).\n   To make things explicit, we set the data type of every column on upload.\n\nWell, let's see how we made out!\n\nA look at our resulting database table.Whoaaa nelly, we did it! With our data\nclean, we can now build something useful! Here's what I built:\n\nFruits of our labor!There we have it: a pipeline that takes a bunch of messy\ndata, cleans it, and puts it somewhere else for proper use.\n\nIf you're interested in how we created the frontend for our Kanban board, check\nout our series on building features with GraphQL\n[https://hackersandslackers.com/series/graphql-hype/]. For the source code,\ncheck out the Github repository\n[https://github.com/toddbirchard/jira-database-etl].","html":"<p>Something we haven't done just yet on this site is walking through the humble process of creating data pipelines: the art of taking a bunch of data, changing said data, and putting it somewhere else. It's kind of a weird thing to be into, hence why the MoMA has been rejecting my submissions of Github repositories. Don't worry; I'll keep at it.</p><p>Something you don't see every day are people sharing their pipelines, which is understandable. Presumably, the other people who do this kind of stuff do it for work; nobody is happily building stupid pipelines in their free time begging to be open sourced. Except me.</p><p>We've recently revamped our <strong><a href=\"https://hackersandslackers.com/projects/\">projects</a></strong> page to include a public-facing Kanban board using GraphQL. To achieve this, we need to extract JIRA data from the JIRA Cloud REST API and place it securely in our database.</p><h2 id=\"structuring-our-pipeline\">Structuring our Pipeline</h2><p>An ETL pipeline which is considered 'well-structured' is in the eyes of the beholder. There are a million different ways to pull and mess with data, so there isn't a \"template\" for building these things out. In my case, the structure of my script just so happened to end up as three modules: one for <em>extracting</em>, one for <em>loading</em>, and one for <em>transforming</em>. This was unplanned, but it's a good sign when our app matches our mindset. Here's the breakdown:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">jira-database-etl\nâââ __main__.py\nâââ jira_etl\nâ   âââ __init__.py\nâ   âââ fetch.py\nâ   âââ data.py\nâ   âââ db.py\nâââ LICENSE\nâââ MANIFEST.in\nâââ Pipfile\nâââ Pipfile.lock\nâââ README.md\nâââ requirements.txt\nâââ setup.cfg\nâââ setup.py\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>main.py</strong> is our application entry point. The logic of our pipeline is stored in three parts under the <strong>jira_etl</strong> directory:</p><ul><li><strong>fetch.py</strong> grabs the data from the source (JIRA Cloud's REST API) and handles fetching all JIRA issues.</li><li><strong>data.py</strong> transforms the data we've fetched and constructs a neat DataFrame containing only the information we're after.</li><li><strong>db.py</strong> finally loads the data into a SQL database.</li></ul><p>Don't look into it too much, but here's our entry point:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    &quot;&quot;&quot;Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    &quot;&quot;&quot;\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n</code></pre>\n<!--kg-card-end: markdown--><p>Without further adieu, let's dig into the logic!</p><h2 id=\"extracting-our-data\">Extracting Our Data</h2><p>Before doing anything, it's essential we become familiar with the data we're about to pull. Firstly, JIRA's REST API returns paginated results which max out at 100 results per page. This means we'll have to loop through the pages recursively until all results are loaded.</p><p>Next, let's look at an example of a <strong><em>single</em></strong> JIRA issue JSON object returned by the API:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n    &quot;expand&quot;: &quot;names,schema&quot;,\n    &quot;startAt&quot;: 0,\n    &quot;maxResults&quot;: 1,\n    &quot;total&quot;: 888,\n    &quot;issues&quot;: [\n        {\n            &quot;expand&quot;: &quot;operations,versionedRepresentations,editmeta,changelog,renderedFields&quot;,\n            &quot;id&quot;: &quot;11718&quot;,\n            &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/11718&quot;,\n            &quot;key&quot;: &quot;HACK-756&quot;,\n            &quot;fields&quot;: {\n                &quot;issuetype&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014&quot;,\n                    &quot;id&quot;: &quot;10014&quot;,\n                    &quot;description&quot;: &quot;Placeholder item for \\&quot;holy shit this is going to be a lot of work\\&quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&amp;avatarId=10311&amp;avatarType=issuetype&quot;,\n                    &quot;name&quot;: &quot;Major Functionality&quot;,\n                    &quot;subtask&quot;: false,\n                    &quot;avatarId&quot;: 10311\n                },\n                &quot;customfield_10070&quot;: null,\n                &quot;customfield_10071&quot;: null,\n                &quot;customfield_10073&quot;: null,\n                &quot;customfield_10074&quot;: null,\n                &quot;customfield_10075&quot;: null,\n                &quot;project&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/project/10015&quot;,\n                    &quot;id&quot;: &quot;10015&quot;,\n                    &quot;key&quot;: &quot;HACK&quot;,\n                    &quot;name&quot;: &quot;Hackers and Slackers&quot;,\n                    &quot;projectTypeKey&quot;: &quot;software&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&amp;avatarId=10535&quot;,\n                        &quot;24x24&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;16x16&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;32x32&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&amp;pid=10015&amp;avatarId=10535&quot;\n                    }\n                },\n                &quot;fixVersions&quot;: [],\n                &quot;resolution&quot;: null,\n                &quot;resolutiondate&quot;: null,\n                &quot;workratio&quot;: -1,\n                &quot;lastViewed&quot;: &quot;2019-03-24T02:01:31.355-0400&quot;,\n                &quot;watches&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers&quot;,\n                    &quot;watchCount&quot;: 1,\n                    &quot;isWatching&quot;: true\n                },\n                &quot;created&quot;: &quot;2019-02-03T00:47:36.677-0500&quot;,\n                &quot;customfield_10062&quot;: null,\n                &quot;customfield_10063&quot;: null,\n                &quot;customfield_10064&quot;: null,\n                &quot;customfield_10065&quot;: null,\n                &quot;customfield_10066&quot;: null,\n                &quot;priority&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/priority/2&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg&quot;,\n                    &quot;name&quot;: &quot;High&quot;,\n                    &quot;id&quot;: &quot;2&quot;\n                },\n                &quot;customfield_10067&quot;: null,\n                &quot;customfield_10068&quot;: null,\n                &quot;customfield_10069&quot;: [],\n                &quot;labels&quot;: [],\n                &quot;versions&quot;: [],\n                &quot;issuelinks&quot;: [],\n                &quot;assignee&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;updated&quot;: &quot;2019-03-24T02:01:30.724-0400&quot;,\n                &quot;status&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/status/10004&quot;,\n                    &quot;description&quot;: &quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/&quot;,\n                    &quot;name&quot;: &quot;To Do&quot;,\n                    &quot;id&quot;: &quot;10004&quot;,\n                    &quot;statusCategory&quot;: {\n                        &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2&quot;,\n                        &quot;id&quot;: 2,\n                        &quot;key&quot;: &quot;new&quot;,\n                        &quot;colorName&quot;: &quot;blue-gray&quot;,\n                        &quot;name&quot;: &quot;To Do&quot;\n                    }\n                },\n                &quot;components&quot;: [],\n                &quot;description&quot;: {\n                    &quot;version&quot;: 1,\n                    &quot;type&quot;: &quot;doc&quot;,\n                    &quot;content&quot;: [\n                        {\n                            &quot;type&quot;: &quot;paragraph&quot;,\n                            &quot;content&quot;: [\n                                {\n                                    &quot;type&quot;: &quot;text&quot;,\n                                    &quot;text&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;,\n                                    &quot;marks&quot;: [\n                                        {\n                                            &quot;type&quot;: &quot;link&quot;,\n                                            &quot;attrs&quot;: {\n                                                &quot;href&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                &quot;customfield_10010&quot;: null,\n                &quot;customfield_10011&quot;: &quot;0|i0064j:i&quot;,\n                &quot;customfield_10012&quot;: null,\n                &quot;customfield_10013&quot;: null,\n                &quot;security&quot;: null,\n                &quot;customfield_10008&quot;: &quot;HACK-143&quot;,\n                &quot;customfield_10009&quot;: {\n                    &quot;hasEpicLinkFieldDependency&quot;: false,\n                    &quot;showField&quot;: false,\n                    &quot;nonEditableReason&quot;: {\n                        &quot;reason&quot;: &quot;PLUGIN_LICENSE_ERROR&quot;,\n                        &quot;message&quot;: &quot;Portfolio for Jira must be licensed for the Parent Link to be available.&quot;\n                    }\n                },\n                &quot;summary&quot;: &quot;Automate newsletter&quot;,\n                &quot;creator&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;subtasks&quot;: [],\n                &quot;reporter&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;customfield_10000&quot;: &quot;{}&quot;,\n                &quot;customfield_10001&quot;: null,\n                &quot;customfield_10004&quot;: null,\n                &quot;environment&quot;: null,\n                &quot;duedate&quot;: null,\n                &quot;votes&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes&quot;,\n                    &quot;votes&quot;: 0,\n                    &quot;hasVoted&quot;: false\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Whoa, mama! That's a ton of BS for a single issue. You can see now why we'd want to transform this data before importing ten million fields into any database. Make note of these important fields:</p><ul><li><code>startAt</code>: An integer which tells us which issue number the paginated results start at.</li><li><code>maxResults</code>: Denotes the maximum number of results page - maxes out at 100 issues.</li><li><code>total</code>: The total number of issues across all pages.</li><li><code>issues</code>: A list of objects which contain the information for exactly one JIRA issue per object</li></ul><p>Great. So the purpose of <strong>fetch.py </strong>will essentially consist of creating a list of all <strong>888</strong> issues (in my case), and passing that off for transformation. Check it the source I came up with:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    &quot;&quot;&quot;Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     &quot;&quot;&quot;\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        &quot;Accept&quot;: &quot;application/json&quot;\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        &quot;&quot;&quot;Gets the total number of results.&quot;&quot;&quot;\n        params = {\n            &quot;jql&quot;: cls.jql,\n            &quot;maxResults&quot;: 0,\n            &quot;startAt&quot;: 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        &quot;&quot;&quot;Recursively retrieve all pages of JIRA issues.&quot;&quot;&quot;\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            &quot;&quot;&quot;Fetch one page of results, and determine if another page exists.&quot;&quot;&quot;\n            params = {\n                &quot;jql&quot;: cls.jql,\n                &quot;maxResults&quot;: cls.results_per_page,\n                &quot;startAt&quot;: len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n</code></pre>\n<!--kg-card-end: markdown--><p>Yep, I'm using classes. This class has two methods:</p><ul><li><code>get_total_number_of_issues</code>: All this does is essentially pull the number of issues (888) from the REST API. We'll use this number in our next function to check if additional pages exist.</li><li><code>fetch_all_results</code>: This is where things start getting fun. <strong>fetch_all_results</strong> is a <em>@classmethod</em> which contains a function within itself. <strong>fetch_all_results</strong> gets the total number of JIRA issues and then calls upon child function <strong>fetch_single_page </strong>to pull JIRA issue JSON objects and dump them into a list called <code>issue_arr</code> until all issues are accounted for.</li></ul><p>Because we have <em>888 issues </em>and can retrieve <em>100 issues</em> at a time, our function Â <code>fetch_single_page</code> should run <em>9 times</em>. And it does!</p><h2 id=\"transforming-our-data\">Transforming Our Data</h2><p>So now we have a list of 888 messy JIRA issues. The scope of <strong>data.py</strong> should be to pull out only the data we want, and make sure that data is clean:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    &quot;&quot;&quot;Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    &quot;&quot;&quot;\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        &quot;&quot;&quot;Make DataFrame out of data received from JIRA API.&quot;&quot;&quot;\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        &quot;&quot;&quot;Convert dict to JSON to string.&quot;&quot;&quot;\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        &quot;&quot;&quot;Create a JSON body for each ticket.&quot;&quot;&quot;\n        updated_date = datetime.strptime(issue['fields']['updated'], &quot;%Y-%m-%dT%H:%M:%S.%f%z&quot;)\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n</code></pre>\n<!--kg-card-end: markdown--><p>Again, let's see the methods at work:</p><ul><li><code>construct_dataframe</code>: The main function we invoke to build our DataFrame (mostly just calls other methods). Once all transformations are completed, creates a DataFrame called <strong>jira_df</strong> by using the Pandas <em>json_normalize()</em> method.</li><li><code>make_issue_body</code>: Creates a new dictionary per singular JIRA issue. Extracts <em>only</em> the fields we want to be imported into our database. Converts each field into either a string or an int as a lazy way of avoiding null values (for example, if <code>issue['fields']['priority']['name']</code> contained a null value, the script would error out. Wrapping this in <strong>str() </strong>is a dirty way of converting <em>null</em> to an empty string).</li><li><code>dict_to_json_string</code> Takes each issue dictionary and converts it to a JSON object, which is then turned into a string (this is done for Pandas).</li></ul><h2 id=\"loading-our-data\">Loading Our Data</h2><p>And now for the final step! Thanks to the joyful marriage of Pandas and SQLAlchemy, turning DataFrames into SQL tables is super simple. We never make things simple, though.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    &quot;&quot;&quot;Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    &quot;&quot;&quot;\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=&quot;hackers$prod&quot;)\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        &quot;&quot;&quot;Clear table of data.&quot;&quot;&quot;\n        sql = text('TRUNCATE TABLE &quot;hackers$prod&quot;.&quot;JiraIssue&quot;')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        &quot;&quot;&quot;Merge epic metadata from existing SQL table.&quot;&quot;&quot;\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        &quot;&quot;&quot;Upload JIRA DataFrame to PostgreSQL database.&quot;&quot;&quot;\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={&quot;assignee&quot;: String(30),\n                                     &quot;assignee_url&quot;: Text,\n                                     &quot;epic_link&quot;: String(50),\n                                     &quot;issuetype_name&quot;: String(50),\n                                     &quot;issuetype_icon&quot;: Text,\n                                     &quot;key&quot;: String(10),\n                                     &quot;priority_name&quot;: String(30),\n                                     &quot;priority_rank&quot;: Integer,\n                                     &quot;priority_url&quot;: Text,\n                                     &quot;project&quot;: String(50),\n                                     &quot;status&quot;: String(30),\n                                     &quot;summary&quot;: Text,\n                                     &quot;updated&quot;: Integer,\n                                     &quot;updatedAt&quot;: TIMESTAMP,\n                                     &quot;createdAt&quot;: TIMESTAMP,\n                                     &quot;epic_color&quot;: String(20),\n                                     &quot;epic_name&quot;: String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><code>merge_epic_metadata</code>: Due to the nature of the JIRA REST API, some metadata is missing per issue. If you're interested, the data missing revolves around <strong>Epics</strong>: JIRA's REST API does not include the <em>Epic Name</em> or <em>Epic Color</em> fields of linked epics.</li><li><code>upload_dataframe</code>: Uses Panda's <strong>to_sql()</strong> method to upload our DataFrame into a SQL table (our target happens to be PostgreSQL, so we pass <em>schema</em> here). To make things explicit, we set the data type of every column on upload.</li></ul><p>Well, let's see how we made out!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.03.51-PM.png\" class=\"kg-image\"><figcaption>A look at our resulting database table.</figcaption></figure><!--kg-card-end: image--><p>Whoaaa nelly, we did it! With our data clean, we can now build something useful! Here's what I built:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.29.20-PM.png\" class=\"kg-image\"><figcaption>Fruits of our labor!</figcaption></figure><!--kg-card-end: image--><p>There we have it: a pipeline that takes a bunch of messy data, cleans it, and puts it somewhere else for proper use.</p><p>If you're interested in how we created the frontend for our Kanban board, check out our series on <a href=\"https://hackersandslackers.com/series/graphql-hype/\">building features with GraphQL</a>. For the source code, check out the <a href=\"https://github.com/toddbirchard/jira-database-etl\">Github repository</a>.</p>","url":"https://hackersandslackers.com/building-an-etl-pipeline-from-jira-to-postgresql/","uuid":"23647abe-9b47-4f58-8206-cff1fb2ae891","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c95e08ef654036aa06c6a02"}},{"node":{"id":"Ghost__Post__5c17ddd4418434084a873d2a","title":"Drawing Mapbox Route Objects via the Directions API","slug":"mapbox-draw-route-objects","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","excerpt":"Using the Mapbox Directions API to visually draw routes.","custom_excerpt":"Using the Mapbox Directions API to visually draw routes.","created_at_pretty":"17 December, 2018","published_at_pretty":"28 February, 2019","updated_at_pretty":"03 March, 2019","created_at":"2018-12-17T12:33:08.000-05:00","published_at":"2019-02-28T09:15:52.000-05:00","updated_at":"2019-03-03T16:21:52.000-05:00","meta_title":"Draw Route Objects with Mapbox Directions API | Hackers and Slackers","meta_description":"Using the Mapbox Directions API to visually draw routes.","og_description":"Using the Mapbox Directions API to visually draw routes.","og_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","og_title":"Drawing Route Objects with Mapbox Directions API","twitter_description":"Using the Mapbox Directions API to visually draw routes.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/mapboxroutes.jpg","twitter_title":"Drawing Route Objects with Mapbox Directions API","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"If you've been here before, you probably already know our affinity for Mapbox \nand the visualization tools it provides data scientists and analysts. In the\npast, we've covered encoding location data from raw addresses\n[https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/], as well\nas an exploration of Mapbox Studio\n[https://hackersandslackers.com/map-data-visualization-with-mapbox/]  for those\ngetting acquainted with the tool. Today we're going a step further: drawing\ndirections on a map.\n\nIt sounds simple enough: we already know how to geocode addresses, so all we\nneed to do is literally go from point A to point B. That said, things always\ntend to get tricky, and if you've never worked with GeoJSON\n[http://geojson.org/]  before, you're in for a treat.\n\nLoad Up Some Data\nI'm going to assume you have a DataFrame ready containing these columns:\n\n * origin_longitude\n * origin_latitude\n * destination_longitude\n * destination_latitude\n * Name/description of this route \n\nIf you want to play along, there are plenty of free datasets out there to play\nwith - I sourced some information from BigQuery while I was testing things out.\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\nSo far so good- all we've done is load our data, and save our Mapbox token from\nan environment variable.\n\nMapbox Directions Endpoint\nNext, we're going to use Mapbox's Directions API\n[https://docs.mapbox.com/api/navigation/#directions]  to return a route for us.\nThe anatomy of a GET call to receive directions looks like this:\n\nhttps://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n\n\n * method_of_transportation refers to one of the three methods that Mapbox\n   offers for creating routes: driving-traffic, driving, walking, and cycling.\n   Note that there is currently no way to draw route objects which follow public\n   transit: this is perhaps Mapbox's biggest downfall at the moment.\n   Nevertheless, if this is something you need, data can be imported from Google\n   maps to be used with Mapbox.\n * access_token  can be either your public token (visible upon login at\n   mapbox.com) or a generated secret token.\n * geometries  accepts the method by which to draw the object. This can be \n   GeoJSON,  polyline, or polyline6. Let's stick with GeoJSON.\n\nConstructing API Requests\nLet's construct a request per row in our DataFrame. By using Pandas' apply, we\nfire a function per row to do just that:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n\n\nHere's where things get a little tricky. You see, GeoJSON abides by a strict\nformat. It looks something like this:\n\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"coordinates\": [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    \"type\": \"LineString\"\n  },\n  \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n  }],\n  \"weight_name\": \"duration\",\n  \"weight\": 3873.3,\n  \"duration\": 3873.3,\n  \"distance\": 9660.2,\n  \"properties\": {\n    \"name\": \"Empire State\"\n  }\n}\n\n\nFor the sake of being difficult, the Mapbox Directions API doesn't return\nresponses in exactly this format. Instead, their response looks like this:\n\n{\n  \"routes\": [{\n    \"geometry\": {\n      \"coordinates\": [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      \"type\": \"LineString\"\n    },\n    \"legs\": [{\n      \"summary\": \"\",\n      \"weight\": 3873.3,\n      \"duration\": 3873.3,\n      \"steps\": [],\n      \"distance\": 9660.2\n    }],\n    \"weight_name\": \"duration\",\n    \"weight\": 3873.3,\n    \"duration\": 3873.3,\n    \"distance\": 9660.2\n  }],\n  \"waypoints\": [{\n      \"distance\": 34.00158252003884,\n      \"name\": \"West 33rd Street\",\n      \"location\": [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      \"distance\": 6.627227256764976,\n      \"name\": \"\",\n      \"location\": [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  \"code\": \"Ok\",\n  \"uuid\": \"cjsomodyl025642o6f1jsddx6\"\n}\n\n\nThe format isn't too  far off, but it's different enough to not work. \n\nFormatting GeoJSON Correctly\nWe need to write a function to take the response Mapbox has given us and\ntransform it into a usable GeoJSON format:\n\nimport os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    \"\"\"Properly formats GeoJson for Mapbox visualization.\"\"\"\n    routes_dict = {\n        \"type\": \"Feature\",\n        \"geometry\": {\n            \"type\": \"LineString\"\n        },\n        \"weight_name\": \"duration\",\n        \"weight\": 718.9,\n        \"duration\": 0,\n        \"distance\": 0,\n        \"properties\": {\n            \"name\": \"\"\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    \"\"\"Get route JSON.\"\"\"\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n\n\nIt's not pretty, but it's reliable: we explicitly create the JSON structure we\nneed with routes_dict, and modify it with the API responses coming back from\nMapbox. Of course, we're still doing this one at a time, for every row in our\nDataFrame.\n\nYou'll notice I save each JSON file locally for now. In the future, we'll write\na script to automate the process of uploading our GeoJSON objects and adding\nthem to the proper Tilesets, but right now I just want to see that our work paid\noff!\n\nBy using Mapbox studio, we can see the result of our first route:\n\nA \"Driving\" Route from the Empire State Building to Battery Park.Aha! Would you\nlook at that- Mapbox knew to take the FDR drive. That's some promising stuff.\n\nDrawing Routes En Masse\nNaturally, this is only the tip of the iceberg: of the DataFrame of information\nwe loaded up, we've so far only viewed a single result. If anything in data is\nworth doing, it must be done thousands of times systematically without fail.\nLuckily, Mapbox provides us with the tools to do this: from lending us an S3\nbucket, to modifying datasets via the API, there's nothing to fear.\n\nTune in next time when do more... stuff!","html":"<p>If you've been here before, you probably already know our affinity for <strong>Mapbox</strong> and the visualization tools it provides data scientists and analysts. In the past, we've covered <a href=\"https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/\">encoding location data from raw addresses</a>, as well as an <a href=\"https://hackersandslackers.com/map-data-visualization-with-mapbox/\">exploration of Mapbox Studio</a> for those getting acquainted with the tool. Today we're going a step further: drawing directions on a map.</p><p>It sounds simple enough: we already know how to geocode addresses, so all we need to do is literally go from point A to point B. That said, things always tend to get tricky, and if you've never worked with <a href=\"http://geojson.org/\">GeoJSON</a> before, you're in for a treat.</p><h2 id=\"load-up-some-data\">Load Up Some Data</h2><p>I'm going to assume you have a DataFrame ready containing these columns:</p><ul><li>origin_longitude</li><li>origin_latitude</li><li>destination_longitude</li><li>destination_latitude</li><li>Name/description of this route </li></ul><p>If you want to play along, there are plenty of free datasets out there to play with - I sourced some information from <strong>BigQuery </strong>while I was testing things out.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n</code></pre>\n<!--kg-card-end: markdown--><p>So far so good- all we've done is load our data, and save our Mapbox token from an environment variable.</p><h2 id=\"mapbox-directions-endpoint\">Mapbox Directions Endpoint</h2><p>Next, we're going to use Mapbox's <a href=\"https://docs.mapbox.com/api/navigation/#directions\">Directions API</a> to return a route for us. The anatomy of a GET call to receive directions looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">https://api.mapbox.com/directions/v5/mapbox/{{method_of_transportation}}/{{origin_longitude}},{{origin_latitude}};{{destination_longitude}},{{destination_latitude}}\n\nPARAMS:\naccess_token={{your_mapbox_access_token}}\ngeometries=geojson\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><strong>method_of_transportation </strong>refers to one of the three methods that Mapbox offers for creating routes: <em>driving-traffic</em>, <em>driving</em>, <em>walking</em>, and <em>cycling</em>. Note that there is currently no way to draw route objects which follow public transit: this is perhaps Mapbox's biggest downfall at the moment. Nevertheless, if this is something you need, data can be imported from Google maps to be used with Mapbox.</li><li><strong>access_token</strong> can be either your public token (visible upon login at mapbox.com) or a generated secret token.</li><li><strong>geometries</strong> accepts the method by which to draw the object. This can be <em>GeoJSON,</em> <em>polyline, </em>or <em>polyline6. </em>Let's stick with GeoJSON.</li></ul><h2 id=\"constructing-api-requests\">Constructing API Requests</h2><p>Let's construct a request per row in our DataFrame. By using Pandas' apply, we fire a function per row to do just that:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\ndef create_route_json(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    # Now what?\n\n\nroutes_df.apply(create_route_json, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's where things get a little tricky. You see, GeoJSON abides by a strict format. It looks something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;type&quot;: &quot;Feature&quot;,\n  &quot;geometry&quot;: {\n    &quot;coordinates&quot;: [\n      [ -73.985897, 40.748133 ], [ -73.985046, 40.747773 ], \n      [ -73.984579, 40.748431 ], [ -73.973437, 40.743885 ],\n      [ -73.972844, 40.744452 ], [ -73.970728, 40.743885 ], \n      [ -73.970611, 40.735137 ], [ -73.9714, 40.733734 ],\n      [ -73.973503, 40.732341 ], [ -73.969823, 40.729864 ], \n      [ -73.969243, 40.727535 ], [ -73.975074, 40.711418 ],\n      [ -73.976603, 40.710276 ], [ -73.978077, 40.710587 ], \n      [ -73.979462, 40.70932 ], [ -73.992664, 40.708145 ],\n      [ -73.996237, 40.707307 ], [ -74.001135, 40.704086 ], \n      [ -74.0055, 40.70243 ], [ -74.006778, 40.703628 ],\n      [ -74.009173, 40.702484 ], [ -74.010637, 40.70371 ], \n      [ -74.014535, 40.703624 ], [ -74.014665, 40.704034 ],\n      [ -74.017057, 40.703259 ]\n    ],\n    &quot;type&quot;: &quot;LineString&quot;\n  },\n  &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n  }],\n  &quot;weight_name&quot;: &quot;duration&quot;,\n  &quot;weight&quot;: 3873.3,\n  &quot;duration&quot;: 3873.3,\n  &quot;distance&quot;: 9660.2,\n  &quot;properties&quot;: {\n    &quot;name&quot;: &quot;Empire State&quot;\n  }\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>For the sake of being difficult, the Mapbox Directions API doesn't return responses in exactly this format. Instead, their response looks like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n  &quot;routes&quot;: [{\n    &quot;geometry&quot;: {\n      &quot;coordinates&quot;: [\n        [-73.985897, 40.748133],\n        [-73.985046, 40.747773],\n        [-73.984579, 40.748431],\n        [-73.973437, 40.743885],\n        [-73.972844, 40.744452],\n        [-73.970728, 40.743885],\n        [-73.970611, 40.735137],\n        [-73.9714, 40.733734],\n        [-73.973503, 40.732341],\n        [-73.969823, 40.729864],\n        [-73.969243, 40.727535],\n        [-73.975074, 40.711418],\n        [-73.976603, 40.710276],\n        [-73.978077, 40.710587],\n        [-73.979462, 40.70932],\n        [-73.992664, 40.708145],\n        [-73.996237, 40.707307],\n        [-74.001135, 40.704086],\n        [-74.0055, 40.70243],\n        [-74.006778, 40.703628],\n        [-74.009173, 40.702484],\n        [-74.010637, 40.70371],\n        [-74.014535, 40.703624],\n        [-74.014665, 40.704034],\n        [-74.017057, 40.703259]\n      ],\n      &quot;type&quot;: &quot;LineString&quot;\n    },\n    &quot;legs&quot;: [{\n      &quot;summary&quot;: &quot;&quot;,\n      &quot;weight&quot;: 3873.3,\n      &quot;duration&quot;: 3873.3,\n      &quot;steps&quot;: [],\n      &quot;distance&quot;: 9660.2\n    }],\n    &quot;weight_name&quot;: &quot;duration&quot;,\n    &quot;weight&quot;: 3873.3,\n    &quot;duration&quot;: 3873.3,\n    &quot;distance&quot;: 9660.2\n  }],\n  &quot;waypoints&quot;: [{\n      &quot;distance&quot;: 34.00158252003884,\n      &quot;name&quot;: &quot;West 33rd Street&quot;,\n      &quot;location&quot;: [\n        -73.985897,\n        40.748133\n      ]\n    },\n    {\n      &quot;distance&quot;: 6.627227256764976,\n      &quot;name&quot;: &quot;&quot;,\n      &quot;location&quot;: [\n        -74.017057,\n        40.703259\n      ]\n    }\n  ],\n  &quot;code&quot;: &quot;Ok&quot;,\n  &quot;uuid&quot;: &quot;cjsomodyl025642o6f1jsddx6&quot;\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>The format isn't <em>too</em> far off, but it's different enough to not work. </p><h2 id=\"formatting-geojson-correctly\">Formatting GeoJSON Correctly</h2><p>We need to write a function to take the response Mapbox has given us and transform it into a usable GeoJSON format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport requests\nimport json\n\nroutes_df = pd.read_csv('datasources/routes.csv').head(10)\ntoken = os.environ.get('mapbox_token')\n\n\ndef create_route_geojson(route_json, name):\n    &quot;&quot;&quot;Properly formats GeoJson for Mapbox visualization.&quot;&quot;&quot;\n    routes_dict = {\n        &quot;type&quot;: &quot;Feature&quot;,\n        &quot;geometry&quot;: {\n            &quot;type&quot;: &quot;LineString&quot;\n        },\n        &quot;weight_name&quot;: &quot;duration&quot;,\n        &quot;weight&quot;: 718.9,\n        &quot;duration&quot;: 0,\n        &quot;distance&quot;: 0,\n        &quot;properties&quot;: {\n            &quot;name&quot;: &quot;&quot;\n        }\n    }\n    routes_dict['geometry']['coordinates'] = route_json['geometry']['coordinates']\n    routes_dict['legs'] = route_json['legs']\n    routes_dict['duration'] = route_json['legs'][0]['duration']\n    routes_dict['distance'] = route_json['legs'][0]['distance']\n    routes_dict['properties']['name'] = name\n    with open('dataoutput/' + name + '.json', 'w') as f:\n        json.dump(routes_dict, \n                  f, \n                  sort_keys=True, \n                  indent=4, \n                  ensure_ascii=False)\n        \n\ndef create_walking_route(row):\n    &quot;&quot;&quot;Get route JSON.&quot;&quot;&quot;\n    base_url = 'https://api.mapbox.com/directions/v5/mapbox/driving/'\n    url = base_url + str(row['home_longitude']) + \\\n        ',' + str(row['home_latitude']) + \\\n        ';' + str(row['destination_longitude']) + \\\n        ',' + str(row['destination_latitude'])\n    params = {\n        'geometries': 'geojson',\n        'access_token': token\n    }\n    req = requests.get(url, params=params)\n    route_json = req.json()['routes'][0]\n    create_route_geojson(route_json, str(int(row['route_id'])))\n\n\nroutes_df.apply(create_walking_route, axis=1)\n</code></pre>\n<!--kg-card-end: markdown--><p>It's not pretty, but it's reliable: we explicitly create the JSON structure we need with <code>routes_dict</code>, and modify it with the API responses coming back from Mapbox. Of course, we're still doing this one at a time, for every row in our DataFrame.</p><p>You'll notice I save each JSON file locally for now. In the future, we'll write a script to automate the process of uploading our GeoJSON objects and adding them to the proper Tilesets, but right now I just want to see that our work paid off!</p><p>By using Mapbox studio, we can see the result of our first route:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/02/Screen-Shot-2019-02-28-at-8.05.21-AM.png\" class=\"kg-image\"><figcaption>A \"Driving\" Route from the Empire State Building to Battery Park.</figcaption></figure><!--kg-card-end: image--><p>Aha! Would you look at that- Mapbox knew to take the FDR drive. That's some promising stuff.</p><h3 id=\"drawing-routes-en-masse\">Drawing Routes En Masse</h3><p>Naturally, this is only the tip of the iceberg: of the DataFrame of information we loaded up, we've so far only viewed a single result. If anything in data is worth doing, it must be done thousands of times systematically without fail. Luckily, Mapbox provides us with the tools to do this: from lending us an S3 bucket, to modifying datasets via the API, there's nothing to fear.</p><p>Tune in next time when do more... stuff!</p>","url":"https://hackersandslackers.com/mapbox-draw-route-objects/","uuid":"ef0a4639-8818-475b-9a25-6a20b13c1ecf","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c17ddd4418434084a873d2a"}},{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * Thereâs no real âexportâ button on Cloudinary. Yes, I know,  theyâve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * Weâre left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\nâââ Posts\nâ   âââ /2017\nâ   â   âââ 11\nâ   âââ /2018\nâ   â   âââ 03\nâ   â   âââ 04\nâ   â   âââ 05\nâ   â   âââ 06\nâ   â   âââ 07\nâ   â   âââ 08\nâ   â   âââ 09\nâ   â   âââ 10\nâ   â   âââ 11\nâ   â   âââ 12\nâ   âââ /2019\nâ   â   âââ 01\nâ   â   âââ 02\nâ   âââ /lynx\nâââ /bunch\nâââ /of\nâââ /other\nâââ /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>Thereâs no real âexportâ button on Cloudinary. <em>Yes, I know,</em> theyâve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>Weâre left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\nâââ Posts\nâ   âââ /2017\nâ   â   âââ 11\nâ   âââ /2018\nâ   â   âââ 03\nâ   â   âââ 04\nâ   â   âââ 05\nâ   â   âââ 06\nâ   â   âââ 07\nâ   â   âââ 08\nâ   â   âââ 09\nâ   â   âââ 10\nâ   â   âââ 11\nâ   â   âââ 12\nâ   âââ /2019\nâ   â   âââ 01\nâ   â   âââ 02\nâ   âââ /lynx\nâââ /bunch\nâââ /of\nâââ /other\nâââ /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},{"node":{"id":"Ghost__Post__5c5a3e362c71af62216fd45e","title":"Manage Database Models with Flask-SQLAlchemy","slug":"manage-database-models-with-flask-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","custom_excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","created_at_pretty":"06 February, 2019","published_at_pretty":"06 February, 2019","updated_at_pretty":"03 April, 2019","created_at":"2019-02-05T20:53:58.000-05:00","published_at":"2019-02-06T08:00:00.000-05:00","updated_at":"2019-04-03T11:38:02.000-04:00","meta_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","meta_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","og_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","twitter_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","twitter_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Pythonâs fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Pythonâs fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"By now you're surely familiar with the benefits of Python's core SQLAlchemy\nlibrary\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/]:\nthe all-in-one solution for basically anything database related. Like most major\nPython libraries, SQLAlchemy has been ported into a version specifically\ncompatible with Flask, aptly named Flask-SQLAlchemy.\n\nSimilar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us\nto modify application data by easily creating defined models. Regardless of what\nyour database of choice might be, Flask-SQLAlchemy will ensure that the models\nwe create in Python will translate to the syntax of our chosen database. Given\nthe ease-of-use and one-size-fits-all Â nature of Flask-SQLAlchemy, it's no\nwonder that the library has been the de facto database library of choice for\nFlask since the very beginning (seriously, is there even another option?)\n\nConfiguring Flask-SQLAlchemy For Your Application\nThere are a few essential configuration variables we need to set upfront before\ninteracting with our database. As is standard, we'll be using a class defined in\n config.py  to handle our Flask config:\n\nimport os\n\n\nclass Config:\n    \"\"\"Set Flask configuration vars from .env file.\"\"\"\n    \n    # General\n    TESTING = os.environ[\"TESTING\"]\n    FLASK_DEBUG = os.environ[\"FLASK_DEBUG\"]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(\"SQLALCHEMY_DATABASE_URI\")\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(\"SQLALCHEMY_TRACK_MODIFICATIONS\")\n\n\nLet's break these down:\n\n * SQLALCHEMY_DATABASE_URI: the connection string we need to connect to our\n   database. This follows the standard convention: \n   [db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]\n * SQLALCHEMY_ECHO: When set to 'True', Flask-SQLAlchemy will log all database\n   activity to Python's stderr for debugging purposes.\n * SQLALCHEMY_TRACK_MODIFICATIONS: Honestly, I just always set this to 'False,'\n   otherwise an obnoxious warning appears every time you run your app reminding\n   you that this option takes a lot of system resources.\n\nThose are the big ones we should worry about. If you're into some next-level\ndatabase shit, there are a few other pro-mode configuration variables which you\ncan find here [http://flask-sqlalchemy.pocoo.org/2.3/config/].\n\nBy using the exact naming conventions for the variables above, simply having\nthem in our config file will automatically configure our database connections\nfor us. We will never have to create engines, sessions, or connections.\nInitiating Flask-SQLAlchemy With Our App\nAs always, we're going to use the Flask Application Factory method\n[https://hackersandslackers.com/structuring-your-flask-app/]  for initiating our\napp. If you're unfamiliar with the term, you're going to find this tutorial to\nbe confusing and pretty much useless.\n\nThe most basic __init__.py  file for Flask applications using Flask-SQLAlchemy\nshould look like this:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n\n\nNote the presence of db  and its location: this our database object being set as\na global  variable outside of create_app(). Inside of create_app(), on the other\nhand, contains the line db.init_app(app). Even though we've set our db object\nglobally, this means nothing until we initialize it after creating our\napplication. We accomplish this by calling init_app()  within create_app(), and\npassing our app as the parameter. Within the actual 'application context' is\nwhere we'll call create_all(), which we'll cover in a bit.\n\nIf that last paragraph sounded like total gibberish to you, you are not alone. \nThe Flask Application Factory is perhaps one of the most odd and poorly\nexplained concepts in Python software development- my best advice is to not\nbecome frustrated, take the copy + paste code above, and blindly accept the\nspoon-fed nonsense enough times until it becomes second nature. That's what I\ndid, and even as I worked through this tutorial, I still  came across obnoxious\nquirks that caught me off-guard.\n\nTake note of import we make inside of the application context called routes.\nThis is one of two files we haven't written just yet: once we create them, our\napplication file structure will look something like this:\n\nmy-app\nâââ /application\nâ   âââ __init__.py\nâ   âââ routes.py\nâ   âââ models.py\nâââ .env\nâââ config.py\nâââ wsgi.py\n\n\nCreating Database Models\nCreate a models.py  file in our application directory. Here we'll import the db \nobject that we created in __init__.py. Now we can create database models by\ndefining classes in this file.\n\nA common example would be to start with a User  model. The first variable we\ncreate is __tablename__, which will correspond to the name of the SQL table new\nusers will be saved. Each additional variable we create within this model class\nwill correspond a column in the database:\n\nfrom . import db\n\n\nclass User(db.Model):\n    \"\"\"Model for user accounts.\"\"\"\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n\nEach \"column\" accepts the following attributes:\n\n * Data Type:  Accepts one of the following: String(size), Text, DateTime, Float\n   , Boolean, PickleType, or LargeBinary.\n * primary_key: Whether or not the column should serve as the primary key.\n * unique: Whether or not to enforce unique values for the column.\n * nullable: Denotes required fields.\n\nWith our first model created, you're already way closer to interacting with your\ndatabase than you might think.\n\nCreating Our First Entry\nLet's create a user in our routes.py  file.\n\nfrom flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(\"User created!\")\n\n\nCheck out how easy this is! All it takes to create a user is create an instance\nof the User  class from models.py, add it to our session via \ndb.session.add(new_user), and commit the changes with db.session.commit()! Let's\nsee what happens when we run this app:\n\nUser Created!\n\n\nThat's what we like to see! If we access our database at this point, we can see\nthat this exact record was created in our users  table.\n\nQuerying Our New Data\nCreating information is dope, but how can we confirm it exists? I've added a few\nthings to routes.py  to show us what's up:\n\nfrom flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=\"Show Users\")\n\n\n  The statement User.query.all()  will return all instances of User  in our\ndatabase. I created a Jinja template to show us all records nicely:\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n  {% for user in users %}\n    <ul id=\"user.username\">\n      <li>Username: {{ user.username }}</li>\n      <li>Email: {{ user.email }}</li>\n      <li>Created: {{ user.created }}</li>\n      <li>Bio: {{ user.bio }}</li>\n      <li>Admin: {{ user.admin }}</li>\n    </ul>\n  {% endfor %}\n{% endblock %}\n\n\nThus, our app gives us:\n\nWe have liftoff!So we can get a single user, but what about a whole table full\nof users? Well, all we need to do is keep changing the username and email\naddress (our unique keys, to avoid a clash) when firing up the app, and each\ntime it runs, it'll create a new user. Here's what comes back after running the\napp a few times with different values:\n\nI Can't Believe It's Not Error Messages.â¢Here, Take All My Stuff\nSure, Flask-SQLAlchemy is great once you get going, but as we've already seen\n\"getting set up\" isn't always a walk in the park. This is one of those things\nthat always seems to be wrong no matter how many times you've done it from\nmemory.\n\nAs a parting gift, I've put the source for this tutorial up on Github\n[https://github.com/toddbirchard/flasksqlalchemy-tutorial]  for you to treasure\nand enjoy. No seriously, take it. Get it away from me. I'm done with Flask for\ntoday. I need to go play some Rocket League.\n\nPS: Add me on PSN","html":"<p>By now you're surely familiar with the benefits of Python's <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">core SQLAlchemy library</a>: the all-in-one solution for basically anything database related. Like most major Python libraries, SQLAlchemy has been ported into a version specifically compatible with Flask, aptly named <strong>Flask-SQLAlchemy</strong>.</p><p>Similar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us to modify application data by easily creating defined models. Regardless of what your database of choice might be, Flask-SQLAlchemy will ensure that the models we create in Python will translate to the syntax of our chosen database. Given the ease-of-use and one-size-fits-all Â nature of Flask-SQLAlchemy, it's no wonder that the library has been the de facto database library of choice for Flask since the very beginning (seriously, is there even another option?)</p><h2 id=\"configuring-flask-sqlalchemy-for-your-application\">Configuring Flask-SQLAlchemy For Your Application</h2><p>There are a few essential configuration variables we need to set upfront before interacting with our database. As is standard, we'll be using a class defined in <code>config.py</code> to handle our Flask config:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\n\n\nclass Config:\n    &quot;&quot;&quot;Set Flask configuration vars from .env file.&quot;&quot;&quot;\n    \n    # General\n    TESTING = os.environ[&quot;TESTING&quot;]\n    FLASK_DEBUG = os.environ[&quot;FLASK_DEBUG&quot;]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(&quot;SQLALCHEMY_DATABASE_URI&quot;)\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(&quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's break these down:</p><ul><li><code>SQLALCHEMY_DATABASE_URI</code>: the connection string we need to connect to our database. This follows the standard convention: <code>[db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]</code></li><li><code>SQLALCHEMY_ECHO</code>: When set to 'True', Flask-SQLAlchemy will log all database activity to Python's stderr for debugging purposes.</li><li><code>SQLALCHEMY_TRACK_MODIFICATIONS</code>: Honestly, I just always set this to 'False,' otherwise an obnoxious warning appears every time you run your app reminding you that this option takes a lot of system resources.</li></ul><p>Those are the big ones we should worry about. If you're into some next-level database shit, there are a few other pro-mode configuration variables which you can find <a href=\"http://flask-sqlalchemy.pocoo.org/2.3/config/\">here</a>.</p><!--kg-card-begin: html--><div class=\"protip\">By using the exact naming conventions for the variables above, simply having them in our config file will automatically configure our database connections for us. We will never have to create engines, sessions, or connections.</div><!--kg-card-end: html--><h2 id=\"initiating-flask-sqlalchemy-with-our-app\">Initiating Flask-SQLAlchemy With Our App</h2><p>As always, we're going to use the <a href=\"https://hackersandslackers.com/structuring-your-flask-app/\">Flask Application Factory method</a> for initiating our app. If you're unfamiliar with the term, you're going to find this tutorial to be confusing and pretty much useless.</p><p>The most basic <code>__init__.py</code> file for Flask applications using Flask-SQLAlchemy should look like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>Note the presence of <code>db</code> and its location: this our database object being set as a <em>global</em> variable outside of <code>create_app()</code>. Inside of <code>create_app()</code>, on the other hand, contains the line <code>db.init_app(app)</code>. Even though we've set our db object globally, this means nothing until we initialize it after creating our application. We accomplish this by calling <code>init_app()</code> within <code>create_app()</code>, and passing our app as the parameter. Within the actual 'application context' is where we'll call <code>create_all()</code>, which we'll cover in a bit.</p><p>If that last paragraph sounded like total gibberish to you, <em>you are not alone.</em> The Flask Application Factory is perhaps one of the most odd and poorly explained concepts in Python software development- my best advice is to not become frustrated, take the copy + paste code above, and blindly accept the spoon-fed nonsense enough times until it becomes second nature. That's what I did, and even as I worked through this tutorial, I <em>still</em> came across obnoxious quirks that caught me off-guard.</p><p>Take note of import we make inside of the application context called <strong>routes</strong>. This is one of two files we haven't written just yet: once we create them, our application file structure will look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-app\nâââ /application\nâ   âââ __init__.py\nâ   âââ routes.py\nâ   âââ models.py\nâââ .env\nâââ config.py\nâââ wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-database-models\">Creating Database Models</h2><p>Create a <code>models.py</code> file in our application directory. Here we'll import the <code>db</code> object that we created in <code>__init__.py</code>. Now we can create database models by defining classes in this file.</p><p>A common example would be to start with a <strong>User</strong> model. The first variable we create is <code>__tablename__</code>, which will correspond to the name of the SQL table new users will be saved. Each additional variable we create within this model class will correspond a column in the database:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from . import db\n\n\nclass User(db.Model):\n    &quot;&quot;&quot;Model for user accounts.&quot;&quot;&quot;\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '&lt;User {}&gt;'.format(self.username)\n</code></pre>\n<!--kg-card-end: markdown--><p>Each \"column\" accepts the following attributes:</p><ul><li><strong>Data Type:</strong> Accepts one of the following: <code>String(size)</code>, <code>Text</code>, <code>DateTime</code>, <code>Float</code>, <code>Boolean</code>, <code>PickleType</code>, or <code>LargeBinary</code>.</li><li><strong>primary_key</strong>: Whether or not the column should serve as the primary key.</li><li><strong>unique</strong>: Whether or not to enforce unique values for the column.</li><li><strong>nullable: </strong>Denotes required fields.</li></ul><p>With our first model created, you're already way closer to interacting with your database than you might think.</p><h2 id=\"creating-our-first-entry\">Creating Our First Entry</h2><p>Let's create a user in our <code>routes.py</code> file.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(&quot;User created!&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out how easy this is! All it takes to create a user is create an instance of the <code>User</code> class from <code>models.py</code>, add it to our session via <code>db.session.add(new_user)</code>, and commit the changes with <code>db.session.commit()</code>! Let's see what happens when we run this app:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">User Created!\n</code></pre>\n<!--kg-card-end: markdown--><p>That's what we like to see! If we access our database at this point, we can see that this exact record was created in our <em>users</em> table.</p><h2 id=\"querying-our-new-data\">Querying Our New Data</h2><p>Creating information is dope, but how can we confirm it exists? I've added a few things to <code>routes.py</code> to show us what's up:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=&quot;Show Users&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p> The statement <code>User.query.all()</code> will return all instances of <code>User</code> in our database. I created a Jinja template to show us all records nicely:</p><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n  {% for user in users %}\n    &lt;ul id=&quot;user.username&quot;&gt;\n      &lt;li&gt;Username: {{ user.username }}&lt;/li&gt;\n      &lt;li&gt;Email: {{ user.email }}&lt;/li&gt;\n      &lt;li&gt;Created: {{ user.created }}&lt;/li&gt;\n      &lt;li&gt;Bio: {{ user.bio }}&lt;/li&gt;\n      &lt;li&gt;Admin: {{ user.admin }}&lt;/li&gt;\n    &lt;/ul&gt;\n  {% endfor %}\n{% endblock %}\n</code></pre>\n<!--kg-card-end: markdown--><p>Thus, our app gives us:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/flasksqlalchemy-test.png\" class=\"kg-image\"><figcaption>We have liftoff!</figcaption></figure><!--kg-card-end: image--><p>So we can get a single user, but what about a whole table full of users? Well, all we need to do is keep changing the username and email address (our unique keys, to avoid a clash) when firing up the app, and each time it runs, it'll create a new user. Here's what comes back after running the app a few times with different values:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-06-at-12.39.07-AM.png\" class=\"kg-image\"><figcaption>I Can't Believe It's Not Error Messages.<b>â¢</b></figcaption></figure><!--kg-card-end: image--><h3 id=\"here-take-all-my-stuff\">Here, Take All My Stuff</h3><p>Sure, Flask-SQLAlchemy is great once you get going, but as we've already seen \"getting set up\" isn't always a walk in the park. This is one of those things that always seems to be wrong no matter how many times you've done it from memory.</p><p>As a parting gift, I've put the source for this tutorial up <a href=\"https://github.com/toddbirchard/flasksqlalchemy-tutorial\">on Github</a> for you to treasure and enjoy. No seriously, take it. Get it away from me. I'm done with Flask for today. I need to go play some Rocket League.</p><!--kg-card-begin: html--><span style=\"color: #a7a7a7;font-style: italic;font-size:.9em;\">PS: Add me on PSN</span><!--kg-card-end: html-->","url":"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/","uuid":"e9fdcb15-3289-472f-8892-2e01cdaced9d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5a3e362c71af62216fd45e"}},{"node":{"id":"Ghost__Post__5c47584f4f3823107c9e8f23","title":"Google BigQuery's Python SDK: Creating Tables Programmatically","slug":"getting-started-google-big-query-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","custom_excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","created_at_pretty":"22 January, 2019","published_at_pretty":"02 February, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-22T12:52:15.000-05:00","published_at":"2019-02-02T09:24:00.000-05:00","updated_at":"2019-03-28T17:06:20.000-04:00","meta_title":"Google BigQuery's Python SDK: Creating Tables | Hackers and Slackers","meta_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","og_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","twitter_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","twitter_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platformâs offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platformâs offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"GCP is on the rise, and it's getting harder and harder to have conversations\naround data without addressing the 500-pound gorilla in the room: Google\nBigQuery. With most enterprises comfortably settled into their Apache-based Big\nData stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers\nAnd Slackers is no such enterprise. Thus, we aren't afraid to ask the Big\nquestion: how much easier would life be with BigQuery?\n\nBig Data, BigQuery\nIn short, BigQuery trivializes the act of querying against multiple,\nunpredictable data sources. To better understand when this is useful, it would\nbetter serve us to identify the types of questions BigQuery can answer. Such as:\n\n * What are our users doing across our multiple systems? How do we leverage log\n   files outputted by multiple systems to find out?\n * How can we consolidate information about employee information, payroll, and\n   benefits, when these all live in isolated systems?\n * What the hell am I supposed to do with all these spreadsheets?\n\nUnlike previous solutions, BigQuery solves these problems in a single product\nand does so with SQL-like query syntax,  a web interface, and 7 native Client\nLibraries.  There are plenty of reasons to love BigQuery, but let's start with\none we've recently already talked about: the auto-generation of table schemas. \n\nMatt has demonstrated how to approach this problem manually with the help of\nPandas\n[https://hackersandslackers.com/downcast-numerical-columns-python-pandas/]. I\nprovided a more gimmicky approach by leveraging the Python table-schema library\n[https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/]. With\nBigQuery, we find yet another alternative which is neither manual or gimmicky:\nperfect for those who are lazy, rich, and demand perfection (AKA: your clients,\nprobably).\n\nFirst, we'll need to get our data into BigQuery\n\nUploading Data into Google Cloud Storage via the Python SDK\nBigQuery requires us to go through Google Cloud Storage as a buffer before\ninputting data into tables. No big deal, we'll write a script!\n\nWe're assuming that you have a basic knowledge of Google Cloud, Google Cloud\nStorage, and how to download a JSON Service Account key\n[https://cloud.google.com/bigquery/docs/reference/libraries]  to store locally\n(hint: click the link).\n\nfrom google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n\n\nThe above is nearly a copy + paste of Google Cloud's sample code for the Google\nCloud Storage Python SDK:\n\n * bucket_uri  is found by inspecting any bucket's information on Google Cloud.\n * bucket_name  is... well, you know.\n * bucket_target  represents the resulting file structure representing the saved\n   CSV when completed.\n * local_dataset  is the path to a CSV we've stored locally: we can assume that\n   we've grabbed some data from somewhere, like an API, and tossed into a local\n   file temporarily.\n\nSuccessfully executing the above results in the following message:\n\nFile data/test.csv uploaded to datasets/data_upload.csv.\n\n\nInserting Data from Cloud Storage to BigQuery\nThat was the easy part. Let's move on to the good stuff:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n\n\nWe've added the function insert_bigquery()  to handle creating a BigQuery table\nout of a CSV.\n\nAfter we set our client, we create a dataset reference. In BigQuery, tables can\nbelong to a 'dataset,' which is a grouping of tables. Compare this concept to\nMongoDB's collections, or PostgreSQL's schemas. Note that this process is made\nmuch easier by the fact that we stored our project key locally: otherwise, we'd\nhave to specify which Google Cloud project we're looking for, etc.\n\nWith the dataset specified, we begin to build our \"job\" object with \nLoadJobConfig. This is like loading a gun before unleashing a shotgun blast into\nthe face of our problems. Alternatively, a more relevant comparison could be\nwith the Python requests  library and the act of prepping an API request before\nexecution.\n\nWe set job_config.autodetect  to be True, obviously. \njob_config.skip_leading_rows  reserves our header row from screwing things up.\n\nload_job  puts our request together, and load_job.result()  executes said job.\nThe .result()  method graciously puts the rest of our script on hold until the\nspecified job is completed. In our case, we want this happen: it simplifies our\nscript so that we don't need to verify this manually before moving on.\n\nLet's see what running that job with our fake data looks like in the BigQuery\nUI:\n\nAll my fake friends are here!Getting Our Flawlessly Inferred Table Schema\nBigQuery surely gets table schemas wrong some of the time. That said, I have yet\nto see it happen. Let's wrap this script up:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    \"\"\"Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n\n\nWith the addition of get_bigquery_schema(), our script is complete!\n\nTableReference()  is similar to the dataset reference we went over earlier, only\nfor tables (duh). This allows us to call upon get_table(), which returns a Table\nclass representing the table we just created. Amongst the methods of that class,\nwe can call .schema(), which gives us precisely what we want: a beautiful\nrepresentation of a Table schema, generated from raw CSV information, where\nthere previously was none.\n\nBehold the fruits of your labor:\n\n[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n\n\nThere you have it; a correctly inferred schema, from data which wasn't entirely\nclean in the first place (our dates are in MM/DD/YY  format as opposed to \nMM/DD/YYYY, but Google still gets it right. How? Because Google).\n\nIt Doesn't End Here\nI hope it goes without saying that abusing Google BigQuery's API to generate\nschemas for you is only a small, obscure use case of what Google BigQuery is\nintended to do, and what it can do for you. That said, I need to stop this\nfanboying post before anybody realizes I'll promote their products for free\nforever (I think I may have passed that point).\n\nIn case you're interested, the source code for this script has been uploaded as\na Gist here\n[https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e]. Have at\nit, and remember to think Bigâ¢*.\n\n*Not a real trademark, I'm making things up again.","html":"<p>GCP is on the rise, and it's getting harder and harder to have conversations around data without addressing the 500-pound gorilla in the room: Google BigQuery. With most enterprises comfortably settled into their Apache-based Big Data stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers And Slackers is no such enterprise. Thus, we aren't afraid to ask the Big question: how much easier would life be with BigQuery?</p><h2 id=\"big-data-bigquery\">Big Data, BigQuery</h2><p>In short, BigQuery trivializes the act of querying against multiple, unpredictable data sources. To better understand when this is useful, it would better serve us to identify the types of questions BigQuery can answer. Such as:</p><ul><li>What are our users doing across our multiple systems? How do we leverage log files outputted by multiple systems to find out?</li><li>How can we consolidate information about employee information, payroll, and benefits, when these all live in isolated systems?</li><li>What the hell am I supposed to do with all these spreadsheets?</li></ul><p>Unlike previous solutions, BigQuery solves these problems in a single product and does so with <strong>SQL-like query syntax,</strong> a <strong>web interface</strong>, and <strong>7 native Client Libraries.</strong> There are plenty of reasons to love BigQuery, but let's start with one we've recently already talked about: the <em>auto-generation of table schemas</em>. </p><p>Matt has demonstrated how to approach this problem <a href=\"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/\">manually with the help of Pandas</a>. I provided a more gimmicky approach by leveraging the <a href=\"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/\">Python table-schema library</a>. With BigQuery, we find yet another alternative which is neither manual or gimmicky: perfect for those who are lazy, rich, and demand perfection (AKA: your clients, probably).</p><p>First, we'll need to get our data into BigQuery</p><h2 id=\"uploading-data-into-google-cloud-storage-via-the-python-sdk\">Uploading Data into Google Cloud Storage via the Python SDK</h2><p>BigQuery requires us to go through Google Cloud Storage as a buffer before inputting data into tables. No big deal, we'll write a script!</p><p>We're assuming that you have a basic knowledge of Google Cloud, Google Cloud Storage, and how to download a <a href=\"https://cloud.google.com/bigquery/docs/reference/libraries\">JSON Service Account key</a> to store locally (hint: click the link).</p><pre><code class=\"language-python\">from google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n</code></pre>\n<p>The above is nearly a copy + paste of Google Cloud's sample code for the Google Cloud Storage Python SDK:</p><ul><li><code>bucket_uri</code> is found by inspecting any bucket's information on Google Cloud.</li><li><code>bucket_name</code> is... well, you know.</li><li><code>bucket_target</code><strong> </strong>represents the resulting file structure representing the saved CSV when completed.</li><li><code>local_dataset</code> is the path to a CSV we've stored locally: we can assume that we've grabbed some data from somewhere, like an API, and tossed into a local file temporarily.</li></ul><p>Successfully executing the above results in the following message:</p><pre><code class=\"language-shell\">File data/test.csv uploaded to datasets/data_upload.csv.\n</code></pre>\n<h2 id=\"inserting-data-from-cloud-storage-to-bigquery\">Inserting Data from Cloud Storage to BigQuery</h2><p>That was the easy part. Let's move on to the good stuff:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n</code></pre>\n<p>We've added the function <code>insert_bigquery()</code> to handle creating a BigQuery table out of a CSV.</p><p>After we set our client, we create a <strong>dataset reference</strong>. In BigQuery, tables can belong to a 'dataset,' which is a grouping of tables. Compare this concept to MongoDB's <strong>collections, </strong>or PostgreSQL's <strong>schemas</strong>. Note that this process is made much easier by the fact that we stored our project key locally: otherwise, we'd have to specify which Google Cloud project we're looking for, etc.</p><p>With the dataset specified, we begin to build our \"job\" object with <code>LoadJobConfig</code>. This is like loading a gun before unleashing a shotgun blast into the face of our problems. Alternatively, a more relevant comparison could be with the Python <code>requests</code> library and the act of prepping an API request before execution.</p><p>We set <code>job_config.autodetect</code> to be <code>True</code>, obviously. <code>job_config.skip_leading_rows</code> reserves our header row from screwing things up.</p><p><code>load_job</code> puts our request together, and <code>load_job.result()</code> executes said job. The <code>.result()</code> method graciously puts the rest of our script on hold until the specified job is completed. In our case, we want this happen: it simplifies our script so that we don't need to verify this manually before moving on.</p><p>Let's see what running that job with our fake data looks like in the BigQuery UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-01-at-7.42.52-PM.png\" class=\"kg-image\"><figcaption>All my fake friends are here!</figcaption></figure><h2 id=\"getting-our-flawlessly-inferred-table-schema\">Getting Our Flawlessly Inferred Table Schema</h2><p>BigQuery surely gets table schemas wrong <em>some </em>of the time. That said, I have yet to see it happen. Let's wrap this script up:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    &quot;&quot;&quot;Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n</code></pre>\n<p>With the addition of <code>get_bigquery_schema()</code>, our script is complete!</p><p><code>TableReference()</code> is similar to the dataset reference we went over earlier, only for tables (duh). This allows us to call upon <code>get_table()</code>, which returns a Table class representing the table we just created. Amongst the methods of that class, we can call <code>.schema()</code>, which gives us precisely what we want: a beautiful representation of a Table schema, generated from raw CSV information, where there previously was none.</p><p>Behold the fruits of your labor:</p><pre><code class=\"language-python\">[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n</code></pre>\n<p>There you have it; a correctly inferred schema, from data which wasn't entirely clean in the first place (our dates are in <strong>MM/DD/YY</strong> format as opposed to <strong>MM/DD/YYYY</strong>, but Google still gets it right. How? Because Google).</p><h3 id=\"it-doesn-t-end-here\">It Doesn't End Here</h3><p>I hope it goes without saying that abusing Google BigQuery's API to generate schemas for you is only a small, obscure use case of what Google BigQuery is intended to do, and what it can do for you. That said, I need to stop this fanboying post before anybody realizes I'll promote their products for free forever (I think I may have passed that point).</p><p>In case you're interested, the source code for this script has been uploaded as a Gist <a href=\"https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e\">here</a>. Have at it, and remember to think Big<strong>â¢*</strong>.</p><span style=\"color: #a6a6a6;font-style: italic; font-size: .8em;\">*Not a real trademark, I'm making things up again.</span>","url":"https://hackersandslackers.com/getting-started-google-big-query-python/","uuid":"30051eb2-7fa7-4a09-91fd-c3f11966b398","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47584f4f3823107c9e8f23"}},{"node":{"id":"Ghost__Post__5c4e57144b23df2da7332b80","title":"Downcast Numerical Data Types with Pandas","slug":"downcast-numerical-columns-python-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","excerpt":"Using an Example Where We Downcast Numerical Columns.","custom_excerpt":"Using an Example Where We Downcast Numerical Columns.","created_at_pretty":"28 January, 2019","published_at_pretty":"28 January, 2019","updated_at_pretty":"14 February, 2019","created_at":"2019-01-27T20:12:52.000-05:00","published_at":"2019-01-28T07:30:00.000-05:00","updated_at":"2019-02-13T22:50:18.000-05:00","meta_title":"Using Pandas' Assign Function on Multiple Columns | Hackers and Slackers","meta_description":"Using Pandas' Assign function on multiple columns via an example: downcasting numerical columns.","og_description":"Using Pandas' Assign by example: downcasting numerical columns.","og_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","og_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","twitter_description":"Using Pandas' Assign by example: downcasting numerical columns.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","twitter_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Recently, I had to find a way to reduce the memory footprint of a Pandas\nDataFrame in order to actually do operations on it. Â Here's a trick that came in\nhandy!\n\nBy default, if you read a DataFrame from a file, it'll cast all the numerical\ncolumns as the float64  type. Â This is in keeping with the philosophy behind\nPandas and NumPy - by using strict types (instead of normal Python \"duck\ntyping\"), you can do things a lot faster. Â The float64  is the most flexible\nnumerical type - it can handle fractions, as well as turning missing values into\na NaN. Â This will let us read it into memory, and then start messing with it.\nÂ The downside is that it consumes a lot of memory.\n\nNow, let's say we want to save memory by manually downcasting our columns into\nthe smallest type that can handle its values? Â And let's ALSO say that we want\nto be really, really lazy and don't want to look at a bunch of numbers by hand.\nÂ And let's say we wanna do this via Method Chaining, because of all the\nadvantages outlined here: https://tomaugspurger.github.io/method-chaining\n\nLet's introduce our example DataFrame. Â We'll convert all the values to floats\nmanually because that's what the default is when we read from a file.\n\ndf = pd.DataFrame({\n    \"stay_float\": [0.5, 3.7, 7.5],\n    \"to_int\": [-5, 7, 5],\n    \"to_uint\": [1, 100, 200]}).astype(float)\n\n\nFirst, let's introduce the workhorse of this exercise - Pandas's to_numeric \nfunction, and its handy optional argument, downcast. Â This will take a numerical\ntype - float, integer  (not int), or unsigned  - and then downcast it to the\nsmallest version available.\n\nNext, let's make a function that checks to see if a column can be downcast from\na float to an integer.\n\ndef float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser\n\nWe're using the try/except pattern here because if we try to make a column with \nNaN  values into an integer column, it'll throw an error. Â If it'd otherwise be\na good candidate for turning into an integer, we should figure a value to impute\nfor those missing values - but that'll be different for every column. Â Sometimes\nit'd make sense to make it 0, other times the mean or median of the column, or\nsomething else entirely.\n\nI'd also like to direct your attention to Line 4, which has a very useful Pandas\npattern - if (ser == int_ser).all(). Â When you do operations on Pandas columns\nlike Equals or Greater Than, you get a new column where the operation was\napplied element-by-element. Â If you're trying to set up a conditional, the\ninterpreter doesn't know what to do with an array containing [True, False, True] \n - you have to boil it down to a single value. Â So, if you wan to check if two\ncolumns are completely equal, you have to call the .all()  method (which has a\nuseful sibling, any()) to make a conditional that can actually be used to\ncontrol execution.\n\nNext, let's make a function that lets us apply a transformation to multiple\ncolumns based on a condition. Â The assign  method is pretty awesome, and it'd be\nfun to not have to leave it (or, if we do, to at least replace it with a\nfunction we can pipe as part of a chain of transformations to the DataFrame as a\nwhole).\n\ndef multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n\n\nassign  lets us do multiple assignments, so long as we make a dictionary of\ncolumn names and target values and then unpack it. Â Really, it'd actually be\neasier to skip the function and go directly to using this syntax, except that\nI'm not aware of a method of accessing a filterable list of the DF's columns\nwhile still \"in\" the chain. Â I think future versions of Pandas' syntax will\ninclude this, as I've read they want to support more Method Chaining.\nÂ Personally, I find the reduction in Cognitive Load is worth it, with having a\nlot of little modular lego-piece transformations chained together.\n\nIt also works as a nice foundation for other little helper functions. Â So,\nhere's one to turn as many float columns to integers as we can.\n\ndef all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[\"float\"])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSee the pattern in action! Â We decide on a transformation function, we decide on\nwhat conditions we want to apply all these transformations (we could have a\nhundred columns, and who wants to make a note of all that?), and then we pass it\nto the multi-assign  function.\n\n(df\n     .pipe(all_float_to_int)).dtypes\n\n\nstay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n\n\nCool! Â But we didn't actually decrease the size of our DataFrame - 64 bytes of\ninteger takes up as many bytes as 64 bytes of float, just like how a hundred\npounds of feathers weighs as much as a hundred pounds of bricks. Â What we did do\nis make it easier to downcast those columns later.\n\nNext, let's make a function that takes a subset of the columns, and tries to\ndowncast it to the smallest version that it can. Â We've got fairly small values\nhere, so it should get some work done.\n\ndef downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSame basic pattern as before! Â But now we have two arguments - one is the \ntarget_type, which tells us what types to try to downcast to. Â By default, this\nwill be the same as the initial_type, with one exception that we'll grab in a\nsecond!\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n\n\nAlright, now we're getting somewhere! Â Wonder if we can do even better, though?\nÂ That last column has a conspicuous name! Â And it has no values lower than 0 -\nmaybe we could save space if we store it as an unsigned integer! Â Let's add a\npipe to our chain that'll try to downcast certain integers into unsigneds...\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n\n\nWhat do ya know, we can!\n\nLet's see how much memory we save by doing this.\n\ndf.info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n\n\nvs\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n\n\n152 down to 98 - we reduced it by more than 1/3rd!","html":"<p>Recently, I had to find a way to reduce the memory footprint of a Pandas DataFrame in order to actually do operations on it. Â Here's a trick that came in handy!</p><p>By default, if you read a DataFrame from a file, it'll cast all the numerical columns as the <code>float64</code> type. Â This is in keeping with the philosophy behind Pandas and NumPy - by using strict types (instead of normal Python \"duck typing\"), you can do things a lot faster. Â The <code>float64</code> is the most flexible numerical type - it can handle fractions, as well as turning missing values into a <code>NaN</code>. Â This will let us read it into memory, and then start messing with it. Â The downside is that it consumes a lot of memory.</p><p>Now, let's say we want to save memory by manually downcasting our columns into the smallest type that can handle its values? Â And let's ALSO say that we want to be really, really lazy and don't want to look at a bunch of numbers by hand. Â And let's say we wanna do this via Method Chaining, because of all the advantages outlined here: <a href=\"https://tomaugspurger.github.io/method-chaining\">https://tomaugspurger.github.io/method-chaining</a></p><p>Let's introduce our example DataFrame. Â We'll convert all the values to floats manually because that's what the default is when we read from a file.</p><pre><code class=\"language-python\">df = pd.DataFrame({\n    &quot;stay_float&quot;: [0.5, 3.7, 7.5],\n    &quot;to_int&quot;: [-5, 7, 5],\n    &quot;to_uint&quot;: [1, 100, 200]}).astype(float)\n</code></pre>\n<p>First, let's introduce the workhorse of this exercise - Pandas's <code>to_numeric</code> function, and its handy optional argument, <code>downcast</code>. Â This will take a numerical type - <code>float</code>, <code>integer</code> (not <code>int</code>), or <code>unsigned</code> - and then downcast it to the smallest version available.</p><p>Next, let's make a function that checks to see if a column can be downcast from a float to an integer.</p><pre><code>def float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser</code></pre><p>We're using the try/except pattern here because if we try to make a column with <code>NaN</code> values into an integer column, it'll throw an error. Â If it'd otherwise be a good candidate for turning into an integer, we should figure a value to impute for those missing values - but that'll be different for every column. Â Sometimes it'd make sense to make it 0, other times the mean or median of the column, or something else entirely.</p><p>I'd also like to direct your attention to Line 4, which has a very useful Pandas pattern - <code>if (ser == int_ser).all()</code>. Â When you do operations on Pandas columns like Equals or Greater Than, you get a new column where the operation was applied element-by-element. Â If you're trying to set up a conditional, the interpreter doesn't know what to do with an array containing <code>[True, False, True]</code> - you have to boil it down to a single value. Â So, if you wan to check if two columns are completely equal, you have to call the <code>.all()</code> method (which has a useful sibling, <code>any()</code>) to make a conditional that can actually be used to control execution.</p><p>Next, let's make a function that lets us apply a transformation to multiple columns based on a condition. Â The <code>assign</code> method is pretty awesome, and it'd be fun to not have to leave it (or, if we do, to at least replace it with a function we can pipe as part of a chain of transformations to the DataFrame as a whole).</p><pre><code class=\"language-python\">def multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n</code></pre>\n<p><code>assign</code> lets us do multiple assignments, so long as we make a dictionary of column names and target values and then unpack it. Â Really, it'd actually be easier to skip the function and go directly to using this syntax, except that I'm not aware of a method of accessing a filterable list of the DF's columns while still \"in\" the chain. Â I think future versions of Pandas' syntax will include this, as I've read they want to support more Method Chaining. Â Personally, I find the reduction in Cognitive Load is worth it, with having a lot of little modular lego-piece transformations chained together. Â </p><p>It also works as a nice foundation for other little helper functions. Â So, here's one to turn as many float columns to integers as we can.</p><pre><code class=\"language-python\">def all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[&quot;float&quot;])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>See the pattern in action! Â We decide on a transformation function, we decide on what conditions we want to apply all these transformations (we could have a hundred columns, and who wants to make a note of all that?), and then we pass it to the <code>multi-assign</code> function. Â </p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n</code></pre>\n<p>Cool! Â But we didn't actually decrease the size of our DataFrame - 64 bytes of integer takes up as many bytes as 64 bytes of float, just like how a hundred pounds of feathers weighs as much as a hundred pounds of bricks. Â What we did do is make it easier to downcast those columns later.</p><p>Next, let's make a function that takes a subset of the columns, and tries to downcast it to the smallest version that it can. Â We've got fairly small values here, so it should get some work done.</p><pre><code class=\"language-python\">def downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>Same basic pattern as before! Â But now we have two arguments - one is the <code>target_type</code>, which tells us what types to try to downcast to. Â By default, this will be the same as the <code>initial_type</code>, with one exception that we'll grab in a second!</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n</code></pre>\n<p>Alright, now we're getting somewhere! Â Wonder if we can do even better, though? Â That last column has a conspicuous name! Â And it has no values lower than 0 - maybe we could save space if we store it as an unsigned integer! Â Let's add a pipe to our chain that'll try to downcast certain integers into unsigneds...</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n</code></pre>\n<p>What do ya know, we can!</p><p>Let's see how much memory we save by doing this.</p><pre><code class=\"language-python\">df.info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n</code></pre>\n<p>vs</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n</code></pre>\n<p>152 down to 98 - we reduced it by more than 1/3rd!</p>","url":"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/","uuid":"58bbb902-99bb-404d-8a3c-232d56b6e776","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c4e57144b23df2da7332b80"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apacheâs suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c3fc99b89c81d4ccc3f64b1","title":"The Hostile Extraction of Tableau Server Data","slug":"hostile-extraction-of-tableau-server-data","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-2.jpg","excerpt":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","custom_excerpt":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","created_at_pretty":"17 January, 2019","published_at_pretty":"17 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-16T19:17:31.000-05:00","published_at":"2019-01-17T07:43:00.000-05:00","updated_at":"2019-03-28T11:06:50.000-04:00","meta_title":"The Hostile Extraction of Tableau Server Data | Hackers and Slackers","meta_description":"How to create a Python application to take back your Tableau data. Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","og_description":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","og_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-2.jpg","og_title":"The Hostile Extraction of Tableau Server Data","twitter_description":"Say no to proprietary software constraints. Say no to vendor lock. Say yes to freedom.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/tableauextraction-1-1.jpg","twitter_title":"The Hostile Extraction of Tableau Server Data","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flaskâs flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"I try my best not to hate on Tableau. It was the softwareâs combination of power\nand ease-of-use that drove me to purchase a license in the first place. Ever\nsince then, Iâm finding new and exciting ways Tableau intentionally locks users\nout of their data. \n\nI gave the Tableau Server Client Python library\n[https://tableau.github.io/server-client-python/docs/]  a spin recently in hopes\nof finding something useful. I decided to (sigh, once more) allow Tableau the\nbenefit of the doubt: after pushing four updates in a single month, maybe things\nhad changed. On the contrary, the Tableau business strategy stands strong: to be\na raging, flaming turd pile. A perfect example of this is the View  object\nTableau allows you to interact with on your server. Those familiar know that \nviews  are slang for sheets  of workbooks  stored on Tableau server. \n\nConnecting to your Tableau instance via Python to retrieve your view objects is\na piece of cake:\n\nimport tableauserverclient as TSC\ntableau_auth = TSC.TableauAuth('username', 'password')\nserver = TSC.Server('http://servername')\n\nwith server.auth.sign_in(tableau_auth):\n  all_views, pagination_item = server.views.get()\n  print([view.name for view in all_views])\n\n\nThis simple snippet lists every view object on your server. Wow! Think of what\nwe can do with all that tabular data we worked so hard to transform, rig- WRONG.\nLook at what Tableau's Python 'View Object' actually contains:\n\n * id  The identifier of the view item.\n * name  The name of the view.\n * owner_id  The id for the owner of the view.\n * preview_image  The thumbnail image for the view.\n * total_views  The usage statistics for the view. Indicates the total number of\n   times the view has been accessed.\n * workbook_id  The id of the workbook associated with the view.\n\nHOLY MOSES STOP THE PRESSES, we can get a thumbnail image  of our data?! THANK\nYOU GENEROUS TABLEAU OVERLORDS!\n\nNotice how there's no mention of, you know, the actual data.\n\nWe're going to play a game. In the wake of my time has been wasted, I feel that\nwarm tickling feeling which seems to say \"Viciously dismantle the ambitions of\nan establishment!\"  May I remind you, we're talking about the kind of\nestablishment that bills customer licenses based on the number of CPUs being\nutilized by their server infrastructure.  This is effectively recognizing the\nhorrifying and inefficient codebase behind Tableau server, and leveraging this\nflaw for monetization. Yes, you're paying more money to incentivize worst\npractices.\n\nLet's Make a Flask App. An Angry One.\nIn our last post I shared a little script to help you get started stealing data\n[https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/] \n off your own Tableau Server. That doesn't quite scratch my itch anymore. I'm\ngoing to build an interface. I want to make it easy as possible for anybody to\nsystemically rob Tableau Server of every penny its got. That's a lot of pennies\nwhen we consider the equation: data = oil + new.\n\nBefore I bore you, here's a quick demo of the MVP we're building:\n\nEach table is a view being pulled from Tableau Server.This POC demonstrates that\nit is very  possible to automate the extraction of Tableau views from Tableau\nServer. The success  message is signaling that we've successfully taken a\nTableau view and created a corresponding table in an external database. Any data\nwe manipulate in Tableau is now truly ours: we can now leverage the transforms\nwe've applied in workbooks, use this data in other applications, and utilize an\nextract scheduler to keep the data coming. We've turned a BI tool into an ETL\ntool. In other words, you can kindly take those thumbnail previews and shove it.\n\nI'll be open sourcing all of this, as is my civic duty. Let us be clear to\nenterprises: withholding freedom to one's own data is an act of war. Pricing\nmodels which reward poor craftsmanship are an insult to our intellect. For every\narrogant atrocity committed against consumers, the war will wage twice as hard.\nI should probably mention these opinions are my own.\n\nThe Proletariat Strikes Back\nGet a feel for where we're heading with the obligatory project-file-structure\ntree:\n\ntableau-exporter\nâââ application\nâ   âââ __init__.py\nâ   âââ database.py\nâ   âââ tableau.py\nâ   âââ routes.py\nâ   âââ static\nâ   â   âââ data\nâ   â   â   âââ view.csv\nâ   â   âââ dist\nâ   â   â   âââ all.css\nâ   â   â   âââ packed.js\nâ   â   âââ img\nâ   â   â   âââ tableaugithub.jpg\nâ   â   âââ js\nâ   â   â   âââ main.js\nâ   â   âââ scss\nâ   â       âââ main.scss\nâ   âââ templates\nâ       âââ export.html\nâ       âââ index.html\nâ       âââ layout.html\nâ       âââ view.html\nâââ config.ini\nâââ config.py\nâââ app.yaml\nâââ start.sh\nâââ wsgi.py\nâââ Pipfile\nâââ README.md\nâââ requirements.txt\n\n\nAs usual, we're using a classic Flask application factory  set up here.\n\nWeapons Of Choice\nLet's have a look at our core arsenal:\n\n * requests: We're achieving our goal by exploiting some loopholes exposed in\n   the Tableau REST API.\n * pandas: Will handle everything from extracting comma-separated data into a\n   CSV, render HTML tables, and output SQL.\n * flask_sqlalchemy: Used in tandem with pandas  to handle shipping our data off\n   elsewhere.\n * flask_redis: To handle session variables.\n\nInitiating our Application\nHere's how we construct our app:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_redis import FlaskRedis\n\n# Set global entities\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Initiate globals\n        db.init_app(app)\n        r.init_app(app, charset=\"utf-8\", decode_responses=True)\n\n        # Set global contexts\n        r.set('uri', app.config['SQLALCHEMY_DATABASE_URI'])\n        r.set('baseurl',  app.config['BASE_URL'])\n        r.set('username',  app.config['USERNAME'])\n        r.set('password', app.config['PASSWORD'])\n\n        # Import our modules\n        from . import routes\n        from . import tableau\n        app.register_blueprint(routes.home_blueprint)\n\n        return app\n\n\nThis should all feel like business-as-usual. The core of our application is\nsplit between routes.py, which handles views, and tableau.py, which handles the\nanti-establishment logic. Let's begin with the latter.\n\nLife, Liberty, and The Pursuit of Sick Data Pipelines\nOur good friend tableau.py  might look familiar to those who joined us last time\n[https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/]\n. tableau.py  has been busy hitting the gym since then and is looking sharp for\nprimetime:\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    \"\"\"Class for working in a Tableau instance.\"\"\"\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        \"\"\"Extract contents of a single view.\"\"\"\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        \"\"\"List all views belonging to a Tableau Site.\"\"\"\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        \"\"\"Receive Auth token to perform API requests.\"\"\"\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        \"\"\"Retrieve ID of Tableau 'site' instance.\"\"\"\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        \"\"\"Retrieve core XML for interacting with Tableau.\"\"\"\n        headers = {'Content-Type': 'application/xml'}\n        body = '<tsRequest><credentials name=\"' + cls.__username + '\" password=\"' + cls.__password + '\" ><site contentUrl=\"' + cls.__contenturl + '\" /></credentials></tsRequest>'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n\n\nI wish I could take full credit for what a shit show this class appears to be at\nfirst glance, but I assure you we've been left with no choice. For example: have\nI mentioned that Tableau's REST API returns XML so malformed that it breaks XML\nparsers? I can't tell incompetence from malicious intent at this point.\n\nHere's a method breakdown of our class:\n\n * initialize_tableau_request(): Handles initial auth and returns valuable\n   information such as site ID and API Token to be used thereafter.\n * get_site(): Extracts the site ID from XML returned by the above.\n * get_token(): Similarly extracts our token.\n * list_views(): Compiles a list of all views within a Tableau site, giving us a\n   chance to select ones for extraction.\n * get_view(): Takes a view of our choice and creates a DataFrame, which is to\n   be shipped off to a foreign database.\n\nOur Routing Logic\nMoving on we have routes.py  building the views and associated logic for our\napp:\n\nfrom flask import current_app as app\nfrom flask import render_template, Blueprint, request, Markup\nfrom flask_assets import Bundle, Environment\nfrom . import tableau\nfrom . import database\nimport pandas as pd\n\nhome_blueprint = Blueprint('home', __name__, template_folder='templates', static_folder='static')\n\nassets = Environment(app)\njs = Bundle('js/*.js', filters='jsmin', output='dist/packed.js')\nscss = Bundle('scss/*.scss', filters='libsass', output='dist/all.css')\nassets.register('scss_all', scss)\nassets.register('js_all', js)\nscss.build()\njs.build()\n\n\n@home_blueprint.route('/', methods=['GET', 'POST'])\ndef entry():\n    \"\"\"Homepage which lists all available views.\"\"\"\n    tableau_view_extractor = tableau.ExtractTableauView()\n    xml = tableau_view_extractor.initialize_tableau_request()\n    token = tableau_view_extractor.get_token(xml)\n    site = tableau_view_extractor.get_site(xml)\n    views = tableau_view_extractor.list_views(site, xml, token)\n    return render_template(\n        'index.html',\n        title=\"Here are your views.\",\n        template=\"home-template\",\n        views=views,\n        token=token,\n        xml=xml,\n        site=site\n    )\n\n\n@home_blueprint.route('/view', methods=['GET', 'POST'])\ndef view():\n    \"\"\"Displays a preview of a selected view.\"\"\"\n    site = request.args.get('site')\n    xml = request.args.get('xml')\n    view = request.args.get('view')\n    token = request.args.get('token')\n    tableau_view_extractor = tableau.ExtractTableauView()\n    view_df = tableau_view_extractor.get_view(site, xml, view, token)\n    view_df.to_csv('application/static/data/view.csv')\n    return render_template(\n        'view.html',\n        title='Your View',\n        template=\"home-template\",\n        view=view,\n        token=token,\n        xml=xml,\n        site=site,\n        view_df=Markup(view_df.to_html(index=False))\n    )\n\n\n@home_blueprint.route('/export', methods=['GET', 'POST'])\ndef export():\n    \"\"\"Exports view to external database.\"\"\"\n    view_df = pd.read_csv('application/static/data/view.csv')\n    view_df.to_sql(name='temp', con=database.engine, if_exists='replace', chunksize=50, index=True)\n    return render_template(\n        'export.html',\n        title='Success!',\n        template=\"success-template\",\n    )\n\n\nWe only have 3 pages to our application. They include our list of views, a\npreview of a single view, and a success page for when said view is exported.\nThis is all core Flask logic.\n\nPutting it On Display\nWe build our pages dynamically based on the values we pass our Jinja templates.\nThe homepage utilizes some nested loops to list the views we returned from \ntableau.py, and also makes use of query strings to pass values on to other\ntemplates.\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n<div class=\"extended-container {{template}}\">\n  <div class=\"container\">\n    <div class=\"row\">\n      <div class=\"col s12\">\n        <h1>{{title}}</h1>\n      </div>\n      <div class=\"col s12 flex-container\">\n        {% for view in views %}\n        <div class=\"download\">\n          <a href=\"{{ url_for('home.view') }}?token={{token}}&site={{site}}&view={{view.id}}&xml={{xml}}\">\n            <ul>\n              {% for key, value in view.items() %}\n              <li><span class=\"key {{key}}\">{{key}}</span> {{ value }}</li>\n              {% endfor %}\n            </ul>\n          </a>\n        </div>\n        {% endfor %}\n      </div>\n    </div>\n  </div>\n  {% endblock %}\n\n\nMoving on: our humble view.html  page has two purposes: display the selected\nview, and export it in the name of justice.\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n<div class=\"extended-container {{template}}\">\n  <div class=\"container\">\n    <div class=\"row\">\n      <div class=\"col s12\">\n        <h1>{{title}}</h1>\n        <a href=\"{{ url_for('home.export') }}\" class=\"export\"><i class=\"far fa-file-export\"></i></a>\n        {{view_df}}\n      </div>\n    </div>\n  </div>\n  {% endblock %}\n\n\nThe War is Not Over\nThis repository is open to the public and can be found here\n[https://github.com/toddbirchard/tableau-extraction]. There are still crusades\nleft ahead of us: for instance, building out this interface to accept\ncredentials via login as opposed to a config file, and the scheduling of view\nexports, as opposed to on-demand.\n\nWhere we go from here depends on what we the people decide. For all I know, I\ncould be shouting to an empty room here (I'm almost positive anybody who pays\nfor enterprise software prefers the blind eye of denial). If the opposite holds\ntrue, I dare say the revolution is only getting started.","html":"<p>I try my best not to hate on Tableau. It was the softwareâs combination of power and ease-of-use that drove me to purchase a license in the first place. Ever since then, Iâm finding new and exciting ways Tableau intentionally locks users out of their data. </p><p>I gave the <a href=\"https://tableau.github.io/server-client-python/docs/\"><strong>Tableau Server Client</strong> Python library</a> a spin recently in hopes of finding something useful. I decided to (sigh, <em>once more</em>) allow Tableau the benefit of the doubt: after pushing <strong>four updates in a single month</strong>, maybe things had changed. On the contrary, the Tableau business strategy stands strong: to be a raging, flaming turd pile. A perfect example of this is the <strong>View</strong> object Tableau allows you to interact with on your server. Those familiar know that <strong>views</strong> are slang for <em>sheets</em> of <em>workbooks</em> stored on Tableau server. </p><p>Connecting to your Tableau instance via Python to retrieve your view objects is a piece of cake:</p><pre><code class=\"language-python\">import tableauserverclient as TSC\ntableau_auth = TSC.TableauAuth('username', 'password')\nserver = TSC.Server('http://servername')\n\nwith server.auth.sign_in(tableau_auth):\n  all_views, pagination_item = server.views.get()\n  print([view.name for view in all_views])\n</code></pre>\n<p>This simple snippet lists every view object on your server. Wow! Think of what we can do with all that tabular data we worked so hard to transform, rig- <strong>WRONG</strong>. Look at what Tableau's Python 'View Object' actually contains:</p><ul><li><code>id</code> The identifier of the view item.</li><li><code>name</code> The name of the view.</li><li><code>owner_id</code> The id for the owner of the view.</li><li><code>preview_image</code> The thumbnail image for the view.</li><li><code>total_views</code> The usage statistics for the view. Indicates the total number of times the view has been accessed.</li><li><code>workbook_id</code> The id of the workbook associated with the view.</li></ul><p>HOLY MOSES STOP THE PRESSES, we can get a <strong><em>thumbnail image</em></strong> of our data?! THANK YOU GENEROUS TABLEAU OVERLORDS!</p><p>Notice how there's no mention of, you know, the <em>actual data</em>.</p><p>We're going to play a game. In the wake of my time has been wasted, I feel that warm tickling feeling which seems to say <em>\"Viciously dismantle the ambitions of an establishment!\"</em> May I remind you, we're talking about the kind of establishment that bills customer licenses based on the <strong><em>number of CPUs being utilized by their server infrastructure.</em></strong> This is effectively recognizing the horrifying and inefficient codebase behind Tableau server, and leveraging this flaw for monetization. Yes, you're paying more money to incentivize worst practices.</p><h2 id=\"let-s-make-a-flask-app-an-angry-one-\">Let's Make a Flask App. An Angry One.</h2><p>In our last post I shared <a href=\"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/\">a little script to help you get started stealing data</a> off your own Tableau Server. That doesn't quite scratch my itch anymore. I'm going to build an interface. I want to make it easy as possible for anybody to systemically rob Tableau Server of every penny its got. That's a lot of pennies when we consider the equation: <strong>data = oil + new</strong>.</p><p>Before I bore you, here's a quick demo of the MVP we're building:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/tableau.gif\" class=\"kg-image\"><figcaption>Each table is a view being pulled from Tableau Server.</figcaption></figure><p>This POC demonstrates that it is <em>very</em> possible to automate the extraction of Tableau views from Tableau Server. The <em>success</em> message is signaling that we've successfully taken a Tableau view and <strong>created a corresponding table in an external database</strong>. Any data we manipulate in Tableau is now truly ours: we can now leverage the transforms we've applied in workbooks, use this data in other applications, and utilize an extract scheduler to keep the data coming. We've turned a BI tool into an ETL tool. In other words, you can kindly take those thumbnail previews and shove it.</p><p>I'll be open sourcing all of this, as is my civic duty. Let us be clear to enterprises: withholding freedom to one's own data is an act of war. Pricing models which reward poor craftsmanship are an insult to our intellect. For every arrogant atrocity committed against consumers, the war will wage twice as hard. I should probably mention these opinions are my own.</p><h2 id=\"the-proletariat-strikes-back\">The Proletariat Strikes Back</h2><p>Get a feel for where we're heading with the obligatory project-file-structure tree:</p><pre><code class=\"language-bash\">tableau-exporter\nâââ application\nâ   âââ __init__.py\nâ   âââ database.py\nâ   âââ tableau.py\nâ   âââ routes.py\nâ   âââ static\nâ   â   âââ data\nâ   â   â   âââ view.csv\nâ   â   âââ dist\nâ   â   â   âââ all.css\nâ   â   â   âââ packed.js\nâ   â   âââ img\nâ   â   â   âââ tableaugithub.jpg\nâ   â   âââ js\nâ   â   â   âââ main.js\nâ   â   âââ scss\nâ   â       âââ main.scss\nâ   âââ templates\nâ       âââ export.html\nâ       âââ index.html\nâ       âââ layout.html\nâ       âââ view.html\nâââ config.ini\nâââ config.py\nâââ app.yaml\nâââ start.sh\nâââ wsgi.py\nâââ Pipfile\nâââ README.md\nâââ requirements.txt\n</code></pre>\n<p>As usual, we're using a classic Flask <em>application factory</em> set up here.</p><h3 id=\"weapons-of-choice\">Weapons Of Choice</h3><p>Let's have a look at our core arsenal:</p><ul><li><code>requests</code>: We're achieving our goal by exploiting some loopholes exposed in the Tableau REST API.</li><li><code>pandas</code>: Will handle everything from extracting comma-separated data into a CSV, render HTML tables, and output SQL.</li><li><code>flask_sqlalchemy</code>: Used in tandem with <em>pandas</em> to handle shipping our data off elsewhere.</li><li><code>flask_redis</code>: To handle session variables.</li></ul><h3 id=\"initiating-our-application\">Initiating our Application</h3><p>Here's how we construct our app:</p><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_redis import FlaskRedis\n\n# Set global entities\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Initiate globals\n        db.init_app(app)\n        r.init_app(app, charset=&quot;utf-8&quot;, decode_responses=True)\n\n        # Set global contexts\n        r.set('uri', app.config['SQLALCHEMY_DATABASE_URI'])\n        r.set('baseurl',  app.config['BASE_URL'])\n        r.set('username',  app.config['USERNAME'])\n        r.set('password', app.config['PASSWORD'])\n\n        # Import our modules\n        from . import routes\n        from . import tableau\n        app.register_blueprint(routes.home_blueprint)\n\n        return app\n</code></pre>\n<p>This should all feel like business-as-usual. The core of our application is split between <code>routes.py</code>, which handles views, and <code>tableau.py</code>, which handles the anti-establishment logic. Let's begin with the latter.</p><h2 id=\"life-liberty-and-the-pursuit-of-sick-data-pipelines\">Life, Liberty, and The Pursuit of Sick Data Pipelines</h2><p>Our good friend <code>tableau.py</code> might look familiar to those who joined us <a href=\"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/\">last time</a>. <code>tableau.py</code> has been busy hitting the gym since then and is looking sharp for primetime:</p><pre><code class=\"language-python\">import requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    &quot;&quot;&quot;Class for working in a Tableau instance.&quot;&quot;&quot;\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        &quot;&quot;&quot;Extract contents of a single view.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        &quot;&quot;&quot;List all views belonging to a Tableau Site.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        &quot;&quot;&quot;Receive Auth token to perform API requests.&quot;&quot;&quot;\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        &quot;&quot;&quot;Retrieve ID of Tableau 'site' instance.&quot;&quot;&quot;\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        &quot;&quot;&quot;Retrieve core XML for interacting with Tableau.&quot;&quot;&quot;\n        headers = {'Content-Type': 'application/xml'}\n        body = '&lt;tsRequest&gt;&lt;credentials name=&quot;' + cls.__username + '&quot; password=&quot;' + cls.__password + '&quot; &gt;&lt;site contentUrl=&quot;' + cls.__contenturl + '&quot; /&gt;&lt;/credentials&gt;&lt;/tsRequest&gt;'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n</code></pre>\n<p>I wish I could take full credit for what a shit show this class appears to be at first glance, but I assure you we've been left with no choice. For example: have I mentioned that Tableau's REST API returns XML so malformed that it breaks XML parsers? I can't tell incompetence from malicious intent at this point.</p><p>Here's a method breakdown of our class:</p><ul><li><code>initialize_tableau_request()</code>: Handles initial auth and returns valuable information such as site ID and API Token to be used thereafter.</li><li><code>get_site()</code>: Extracts the site ID from XML returned by the above.</li><li><code>get_token()</code>: Similarly extracts our token.</li><li><code>list_views()</code>: Compiles a list of all views within a Tableau site, giving us a chance to select ones for extraction.</li><li><code>get_view()</code>: Takes a view of our choice and creates a DataFrame, which is to be shipped off to a foreign database.</li></ul><h2 id=\"our-routing-logic\">Our Routing Logic</h2><p>Moving on we have <code>routes.py</code> building the views and associated logic for our app:</p><pre><code class=\"language-python\">from flask import current_app as app\nfrom flask import render_template, Blueprint, request, Markup\nfrom flask_assets import Bundle, Environment\nfrom . import tableau\nfrom . import database\nimport pandas as pd\n\nhome_blueprint = Blueprint('home', __name__, template_folder='templates', static_folder='static')\n\nassets = Environment(app)\njs = Bundle('js/*.js', filters='jsmin', output='dist/packed.js')\nscss = Bundle('scss/*.scss', filters='libsass', output='dist/all.css')\nassets.register('scss_all', scss)\nassets.register('js_all', js)\nscss.build()\njs.build()\n\n\n@home_blueprint.route('/', methods=['GET', 'POST'])\ndef entry():\n    &quot;&quot;&quot;Homepage which lists all available views.&quot;&quot;&quot;\n    tableau_view_extractor = tableau.ExtractTableauView()\n    xml = tableau_view_extractor.initialize_tableau_request()\n    token = tableau_view_extractor.get_token(xml)\n    site = tableau_view_extractor.get_site(xml)\n    views = tableau_view_extractor.list_views(site, xml, token)\n    return render_template(\n        'index.html',\n        title=&quot;Here are your views.&quot;,\n        template=&quot;home-template&quot;,\n        views=views,\n        token=token,\n        xml=xml,\n        site=site\n    )\n\n\n@home_blueprint.route('/view', methods=['GET', 'POST'])\ndef view():\n    &quot;&quot;&quot;Displays a preview of a selected view.&quot;&quot;&quot;\n    site = request.args.get('site')\n    xml = request.args.get('xml')\n    view = request.args.get('view')\n    token = request.args.get('token')\n    tableau_view_extractor = tableau.ExtractTableauView()\n    view_df = tableau_view_extractor.get_view(site, xml, view, token)\n    view_df.to_csv('application/static/data/view.csv')\n    return render_template(\n        'view.html',\n        title='Your View',\n        template=&quot;home-template&quot;,\n        view=view,\n        token=token,\n        xml=xml,\n        site=site,\n        view_df=Markup(view_df.to_html(index=False))\n    )\n\n\n@home_blueprint.route('/export', methods=['GET', 'POST'])\ndef export():\n    &quot;&quot;&quot;Exports view to external database.&quot;&quot;&quot;\n    view_df = pd.read_csv('application/static/data/view.csv')\n    view_df.to_sql(name='temp', con=database.engine, if_exists='replace', chunksize=50, index=True)\n    return render_template(\n        'export.html',\n        title='Success!',\n        template=&quot;success-template&quot;,\n    )\n</code></pre>\n<p>We only have 3 pages to our application. They include our list of views, a preview of a single view, and a success page for when said view is exported. This is all core Flask logic.</p><h2 id=\"putting-it-on-display\">Putting it On Display</h2><p>We build our pages dynamically based on the values we pass our Jinja templates. The homepage utilizes some nested loops to list the views we returned from <code>tableau.py</code>, and also makes use of query strings to pass values on to other templates.</p><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n&lt;div class=&quot;extended-container {{template}}&quot;&gt;\n  &lt;div class=&quot;container&quot;&gt;\n    &lt;div class=&quot;row&quot;&gt;\n      &lt;div class=&quot;col s12&quot;&gt;\n        &lt;h1&gt;{{title}}&lt;/h1&gt;\n      &lt;/div&gt;\n      &lt;div class=&quot;col s12 flex-container&quot;&gt;\n        {% for view in views %}\n        &lt;div class=&quot;download&quot;&gt;\n          &lt;a href=&quot;{{ url_for('home.view') }}?token={{token}}&amp;site={{site}}&amp;view={{view.id}}&amp;xml={{xml}}&quot;&gt;\n            &lt;ul&gt;\n              {% for key, value in view.items() %}\n              &lt;li&gt;&lt;span class=&quot;key {{key}}&quot;&gt;{{key}}&lt;/span&gt; {{ value }}&lt;/li&gt;\n              {% endfor %}\n            &lt;/ul&gt;\n          &lt;/a&gt;\n        &lt;/div&gt;\n        {% endfor %}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  {% endblock %}\n</code></pre>\n<p>Moving on: our humble <code>view.html</code> page has two purposes: display the selected view, and export it in the name of justice.</p><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n&lt;div class=&quot;extended-container {{template}}&quot;&gt;\n  &lt;div class=&quot;container&quot;&gt;\n    &lt;div class=&quot;row&quot;&gt;\n      &lt;div class=&quot;col s12&quot;&gt;\n        &lt;h1&gt;{{title}}&lt;/h1&gt;\n        &lt;a href=&quot;{{ url_for('home.export') }}&quot; class=&quot;export&quot;&gt;&lt;i class=&quot;far fa-file-export&quot;&gt;&lt;/i&gt;&lt;/a&gt;\n        {{view_df}}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  {% endblock %}\n</code></pre>\n<h2 id=\"the-war-is-not-over\">The War is Not Over</h2><p>This repository is open to the public and can be found <a href=\"https://github.com/toddbirchard/tableau-extraction\">here</a>. There are still crusades left ahead of us: for instance, building out this interface to accept credentials via login as opposed to a config file, and the scheduling of view exports, as opposed to on-demand.</p><p>Where we go from here depends on what we the people decide. For all I know, I could be shouting to an empty room here (I'm almost positive anybody who pays for enterprise software prefers the blind eye of denial). If the opposite holds true, I dare say the revolution is only getting started.</p>","url":"https://hackersandslackers.com/hostile-extraction-of-tableau-server-data/","uuid":"23914fde-b90e-4496-9a7d-56d6ae3765d9","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3fc99b89c81d4ccc3f64b1"}},{"node":{"id":"Ghost__Post__5c3d0b441719dc6b38ee53b6","title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","slug":"psycopg2-postgres-python-the-old-fashioned-way","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","custom_excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","created_at_pretty":"14 January, 2019","published_at_pretty":"15 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-14T17:20:52.000-05:00","published_at":"2019-01-15T15:57:34.000-05:00","updated_at":"2019-03-28T14:46:14.000-04:00","meta_title":"Psycopg2: PostgreSQL & Python the Old Way | Hackers and Slackers","meta_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","og_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","twitter_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","twitter_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Last time we met, we joyfully shared a little tirade\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/] \nabout missing out on functionality provided to us by libraries such as \nSQLAlchemy, and the advantages of interacting with databases where ORMs are\ninvolved. I stand by that sentiment, but Iâll now directly contradict myself by\nsharing some tips on using vanilla Psycopg2  to interact with databases. \n\nWe never know when weâll be stranded on a desert island without access to\nSQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps\nyouâre part of a development team stuck in a certain way of doing things which\ndoesn't include utilize SQLAlchemy. Whatever the situation may be, weâre here\nfor you. \n\nThe Quintessential Boilerplate\nNo matter the type of database or the library, the boilerplate code for\nconnecting to databases remains mostly the same. To some extent, this even holds\ntrue across programming languages. Let's look at a barebones example while\nignoring the library at hand:\n\nimport SomeDatabaseLibrary\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nIn the above example, we could swap SomeDatabaseLibrary  with either Psycopg2 \nor PyMySQL  just the same. If we compare this to our example with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], it's easy to see that the\nbasics of utilizing connections, cursors, and the methods to close them\ntranscend libraries. If you know the basics of one, you know them all.\n\nIf you'd like to keep your connection logic separate (as I do), we can cleanly\nbreak the logic of handling connections out to a separate function. This time,\nwe'll replace SomeDatabaseLibrary  with Psycopg2  to produce some working code:\n\nimport psycopg2\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        \"\"\"Encapsulated connection management logic.\"\"\"\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(\"ERROR: Could not connect to Postgres.\")\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nPsycopg2 Extras\nPsycopg2 has many useful features via a library called psycopg2.extras\n[http://initd.org/psycopg/docs/extras.html]. My personal favorite of these\nextras is the DictCursor, which renders the rows being returned by our query as\nPython dictionaries  as opposed to lists. \n\nUsing DictCursor to Return More Useful Results\nWhen using a DictCursor, the key  is always the column name, and the value is\nthe value of that column in that particular row.\n\nTo use extras, we import psycopg2.extras.\n\nThen, we turn our attention to the following line:\n\nself.conn.cursor() as cur:\n\n\nWithin cursor, we can pass an attribute named cursor_factory  Â and set it as\nsuch:\n\nconn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n\n\nWhile our cursor is open, all rows returned by the query will be returned as\ndictionaries. For example, the row  in the above example will be returned as a\ndict. To demonstrate, here's what a query on this exact post  you're reading now\nlooks like when returned as a Dict:\n\n{\n    title: \"Psycopg2: Postgres & Python the Old Fashioned Way\",\n    slug: \"psycopg2-postgres-python-the-old-fashioned-way\",\n    feature_image: \"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n    status: \"draft\",\n    created_at: \"2019-01-14 22:20:52\",\n    custom_excerpt: \"Managing Postgres Database connections with Psycopg2\"\n}\n\n\nCompare this to what we would've seen had we not used DictCursor:\n\n[\"Psycopg2: Postgres & Python the Old Fashioned Way\",\n\"psycopg2-postgres-python-the-old-fashioned-way\",\n\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n\"draft\",\n\"2019-01-14 22:20:52\",\n\"Managing Postgres Database connections with Psycopg2\"]\n\n\nYes, it's a list, and thereby much less useful. Even from a readability\nstandpoint, I (the human user) have no idea what these values represent unless\ncomparing them to the table schema. Even worse would be compiling CSVs or even\nPandas Dataframes this way. When building a table made of lists, you set your\nheaders and hope that every row to come matches the number of header columns\none-to-one. Otherwise, it's entirely unclear as to which value belongs to which\ncolumn.\n\nOther Psycopg2 Extras\nThere are plenty more Psycopg2 extras where that came from; it's mostly up to\nyou to decide which are worth your while.\n\nFor example, another extra which might be of interest could be \npsycopg2.extras.LoggingConnection, useful for debugging connection statuses and\nerrors as you work through your program.\n\nThere's even a JSON Adaptation  extra, which provides support for leveraging\nJSON data in building queries:\n\ncur.execute(\"insert into mytable (jsondata) values (%s)\",\n    [Json({'a': 100})])\n\n\nI don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike\nextras I'm missing, feel free to call them out in the COMMENTS BELOW!  (Hah!\nI've always wanted to say that).\n\nA Few More Fundamental Useful Things\nSomething worth visiting is the ability to upload CSVs into Postgres to create\ntables. We can accomplish this via the built-in method copy_expert.\n\nFrom CSV to Postgres Table\nTo save a CSV to Postgres table, we need to begin with a basic SQL query saved\nin our project as a variable:\n\nCOPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n\n\nAs should be familiar, %s  represents a value we can pass in later. With this\nraw query, we're only missing two more values:\n\n * The path of our CSV file to be uploaded\n * The name of the table we'd like to upload to in Postgres\n\nCheck out how we use copy_expert  here to put it all together:\n\nsql = \"COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','\"\nfile = open('files/myfile.csv', \"r\")\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(\"truncate \" + table + \";\")\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n\n\nNotice that I opt to truncate the existing table before uploading the new data,\nas seen by cur.execute(\"truncate \" + table + \";\"). Without doing this, we would\nbe uploading the same CSV to the same table forever, creating duplicate rows\nover and over.\n\nWhat if The Table Doesn't Exist?\nUgh, of course  this would come up. The truth is (to the best of my knowledge),\nthere aren't many native things Psycopg2 has to offer to make this process easy.\n \n\nRecall that creating a table has a syntax similar to this:\n\nCREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n\n\nIt's not impossible to build this string yourself in Python. It just entails a\nlot of iterating over whichever dynamic data structure you have coming through,\ndetermining the correct data type per column, and then the unavoidable task of\nsetting your Primary  and Unique  keys if applicable. This is where my patience\nends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey,\nit's possible! I just don't feel like writing about it. :).\n\nGodspeed to You, Brave Warrior\nFor those about to Psycopg2, we salute you. That is unless the choice is\nself-inflicted. In that case, perhaps it's best we don't work together any time\nsoon.","html":"<p>Last time we met, we joyfully <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">shared a little tirade</a> about missing out on functionality provided to us by libraries such as <strong>SQLAlchemy</strong>, and the advantages of interacting with databases where ORMs are involved. I stand by that sentiment, but Iâll now directly contradict myself by sharing some tips on using vanilla <strong>Psycopg2</strong> to interact with databases. </p><p>We never know when weâll be stranded on a desert island without access to SQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps youâre part of a development team stuck in a certain way of doing things which doesn't include utilize SQLAlchemy. Whatever the situation may be, weâre here for you. </p><h2 id=\"the-quintessential-boilerplate\">The Quintessential Boilerplate</h2><p>No matter the type of database or the library, the boilerplate code for connecting to databases remains mostly the same. To some extent, this even holds true across programming languages. Let's look at a barebones example while ignoring the library at hand:</p><pre><code class=\"language-python\">import SomeDatabaseLibrary\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<p>In the above example, we could swap <code>SomeDatabaseLibrary</code> with either <code>Psycopg2</code> or <code>PyMySQL</code> just the same. If we compare this to <a href=\"https://hackersandslackers.com/using-pymysql/\">our example with PyMySQL</a>, it's easy to see that the basics of utilizing <strong>connections</strong>, <strong>cursors</strong>, and the methods to close them transcend libraries. If you know the basics of one, you know them all.</p><p>If you'd like to keep your connection logic separate (as I do), we can cleanly break the logic of handling connections out to a separate function. This time, we'll replace <code>SomeDatabaseLibrary</code> with <code>Psycopg2</code> to produce some working code:</p><pre><code class=\"language-python\">import psycopg2\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        &quot;&quot;&quot;Encapsulated connection management logic.&quot;&quot;&quot;\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(&quot;ERROR: Could not connect to Postgres.&quot;)\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<h2 id=\"psycopg2-extras\">Psycopg2 Extras</h2><p>Psycopg2 has many useful features via a library called <a href=\"http://initd.org/psycopg/docs/extras.html\">psycopg2.extras</a>. My personal favorite of these extras is the <code>DictCursor</code>, which renders the rows being returned by our query as Python <em>dictionaries</em> as opposed to <em>lists. </em></p><h3 id=\"using-dictcursor-to-return-more-useful-results\">Using DictCursor to Return More Useful Results</h3><p>When using a DictCursor, the <em>key</em> is always the column name, and the <em>value </em>is the value of that column in that particular row.</p><p>To use extras, we <code>import psycopg2.extras</code>.</p><p>Then, we turn our attention to the following line:</p><pre><code class=\"language-python\">self.conn.cursor() as cur:\n</code></pre>\n<p>Within <code>cursor</code>, we can pass an attribute named <code>cursor_factory</code> Â and set it as such:</p><pre><code class=\"language-python\">conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n</code></pre>\n<p>While our cursor is open, all rows returned by the query will be returned as dictionaries. For example, the <strong>row</strong> in the above example will be returned as a dict. To demonstrate, here's what a query on this <em>exact post</em> you're reading now looks like when returned as a Dict:</p><pre><code class=\"language-python\">{\n    title: &quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n    slug: &quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n    feature_image: &quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n    status: &quot;draft&quot;,\n    created_at: &quot;2019-01-14 22:20:52&quot;,\n    custom_excerpt: &quot;Managing Postgres Database connections with Psycopg2&quot;\n}\n</code></pre>\n<p>Compare this to what we would've seen had we not used <code>DictCursor</code>:</p><pre><code class=\"language-python\">[&quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n&quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n&quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n&quot;draft&quot;,\n&quot;2019-01-14 22:20:52&quot;,\n&quot;Managing Postgres Database connections with Psycopg2&quot;]\n</code></pre>\n<p>Yes, it's a list, and thereby much less useful. Even from a readability standpoint, I (the human user) have no idea what these values represent unless comparing them to the table schema. Even worse would be compiling CSVs or even Pandas Dataframes this way. When building a table made of lists, you set your headers and hope that every row to come matches the number of header columns one-to-one. Otherwise, it's entirely unclear as to which value belongs to which column.</p><h3 id=\"other-psycopg2-extras\">Other Psycopg2 Extras</h3><p>There are plenty more Psycopg2 extras where that came from; it's mostly up to you to decide which are worth your while.</p><p>For example, another extra which might be of interest could be <code>psycopg2.extras.LoggingConnection</code>, useful for debugging connection statuses and errors as you work through your program.</p><p>There's even a <strong>JSON Adaptation</strong> extra, which provides support for leveraging JSON data in building queries:</p><pre><code class=\"language-python\">cur.execute(&quot;insert into mytable (jsondata) values (%s)&quot;,\n    [Json({'a': 100})])\n</code></pre>\n<p>I don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike extras I'm missing, feel free to call them out in the <strong><em>COMMENTS BELOW!</em></strong> (Hah! I've always wanted to say that).</p><h2 id=\"a-few-more-fundamental-useful-things\">A Few More Fundamental Useful Things</h2><p>Something worth visiting is the ability to upload CSVs into Postgres to create tables. We can accomplish this via the built-in method <code>copy_expert</code>.</p><h3 id=\"from-csv-to-postgres-table\">From CSV to Postgres Table</h3><p>To save a CSV to Postgres table, we need to begin with a basic SQL query saved in our project as a variable:</p><pre><code class=\"language-sql\">COPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n</code></pre>\n<p>As should be familiar, <code>%s</code> represents a value we can pass in later. With this raw query, we're only missing two more values:</p><ul><li>The path of our CSV file to be uploaded</li><li>The name of the table we'd like to upload to in Postgres</li></ul><p>Check out how we use <code>copy_expert</code> here to put it all together:</p><pre><code class=\"language-python\">sql = &quot;COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','&quot;\nfile = open('files/myfile.csv', &quot;r&quot;)\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(&quot;truncate &quot; + table + &quot;;&quot;)\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n</code></pre>\n<p>Notice that I opt to truncate the existing table before uploading the new data, as seen by <code>cur.execute(\"truncate \" + table + \";\")</code>. Without doing this, we would be uploading the same CSV to the same table forever, creating duplicate rows over and over.</p><h3 id=\"what-if-the-table-doesn-t-exist\">What if The Table Doesn't Exist?</h3><p>Ugh, of <em>course</em> this would come up. The truth is (to the best of my knowledge), there aren't many native things Psycopg2 has to offer to make this process easy. </p><p>Recall that creating a table has a syntax similar to this:</p><pre><code class=\"language-sql\">CREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n</code></pre>\n<p>It's not impossible to build this string yourself in Python. It just entails a lot of iterating over whichever dynamic data structure you have coming through, determining the correct data type per column, and then the unavoidable task of setting your <strong>Primary</strong> and <strong>Unique</strong> keys if applicable. This is where my patience ends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey, it's possible! I just don't feel like writing about it. :).</p><h2 id=\"godspeed-to-you-brave-warrior\">Godspeed to You, Brave Warrior</h2><p>For those about to Psycopg2, we salute you. That is unless the choice is self-inflicted. In that case, perhaps it's best we don't work together any time soon.</p>","url":"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/","uuid":"f07736c5-c167-4fe9-b932-1b6b4d95e3ff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3d0b441719dc6b38ee53b6"}},{"node":{"id":"Ghost__Post__5c3409a094d3e847951adf44","title":"Pythonic Database Management with SQLAlchemy","slug":"pythonic-database-management-with-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","excerpt":"The iconic Python library for handling any conceivable database interaction.","custom_excerpt":"The iconic Python library for handling any conceivable database interaction.","created_at_pretty":"08 January, 2019","published_at_pretty":"09 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-07T21:23:28.000-05:00","published_at":"2019-01-09T08:00:00.000-05:00","updated_at":"2019-03-28T11:17:45.000-04:00","meta_title":"Pythonic Database Management with SQLAlchemy | Hackers and Slackers","meta_description":"The iconic Python library for handling any conceivable database interaction.","og_description":"The iconic Python library for handling any conceivable database interaction.","og_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","og_title":"Pythonic Database Management with SQLAlchemy","twitter_description":"The iconic Python library for handling any conceivable database interaction.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-1.jpg","twitter_title":"Pythonic Database Management with SQLAlchemy","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we've taken for granted thus far on Hackers and Slackers is a library\nmost data professionals have accepted as an undisputed standard: SQLAlchemy\n[https://www.sqlalchemy.org/].\n\nIn the past, we've covered database connection management and querying using\nlibraries such as PyMySQL [https://hackersandslackers.com/using-pymysql/]  and \nPsycopg2\n[https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/]\n, both of which do an excellent job of interacting with databases just as we'd\nexpect them to. The nature of opening/closing DB connections and working with\ncursors hasn't changed much in the past few decades (nearly the lifespan of\nrelational databases themselves). While boilerplate is boring, at least it has\nremained consistent, one might figure. That may  have been the case, but the\nphilosophical boom of MVC frameworks nearly a decade ago sparked the emergence\nof popularity for ORMs. While the world was singing praises of object-oriented\nprogramming, containing database-oriented functionality within objects must have\nbeen a wet dream.\n\nThe only thing shocking about SQLAlchemy's popularity is its flip side: the\ncontingency of those functioning without  SQLAlchemy as a part of their regular\nstack. Whether this stems from unawareness or active reluctance to change, data\nteams using Python without a proper ORM are surprisingly prevalent. It's easy to\nforget the reality of the workforce when our interactions with other\nprofessionals come mostly from blogs published by those at the top of their\nfield.\n\nI realize the \"this is how we've always done it\" attitude is a clichÃ© with no\nshortage of commentary. Tales of adopting new (relatively speaking) practices\ndominate Silicon Valley blogs every day- it's the manner in which this is\nmanifested, however, that catches me off guard. In this case, resistance to a\nsingle Python library can shed light on a frightening mental model that has\nimplications up and down a corporation's stack.\n\nPutting The 'M' In MVC\nFrameworks which enforce a Model-View-Controller have held undisputed consensus\nfor long enough: none of us need to recap why creating apps this way is\nunequivocally correct. To understand why side-stepping an ORM is so significant,\nlet's recall what ORM stands for:\n\n> Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a\ntechnique that connects the rich objects of an application to tables in a\nrelational database management system. Using ORM, the properties and\nrelationships of the objects in an application can be easily stored and\nretrieved from a database without writing SQL statements directly and with less\noverall database access code. - Active Record\n[https://guides.rubyonrails.org/active_record_basics.html]\nORMs allow us to interact with databases simply by modifying objects in code\n(such as classes) as opposed to generating SQL queries by hand for each database\ninteraction. Bouncing from application code to SQL is a major context switch,\nand the more interactions we introduce, the more out of control our app becomes.\n \n\nTo illustrate the alternative to this using models, I'll use an example offered\nby Flask-SQLAlchemy. Let's say we have a table of users which contains columns\nfor id, username,  and email. A model for such a table would look as such:\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '<User %r>' % self.username\n\n\nThe 'model' is an object representing the structure of a single entry in our\ntable. Once our model exists, this is all it takes to create an entry:\n\nnewuser = User(username='admin', email='admin@example.com')\n\n\nThat's a single readable line of code without writing a single line of SQL.\nCompare this to the alternative, which would be to use Psycopg2:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\ndef query_function(query):\n  \"\"\"Runs a database query.\"\"\"\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n\n\nSure, query_function()  only needs to be set once, but compare the readability\nof using a model to the following:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\nquery_function(query)\n\n\nDespite achieving the same effect, the latter is much less readable or\nmaintainable by human beings. Building an application around raw string queries\ncan quickly become a nightmare.\n\nIntegration With Other Data Libraries\nWhen it comes to golden standards of Python libraries, there is none more\nquintessential to data analysis than Pandas. The pairing of Pandas and\nSQLAlchemy is standard to the point where Pandas has built-in integrations to\ninteract with data from SQLAlchemy. Here's what it takes to turn a database\ntable into a Pandas dataframe with SQLAlchemy as our connector:\n\ndf = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n\n\nOnce again, a single line of Python code!\n\nWriting Queries Purely in Python\nSo far by using SQLAlchemy, we haven't needed to write a single line of SQL: how\nfar could we take this? As far as we want, in fact. SQLAlchemy contains what\nthey've dubbed as function-based query construction, which is to say we can\nconstruct nearly any conceivable SQL query purely in Python by using the methods\noffered to us. For example, here's an update query:\n\nstmt = users.update().values(fullname=\"Fullname: \" + users.c.name)\nconn.execute(stmt)\n\n\nCheck the  full reference to see what I mean\n[https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates].\nEvery query you've ever needed to write: it's all there. All of it.\n\nSimple Connection Management\nSeeing as how we all now agree that SQLAlchemy is beneficial to our workflow,\nlet's visit square one and see how simple it is to manage connections. The two\nkey words to remember here are engines  and sessions.\n\nThe Engine\nAn engine in SQLAlchemy is merely a bare-bones object representing our database.\nMaking SQLAlchemy aware of our database is as simple as these two lines:\n\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\nThe Engine can interact with our database by accepting a simple URI. Once engine \n exists, we could in theory use engine exclusively via functions such as \nengine.connect()  and engine.execute().\n\nSessions\nTo interact with our database in a Pythonic manner via the ORM, we'll need to\ncreate a session  from the engine we just declared. Thus our code expands:\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n\n\nThat's all it takes! Now just as before, we can use SQLAlchemy's ORM and\nbuilt-in functions to make simple interacts:\n\nnew_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n\n\nTakeaway Goodies\nIt's worth mentioning that SQLAlchemy works with nearly every type of database,\nand does so by leveraging the base Python library for the respective type of\ndatabase. For example, it probably seems to the outsider that we've spent some\ntime shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a\nPostgres database, it is using the Psycopg2 library under the hood to manage the\nboilerplate for us. The same goes for every other type of relational database\n[https://docs.sqlalchemy.org/en/latest/core/engines.html]  along with their\nstandard libraries.\n\nThere are plenty of more reasons [https://www.sqlalchemy.org/features.html]  why\nSQLAlchemy is beneficial to the point where it is arguably critical to data\nanalysis workflows. The critical point to be made here is that leaving\nSQLAlchemy out of any data workflow only hurts the person writing the code, or\nmore importantly, all those who come after.","html":"<p>Something we've taken for granted thus far on Hackers and Slackers is a library most data professionals have accepted as an undisputed standard: <strong><a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a></strong>.</p><p>In the past, we've covered database connection management and querying using libraries such as <a href=\"https://hackersandslackers.com/using-pymysql/\"><strong>PyMySQL</strong></a> and <strong><a href=\"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/\">Psycopg2</a></strong>, both of which do an excellent job of interacting with databases just as we'd expect them to. The nature of opening/closing DB connections and working with cursors hasn't changed much in the past few decades (nearly the lifespan of relational databases themselves). While boilerplate is boring, at least it has remained consistent, one might figure. That <strong><em>may</em></strong> have been the case, but the philosophical boom of MVC frameworks nearly a decade ago sparked the emergence of popularity for ORMs. While the world was singing praises of object-oriented programming, containing database-oriented functionality within objects must have been a wet dream.</p><p>The only thing shocking about SQLAlchemy's popularity is its flip side: the contingency of those functioning <em>without</em> SQLAlchemy as a part of their regular stack. Whether this stems from unawareness or active reluctance to change, data teams using Python without a proper ORM are surprisingly prevalent. It's easy to forget the reality of the workforce when our interactions with other professionals come mostly from blogs published by those at the top of their field.</p><p>I realize the \"this is how we've always done it\" attitude is a clichÃ© with no shortage of commentary. Tales of adopting new (relatively speaking) practices dominate Silicon Valley blogs every day- it's the manner in which this is manifested, however, that catches me off guard. In this case, resistance to a single Python library can shed light on a frightening mental model that has implications up and down a corporation's stack.</p><h2 id=\"putting-the-m-in-mvc\">Putting The 'M' In MVC</h2><p>Frameworks which enforce a Model-View-Controller have held undisputed consensus for long enough: none of us need to recap why creating apps this way is unequivocally correct. To understand why side-stepping an ORM is so significant, let's recall what ORM stands for:</p><blockquote><em>Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a technique that connects the rich objects of an application to tables in a relational database management system. Using ORM, the properties and relationships of the objects in an application can be easily stored and retrieved from a database without writing SQL statements directly and with less overall database access code. <strong>- <a href=\"https://guides.rubyonrails.org/active_record_basics.html\">Active Record</a></strong></em></blockquote><p>ORMs allow us to interact with databases simply by modifying objects in code (such as classes) as opposed to generating SQL queries by hand for each database interaction. Bouncing from application code to SQL is a <em>major context switch</em>, and the more interactions we introduce, the more out of control our app becomes. </p><p>To illustrate the alternative to this using models, I'll use an example offered by <strong>Flask-SQLAlchemy</strong>. Let's say we have a table of users which contains columns for <strong>id, username,</strong> and <strong>email. </strong>A model for such a table would look as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">class User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '&lt;User %r&gt;' % self.username\n</code></pre>\n<!--kg-card-end: markdown--><p>The 'model' is an object representing the structure of a single entry in our table. Once our model exists, this is all it takes to create an entry:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">newuser = User(username='admin', email='admin@example.com')\n</code></pre>\n<!--kg-card-end: markdown--><p>That's a single readable line of code without writing a single line of SQL. Compare this to the alternative, which would be to use Psycopg2:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\ndef query_function(query):\n  &quot;&quot;&quot;Runs a database query.&quot;&quot;&quot;\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Sure, <code>query_function()</code> only needs to be set once, but compare the readability of using a model to the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Despite achieving the same effect, the latter is much less readable or maintainable by human beings. Building an application around raw string queries can quickly become a nightmare.</p><h2 id=\"integration-with-other-data-libraries\">Integration With Other Data Libraries</h2><p>When it comes to golden standards of Python libraries, there is none more quintessential to data analysis than Pandas. The pairing of Pandas and SQLAlchemy is standard to the point where Pandas has built-in integrations to interact with data from SQLAlchemy. Here's what it takes to turn a database table into a Pandas dataframe with SQLAlchemy as our connector:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n</code></pre>\n<!--kg-card-end: markdown--><p>Once again, a single line of Python code!</p><h2 id=\"writing-queries-purely-in-python\">Writing Queries Purely in Python</h2><p>So far by using SQLAlchemy, we haven't needed to write a single line of SQL: how far could we take this? As far as we want, in fact. SQLAlchemy contains what they've dubbed as <strong>function-based query construction, </strong>which is to say we can construct nearly any conceivable SQL query purely in Python by using the methods offered to us. For example, here's an update query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">stmt = users.update().values(fullname=&quot;Fullname: &quot; + users.c.name)\nconn.execute(stmt)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check the<a href=\"https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates\"> full reference to see what I mean</a>. Every query you've ever needed to write: it's all there. All of it.</p><h2 id=\"simple-connection-management\">Simple Connection Management</h2><p>Seeing as how we all now agree that SQLAlchemy is beneficial to our workflow, let's visit square one and see how simple it is to manage connections. The two key words to remember here are <strong>engines</strong> and <strong>sessions</strong>.</p><h3 id=\"the-engine\">The Engine</h3><p>An engine in SQLAlchemy is merely a bare-bones object representing our database. Making SQLAlchemy aware of our database is as simple as these two lines:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>The Engine can interact with our database by accepting a simple URI. Once <code>engine</code> exists, we could in theory use engine exclusively via functions such as <code>engine.connect()</code> and <code>engine.execute()</code>.</p><h3 id=\"sessions\">Sessions</h3><p>To interact with our database in a Pythonic manner via the ORM, we'll need to create a <strong>session</strong> from the <strong>engine </strong>we just declared. Thus our code expands:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n</code></pre>\n<!--kg-card-end: markdown--><p>That's all it takes! Now just as before, we can use SQLAlchemy's ORM and built-in functions to make simple interacts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">new_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"takeaway-goodies\">Takeaway Goodies</h2><p>It's worth mentioning that SQLAlchemy works with nearly every type of database, and does so by leveraging the base Python library for the respective type of database. For example, it probably seems to the outsider that we've spent some time shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a Postgres database, it is using the Psycopg2 library under the hood to manage the boilerplate for us. The same goes for <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\">every other type of relational database</a> along with their standard libraries.</p><p>There are <a href=\"https://www.sqlalchemy.org/features.html\">plenty of more reasons</a> why SQLAlchemy is beneficial to the point where it is arguably critical to data analysis workflows. The critical point to be made here is that leaving SQLAlchemy out of any data workflow only hurts the person writing the code, or more importantly, all those who come after.</p>","url":"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/","uuid":"7246d9db-39cb-44aa-9da8-cf87df00eeff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3409a094d3e847951adf44"}}]}},"pageContext":{"slug":"python","limit":12,"skip":0,"numberOfPages":7,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/python/page/2/"}}