{"data":{"ghostTag":{"slug":"pandas","name":"Pandas","visibility":"public","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c95e08ef654036aa06c6a02","title":"Building an ETL Pipeline: From JIRA to SQL","slug":"building-an-etl-pipeline-from-jira-to-postgresql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-3.jpg","excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","custom_excerpt":"An example data pipeline which extracts data from the JIRA Cloud API and loads it to a SQL database.","created_at_pretty":"23 March, 2019","published_at_pretty":"28 March, 2019","updated_at_pretty":"09 April, 2019","created_at":"2019-03-23T03:30:22.000-04:00","published_at":"2019-03-28T04:15:00.000-04:00","updated_at":"2019-04-08T23:34:47.000-04:00","meta_title":"Building an ETL Pipeline: From JIRA to SQL | Hackers and Slackers","meta_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","og_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-2.jpg","og_title":"Building an ETL Pipeline: From JIRA to SQL","twitter_description":"How to build and structure a data pipeline. This example takes issue data extracted from the JIRA Cloud API, transforms it, and loads it to a SQL database.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jira-etl-3-1.jpg","twitter_title":"Building an ETL Pipeline: From JIRA to SQL","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Atlassian","slug":"atlassian","description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","feature_image":null,"meta_description":"Beef up JIRA and Confluence by scripting and automating nearly anything. Empower teams with customized workflows and philosophies.","meta_title":"Atlassian Development for JIRA and Confluence. | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we haven't done just yet on this site is walking through the humble\nprocess of creating data pipelines: the art of taking a bunch of data, changing\nsaid data, and putting it somewhere else. It's kind of a weird thing to be into,\nhence why the MoMA has been rejecting my submissions of Github repositories.\nDon't worry; I'll keep at it.\n\nSomething you don't see every day are people sharing their pipelines, which is\nunderstandable. Presumably, the other people who do this kind of stuff do it for\nwork; nobody is happily building stupid pipelines in their free time begging to\nbe open sourced. Except me.\n\nWe've recently revamped our projects [https://hackersandslackers.com/projects/] \npage to include a public-facing Kanban board using GraphQL. To achieve this, we\nneed to extract JIRA data from the JIRA Cloud REST API and place it securely in\nour database.\n\nStructuring our Pipeline\nAn ETL pipeline which is considered 'well-structured' is in the eyes of the\nbeholder. There are a million different ways to pull and mess with data, so\nthere isn't a \"template\" for building these things out. In my case, the\nstructure of my script just so happened to end up as three modules: one for \nextracting, one for loading, and one for transforming. This was unplanned, but\nit's a good sign when our app matches our mindset. Here's the breakdown:\n\njira-database-etl\n├── __main__.py\n├── jira_etl\n│   ├── __init__.py\n│   ├── fetch.py\n│   ├── data.py\n│   └── db.py\n├── LICENSE\n├── MANIFEST.in\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── requirements.txt\n├── setup.cfg\n└── setup.py\n\n\nmain.py  is our application entry point. The logic of our pipeline is stored in\nthree parts under the jira_etl  directory:\n\n * fetch.py  grabs the data from the source (JIRA Cloud's REST API) and handles\n   fetching all JIRA issues.\n * data.py  transforms the data we've fetched and constructs a neat DataFrame\n   containing only the information we're after.\n * db.py  finally loads the data into a SQL database.\n\nDon't look into it too much, but here's our entry point:\n\nfrom jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    \"\"\"Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    \"\"\"\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n\n\nWithout further adieu, let's dig into the logic!\n\nExtracting Our Data\nBefore doing anything, it's essential we become familiar with the data we're\nabout to pull. Firstly, JIRA's REST API returns paginated results which max out\nat 100 results per page. This means we'll have to loop through the pages\nrecursively until all results are loaded.\n\nNext, let's look at an example of a single  JIRA issue JSON object returned by\nthe API:\n\n{\n    \"expand\": \"names,schema\",\n    \"startAt\": 0,\n    \"maxResults\": 1,\n    \"total\": 888,\n    \"issues\": [\n        {\n            \"expand\": \"operations,versionedRepresentations,editmeta,changelog,renderedFields\",\n            \"id\": \"11718\",\n            \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/11718\",\n            \"key\": \"HACK-756\",\n            \"fields\": {\n                \"issuetype\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014\",\n                    \"id\": \"10014\",\n                    \"description\": \"Placeholder item for \\\"holy shit this is going to be a lot of work\\\"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&avatarId=10311&avatarType=issuetype\",\n                    \"name\": \"Major Functionality\",\n                    \"subtask\": false,\n                    \"avatarId\": 10311\n                },\n                \"customfield_10070\": null,\n                \"customfield_10071\": null,\n                \"customfield_10073\": null,\n                \"customfield_10074\": null,\n                \"customfield_10075\": null,\n                \"project\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/project/10015\",\n                    \"id\": \"10015\",\n                    \"key\": \"HACK\",\n                    \"name\": \"Hackers and Slackers\",\n                    \"projectTypeKey\": \"software\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&avatarId=10535\",\n                        \"24x24\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&pid=10015&avatarId=10535\",\n                        \"16x16\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&pid=10015&avatarId=10535\",\n                        \"32x32\": \"https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&pid=10015&avatarId=10535\"\n                    }\n                },\n                \"fixVersions\": [],\n                \"resolution\": null,\n                \"resolutiondate\": null,\n                \"workratio\": -1,\n                \"lastViewed\": \"2019-03-24T02:01:31.355-0400\",\n                \"watches\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers\",\n                    \"watchCount\": 1,\n                    \"isWatching\": true\n                },\n                \"created\": \"2019-02-03T00:47:36.677-0500\",\n                \"customfield_10062\": null,\n                \"customfield_10063\": null,\n                \"customfield_10064\": null,\n                \"customfield_10065\": null,\n                \"customfield_10066\": null,\n                \"priority\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/priority/2\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg\",\n                    \"name\": \"High\",\n                    \"id\": \"2\"\n                },\n                \"customfield_10067\": null,\n                \"customfield_10068\": null,\n                \"customfield_10069\": [],\n                \"labels\": [],\n                \"versions\": [],\n                \"issuelinks\": [],\n                \"assignee\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"updated\": \"2019-03-24T02:01:30.724-0400\",\n                \"status\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/status/10004\",\n                    \"description\": \"\",\n                    \"iconUrl\": \"https://hackersandslackers.atlassian.net/\",\n                    \"name\": \"To Do\",\n                    \"id\": \"10004\",\n                    \"statusCategory\": {\n                        \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2\",\n                        \"id\": 2,\n                        \"key\": \"new\",\n                        \"colorName\": \"blue-gray\",\n                        \"name\": \"To Do\"\n                    }\n                },\n                \"components\": [],\n                \"description\": {\n                    \"version\": 1,\n                    \"type\": \"doc\",\n                    \"content\": [\n                        {\n                            \"type\": \"paragraph\",\n                            \"content\": [\n                                {\n                                    \"type\": \"text\",\n                                    \"text\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\",\n                                    \"marks\": [\n                                        {\n                                            \"type\": \"link\",\n                                            \"attrs\": {\n                                                \"href\": \"https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/\"\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                \"customfield_10010\": null,\n                \"customfield_10011\": \"0|i0064j:i\",\n                \"customfield_10012\": null,\n                \"customfield_10013\": null,\n                \"security\": null,\n                \"customfield_10008\": \"HACK-143\",\n                \"customfield_10009\": {\n                    \"hasEpicLinkFieldDependency\": false,\n                    \"showField\": false,\n                    \"nonEditableReason\": {\n                        \"reason\": \"PLUGIN_LICENSE_ERROR\",\n                        \"message\": \"Portfolio for Jira must be licensed for the Parent Link to be available.\"\n                    }\n                },\n                \"summary\": \"Automate newsletter\",\n                \"creator\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"subtasks\": [],\n                \"reporter\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"name\": \"bro\",\n                    \"key\": \"admin\",\n                    \"accountId\": \"557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8\",\n                    \"emailAddress\": \"toddbirchard@gmail.com\",\n                    \"avatarUrls\": {\n                        \"48x48\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue\",\n                        \"24x24\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue\",\n                        \"16x16\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue\",\n                        \"32x32\": \"https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue\"\n                    },\n                    \"displayName\": \"Todd Birchard\",\n                    \"active\": true,\n                    \"timeZone\": \"America/New_York\",\n                    \"accountType\": \"atlassian\"\n                },\n                \"customfield_10000\": \"{}\",\n                \"customfield_10001\": null,\n                \"customfield_10004\": null,\n                \"environment\": null,\n                \"duedate\": null,\n                \"votes\": {\n                    \"self\": \"https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes\",\n                    \"votes\": 0,\n                    \"hasVoted\": false\n                }\n            }\n        }\n    ]\n}\n\n\nWhoa, mama! That's a ton of BS for a single issue. You can see now why we'd want\nto transform this data before importing ten million fields into any database.\nMake note of these important fields:\n\n * startAt: An integer which tells us which issue number the paginated results\n   start at.\n * maxResults: Denotes the maximum number of results page - maxes out at 100\n   issues.\n * total: The total number of issues across all pages.\n * issues: A list of objects which contain the information for exactly one JIRA\n   issue per object\n\nGreat. So the purpose of fetch.py will essentially consist of creating a list of\nall 888  issues (in my case), and passing that off for transformation. Check it\nthe source I came up with:\n\nimport os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    \"\"\"Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     \"\"\"\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        \"Accept\": \"application/json\"\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        \"\"\"Gets the total number of results.\"\"\"\n        params = {\n            \"jql\": cls.jql,\n            \"maxResults\": 0,\n            \"startAt\": 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        \"\"\"Recursively retrieve all pages of JIRA issues.\"\"\"\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            \"\"\"Fetch one page of results, and determine if another page exists.\"\"\"\n            params = {\n                \"jql\": cls.jql,\n                \"maxResults\": cls.results_per_page,\n                \"startAt\": len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n\n\nYep, I'm using classes. This class has two methods:\n\n * get_total_number_of_issues: All this does is essentially pull the number of\n   issues (888) from the REST API. We'll use this number in our next function to\n   check if additional pages exist.\n * fetch_all_results: This is where things start getting fun. fetch_all_results \n   is a @classmethod  which contains a function within itself. fetch_all_results \n    gets the total number of JIRA issues and then calls upon child function \n   fetch_single_page to pull JIRA issue JSON objects and dump them into a list\n   called issue_arr  until all issues are accounted for.\n\nBecause we have 888 issues and can retrieve 100 issues  at a time, our function\nfetch_single_page  should run 9 times. And it does!\n\nTransforming Our Data\nSo now we have a list of 888 messy JIRA issues. The scope of data.py  should be\nto pull out only the data we want, and make sure that data is clean:\n\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    \"\"\"Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    \"\"\"\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        \"\"\"Make DataFrame out of data received from JIRA API.\"\"\"\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        \"\"\"Convert dict to JSON to string.\"\"\"\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        \"\"\"Create a JSON body for each ticket.\"\"\"\n        updated_date = datetime.strptime(issue['fields']['updated'], \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n\n\nAgain, let's see the methods at work:\n\n * construct_dataframe: The main function we invoke to build our DataFrame\n   (mostly just calls other methods). Once all transformations are completed,\n   creates a DataFrame called jira_df  by using the Pandas json_normalize() \n   method.\n * make_issue_body: Creates a new dictionary per singular JIRA issue. Extracts \n   only  the fields we want to be imported into our database. Converts each\n   field into either a string or an int as a lazy way of avoiding null values\n   (for example, if issue['fields']['priority']['name']  contained a null value,\n   the script would error out. Wrapping this in str() is a dirty way of\n   converting null  to an empty string).\n * dict_to_json_string  Takes each issue dictionary and converts it to a JSON\n   object, which is then turned into a string (this is done for Pandas).\n\nLoading Our Data\nAnd now for the final step! Thanks to the joyful marriage of Pandas and\nSQLAlchemy, turning DataFrames into SQL tables is super simple. We never make\nthings simple, though.\n\nimport os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    \"\"\"Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    \"\"\"\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=\"hackers$prod\")\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        \"\"\"Clear table of data.\"\"\"\n        sql = text('TRUNCATE TABLE \"hackers$prod\".\"JiraIssue\"')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        \"\"\"Merge epic metadata from existing SQL table.\"\"\"\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        \"\"\"Upload JIRA DataFrame to PostgreSQL database.\"\"\"\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={\"assignee\": String(30),\n                                     \"assignee_url\": Text,\n                                     \"epic_link\": String(50),\n                                     \"issuetype_name\": String(50),\n                                     \"issuetype_icon\": Text,\n                                     \"key\": String(10),\n                                     \"priority_name\": String(30),\n                                     \"priority_rank\": Integer,\n                                     \"priority_url\": Text,\n                                     \"project\": String(50),\n                                     \"status\": String(30),\n                                     \"summary\": Text,\n                                     \"updated\": Integer,\n                                     \"updatedAt\": TIMESTAMP,\n                                     \"createdAt\": TIMESTAMP,\n                                     \"epic_color\": String(20),\n                                     \"epic_name\": String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n\n\n * merge_epic_metadata: Due to the nature of the JIRA REST API, some metadata is\n   missing per issue. If you're interested, the data missing revolves around \n   Epics: JIRA's REST API does not include the Epic Name  or Epic Color  fields\n   of linked epics.\n * upload_dataframe: Uses Panda's to_sql()  method to upload our DataFrame into\n   a SQL table (our target happens to be PostgreSQL, so we pass schema  here).\n   To make things explicit, we set the data type of every column on upload.\n\nWell, let's see how we made out!\n\nA look at our resulting database table.Whoaaa nelly, we did it! With our data\nclean, we can now build something useful! Here's what I built:\n\nFruits of our labor!There we have it: a pipeline that takes a bunch of messy\ndata, cleans it, and puts it somewhere else for proper use.\n\nIf you're interested in how we created the frontend for our Kanban board, check\nout our series on building features with GraphQL\n[https://hackersandslackers.com/series/graphql-hype/]. For the source code,\ncheck out the Github repository\n[https://github.com/toddbirchard/jira-database-etl].","html":"<p>Something we haven't done just yet on this site is walking through the humble process of creating data pipelines: the art of taking a bunch of data, changing said data, and putting it somewhere else. It's kind of a weird thing to be into, hence why the MoMA has been rejecting my submissions of Github repositories. Don't worry; I'll keep at it.</p><p>Something you don't see every day are people sharing their pipelines, which is understandable. Presumably, the other people who do this kind of stuff do it for work; nobody is happily building stupid pipelines in their free time begging to be open sourced. Except me.</p><p>We've recently revamped our <strong><a href=\"https://hackersandslackers.com/projects/\">projects</a></strong> page to include a public-facing Kanban board using GraphQL. To achieve this, we need to extract JIRA data from the JIRA Cloud REST API and place it securely in our database.</p><h2 id=\"structuring-our-pipeline\">Structuring our Pipeline</h2><p>An ETL pipeline which is considered 'well-structured' is in the eyes of the beholder. There are a million different ways to pull and mess with data, so there isn't a \"template\" for building these things out. In my case, the structure of my script just so happened to end up as three modules: one for <em>extracting</em>, one for <em>loading</em>, and one for <em>transforming</em>. This was unplanned, but it's a good sign when our app matches our mindset. Here's the breakdown:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">jira-database-etl\n├── __main__.py\n├── jira_etl\n│   ├── __init__.py\n│   ├── fetch.py\n│   ├── data.py\n│   └── db.py\n├── LICENSE\n├── MANIFEST.in\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── requirements.txt\n├── setup.cfg\n└── setup.py\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>main.py</strong> is our application entry point. The logic of our pipeline is stored in three parts under the <strong>jira_etl</strong> directory:</p><ul><li><strong>fetch.py</strong> grabs the data from the source (JIRA Cloud's REST API) and handles fetching all JIRA issues.</li><li><strong>data.py</strong> transforms the data we've fetched and constructs a neat DataFrame containing only the information we're after.</li><li><strong>db.py</strong> finally loads the data into a SQL database.</li></ul><p>Don't look into it too much, but here's our entry point:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from jira_etl import fetch\nfrom jira_etl import data\nfrom jira_etl import db\n\n\ndef main():\n    &quot;&quot;&quot;Application Entry Point.\n\n    1. Fetch all desired JIRA issues from an instance's REST API.\n    2. Sanitize the data and add secondary metadata.\n    3. Upload resulting DataFrame to database.\n    &quot;&quot;&quot;\n    jira_issues_json = fetch.FetchJiraIssues.fetch_all_results()\n    jira_issues_df = data.TransformData.construct_dataframe(jira_issues_json)\n    upload_status = db.DatabaseImport.upload_dataframe(jira_issues_df)\n    return upload_status\n</code></pre>\n<!--kg-card-end: markdown--><p>Without further adieu, let's dig into the logic!</p><h2 id=\"extracting-our-data\">Extracting Our Data</h2><p>Before doing anything, it's essential we become familiar with the data we're about to pull. Firstly, JIRA's REST API returns paginated results which max out at 100 results per page. This means we'll have to loop through the pages recursively until all results are loaded.</p><p>Next, let's look at an example of a <strong><em>single</em></strong> JIRA issue JSON object returned by the API:</p><!--kg-card-begin: markdown--><pre><code class=\"language-json\">{\n    &quot;expand&quot;: &quot;names,schema&quot;,\n    &quot;startAt&quot;: 0,\n    &quot;maxResults&quot;: 1,\n    &quot;total&quot;: 888,\n    &quot;issues&quot;: [\n        {\n            &quot;expand&quot;: &quot;operations,versionedRepresentations,editmeta,changelog,renderedFields&quot;,\n            &quot;id&quot;: &quot;11718&quot;,\n            &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/11718&quot;,\n            &quot;key&quot;: &quot;HACK-756&quot;,\n            &quot;fields&quot;: {\n                &quot;issuetype&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issuetype/10014&quot;,\n                    &quot;id&quot;: &quot;10014&quot;,\n                    &quot;description&quot;: &quot;Placeholder item for \\&quot;holy shit this is going to be a lot of work\\&quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/secure/viewavatar?size=xsmall&amp;avatarId=10311&amp;avatarType=issuetype&quot;,\n                    &quot;name&quot;: &quot;Major Functionality&quot;,\n                    &quot;subtask&quot;: false,\n                    &quot;avatarId&quot;: 10311\n                },\n                &quot;customfield_10070&quot;: null,\n                &quot;customfield_10071&quot;: null,\n                &quot;customfield_10073&quot;: null,\n                &quot;customfield_10074&quot;: null,\n                &quot;customfield_10075&quot;: null,\n                &quot;project&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/project/10015&quot;,\n                    &quot;id&quot;: &quot;10015&quot;,\n                    &quot;key&quot;: &quot;HACK&quot;,\n                    &quot;name&quot;: &quot;Hackers and Slackers&quot;,\n                    &quot;projectTypeKey&quot;: &quot;software&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?pid=10015&amp;avatarId=10535&quot;,\n                        &quot;24x24&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=small&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;16x16&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=xsmall&amp;pid=10015&amp;avatarId=10535&quot;,\n                        &quot;32x32&quot;: &quot;https://hackersandslackers.atlassian.net/secure/projectavatar?size=medium&amp;pid=10015&amp;avatarId=10535&quot;\n                    }\n                },\n                &quot;fixVersions&quot;: [],\n                &quot;resolution&quot;: null,\n                &quot;resolutiondate&quot;: null,\n                &quot;workratio&quot;: -1,\n                &quot;lastViewed&quot;: &quot;2019-03-24T02:01:31.355-0400&quot;,\n                &quot;watches&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/watchers&quot;,\n                    &quot;watchCount&quot;: 1,\n                    &quot;isWatching&quot;: true\n                },\n                &quot;created&quot;: &quot;2019-02-03T00:47:36.677-0500&quot;,\n                &quot;customfield_10062&quot;: null,\n                &quot;customfield_10063&quot;: null,\n                &quot;customfield_10064&quot;: null,\n                &quot;customfield_10065&quot;: null,\n                &quot;customfield_10066&quot;: null,\n                &quot;priority&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/priority/2&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/images/icons/priorities/high.svg&quot;,\n                    &quot;name&quot;: &quot;High&quot;,\n                    &quot;id&quot;: &quot;2&quot;\n                },\n                &quot;customfield_10067&quot;: null,\n                &quot;customfield_10068&quot;: null,\n                &quot;customfield_10069&quot;: [],\n                &quot;labels&quot;: [],\n                &quot;versions&quot;: [],\n                &quot;issuelinks&quot;: [],\n                &quot;assignee&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;updated&quot;: &quot;2019-03-24T02:01:30.724-0400&quot;,\n                &quot;status&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/status/10004&quot;,\n                    &quot;description&quot;: &quot;&quot;,\n                    &quot;iconUrl&quot;: &quot;https://hackersandslackers.atlassian.net/&quot;,\n                    &quot;name&quot;: &quot;To Do&quot;,\n                    &quot;id&quot;: &quot;10004&quot;,\n                    &quot;statusCategory&quot;: {\n                        &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/statuscategory/2&quot;,\n                        &quot;id&quot;: 2,\n                        &quot;key&quot;: &quot;new&quot;,\n                        &quot;colorName&quot;: &quot;blue-gray&quot;,\n                        &quot;name&quot;: &quot;To Do&quot;\n                    }\n                },\n                &quot;components&quot;: [],\n                &quot;description&quot;: {\n                    &quot;version&quot;: 1,\n                    &quot;type&quot;: &quot;doc&quot;,\n                    &quot;content&quot;: [\n                        {\n                            &quot;type&quot;: &quot;paragraph&quot;,\n                            &quot;content&quot;: [\n                                {\n                                    &quot;type&quot;: &quot;text&quot;,\n                                    &quot;text&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;,\n                                    &quot;marks&quot;: [\n                                        {\n                                            &quot;type&quot;: &quot;link&quot;,\n                                            &quot;attrs&quot;: {\n                                                &quot;href&quot;: &quot;https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/&quot;\n                                            }\n                                        }\n                                    ]\n                                }\n                            ]\n                        }\n                    ]\n                },\n                &quot;customfield_10010&quot;: null,\n                &quot;customfield_10011&quot;: &quot;0|i0064j:i&quot;,\n                &quot;customfield_10012&quot;: null,\n                &quot;customfield_10013&quot;: null,\n                &quot;security&quot;: null,\n                &quot;customfield_10008&quot;: &quot;HACK-143&quot;,\n                &quot;customfield_10009&quot;: {\n                    &quot;hasEpicLinkFieldDependency&quot;: false,\n                    &quot;showField&quot;: false,\n                    &quot;nonEditableReason&quot;: {\n                        &quot;reason&quot;: &quot;PLUGIN_LICENSE_ERROR&quot;,\n                        &quot;message&quot;: &quot;Portfolio for Jira must be licensed for the Parent Link to be available.&quot;\n                    }\n                },\n                &quot;summary&quot;: &quot;Automate newsletter&quot;,\n                &quot;creator&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;subtasks&quot;: [],\n                &quot;reporter&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/user?accountId=557058%3A713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;name&quot;: &quot;bro&quot;,\n                    &quot;key&quot;: &quot;admin&quot;,\n                    &quot;accountId&quot;: &quot;557058:713aac6d-44ef-416d-9a1d-3e524a5c4dc8&quot;,\n                    &quot;emailAddress&quot;: &quot;toddbirchard@gmail.com&quot;,\n                    &quot;avatarUrls&quot;: {\n                        &quot;48x48&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=48&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D48%26noRedirect%3Dtrue&quot;,\n                        &quot;24x24&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=24&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D24%26noRedirect%3Dtrue&quot;,\n                        &quot;16x16&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=16&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D16%26noRedirect%3Dtrue&quot;,\n                        &quot;32x32&quot;: &quot;https://avatar-cdn.atlassian.com/9eb3868db428fb602e03b3059608199b?s=32&amp;d=https%3A%2F%2Fsecure.gravatar.com%2Favatar%2F9eb3868db428fb602e03b3059608199b%3Fd%3Dmm%26s%3D32%26noRedirect%3Dtrue&quot;\n                    },\n                    &quot;displayName&quot;: &quot;Todd Birchard&quot;,\n                    &quot;active&quot;: true,\n                    &quot;timeZone&quot;: &quot;America/New_York&quot;,\n                    &quot;accountType&quot;: &quot;atlassian&quot;\n                },\n                &quot;customfield_10000&quot;: &quot;{}&quot;,\n                &quot;customfield_10001&quot;: null,\n                &quot;customfield_10004&quot;: null,\n                &quot;environment&quot;: null,\n                &quot;duedate&quot;: null,\n                &quot;votes&quot;: {\n                    &quot;self&quot;: &quot;https://hackersandslackers.atlassian.net/rest/api/3/issue/HACK-756/votes&quot;,\n                    &quot;votes&quot;: 0,\n                    &quot;hasVoted&quot;: false\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Whoa, mama! That's a ton of BS for a single issue. You can see now why we'd want to transform this data before importing ten million fields into any database. Make note of these important fields:</p><ul><li><code>startAt</code>: An integer which tells us which issue number the paginated results start at.</li><li><code>maxResults</code>: Denotes the maximum number of results page - maxes out at 100 issues.</li><li><code>total</code>: The total number of issues across all pages.</li><li><code>issues</code>: A list of objects which contain the information for exactly one JIRA issue per object</li></ul><p>Great. So the purpose of <strong>fetch.py </strong>will essentially consist of creating a list of all <strong>888</strong> issues (in my case), and passing that off for transformation. Check it the source I came up with:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport math\nimport requests\n\n\nclass FetchJiraIssues:\n    &quot;&quot;&quot;Fetch all public-facing issues from JIRA instance.\n\n    1. Retrieve all values from env vars.\n    2. Construct request against JIRA REST API.\n    3. Fetch paginated issues via recursion.\n    4. Pass final JSON to be transformed into a DataFrame.\n     &quot;&quot;&quot;\n    results_per_page = 100\n    username = os.environ.get('JIRA_USERNAME')\n    password = os.environ.get('JIRA_PASSWORD')\n    endpoint = os.environ.get('JIRA_ENDPOINT')\n    jql = os.environ.get('JIRA_QUERY')\n    headers = {\n        &quot;Accept&quot;: &quot;application/json&quot;\n    }\n\n    @classmethod\n    def get_total_number_of_issues(cls):\n        &quot;&quot;&quot;Gets the total number of results.&quot;&quot;&quot;\n        params = {\n            &quot;jql&quot;: cls.jql,\n            &quot;maxResults&quot;: 0,\n            &quot;startAt&quot;: 0\n        }\n        req = requests.get(cls.endpoint,\n                           headers=cls.headers,\n                           params=params,\n                           auth=(cls.username, cls.password)\n                           )\n        response = req.json()\n        try:\n            total_results = response['total']\n            return total_results\n        except KeyError:\n            print('Could not find any issues!')\n\n    @classmethod\n    def fetch_all_results(cls):\n        &quot;&quot;&quot;Recursively retrieve all pages of JIRA issues.&quot;&quot;&quot;\n        total_results = cls.get_total_number_of_issues()\n        issue_arr = []\n\n        def fetch_single_page():\n            &quot;&quot;&quot;Fetch one page of results, and determine if another page exists.&quot;&quot;&quot;\n            params = {\n                &quot;jql&quot;: cls.jql,\n                &quot;maxResults&quot;: cls.results_per_page,\n                &quot;startAt&quot;: len(issue_arr)\n            }\n            req = requests.get(cls.endpoint,\n                               headers=cls.headers,\n                               params=params,\n                               auth=(cls.username, cls.password)\n                               )\n            response = req.json()\n            issues = response['issues']\n            issues_so_far = len(issue_arr) + cls.results_per_page\n            print(issues_so_far, ' out of', total_results)\n            issue_arr.extend(issues)\n            # Check if additional pages of results exist.\n        count = math.ceil(total_results/cls.results_per_page)\n        for x in range(0, count):\n            fetch_single_page()\n        return issue_arr\n</code></pre>\n<!--kg-card-end: markdown--><p>Yep, I'm using classes. This class has two methods:</p><ul><li><code>get_total_number_of_issues</code>: All this does is essentially pull the number of issues (888) from the REST API. We'll use this number in our next function to check if additional pages exist.</li><li><code>fetch_all_results</code>: This is where things start getting fun. <strong>fetch_all_results</strong> is a <em>@classmethod</em> which contains a function within itself. <strong>fetch_all_results</strong> gets the total number of JIRA issues and then calls upon child function <strong>fetch_single_page </strong>to pull JIRA issue JSON objects and dump them into a list called <code>issue_arr</code> until all issues are accounted for.</li></ul><p>Because we have <em>888 issues </em>and can retrieve <em>100 issues</em> at a time, our function  <code>fetch_single_page</code> should run <em>9 times</em>. And it does!</p><h2 id=\"transforming-our-data\">Transforming Our Data</h2><p>So now we have a list of 888 messy JIRA issues. The scope of <strong>data.py</strong> should be to pull out only the data we want, and make sure that data is clean:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport json\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\n\nclass TransformData:\n    &quot;&quot;&quot;Build JIRA issue DataFrame.\n\n    1. Loop through JIRA issues and create a dictionary of desired data.\n    2. Convert each issue dictionary into a JSON object.\n    3. Load all issues into a Pandas DataFrame.\n    &quot;&quot;&quot;\n\n    issue_count = 0\n\n    @classmethod\n    def construct_dataframe(cls, issue_list_chunk):\n        &quot;&quot;&quot;Make DataFrame out of data received from JIRA API.&quot;&quot;&quot;\n        issue_list = [cls.make_issue_body(issue) for issue in issue_list_chunk]\n        issue_json_list = [cls.dict_to_json_string(issue) for issue in issue_list]\n        jira_issues_df = json_normalize(issue_json_list)\n        return jira_issues_df\n\n    @staticmethod\n    def dict_to_json_string(issue_dict):\n        &quot;&quot;&quot;Convert dict to JSON to string.&quot;&quot;&quot;\n        issue_json_string = json.dumps(issue_dict)\n        issue_json = json.loads(issue_json_string)\n        return issue_json\n\n    @classmethod\n    def make_issue_body(cls, issue):\n        &quot;&quot;&quot;Create a JSON body for each ticket.&quot;&quot;&quot;\n        updated_date = datetime.strptime(issue['fields']['updated'], &quot;%Y-%m-%dT%H:%M:%S.%f%z&quot;)\n        body = {\n            'id': str(cls.issue_count),\n            'key': str(issue['key']),\n            'assignee_name': str(issue['fields']['assignee']['displayName']),\n            'assignee_url': str(issue['fields']['assignee']['avatarUrls']['48x48']),\n            'summary': str(issue['fields']['summary']),\n            'status': str(issue['fields']['status']['name']),\n            'priority_url': str(issue['fields']['priority']['iconUrl']),\n            'priority_rank': int(issue['fields']['priority']['id']),\n            'issuetype_name': str(issue['fields']['issuetype']['name']),\n            'issuetype_icon': str(issue['fields']['issuetype']['iconUrl']),\n            'epic_link': str(issue['fields']['customfield_10008']),\n            'project': str(issue['fields']['project']['name']),\n            'updated': int(datetime.timestamp(updated_date)),\n            'updatedAt': str(updated_date)\n        }\n        cls.issue_count += 1\n        return body\n</code></pre>\n<!--kg-card-end: markdown--><p>Again, let's see the methods at work:</p><ul><li><code>construct_dataframe</code>: The main function we invoke to build our DataFrame (mostly just calls other methods). Once all transformations are completed, creates a DataFrame called <strong>jira_df</strong> by using the Pandas <em>json_normalize()</em> method.</li><li><code>make_issue_body</code>: Creates a new dictionary per singular JIRA issue. Extracts <em>only</em> the fields we want to be imported into our database. Converts each field into either a string or an int as a lazy way of avoiding null values (for example, if <code>issue['fields']['priority']['name']</code> contained a null value, the script would error out. Wrapping this in <strong>str() </strong>is a dirty way of converting <em>null</em> to an empty string).</li><li><code>dict_to_json_string</code> Takes each issue dictionary and converts it to a JSON object, which is then turned into a string (this is done for Pandas).</li></ul><h2 id=\"loading-our-data\">Loading Our Data</h2><p>And now for the final step! Thanks to the joyful marriage of Pandas and SQLAlchemy, turning DataFrames into SQL tables is super simple. We never make things simple, though.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nimport logging\nfrom sqlalchemy import create_engine, text, MetaData\nfrom sqlalchemy.types import Integer, Text, TIMESTAMP, String\nimport pandas as pd\n\nlogging.basicConfig()\nlogging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)\n\n\nclass DatabaseImport:\n    &quot;&quot;&quot;Merge Epic metadata and upload JIRA issues.\n\n    1. Merge Epic metadata by fetching an existing table.\n    2. Explicitly set data types for all columns found in jira_issues_df.\n    2. Create a new table from the final jira_issues_df.\n    &quot;&quot;&quot;\n\n    URI = os.environ.get('SQLALCHEMY_DATABASE_URI')\n    db_epic_table = os.environ.get('SQLALCHEMY_EPIC_TABLE')\n    db_jira_table = os.environ.get('SQLALCHEMY_JIRA_TABLE')\n    db_schema = os.environ.get('SQLALCHEMY_DB_SCHEMA')\n\n    # Create Engine\n    meta = MetaData(schema=&quot;hackers$prod&quot;)\n    engine = create_engine(URI, echo=True)\n\n    @staticmethod\n    def truncate_table(engine):\n        &quot;&quot;&quot;Clear table of data.&quot;&quot;&quot;\n        sql = text('TRUNCATE TABLE &quot;hackers$prod&quot;.&quot;JiraIssue&quot;')\n        engine.execute(sql)\n\n    @classmethod\n    def merge_epic_metadata(cls, jira_issues_df):\n        &quot;&quot;&quot;Merge epic metadata from existing SQL table.&quot;&quot;&quot;\n        cls.truncate_table(cls.engine)\n        epics_df = pd.read_sql_table(cls.db_epic_table,\n                                     cls.engine,\n                                     schema=cls.db_schema)\n        jira_issues_df = pd.merge(jira_issues_df,\n                                  epics_df[['epic_link', 'epic_name', 'epic_color']],\n                                  how='left',\n                                  on='epic_link',\n                                  copy=False)\n        return jira_issues_df\n\n    @classmethod\n    def upload_dataframe(cls, jira_issues_df):\n        &quot;&quot;&quot;Upload JIRA DataFrame to PostgreSQL database.&quot;&quot;&quot;\n        jira_issues_df = cls.merge_epic_metadata(jira_issues_df)\n        jira_issues_df.to_sql(cls.db_jira_table,\n                              cls.engine,\n                              if_exists='append',\n                              schema=cls.db_schema,\n                              index=False,\n                              dtype={&quot;assignee&quot;: String(30),\n                                     &quot;assignee_url&quot;: Text,\n                                     &quot;epic_link&quot;: String(50),\n                                     &quot;issuetype_name&quot;: String(50),\n                                     &quot;issuetype_icon&quot;: Text,\n                                     &quot;key&quot;: String(10),\n                                     &quot;priority_name&quot;: String(30),\n                                     &quot;priority_rank&quot;: Integer,\n                                     &quot;priority_url&quot;: Text,\n                                     &quot;project&quot;: String(50),\n                                     &quot;status&quot;: String(30),\n                                     &quot;summary&quot;: Text,\n                                     &quot;updated&quot;: Integer,\n                                     &quot;updatedAt&quot;: TIMESTAMP,\n                                     &quot;createdAt&quot;: TIMESTAMP,\n                                     &quot;epic_color&quot;: String(20),\n                                     &quot;epic_name&quot;: String(50)\n                                     })\n        success_message = 'Successfully uploaded' \\\n                          + str(len(jira_issues_df.index)) \\\n                          + ' rows to ' + cls.db_jira_table\n        return success_message\n</code></pre>\n<!--kg-card-end: markdown--><ul><li><code>merge_epic_metadata</code>: Due to the nature of the JIRA REST API, some metadata is missing per issue. If you're interested, the data missing revolves around <strong>Epics</strong>: JIRA's REST API does not include the <em>Epic Name</em> or <em>Epic Color</em> fields of linked epics.</li><li><code>upload_dataframe</code>: Uses Panda's <strong>to_sql()</strong> method to upload our DataFrame into a SQL table (our target happens to be PostgreSQL, so we pass <em>schema</em> here). To make things explicit, we set the data type of every column on upload.</li></ul><p>Well, let's see how we made out!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.03.51-PM.png\" class=\"kg-image\"><figcaption>A look at our resulting database table.</figcaption></figure><!--kg-card-end: image--><p>Whoaaa nelly, we did it! With our data clean, we can now build something useful! Here's what I built:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-27-at-9.29.20-PM.png\" class=\"kg-image\"><figcaption>Fruits of our labor!</figcaption></figure><!--kg-card-end: image--><p>There we have it: a pipeline that takes a bunch of messy data, cleans it, and puts it somewhere else for proper use.</p><p>If you're interested in how we created the frontend for our Kanban board, check out our series on <a href=\"https://hackersandslackers.com/series/graphql-hype/\">building features with GraphQL</a>. For the source code, check out the <a href=\"https://github.com/toddbirchard/jira-database-etl\">Github repository</a>.</p>","url":"https://hackersandslackers.com/building-an-etl-pipeline-from-jira-to-postgresql/","uuid":"23647abe-9b47-4f58-8206-cff1fb2ae891","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c95e08ef654036aa06c6a02"}},{"node":{"id":"Ghost__Post__5c4e57144b23df2da7332b80","title":"Downcast Numerical Data Types with Pandas","slug":"downcast-numerical-columns-python-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","excerpt":"Using an Example Where We Downcast Numerical Columns.","custom_excerpt":"Using an Example Where We Downcast Numerical Columns.","created_at_pretty":"28 January, 2019","published_at_pretty":"28 January, 2019","updated_at_pretty":"14 February, 2019","created_at":"2019-01-27T20:12:52.000-05:00","published_at":"2019-01-28T07:30:00.000-05:00","updated_at":"2019-02-13T22:50:18.000-05:00","meta_title":"Using Pandas' Assign Function on Multiple Columns | Hackers and Slackers","meta_description":"Using Pandas' Assign function on multiple columns via an example: downcasting numerical columns.","og_description":"Using Pandas' Assign by example: downcasting numerical columns.","og_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","og_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","twitter_description":"Using Pandas' Assign by example: downcasting numerical columns.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","twitter_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Recently, I had to find a way to reduce the memory footprint of a Pandas\nDataFrame in order to actually do operations on it.  Here's a trick that came in\nhandy!\n\nBy default, if you read a DataFrame from a file, it'll cast all the numerical\ncolumns as the float64  type.  This is in keeping with the philosophy behind\nPandas and NumPy - by using strict types (instead of normal Python \"duck\ntyping\"), you can do things a lot faster.  The float64  is the most flexible\nnumerical type - it can handle fractions, as well as turning missing values into\na NaN.  This will let us read it into memory, and then start messing with it.\n The downside is that it consumes a lot of memory.\n\nNow, let's say we want to save memory by manually downcasting our columns into\nthe smallest type that can handle its values?  And let's ALSO say that we want\nto be really, really lazy and don't want to look at a bunch of numbers by hand.\n And let's say we wanna do this via Method Chaining, because of all the\nadvantages outlined here: https://tomaugspurger.github.io/method-chaining\n\nLet's introduce our example DataFrame.  We'll convert all the values to floats\nmanually because that's what the default is when we read from a file.\n\ndf = pd.DataFrame({\n    \"stay_float\": [0.5, 3.7, 7.5],\n    \"to_int\": [-5, 7, 5],\n    \"to_uint\": [1, 100, 200]}).astype(float)\n\n\nFirst, let's introduce the workhorse of this exercise - Pandas's to_numeric \nfunction, and its handy optional argument, downcast.  This will take a numerical\ntype - float, integer  (not int), or unsigned  - and then downcast it to the\nsmallest version available.\n\nNext, let's make a function that checks to see if a column can be downcast from\na float to an integer.\n\ndef float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser\n\nWe're using the try/except pattern here because if we try to make a column with \nNaN  values into an integer column, it'll throw an error.  If it'd otherwise be\na good candidate for turning into an integer, we should figure a value to impute\nfor those missing values - but that'll be different for every column.  Sometimes\nit'd make sense to make it 0, other times the mean or median of the column, or\nsomething else entirely.\n\nI'd also like to direct your attention to Line 4, which has a very useful Pandas\npattern - if (ser == int_ser).all().  When you do operations on Pandas columns\nlike Equals or Greater Than, you get a new column where the operation was\napplied element-by-element.  If you're trying to set up a conditional, the\ninterpreter doesn't know what to do with an array containing [True, False, True] \n - you have to boil it down to a single value.  So, if you wan to check if two\ncolumns are completely equal, you have to call the .all()  method (which has a\nuseful sibling, any()) to make a conditional that can actually be used to\ncontrol execution.\n\nNext, let's make a function that lets us apply a transformation to multiple\ncolumns based on a condition.  The assign  method is pretty awesome, and it'd be\nfun to not have to leave it (or, if we do, to at least replace it with a\nfunction we can pipe as part of a chain of transformations to the DataFrame as a\nwhole).\n\ndef multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n\n\nassign  lets us do multiple assignments, so long as we make a dictionary of\ncolumn names and target values and then unpack it.  Really, it'd actually be\neasier to skip the function and go directly to using this syntax, except that\nI'm not aware of a method of accessing a filterable list of the DF's columns\nwhile still \"in\" the chain.  I think future versions of Pandas' syntax will\ninclude this, as I've read they want to support more Method Chaining.\n Personally, I find the reduction in Cognitive Load is worth it, with having a\nlot of little modular lego-piece transformations chained together.\n\nIt also works as a nice foundation for other little helper functions.  So,\nhere's one to turn as many float columns to integers as we can.\n\ndef all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[\"float\"])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSee the pattern in action!  We decide on a transformation function, we decide on\nwhat conditions we want to apply all these transformations (we could have a\nhundred columns, and who wants to make a note of all that?), and then we pass it\nto the multi-assign  function.\n\n(df\n     .pipe(all_float_to_int)).dtypes\n\n\nstay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n\n\nCool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of\ninteger takes up as many bytes as 64 bytes of float, just like how a hundred\npounds of feathers weighs as much as a hundred pounds of bricks.  What we did do\nis make it easier to downcast those columns later.\n\nNext, let's make a function that takes a subset of the columns, and tries to\ndowncast it to the smallest version that it can.  We've got fairly small values\nhere, so it should get some work done.\n\ndef downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSame basic pattern as before!  But now we have two arguments - one is the \ntarget_type, which tells us what types to try to downcast to.  By default, this\nwill be the same as the initial_type, with one exception that we'll grab in a\nsecond!\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n\n\nAlright, now we're getting somewhere!  Wonder if we can do even better, though?\n That last column has a conspicuous name!  And it has no values lower than 0 -\nmaybe we could save space if we store it as an unsigned integer!  Let's add a\npipe to our chain that'll try to downcast certain integers into unsigneds...\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n\n\nWhat do ya know, we can!\n\nLet's see how much memory we save by doing this.\n\ndf.info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n\n\nvs\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n\n\n152 down to 98 - we reduced it by more than 1/3rd!","html":"<p>Recently, I had to find a way to reduce the memory footprint of a Pandas DataFrame in order to actually do operations on it.  Here's a trick that came in handy!</p><p>By default, if you read a DataFrame from a file, it'll cast all the numerical columns as the <code>float64</code> type.  This is in keeping with the philosophy behind Pandas and NumPy - by using strict types (instead of normal Python \"duck typing\"), you can do things a lot faster.  The <code>float64</code> is the most flexible numerical type - it can handle fractions, as well as turning missing values into a <code>NaN</code>.  This will let us read it into memory, and then start messing with it.  The downside is that it consumes a lot of memory.</p><p>Now, let's say we want to save memory by manually downcasting our columns into the smallest type that can handle its values?  And let's ALSO say that we want to be really, really lazy and don't want to look at a bunch of numbers by hand.  And let's say we wanna do this via Method Chaining, because of all the advantages outlined here: <a href=\"https://tomaugspurger.github.io/method-chaining\">https://tomaugspurger.github.io/method-chaining</a></p><p>Let's introduce our example DataFrame.  We'll convert all the values to floats manually because that's what the default is when we read from a file.</p><pre><code class=\"language-python\">df = pd.DataFrame({\n    &quot;stay_float&quot;: [0.5, 3.7, 7.5],\n    &quot;to_int&quot;: [-5, 7, 5],\n    &quot;to_uint&quot;: [1, 100, 200]}).astype(float)\n</code></pre>\n<p>First, let's introduce the workhorse of this exercise - Pandas's <code>to_numeric</code> function, and its handy optional argument, <code>downcast</code>.  This will take a numerical type - <code>float</code>, <code>integer</code> (not <code>int</code>), or <code>unsigned</code> - and then downcast it to the smallest version available.</p><p>Next, let's make a function that checks to see if a column can be downcast from a float to an integer.</p><pre><code>def float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser</code></pre><p>We're using the try/except pattern here because if we try to make a column with <code>NaN</code> values into an integer column, it'll throw an error.  If it'd otherwise be a good candidate for turning into an integer, we should figure a value to impute for those missing values - but that'll be different for every column.  Sometimes it'd make sense to make it 0, other times the mean or median of the column, or something else entirely.</p><p>I'd also like to direct your attention to Line 4, which has a very useful Pandas pattern - <code>if (ser == int_ser).all()</code>.  When you do operations on Pandas columns like Equals or Greater Than, you get a new column where the operation was applied element-by-element.  If you're trying to set up a conditional, the interpreter doesn't know what to do with an array containing <code>[True, False, True]</code> - you have to boil it down to a single value.  So, if you wan to check if two columns are completely equal, you have to call the <code>.all()</code> method (which has a useful sibling, <code>any()</code>) to make a conditional that can actually be used to control execution.</p><p>Next, let's make a function that lets us apply a transformation to multiple columns based on a condition.  The <code>assign</code> method is pretty awesome, and it'd be fun to not have to leave it (or, if we do, to at least replace it with a function we can pipe as part of a chain of transformations to the DataFrame as a whole).</p><pre><code class=\"language-python\">def multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n</code></pre>\n<p><code>assign</code> lets us do multiple assignments, so long as we make a dictionary of column names and target values and then unpack it.  Really, it'd actually be easier to skip the function and go directly to using this syntax, except that I'm not aware of a method of accessing a filterable list of the DF's columns while still \"in\" the chain.  I think future versions of Pandas' syntax will include this, as I've read they want to support more Method Chaining.  Personally, I find the reduction in Cognitive Load is worth it, with having a lot of little modular lego-piece transformations chained together.  </p><p>It also works as a nice foundation for other little helper functions.  So, here's one to turn as many float columns to integers as we can.</p><pre><code class=\"language-python\">def all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[&quot;float&quot;])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>See the pattern in action!  We decide on a transformation function, we decide on what conditions we want to apply all these transformations (we could have a hundred columns, and who wants to make a note of all that?), and then we pass it to the <code>multi-assign</code> function.  </p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n</code></pre>\n<p>Cool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of integer takes up as many bytes as 64 bytes of float, just like how a hundred pounds of feathers weighs as much as a hundred pounds of bricks.  What we did do is make it easier to downcast those columns later.</p><p>Next, let's make a function that takes a subset of the columns, and tries to downcast it to the smallest version that it can.  We've got fairly small values here, so it should get some work done.</p><pre><code class=\"language-python\">def downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>Same basic pattern as before!  But now we have two arguments - one is the <code>target_type</code>, which tells us what types to try to downcast to.  By default, this will be the same as the <code>initial_type</code>, with one exception that we'll grab in a second!</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n</code></pre>\n<p>Alright, now we're getting somewhere!  Wonder if we can do even better, though?  That last column has a conspicuous name!  And it has no values lower than 0 - maybe we could save space if we store it as an unsigned integer!  Let's add a pipe to our chain that'll try to downcast certain integers into unsigneds...</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n</code></pre>\n<p>What do ya know, we can!</p><p>Let's see how much memory we save by doing this.</p><pre><code class=\"language-python\">df.info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n</code></pre>\n<p>vs</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n</code></pre>\n<p>152 down to 98 - we reduced it by more than 1/3rd!</p>","url":"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/","uuid":"58bbb902-99bb-404d-8a3c-232d56b6e776","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c4e57144b23df2da7332b80"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673731","title":"Liberating Data from PDFs with Tabula and Pandas","slug":"liberating-data-from-pdfs-with-tabula-and-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","excerpt":"Making 'open' data more open.","custom_excerpt":"Making 'open' data more open.","created_at_pretty":"03 November, 2018","published_at_pretty":"04 November, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-11-03T13:04:50.000-04:00","published_at":"2018-11-04T14:23:41.000-05:00","updated_at":"2019-02-02T08:19:55.000-05:00","meta_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","meta_description":"Making 'open' data more open: use Python's Pandas library and Tabula to extract data from PDFs.","og_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","og_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","og_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","twitter_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","twitter_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Check out the accompanying GitHub repo for this article here\n[https://github.com/grahamalama/school_budget_aggregator].\n\nTechnically, the School District of Philadelphia's budget data for the 2019\nfiscal year is \"open\". It is, after all, made available through the district's \nOpen Data portal  and is freely available to download.\n\nBut just because data is freely available, doesn't mean it's easy to work with.\nThat's what found out when I downloaded the zipped folder, opened it up, and\nfound a heap of PDFs. Joy.\n\nAs a member of Code for Philly [https://codeforphilly.org/], I thought of my\ncompatriots who might want to use school district data in their projects. I knew\nwith a bit of data munging, I could provide a data set that would be more easily\nusable.\n\nData Liberation\nThe first hurdle was to find a way to get the data from the PDFs. After a bit\nGoogling, I came across tabula-py [https://github.com/chezou/tabula-py], a\nPython wrapper for Tabula [https://tabula.technology/].\n\nEach budget is composed of 5 tables:\n\n * General information about the school\n * Enrollment information\n * Operating Funded budget allotments\n * Grant Funded budget allotments\n * A summary table of allotment totals\n\nExtracting these tables from a budget with Tabula was as simple as:\n\ntabula.read_pdf(path_to_budget, multiple_tables=True)\n\n\nWhich returned a list of DataFrames, one for each table mentioned above.\nPerfect! \nSo, I iterated over all of the files in folder and appended them to a list:\n\nimport os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f\"{directory}/{filename}\", \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(\"SY1819_School_Budgets\")\n\n\nInitial Cleaning\nWhile this gave me a good start, I knew it wouldn't be that easy to liberate the\ndata from the PDFs. I took a look at each of the DataFrames to see what I'd be\nworking with. \n\n# an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n\n\nAfter I saw the output, I wrote a function to perform the same cleaning\noperation for each table in each budget.\n\nFor each table below, first I'll introduce the \"raw\" output that Tabula\nreturned, then I'll show the function that I wrote to fix that output.\n\nBasic Information\nRaw Output\nbasic_information = sample_budget[0] #basic information\nbasic_information\n\n\n0\n 1\n 0\n Basic Information\n NaN\n 1\n Council District\n 2nd\n 2\n Organization Code\n 1380\n 3\n School Level\n Elementary School\n 4\n Economically\n NaN\n 5\n Disadvantaged Rate*\n NaN\n 6\n NaN\n 83.44%\n Cleanup Function\ndef generate_basic_information_table(df):\n    '''Series representing the \"basic information\" table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [\"Economically Disadvantaged Rate\", df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n\n\nCleaned\nbasic_information = generate_basic_information_table(basic_information)\nbasic_information\n\n\n# Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n\n\nEnrollment\nRaw Output\n# Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n\n\n0\n 1\n 2\n 3\n 0\n NaN\n FY14\n FY18\n FY19 Projected\n 1\n Enrollment**\n 842\n 640\n 602\n Cleanup Function\ndef generate_enrollment_table(df):\n    '''returns a series representing the \"enrollment\" table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=\"enrollment\")\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n\n\nCleaned\n# Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n\n\nAllotments\nLuckily, both allotment tables were identical, so I could apply to the same\ncleanup steps to both.\n\nRaw Output\noperating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n\n\n0\n 1\n 2\n 3\n 0\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n 2\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n 3\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n 4\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n 5\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n 6\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n 7\n Secretaries\n 1.0\n 1.0\n 1.0\n 8\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n 9\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n 10\n Other\n 0.0\n 1.2\n 1.0\n 11\n Total Positions\n 60.0\n 43.8\n 50.1\n 12\n Supplies/Equipment/Non Full‐Time Salaries/Other\n $32,272\n $100,159\n $97,553\n grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n\n\nCleanup Function\nI decided to merge the two allotment tables into one DataFrame while building a\nMultiIndex to keep things in order. This would allow me to ask some more\ninteresting questions further on down the road.\n\ndef generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + \"ed_allotments\"\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(\"org_code\", \"fund\", \"allotment\")\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n\n\nCleaned\npd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, \"1410\", \"operating_fund\"\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, \"1410\", \"grant_fund\"\n    )\n])\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment\n 1410\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n Totals\nSince the final \"totals\" table could be calculated from the data already in the\nnew allotment table, I didn't bother transforming it in any way.\n\n# same data can be derived from the allotments table directly\nsample_budget[4]\n\n\n0\n 1\n 2\n 3\n 4\n 0\n NaN\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Total Positions\n NaN\n 76.1\n 64.8\n 67.8\n 2\n Total Supplies/Equipment/Non Full‐Time Salarie...\n NaN\n $230,726\n $120,136\n $112,719\n Once I figured out what transformations I needed for each table, I combined\nthem into a function so that, given a list of DataFames from Tabula, I'd get\nthose same tables back neatly formatted.\n\ndef generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n\n\nAggregation Time\nNow that I had cleaned the tables that Tabula produced, it was time to combine\nthem into some aggregated tables.\n\nFirst I wrote a function that would output a Series (representing one row) of\ninformation from all tables for a given school in a given fiscal year. \n\ndef generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +\": \" + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == \"FY19\" else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == \"FY19\" else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(\"FY18\", basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n\n\n# Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n\n\nThen, I applied this function to each list of budgets in the collection and\ncompiled them into a DataFrame.\n\ndef generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = \"FY14\"\nfy14.to_csv(\"output/combined_fy14.csv\")\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = \"FY18\"\nfy18.to_csv(\"output/combined_fy18.csv\")\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = \"FY19\"\nfy19.to_csv(\"output/combined_fy19.csv\")\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(\"output/all_budgets_tabular.csv\")\n\n\nFinally, I wanted to output a CSV that would preserve some of the multi-indexed\nnature of the allotment tables. Here's what I wrote for that.\n\ndef generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(\"output/all_budgets_hierarchical.csv\")\n\nhierarchical_budget\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment  \n 1380\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n grant_fund\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n 5160 rows × 3 columnsThis makes it easier to aggregate in interesting ways:\n\nhierarchical_budget.groupby('allotment').sum()\n\n\nFY14\n FY18\n FY19\n allotment\n Classroom Assistants/Teacher Assistants\n 2365.0\n 1720.0\n 1935.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 258.0\n 215.0\n 258.0\n Nurses/Health Services\n 129.0\n 215.0\n 215.0\n Other\n 215.0\n 258.0\n 215.0\n Principals/Assistant Principals\n 430.0\n 215.0\n 215.0\n Secretaries\n 215.0\n 215.0\n 215.0\n Student Climate Staff\n 1720.0\n 1720.0\n 1505.0\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 49606090.0\n 25829240.0\n 24234585.0\n Support Services Assistants\n 1505.0\n 1505.0\n 1720.0\n Teachers ‐ Regular Education\n 8234.5\n 7224.0\n 7224.0\n Teachers ‐ Special Education\n 1290.0\n 645.0\n 1075.0\n Total Positions\n 16361.5\n 13932.0\n 14577.0\n More Cleaning to be Done\nMy work here is done. I saved the data from their not-so-accessible PDF prisons.\nBut now it's time for someone with some domain-specific knowledge to make it\nactionable.\n\nThe biggest weakness with the data in its current form is that there is some\namount of ambiguity as to what the different allotments numbers represent in\nreal-dollar amounts. Only the Supplies/Equipment/Non Full‐Time Salaries/Other \nallotment category came in currency notation – the rest of the allotments were\nrepresented as simple decimal amounts with no context to help interpret what\nthey mean. Do they represent FTE\n[https://en.wikipedia.org/wiki/Full-time_equivalent]? Dollar amounts in\nscientific notation? I'm not sure, but I hope by handing this work off to the\nright people, these questions and more can be answered more easily thanks to a\ncleaner, more accessible data set.","html":"<p><em>Check out the accompanying GitHub repo for this article <a href=\"https://github.com/grahamalama/school_budget_aggregator\">here</a>.</em></p><p>Technically, the School District of Philadelphia's budget data for the 2019 fiscal year is \"open\". It is, after all, made available through the district's <a href=\"https://www.philasd.org/performance/programsservices/open-data/district-information/#budget\">Open Data portal</a> and is freely available to download.</p><p>But just because data is freely available, doesn't mean it's easy to work with. That's what found out when I downloaded the zipped folder, opened it up, and found a heap of PDFs. Joy.</p><p>As a member of <a href=\"https://codeforphilly.org/\">Code for Philly</a>, I thought of my compatriots who might want to use school district data in their projects. I knew with a bit of data munging, I could provide a data set that would be more easily usable.</p><h2 id=\"data-liberation\">Data Liberation</h2><p>The first hurdle was to find a way to get the data from the PDFs. After a bit Googling, I came across <a href=\"https://github.com/chezou/tabula-py\"><strong>tabula-py</strong></a>, a Python wrapper for <a href=\"https://tabula.technology/\">Tabula</a>.</p><p>Each budget is composed of 5 tables:</p><ul><li>General information about the school</li><li>Enrollment information</li><li>Operating Funded budget allotments</li><li>Grant Funded budget allotments</li><li>A summary table of allotment totals</li></ul><p>Extracting these tables from a budget with Tabula was as simple as:</p><pre><code class=\"language-python\">tabula.read_pdf(path_to_budget, multiple_tables=True)\n</code></pre>\n<p>Which returned a list of DataFrames, one for each table mentioned above. Perfect! <br>So, I iterated over all of the files in folder and appended them to a list:</p><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f&quot;{directory}/{filename}&quot;, \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(&quot;SY1819_School_Budgets&quot;)\n</code></pre>\n<h2 id=\"initial-cleaning\">Initial Cleaning</h2><p>While this gave me a good start, I knew it wouldn't be that easy to liberate the data from the PDFs. I took a look at each of the DataFrames to see what I'd be working with. </p><pre><code class=\"language-python\"># an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n</code></pre>\n<p>After I saw the output, I wrote a function to perform the same cleaning operation for each table in each budget.</p><p>For each table below, first I'll introduce the \"raw\" output that Tabula returned, then I'll show the function that I wrote to fix that output.</p><h2 id=\"basic-information\">Basic Information</h2><h3 id=\"raw-output\">Raw Output</h3><pre><code class=\"language-python\">basic_information = sample_budget[0] #basic information\nbasic_information\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Basic Information</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Council District</td>\n      <td>2nd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Organization Code</td>\n      <td>1380</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>School Level</td>\n      <td>Elementary School</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Economically</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Disadvantaged Rate*</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>83.44%</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_basic_information_table(df):\n    '''Series representing the &quot;basic information&quot; table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [&quot;Economically Disadvantaged Rate&quot;, df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n</code></pre>\n<h3 id=\"cleaned\">Cleaned</h3><pre><code class=\"language-python\">basic_information = generate_basic_information_table(basic_information)\nbasic_information\n</code></pre>\n<pre><code class=\"language-python\"># Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n</code></pre>\n<h2 id=\"enrollment\">Enrollment</h2><h4 id=\"raw-output-1\">Raw Output</h4><pre><code class=\"language-python\"># Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>FY14</td>\n      <td>FY18</td>\n      <td>FY19 Projected</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Enrollment**</td>\n      <td>842</td>\n      <td>640</td>\n      <td>602</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function-1\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_enrollment_table(df):\n    '''returns a series representing the &quot;enrollment&quot; table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=&quot;enrollment&quot;)\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n</code></pre>\n<h4 id=\"cleaned-1\">Cleaned</h4><pre><code class=\"language-python\"># Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n</code></pre>\n<h2 id=\"allotments\">Allotments</h2><p>Luckily, both allotment tables were identical, so I could apply to the same cleanup steps to both.</p><h4 id=\"raw-output-2\">Raw Output</h4><pre><code class=\"language-python\">operating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Principals/Assistant Principals</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Teachers ‐ Regular Education</td>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Teachers ‐ Special Education</td>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Counselors/Student Adv./ Soc. Serv. Liaisons</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nurses/Health Services</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Classroom Assistants/Teacher Assistants</td>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Secretaries</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Support Services Assistants</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Student Climate Staff</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Other</td>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Total Positions</td>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Supplies/Equipment/Non Full‐Time Salaries/Other</td>\n      <td>$32,272</td>\n      <td>$100,159</td>\n      <td>$97,553</td>\n    </tr>\n  </tbody>\n</table>\n</div><pre><code class=\"language-python\">grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n</code></pre>\n<h3 id=\"cleanup-function-2\">Cleanup Function</h3><p>I decided to merge the two allotment tables into one DataFrame while building a MultiIndex to keep things in order. This would allow me to ask some more interesting questions further on down the road.</p><pre><code class=\"language-python\">def generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + &quot;ed_allotments&quot;\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(&quot;org_code&quot;, &quot;fund&quot;, &quot;allotment&quot;)\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n</code></pre>\n<h4 id=\"cleaned-2\">Cleaned</h4><pre><code class=\"language-python\">pd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, &quot;1410&quot;, &quot;operating_fund&quot;\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, &quot;1410&quot;, &quot;grant_fund&quot;\n    )\n])\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"24\" valign=\"top\">1410</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"totals\">Totals</h2><p>Since the final \"totals\" table could be calculated from the data already in the new allotment table, I didn't bother transforming it in any way.</p><pre><code class=\"language-python\"># same data can be derived from the allotments table directly\nsample_budget[4]\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Total Positions</td>\n      <td>NaN</td>\n      <td>76.1</td>\n      <td>64.8</td>\n      <td>67.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total Supplies/Equipment/Non Full‐Time Salarie...</td>\n      <td>NaN</td>\n      <td>$230,726</td>\n      <td>$120,136</td>\n      <td>$112,719</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Once I figured out what transformations I needed for each table, I combined them into a function so that, given a list of DataFames from Tabula, I'd get those same tables back neatly formatted.</p><pre><code class=\"language-python\">def generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n</code></pre>\n<h2 id=\"aggregation-time\">Aggregation Time</h2><p>Now that I had cleaned the tables that Tabula produced, it was time to combine them into some aggregated tables.</p><p>First I wrote a function that would output a Series (representing one row) of information from all tables for a given school in a given fiscal year. </p><pre><code class=\"language-python\">def generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +&quot;: &quot; + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == &quot;FY19&quot; else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == &quot;FY19&quot; else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(&quot;FY18&quot;, basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n</code></pre>\n<pre><code class=\"language-python\"># Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n</code></pre>\n<p>Then, I applied this function to each list of budgets in the collection and compiled them into a DataFrame.</p><pre><code class=\"language-python\">def generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = &quot;FY14&quot;\nfy14.to_csv(&quot;output/combined_fy14.csv&quot;)\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = &quot;FY18&quot;\nfy18.to_csv(&quot;output/combined_fy18.csv&quot;)\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = &quot;FY19&quot;\nfy19.to_csv(&quot;output/combined_fy19.csv&quot;)\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(&quot;output/all_budgets_tabular.csv&quot;)\n</code></pre>\n<p>Finally, I wanted to output a CSV that would preserve some of the multi-indexed nature of the allotment tables. Here's what I wrote for that.</p><pre><code class=\"language-python\">def generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(&quot;output/all_budgets_hierarchical.csv&quot;)\n\nhierarchical_budget\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>       \n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"61\" valign=\"top\">1380</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">grant_fund</th>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n<div style=\"text-align: right;\n    width: 100%;\n    font-family: Gordita-Medium,sans-serif;\n    font-size: .9em;\n    margin-top: -20px;\">5160 rows × 3 columns</div>\n</div><p>This makes it easier to aggregate in interesting ways:</p><pre><code class=\"language-python\">hierarchical_budget.groupby('allotment').sum()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>2365.0</td>\n      <td>1720.0</td>\n      <td>1935.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>258.0</td>\n      <td>215.0</td>\n      <td>258.0</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>129.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>215.0</td>\n      <td>258.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Principals/Assistant Principals</th>\n      <td>430.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>215.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>1720.0</td>\n      <td>1720.0</td>\n      <td>1505.0</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>49606090.0</td>\n      <td>25829240.0</td>\n      <td>24234585.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>1505.0</td>\n      <td>1505.0</td>\n      <td>1720.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8234.5</td>\n      <td>7224.0</td>\n      <td>7224.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>1290.0</td>\n      <td>645.0</td>\n      <td>1075.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16361.5</td>\n      <td>13932.0</td>\n      <td>14577.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"more-cleaning-to-be-done\">More Cleaning to be Done</h2><p>My work here is done. I saved the data from their not-so-accessible PDF prisons. But now it's time for someone with some domain-specific knowledge to make it actionable.</p><p>The biggest weakness with the data in its current form is that there is some amount of ambiguity as to what the different allotments numbers represent in real-dollar amounts. Only the <strong>Supplies/Equipment/Non Full‐Time Salaries/Other</strong> allotment category came in currency notation – the rest of the allotments were represented as simple decimal amounts with no context to help interpret what they mean. Do they represent <a href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\">FTE</a>? Dollar amounts in scientific notation? I'm not sure, but I hope by handing this work off to the right people, these questions and more can be answered more easily thanks to a cleaner, more accessible data set.</p>","url":"https://hackersandslackers.com/liberating-data-from-pdfs-with-tabula-and-pandas/","uuid":"ab1a4ee3-9cc3-43a6-9ebe-5a885ae264a2","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bddd5323ea1e4769817c4c9"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736d6","title":"Importing Excel Datetimes Into Pandas, Part II","slug":"importing-excel-datetimes-into-pandas-part-2","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas.jpg","excerpt":"Pandas and Excel Pt. 2","custom_excerpt":"Pandas and Excel Pt. 2","created_at_pretty":"13 August, 2018","published_at_pretty":"20 August, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-08-12T22:28:46.000-04:00","published_at":"2018-08-20T07:30:00.000-04:00","updated_at":"2019-04-10T00:47:31.000-04:00","meta_title":"Pandas and Excel Pt. 2 | Hackers And Slackers","meta_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","og_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","og_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas-2.jpg","og_title":"Importing Excel Datetimes Into Pandas II","twitter_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas-1.jpg","twitter_title":"Importing Excel Datetimes Into Pandas II","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"What if, like during my data import task a few months back, the dates & times\nare in separate columns?  This gives us a few new issues.  Let's import that\nExcel file!\n\nimport pandas as pd\nimport xlrd\nimport datetime\n\ndf = pd.read_excel(\"hasDatesAndTimes.xlsx\", sheet_name=\"Sheet1\")\n\nbook = xlrd.open_workbook(\"hasDatesAndTimes.xlsx\")\ndatemode = book.datemode\n\n\n\nAnd let's see that time variable!\n\ndf[\"Time\"]\n\n\nIndex\n Time\n 0\n 0.909907\n 1\n 0.909919\n 2\n 0.909931\n 3\n 0.909942\n 4\n 0.909954\n df[\"Time\"].map(lambda x: xlrd.xldate_as_tuple(x, datemode))\n\n\nSo far, so good....\n\ndf[\"Time\"].map(lambda x: datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                              datemode)))\nValueError: year 0 is out of range\n\n\nAgh!  Plain datetime won't let us have 0 as our year.\n\nWe'll want two packages to fix this.  One is an awesome package for handling\ndates called arrow.  In order for arrow  to recognize what we want it to,\nthough, we'll need some more manipulations - I'll be using the pipe  function\nfrom toolz  in order to make that more readable.\n\nIndex\n Time\n 0\n (0, 0, 0, 21, 50, 16)\n 1\n (0, 0, 0, 21, 50, 17)\n 2\n (0, 0, 0, 21, 50, 18)\n 3\n (0, 0, 0, 21, 50, 19)\n 4\n (0, 0, 0, 21, 50, 20)\n Pipe lets us have a nice workflow where we just list the transformations we\nwant, and our value will be \"piped\" sequentially through each one.\n\nfns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n     lambda x: x[3:6],\n      lambda x: map(str, x),\n      lambda x: \"-\".join(x),\n       lambda x: arrow.get(x, \"H-m-s\"),\n       lambda x: x.format('HH:mm:ss')\n      ]\n\n\nLet's see a blow-by-blow of how one of our values gets transformed by that.\n\nfnRanges = [fns[:i+1] for i in range(len(fns))]\n[pipe(0.909907, *x) for x in fnRanges]\n\n[(0, 0, 0, 21, 50, 16),\n (21, 50, 16),\n <map at 0x7f105151af98>,\n '21-50-16',\n <Arrow [0001-01-01T21:50:16+00:00]>,\n '21:50:16']\n\n\nThe first function takes us from an Excel datetime to a datetime tuple.\n\nThe next extracts just the time variables.\n\nWe then map that all to a string (which shows up as nothing because map  is\nevaluated lazily).\n\nThen we put a dash between all those elements so it'll be easier to parse as a\ntime.\n\nThen arrow  consumes the value, with the format we specified.\n\nAnd finally gives us a neatly-formatted time, ready to be consumed by a\ndatabase!\n\nHelper Functions\ndef mapPipe(ser, fns):\n    return ser.map(lambda a: pipe(a, *fns),\n        na_action=\"ignore\" )\n\nmapPipe(df['Time'],\n   fns)\n\n\nIndex\n Time\n 0\n 21:50:16\n 1\n 21:50:17\n 2\n 21:50:18\n 3\n 21:50:19\n 4\n 21:50:20\n Dates are a bit easier - though the pipe  syntax is still helpful!\n\ndateFns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n      lambda x: arrow.get(*x),\n      lambda x: x.format('YYYY-MM-DD')\n      ]\nmapPipe(df['Date'],\n       dateFns)\n\n\nIndex\n Date\n 0\n 2018-08-12\n 1\n 2018-08-12\n 2\n 2018-08-12\n 3\n 2018-08-12\n 4\n 2018-08-12\n Put it all together....\n\n(df.assign(Date = mapPipe(df['Date'],\n       dateFns))\n   .assign(Time = mapPipe(df['Time'],\n       fns)))\n\n\nIndex\n Date\n Time\n 0\n 2018-08-12\n 21:50:16\n 1\n 2018-08-12\n 21:50:17\n 2\n 2018-08-12\n 21:50:18\n 3\n 2018-08-12\n 21:50:19\n 4\n 2018-08-12\n 21:50:20","html":"<p>What if, like during my data import task a few months back, the dates &amp; times are in separate columns?  This gives us a few new issues.  Let's import that Excel file!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\nimport xlrd\nimport datetime\n\ndf = pd.read_excel(&quot;hasDatesAndTimes.xlsx&quot;, sheet_name=&quot;Sheet1&quot;)\n\nbook = xlrd.open_workbook(&quot;hasDatesAndTimes.xlsx&quot;)\ndatemode = book.datemode\n\n</code></pre>\n<!--kg-card-end: markdown--><p>And let's see that time variable!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;]\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.909907</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.909919</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.909931</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.909942</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.909954</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;].map(lambda x: xlrd.xldate_as_tuple(x, datemode))\n</code></pre>\n<!--kg-card-end: markdown--><p>So far, so good....</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;].map(lambda x: datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                              datemode)))\nValueError: year 0 is out of range\n</code></pre>\n<!--kg-card-end: markdown--><p>Agh!  Plain datetime won't let us have 0 as our year.</p><p>We'll want two packages to fix this.  One is an awesome package for handling dates called <code>arrow</code>.  In order for <code>arrow</code> to recognize what we want it to, though, we'll need some more manipulations - I'll be using the <code>pipe</code> function from <code>toolz</code> in order to make that more readable.</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>(0, 0, 0, 21, 50, 16)</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>(0, 0, 0, 21, 50, 17)</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>(0, 0, 0, 21, 50, 18)</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>(0, 0, 0, 21, 50, 19)</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>(0, 0, 0, 21, 50, 20)</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Pipe lets us have a nice workflow where we just list the transformations we want, and our value will be \"piped\" sequentially through each one.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">fns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n     lambda x: x[3:6],\n      lambda x: map(str, x),\n      lambda x: &quot;-&quot;.join(x),\n       lambda x: arrow.get(x, &quot;H-m-s&quot;),\n       lambda x: x.format('HH:mm:ss')\n      ]\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see a blow-by-blow of how one of our values gets transformed by that.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">fnRanges = [fns[:i+1] for i in range(len(fns))]\n[pipe(0.909907, *x) for x in fnRanges]\n\n[(0, 0, 0, 21, 50, 16),\n (21, 50, 16),\n &lt;map at 0x7f105151af98&gt;,\n '21-50-16',\n &lt;Arrow [0001-01-01T21:50:16+00:00]&gt;,\n '21:50:16']\n</code></pre>\n<!--kg-card-end: markdown--><p>The first function takes us from an Excel datetime to a datetime tuple.</p><p>The next extracts just the time variables.</p><p>We then map that all to a string (which shows up as nothing because <code>map</code> is evaluated lazily).</p><p>Then we put a dash between all those elements so it'll be easier to parse as a time.</p><p>Then <code>arrow</code> consumes the value, with the format we specified.</p><p>And finally gives us a neatly-formatted time, ready to be consumed by a database!</p><h2 id=\"helper-functions\">Helper Functions</h2><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def mapPipe(ser, fns):\n    return ser.map(lambda a: pipe(a, *fns),\n        na_action=&quot;ignore&quot; )\n\nmapPipe(df['Time'],\n   fns)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>21:50:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>21:50:17</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>21:50:18</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>21:50:19</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Dates are a bit easier - though the <code>pipe</code> syntax is still helpful!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">dateFns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n      lambda x: arrow.get(*x),\n      lambda x: x.format('YYYY-MM-DD')\n      ]\nmapPipe(df['Date'],\n       dateFns)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2018-08-12</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Put it all together....</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">(df.assign(Date = mapPipe(df['Date'],\n       dateFns))\n   .assign(Time = mapPipe(df['Time'],\n       fns)))\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Date</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2018-08-12</td>\n      <td>21:50:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2018-08-12</td>\n      <td>21:50:17</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2018-08-12</td>\n      <td>21:50:18</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2018-08-12</td>\n      <td>21:50:19</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2018-08-12</td>\n      <td>21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html-->","url":"https://hackersandslackers.com/importing-excel-datetimes-into-pandas-part-2/","uuid":"f106291a-af02-4b8a-87b4-7afe333a9548","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b70ecde0230162100a1daa5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736d3","title":"Importing Excel Datetimes Into Pandas, Part I","slug":"importing-excel-dates-times-into-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1.jpg","excerpt":"Pandas & Excel, Part 1.","custom_excerpt":"Pandas & Excel, Part 1.","created_at_pretty":"13 August, 2018","published_at_pretty":"13 August, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-08-12T20:21:40.000-04:00","published_at":"2018-08-13T07:30:00.000-04:00","updated_at":"2019-04-09T23:40:15.000-04:00","meta_title":"Pandas & Excel, Part 1 | Hackers And Slackers","meta_description":"Import dates & times from Excel .xlsx files into Pandas!","og_description":"Import dates & times from Excel .xlsx files into Pandas!","og_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1-2.jpg","og_title":"Importing Excel Datetimes Into Pandas","twitter_description":"Import dates & times from Excel .xlsx files into Pandas!","twitter_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1-1.jpg","twitter_title":"Importing Excel Datetimes Into Pandas","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Different file formats are different!  For all kinds of reasons!\n\nA few months back, I had to import some Excel files into a database. In this\nprocess I learned so much about the delightfully unique way Excel stores dates &\ntimes!\n\nThe basic datetime will be a decimal number, like 43324.909907407404.  The\nnumber before the decimal is the day, the number afterwards is the time.  So\nfar, so good - this is pretty common for computers.  The date is often the\nnumber of days past a certain date, and the time is the number of seconds.\n\nSo, let's load our excel sheet!  Pandas of course has a painless way of doing\nthis.\n\nimport pandas as pd\n\ndfRaw = pd.read_excel(\"hasDates.xlsx\", sheet_name=\"Sheet1\")\n\ndfRaw[\"dateTimes\"]\n\n\n0\n 0\n 43324.909907\n 1\n 43324.909919\n 2\n 43324.909931\n 3\n 43324.909942\n 4\n 43324.909954\n Sadly, we can't yet convert these.  Different Excel files start at different\ndates, and you'll get a very wrong result if you use the wrong one.  Luckily,\nthere are tools that'll go into the file and extract what we need!  Enter xlrd:\n\nimport xlrd\n\nbook = xlrd.open_workbook(\"hasDates.xlsx\")\ndatemode = book.datemode\n\n\nxlrd  also has a handy function for turning those dates into a datetime  tuple\nthat'll play nicely with Python.\n\ndfRaw[\"dateTimes\"].map(lambda x: \n          xlrd.xldate_as_tuple(x, datemode))\n\n\n0\n 0\n (2018, 8, 12, 21, 50, 16)\n 1\n (2018, 8, 12, 21, 50, 17)\n 2\n (2018, 8, 12, 21, 50, 18)\n 3\n (2018, 8, 12, 21, 50, 19)\n 4\n (2018, 8, 12, 21, 50, 20)\n And once we've got that, simple enough to convert to proper datetimes!\n\nimport datetime\n\ndfRaw[\"dateTimes\"].map(lambda x: \n          datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                                  datemode)))\n\n\n0\n 0\n 2018-08-12 21:50:16\n 1\n 2018-08-12 21:50:17\n 2\n 2018-08-12 21:50:18\n 3\n 2018-08-12 21:50:19\n 4\n 2018-08-12 21:50:20\n Stick around for Part 2, where we look at some messier situations.","html":"<p>Different file formats are different!  For all kinds of reasons!</p><p>A few months back, I had to import some Excel files into a database. In this process I learned so much about the delightfully unique way Excel stores dates &amp; times!  </p><p>The basic datetime will be a decimal number, like <code>43324.909907407404</code>.  The number before the decimal is the day, the number afterwards is the time.  So far, so good - this is pretty common for computers.  The date is often the number of days past a certain date, and the time is the number of seconds.  </p><p>So, let's load our excel sheet!  Pandas of course has a painless way of doing this.</p><pre><code class=\"language-python\">import pandas as pd\n\ndfRaw = pd.read_excel(&quot;hasDates.xlsx&quot;, sheet_name=&quot;Sheet1&quot;)\n\ndfRaw[&quot;dateTimes&quot;]\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43324.909907</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43324.909919</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43324.909931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43324.909942</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>43324.909954</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Sadly, we can't yet convert these.  Different Excel files start at different dates, and you'll get a very wrong result if you use the wrong one.  Luckily, there are tools that'll go into the file and extract what we need!  Enter <code>xlrd</code>:</p><pre><code class=\"language-python\">import xlrd\n\nbook = xlrd.open_workbook(&quot;hasDates.xlsx&quot;)\ndatemode = book.datemode\n</code></pre>\n<p><code>xlrd</code> also has a handy function for turning those dates into a <code>datetime</code> tuple that'll play nicely with Python.</p><pre><code class=\"language-python\">dfRaw[&quot;dateTimes&quot;].map(lambda x: \n          xlrd.xldate_as_tuple(x, datemode))\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(2018, 8, 12, 21, 50, 16)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(2018, 8, 12, 21, 50, 17)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(2018, 8, 12, 21, 50, 18)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(2018, 8, 12, 21, 50, 19)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(2018, 8, 12, 21, 50, 20)</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>And once we've got that, simple enough to convert to proper datetimes!</p><pre><code class=\"language-python\">import datetime\n\ndfRaw[&quot;dateTimes&quot;].map(lambda x: \n          datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                                  datemode)))\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-08-12 21:50:16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-08-12 21:50:17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-08-12 21:50:18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-08-12 21:50:19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-08-12 21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Stick around for Part 2, where we look at some messier situations.</p>","url":"https://hackersandslackers.com/importing-excel-dates-times-into-pandas/","uuid":"727f6571-8ca4-4abc-b278-c7517cdaa29b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b70cf140230162100a1da9b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c8","title":"Lazy Pandas and Dask","slug":"cutting-a-file-down-to-size-with-dask","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","excerpt":"Increase the performance of Pandas with Dask.","custom_excerpt":"Increase the performance of Pandas with Dask.","created_at_pretty":"05 August, 2018","published_at_pretty":"06 August, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-08-05T17:09:51.000-04:00","published_at":"2018-08-06T07:30:00.000-04:00","updated_at":"2019-02-19T05:19:53.000-05:00","meta_title":"Picking Low-Hanging Fruit With Dask | Hackers And Slackers","meta_description":"Dask is library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core. ","og_description":"Lazy Pandas and Dask","og_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","og_title":"Lazy Pandas and Dask","twitter_description":"Picking Low-Hanging Fruit With Dask","twitter_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","twitter_title":"Lazy Pandas and Dask","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Ah, laziness.  You love it, I love it, everyone agrees it's just better.\n\nFlesh-and-blood are famously lazy.  Pandas the package, however, uses Eager\nEvaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging\nout on the street corner and being fierce to the style choices of people walking\nby?  Well, yes, but that's not the most relevant sense in which I mean it here.\n\nEager evaluation means that once you call pd.read_csv(), Pandas immediately\njumps to read the whole CSV into memory.\n\n\"Wait!\" I hear you ask.\"Isn't that what we want?  Why would I call the function\nif I didn't want it to happen?\"\n\nEventually, yes that is what you want.  But sometimes you want some time in\nbetween when you give the command and when the computer hops to it.  That's why\nit's Lazy and not Inactive - it will get to the job at some point, it'll just\nprocrastinate a bit.\n\nFor example, last week I was tasked with searching through the output of a\ngovernment API.  It had records since the 90s, and was about 300MB.  Now, this\nisn't actually outside the realm of what Pandas can handle - it's quite\noptimized, and as long as the file can fit into memory, Pandas can mess with.\n However, it won't necessarily be fast.  Furthermore, my laptop is old and I\ndidn't feel like offloading what I was doing to a remote machine.\n\nFurthermore, I knew I actually only needed a subset of the file.  Here's where\nLaziness comes in handy.  With Eager evaluation, Pandas would have to load the\nwhole thing into memory, then filter based on my criteria.\n\nEnter Dask:  Dask is a very cool little library that seamlessly allows you to\nparallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed\nto only work on one core.  Dask, on the other hand, lets you split the work\nbetween different cores - both on a single machine, or on a distributed system.\n It doesn't implement every single thing you can do with Pandas, though, so only\nuse it when you have to.\n\n  I probably should have titled this post \"Parallel Pandas\", but hey, too late\nnow - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.\n Anyway, Laziness is  part of the process.  Because Dask is lazy by default\n(much like your humble narrator), we can define our fileout loading it, like so:\n\nimport dask.dataframe as dd\n\ndf = dd.read_csv(\"giantThing.csv\")\n\n\nPandas was taking a long time to parse the file.  What's more is that this file\nhad a few quirks - I'd figured out that it needed a special text encoding, and I\nwasn't sure if there was other weirdness going on.  Was my computer just taking\na long time to nom the file, or was I going to wait there for a few minutes to\nfind an error message?  It's a catch-22 - I needed to figure out how to cut the\nfile down to size, but in order to do that I would have needed to be able to\nload it into memory.  Dask to the rescue!\n\nThis file wasn't terribly well-documented - I had an inkling as to what column\nwould tell me which rows I wanted, but I wasn't sure.  So, first thing I did was\ncheck out the first few rows.  Remember, in order to see these first 5 rows in\nPandas, I'd have to load the whole thing into memory (which might or might not\neven work!).\n\ndf.head()\n\nWith that, I was able to do a simple spot-check and see if there were any weird\ncolumns that might get in the way of parsing.  Furthermore, I confirmed that the\nID columns I was looking at contained something vaguely like what I was looking\nfor.  Even MORE interestingly, I found that it was formatted slightly\nirregularly.  Even more use for laziness!  Let's load just that one column into\nmemory (you could do this with a loop, sure - but selecting a single column is a\nlot clumsier)\n\ndf[\"ORG_NAME\"].compute()\n\nNote the .compute()  method at the end.  That's necessary because of the Lazy\nEvaluation - just calling a column name doesn't make Dask think you necessarily\nwant the thing now.  I'm not sure why I didn't have to call it with df.head(),\nthough (that's the Hackers & Slackers Codeblogging vérité style!).\n\nSo, now that I've seen the formatting, I found out that I'm going to have to\nfilter it with a call of a str.contains()  method instead of an exact value.\n Let's poke around a teensy bit more.\n\norgDF = df[\"ORG_NAME\"]\norgFiltered = corp[corp.str.contains(\"baseName\", na=False)].compute().shape\n\n\nTurns out it was only about 800 rows!So, let's filter that and make a regular\nPandas Dataframe (and probably a new CSV for later!)\n\ndf = dd.read_csv(\"giantThing.csv\")\n\norgFiltered = df[df[\"ORG_NAME\"].str.contains(\"baseName\", na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(\"filteredThing.csv\")\n\n\nNote that I actually could have done this with base Pandas, through use of the\niterator flag.  However, I didn't realize that it's only wind up being so few\nrows.  It also would have been slower - and the speed difference makes a huge\ndifference in terms of how fluidly you can explore.\n\nFor instance, the na=False  flag was something I discovered would be needed\nbecause of a quirk in the file - again, this sort of thing becomes a lot easier\ndo diagnose when you can iterate quickly, and you know you're not going to just\ntimeout from running out of memory.\n\nFor comparison's sake, here's the code for filtering on the fly and loading into\nPandas:\n\niter_csv = pd.read_csv(\"giantThing.csv\",\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[\"ORG_NAME\"].str.contains(\"baseName\", na=False)] \n                for chunk in iter_csv])\n\n\nOn my computer, that took a little over 3 minutes.  While the Dask code took\nabout a minute.","html":"<p>Ah, laziness.  You love it, I love it, everyone agrees it's just better.</p><p>Flesh-and-blood are famously lazy.  Pandas the package, however, uses Eager Evaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging out on the street corner and being fierce to the style choices of people walking by?  Well, yes, but that's not the most relevant sense in which I mean it here.  </p><p>Eager evaluation means that once you call <code>pd.read_csv()</code>, Pandas immediately jumps to read the whole CSV into memory.</p><p><strong>\"Wait!\" </strong>I hear you ask.  <strong>\"Isn't that what we want?  Why would I call the function if I didn't want it to happen?\"</strong></p><p><em>Eventually</em>, yes that is what you want.  But sometimes you want some time in between when you give the command and when the computer hops to it.  That's why it's Lazy and not Inactive - it will get to the job at some point, it'll just procrastinate a bit.</p><p>For example, last week I was tasked with searching through the output of a government API.  It had records since the 90s, and was about 300MB.  Now, this isn't actually outside the realm of what Pandas can handle - it's quite optimized, and as long as the file can fit into memory, Pandas can mess with.  However, it won't necessarily be fast.  Furthermore, my laptop is old and I didn't feel like offloading what I was doing to a remote machine.  </p><p>Furthermore, I knew I actually only needed a subset of the file.  Here's where Laziness comes in handy.  With Eager evaluation, Pandas would have to load the whole thing into memory, then filter based on my criteria.</p><p>Enter Dask:  Dask is a very cool little library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core.  Dask, on the other hand, lets you split the work between different cores - both on a single machine, or on a distributed system.  It doesn't implement every single thing you can do with Pandas, though, so only use it when you have to.</p><p> I probably should have titled this post \"Parallel Pandas\", but hey, too late now - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.  Anyway, Laziness <em>is</em> part of the process.  Because Dask is lazy by default (much like your humble narrator), we can define our fileout loading it, like so:</p><pre><code class=\"language-python\">import dask.dataframe as dd\n\ndf = dd.read_csv(&quot;giantThing.csv&quot;)\n</code></pre>\n<p>Pandas was taking a long time to parse the file.  What's more is that this file had a few quirks - I'd figured out that it needed a special text encoding, and I wasn't sure if there was other weirdness going on.  Was my computer just taking a long time to nom the file, or was I going to wait there for a few minutes to find an error message?  It's a catch-22 - I needed to figure out how to cut the file down to size, but in order to do that I would have needed to be able to load it into memory.  Dask to the rescue!</p><p>This file wasn't terribly well-documented - I had an inkling as to what column would tell me which rows I wanted, but I wasn't sure.  So, first thing I did was check out the first few rows.  Remember, in order to see these first 5 rows in Pandas, I'd have to load the whole thing into memory (which might or might not even work!).</p><p><code>df.head()</code></p><p>With that, I was able to do a simple spot-check and see if there were any weird columns that might get in the way of parsing.  Furthermore, I confirmed that the ID columns I was looking at contained something vaguely like what I was looking for.  Even MORE interestingly, I found that it was formatted slightly irregularly.  Even more use for laziness!  Let's load just that one column into memory (you could do this with a loop, sure - but selecting a single column is a lot clumsier)</p><p><code>df[\"ORG_NAME\"].compute()</code></p><p>Note the <code>.compute()</code> method at the end.  That's necessary because of the Lazy Evaluation - just calling a column name doesn't make Dask think you necessarily want the thing now.  I'm not sure why I didn't have to call it with <code>df.head()</code>, though (that's the Hackers &amp; Slackers Codeblogging vérité style!).</p><p>So, now that I've seen the formatting, I found out that I'm going to have to filter it with a call of a <code>str.contains()</code> method instead of an exact value.  Let's poke around a teensy bit more.</p><pre><code class=\"language-python\">orgDF = df[&quot;ORG_NAME&quot;]\norgFiltered = corp[corp.str.contains(&quot;baseName&quot;, na=False)].compute().shape\n</code></pre>\n<blockquote>Turns out it was only about 800 rows!</blockquote><p>So, let's filter that and make a regular Pandas Dataframe (and probably a new CSV for later!)</p><pre><code class=\"language-python\">df = dd.read_csv(&quot;giantThing.csv&quot;)\n\norgFiltered = df[df[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(&quot;filteredThing.csv&quot;)\n</code></pre>\n<p>Note that I actually could have done this with base Pandas, through use of the iterator flag.  However, I didn't realize that it's only wind up being so few rows.  It also would have been slower - and the speed difference makes a huge difference in terms of how fluidly you can explore.</p><p>For instance, the <code>na=False</code> flag was something I discovered would be needed because of a quirk in the file - again, this sort of thing becomes a lot easier do diagnose when you can iterate quickly, and you know you're not going to just timeout from running out of memory.</p><p>For comparison's sake, here's the code for filtering on the fly and loading into Pandas:</p><pre><code class=\"language-python\">iter_csv = pd.read_csv(&quot;giantThing.csv&quot;,\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)] \n                for chunk in iter_csv])\n</code></pre>\n<p>On my computer, that took a little over 3 minutes.  While the Dask code took about a minute.</p>","url":"https://hackersandslackers.com/cutting-a-file-down-to-size-with-dask/","uuid":"d4270325-d03f-46fd-a0ae-1b3cbfe1b527","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b67679f17f6083e60a44c5d"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c4","title":"All That Is Solid Melts Into Graphs","slug":"all-that-is-solid-melts-into-graphs","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","excerpt":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair.","custom_excerpt":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair.","created_at_pretty":"26 July, 2018","published_at_pretty":"30 July, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-07-25T21:53:23.000-04:00","published_at":"2018-07-30T07:30:00.000-04:00","updated_at":"2019-02-02T04:07:03.000-05:00","meta_title":"All That Is Solid Melts Into Graphs | Hackers and Slackers","meta_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair","og_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with #Altair","og_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","og_title":"All That Is Solid Melts Into Graphs","twitter_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with #Altair","twitter_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","twitter_title":"All That Is Solid Melts Into Graphs","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Last few Code Snippet Corners were about using Pandas as an easy way to handle\ninput and output between files & databases.  Let's shift gears a little bit!\n Among other reasons, because earlier today I discovered a package that\nexclusively does that, which means I can stop importing the massive Pandas\npackage when all I really wanted to do with it was take advantage of its I/O\nmodules.Check it out [https://github.com/insightindustry/sqlathanor]! \n\nSo, rather than the entrances & exits, let's focus on all the crazy ways you can\nreshape data with Pandas!\n\nOur Data\nFor our demonstration, I'll use a dataset based on something I was once actually\nsent.  It was a big CSV with sensor readings from HVAC systems.  Each line had a\ndifferent house, a different room, a datetime, and readings from a bunch of\ndifferent types of sensors.  Oh, hrm, I probably shouldn't use data I got from a\nclient.  Uh...\n\nBONUS SECTION!\nGENERATING DUMMY TEMPERATURE DATA\n(Feel free to skip to next part if you don't care)\n\nWe want it to fluctuate, but we don't want to just make a bunch of totally\nrandom values - a reading should have some relationship to the reading taken a\nsecond earlier.\n\nLet's use NumPy  for some Randomness, and the accumulate  and repeat  functions\nfrom itertools.  Maybe I'll do an in-depth post on these at some point, but the\ncode I'll be writing with them will be pretty short and hopefully somewhat\nself-demonstrating.  If you wanna go deeper here's some good material: Official\nDocs [https://docs.python.org/3/library/itertools.html], Good article\n[https://realpython.com/python-itertools/]\n\nimport numpy as np\nfrom itertools import accumulate, repeat\n\n\nWe want there to be some random \"noise\", but we also want the occasional\nsubstantive change.  We'll reflect this by having it so that 90% of the time we\nget a small fluctuation, with a 10% chance of a smaller fluctuation. \n\ndef genTempDataPoint(x, *args):\n    if np.random.rand(1) <= 0.9:\n        return x + np.random.uniform(-3,3,1)[0]\n    else:\n        return x + np.random.uniform(-10,10,1)[0]\n\n\n  Now let's see some test points!\n\nlist(accumulate(repeat(70, 5), genTempDataPoint))\n[70,\n 69.00258239202094,\n 59.34919781643355,\n 56.60722073795931,\n 57.265078261782946]\n\n\nSure, fine, why not.  Good enough for our purposes!   Now let's put it all\ntogether so we can just call it with a base temp and the number of points we\nwant.\n\ndef genTempData(base, n):\n    return list(accumulate(repeat(base, n), \n                           genTempDataPoint))\n\n\nTo simulate the dataset, we actually need to mix it up.  Or else what good are\nthe GroupBys gonna be?  So, let's create a problem to fix later!  Here's a\nfunction to create a simplified version of the dataset - each row will have a\nlocation ID, a number corresponding to time (just raw ints, I'm not making\nactual datetimes - I've spent too much time on this part already).  We'll also\ngenerate humidity values, to add another monkey wrench to fix later (we'll still\nuse the genTempData  function).\n\nfrom itertools import chain\n\ndef makeLocation(name, base1, n1, base2, n2):\n    return [(x[0], name, x[1][0], x[1][1]) \n        for x in enumerate(zip(genTempData(base1, n1),\n              genTempData(base2, n2)) )]\n\nbigList = list(chain.from_iterable(makeLocation(str(x), \n                                                70, \n                                                15,\n                                                40, \n                                                15) \n                         for x in range(5)))\nnp.random.shuffle(bigList)\n\ndf = pd.DataFrame(bigList, \n                  columns = [\"Time\", \"Loc\", \"Temp\", \"Hum\"])\n\n\nBack To The Main Plot\nLet's look at some test rows!\n\n# Viewing test rows\n\ndf.iloc[:5]\nTime\tLoc\tTemp     \tHum\n10\t4\t68.396970\t34.169753\n13\t0\t80.288846\t42.076786\n7\t4\t69.923273\t37.967951\n6\t0\t71.781362\t41.186802\n5\t2\t62.678844\t37.321636\n\n\nNow, when I'm getting started with a new dataset, one of the first things I like\nto do is make some graphs.  As of late, my favorite package has been Altair\n[https://altair-viz.github.io/].  Looks very nice by default, is pretty easy to\niterate with, and has nice declarative syntax.\n\nOnly one problem!  It wants date in \"long-form\" - as in, rather than each row\nhaving several variables of interest, each row has one (or more) \"ID\" variables,\none numerical value, and the name of the variable we're measuring.  So for\ninstance, something more like this:\n\nTime\tLoc\tvariable\tvalue\n10\t4\tTemp\t        68.396970\n13\t0\tTemp\t        80.288846\n7\t4\tTemp\t        69.923273\n6\t0\tTemp\t        71.781362\n5\t2\tTemp\t        62.678844\n\n\nNot quite sure why!  Buuut, that's kind of a feature of modern coding - we're\nsitting on an inheritance of libraries that have built up over the years, and so\nmore often than not we're just building the \"plumbing\" between existing stuff.\n It's cool!  And good!  It lets us separate Function from Implementation.  We\ndon't need to know what's going on under the hood - we just need to know thing X\nwill produce an output we want, and that in order to get it we first need to\nreshape what we've already got into an input that it'll accept.  Since that's\nsuch a huge part of coding these days, Pandas' power in that realm is super\nuseful.\n\nSooo, how do we get from here to there?  Shockingly easily!\n\nmelted = pd.melt(df, id_vars=[\"Time\", \"Loc\"])\n\n\nDone!\n\nWell, obviously we're not REALLY done yet.  Half the point of having such terse,\nexpressive code is that we can do MORE things!\n\nLet's say we want to see how humidity & temperature change over the course of\nthe day.  First, we'll have to grab all the readings from a single location.\n Let's say Location 3!\n\nloc3 = melted[melted[\"Loc\"]==\"3\"]\n\n\nAltair's pretty neat.\n\n(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O', #We're encoding time as an Ordinal \n         y='value:Q',\n         color='variable:N'))\n\n\nHrm, lot of room there at the bottom.  If we were in an interactive session, we\ncould make this interactive (zoomable and navigable!) by just adding the \n.interactive()  method to the end, but I don't know how to do that in the blog.\n Regardless, it's pretty easy to rescale if we want a closer look!\n\n(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='variable:N'))\n\n\nLet's try it with just temperature, and color will encode the location!\n\nmeltedJustTemp = pd.melt(df, \n                         id_vars=[\"Time\", \"Loc\"],\n                        value_vars= [\"Temp\"])\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y='value:Q',\n         color='Loc:N'))\n\n\nLet's zoom in again...\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='Loc:N'))\n\n\nAltair also lets us Facet our graphs extremely flexibly & painlessly.\n\nalt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='Loc:N',\n      column=\"variable\")\n\n\nOr how about another way!  Let's see humidity & temp, location by location.\n\nalt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='variable:N',\n      row=\"Loc\")\n\n\nWe could make them nicer (there's a WIDE array of customizations), but I'm\nlooking to simulate Exploratory Data Analysis.  I can't think of another\ngraphing package in Python that has quite this level of \"instant gratification\"\nfor so many different variations.","html":"<p>Last few Code Snippet Corners were about using Pandas as an easy way to handle input and output between files &amp; databases.  Let's shift gears a little bit!  Among other reasons, because earlier today I discovered a package that exclusively does that, which means I can stop importing the massive Pandas package when all I really wanted to do with it was take advantage of its I/O modules.  <a href=\"https://github.com/insightindustry/sqlathanor\">Check it out</a>! </p><p>So, rather than the entrances &amp; exits, let's focus on all the crazy ways you can reshape data with Pandas!</p><h2 id=\"our-data\">Our Data</h2><p>For our demonstration, I'll use a dataset based on something I was once actually sent.  It was a big CSV with sensor readings from HVAC systems.  Each line had a different house, a different room, a datetime, and readings from a bunch of different types of sensors.  Oh, hrm, I probably shouldn't use data I got from a client.  Uh...</p><h2 id=\"bonus-section-\">BONUS SECTION!</h2><h3 id=\"generating-dummy-temperature-data\">GENERATING DUMMY TEMPERATURE DATA</h3><p><strong>(Feel free to skip to next part if you don't care)</strong></p><p>We want it to fluctuate, but we don't want to just make a bunch of totally random values - a reading should have some relationship to the reading taken a second earlier.</p><p>Let's use <code>NumPy</code> for some Randomness, and the <code>accumulate</code> and <code>repeat</code> functions from <code>itertools</code>.  Maybe I'll do an in-depth post on these at some point, but the code I'll be writing with them will be pretty short and hopefully somewhat self-demonstrating.  If you wanna go deeper here's some good material: <a href=\"https://docs.python.org/3/library/itertools.html\">Official Docs</a>, <a href=\"https://realpython.com/python-itertools/\">Good article</a></p><pre><code class=\"language-python\">import numpy as np\nfrom itertools import accumulate, repeat\n</code></pre>\n<p>We want there to be some random \"noise\", but we also want the occasional substantive change.  We'll reflect this by having it so that 90% of the time we get a small fluctuation, with a 10% chance of a smaller fluctuation. </p><pre><code class=\"language-python\">def genTempDataPoint(x, *args):\n    if np.random.rand(1) &lt;= 0.9:\n        return x + np.random.uniform(-3,3,1)[0]\n    else:\n        return x + np.random.uniform(-10,10,1)[0]\n</code></pre>\n<p> Now let's see some test points!</p><pre><code class=\"language-python\">list(accumulate(repeat(70, 5), genTempDataPoint))\n[70,\n 69.00258239202094,\n 59.34919781643355,\n 56.60722073795931,\n 57.265078261782946]\n</code></pre>\n<p>Sure, fine, why not.  Good enough for our purposes!   Now let's put it all together so we can just call it with a base temp and the number of points we want.</p><pre><code class=\"language-python\">def genTempData(base, n):\n    return list(accumulate(repeat(base, n), \n                           genTempDataPoint))\n</code></pre>\n<p>To simulate the dataset, we actually need to mix it up.  Or else what good are the GroupBys gonna be?  So, let's create a problem to fix later!  Here's a function to create a simplified version of the dataset - each row will have a location ID, a number corresponding to time (just raw ints, I'm not making actual datetimes - I've spent too much time on this part already).  We'll also generate humidity values, to add another monkey wrench to fix later (we'll still use the <code>genTempData</code> function).</p><pre><code class=\"language-python\">from itertools import chain\n\ndef makeLocation(name, base1, n1, base2, n2):\n    return [(x[0], name, x[1][0], x[1][1]) \n        for x in enumerate(zip(genTempData(base1, n1),\n              genTempData(base2, n2)) )]\n\nbigList = list(chain.from_iterable(makeLocation(str(x), \n                                                70, \n                                                15,\n                                                40, \n                                                15) \n                         for x in range(5)))\nnp.random.shuffle(bigList)\n\ndf = pd.DataFrame(bigList, \n                  columns = [&quot;Time&quot;, &quot;Loc&quot;, &quot;Temp&quot;, &quot;Hum&quot;])\n</code></pre>\n<h2 id=\"back-to-the-main-plot\">Back To The Main Plot</h2><p>Let's look at some test rows!</p><pre><code class=\"language-python\"># Viewing test rows\n\ndf.iloc[:5]\nTime\tLoc\tTemp     \tHum\n10\t4\t68.396970\t34.169753\n13\t0\t80.288846\t42.076786\n7\t4\t69.923273\t37.967951\n6\t0\t71.781362\t41.186802\n5\t2\t62.678844\t37.321636\n</code></pre>\n<p>Now, when I'm getting started with a new dataset, one of the first things I like to do is make some graphs.  As of late, my favorite package has been <a href=\"https://altair-viz.github.io/\">Altair</a>.  Looks very nice by default, is pretty easy to iterate with, and has nice declarative syntax.</p><p>Only one problem!  It wants date in \"long-form\" - as in, rather than each row having several variables of interest, each row has one (or more) \"ID\" variables, one numerical value, and the name of the variable we're measuring.  So for instance, something more like this:</p><pre><code class=\"language-python\">Time\tLoc\tvariable\tvalue\n10\t4\tTemp\t        68.396970\n13\t0\tTemp\t        80.288846\n7\t4\tTemp\t        69.923273\n6\t0\tTemp\t        71.781362\n5\t2\tTemp\t        62.678844\n</code></pre>\n<p>Not quite sure why!  Buuut, that's kind of a feature of modern coding - we're sitting on an inheritance of libraries that have built up over the years, and so more often than not we're just building the \"plumbing\" between existing stuff.  It's cool!  And good!  It lets us separate Function from Implementation.  We don't need to know what's going on under the hood - we just need to know thing X will produce an output we want, and that in order to get it we first need to reshape what we've already got into an input that it'll accept.  Since that's such a huge part of coding these days, Pandas' power in that realm is super useful.</p><p>Sooo, how do we get from here to there?  Shockingly easily!</p><pre><code class=\"language-python\">melted = pd.melt(df, id_vars=[&quot;Time&quot;, &quot;Loc&quot;])\n</code></pre>\n<p>Done!</p><p>Well, obviously we're not REALLY done yet.  Half the point of having such terse, expressive code is that we can do MORE things!</p><p>Let's say we want to see how humidity &amp; temperature change over the course of the day.  First, we'll have to grab all the readings from a single location.  Let's say Location 3!</p><pre><code class=\"language-python\">loc3 = melted[melted[&quot;Loc&quot;]==&quot;3&quot;]\n</code></pre>\n<p>Altair's pretty neat.</p><pre><code class=\"language-python\">(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O', #We're encoding time as an Ordinal \n         y='value:Q',\n         color='variable:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--2--1.png\" class=\"kg-image\"></figure><p>Hrm, lot of room there at the bottom.  If we were in an interactive session, we could make this interactive (zoomable and navigable!) by just adding the <code>.interactive()</code> method to the end, but I don't know how to do that in the blog.  Regardless, it's pretty easy to rescale if we want a closer look!</p><pre><code class=\"language-python\">(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='variable:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--3--2.png\" class=\"kg-image\"></figure><p>Let's try it with just temperature, and color will encode the location!</p><pre><code class=\"language-python\">meltedJustTemp = pd.melt(df, \n                         id_vars=[&quot;Time&quot;, &quot;Loc&quot;],\n                        value_vars= [&quot;Temp&quot;])\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y='value:Q',\n         color='Loc:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--4--2.png\" class=\"kg-image\"></figure><p>Let's zoom in again...</p><pre><code class=\"language-python\">(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='Loc:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--4--2.png\" class=\"kg-image\"></figure><p>Altair also lets us Facet our graphs extremely flexibly &amp; painlessly.</p><pre><code class=\"language-python\">alt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='Loc:N',\n      column=&quot;variable&quot;)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--5--1.png\" class=\"kg-image\"></figure><p>Or how about another way!  Let's see humidity &amp; temp, location by location.</p><pre><code class=\"language-python\">alt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='variable:N',\n      row=&quot;Loc&quot;)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--6--1.png\" class=\"kg-image\"></figure><p>We could make them nicer (there's a WIDE array of customizations), but I'm looking to simulate Exploratory Data Analysis.  I can't think of another graphing package in Python that has quite this level of \"instant gratification\" for so many different variations.</p>","url":"https://hackersandslackers.com/all-that-is-solid-melts-into-graphs/","uuid":"603156b0-ee55-4aaa-b5cd-34950389cd08","page":false,"codeinjection_foot":"<script>\n    hljs.configure({language: ['python']})\n </script>","codeinjection_head":"","comment_id":"5b5929932714bc41b8a370c5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c3","title":"Automagically Turn JSON into Pandas DataFrames","slug":"json-into-pandas-dataframes","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","custom_excerpt":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame.","created_at_pretty":"25 July, 2018","published_at_pretty":"28 July, 2018","updated_at_pretty":"21 February, 2019","created_at":"2018-07-25T10:52:35.000-04:00","published_at":"2018-07-28T08:00:00.000-04:00","updated_at":"2019-02-20T21:35:53.000-05:00","meta_title":"Turn JSON into Pandas DataFrames | Hackers And Slackers","meta_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","og_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","og_title":"Automagically Turn JSON into Pandas DataFrames","twitter_description":"Let Pandas do the heavy lifting for you when turning JSON into a DataFrame, especially when that JSON is heavily nested.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/json@2x.jpg","twitter_title":"Automagically Turn JSON into Pandas DataFrames","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"In his post about extracting data from APIs\n[https://hackersandslackers.com/extracting-massive-datasets-from-apis/], Todd\n[https://hackersandslackers.com/author/todd/]  demonstrated a nice way to\nmassage JSON into a pandas DataFrame. This method works great when our JSON\nresponse is flat, because dict.keys()  only gets the keys on the first \"level\"\nof a dictionary. It gets a little trickier when our JSON starts to become nested\nthough, as I experienced when working with Spotify's API\n[https://developer.spotify.com/documentation/web-api/]  via the Spotipy\n[https://spotipy.readthedocs.io/en/latest/]  library. For example, take a look\nat a response from their https://api.spotify.com/v1/tracks/{id}  endpoint:\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n\ntrack_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n\n\nOutput:\n{\n  \"album\": {\n    \"album_type\": \"album\",\n    \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n        \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n        \"name\": \"Stephen Malkmus\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      {\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n        \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n        \"name\": \"The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n      }\n    ],\n    \"available_markets\": [\"AR\",\n      \"BO\",\n      \"BR\",\n      \"CA\",\n      \"...\",\n      \"US\",\n      \"UY\",\n      \"VN\"\n    ],\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n    },\n    \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n    \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n    \"images\": [{\n        \"height\": 640,\n        \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n        \"width\": 640\n      },\n      {\n        \"height\": 300,\n        \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n        \"width\": 300\n      },\n      {\n        \"height\": 64,\n        \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n        \"width\": 64\n      }\n    ],\n    \"name\": \"Real Emotional Trash\",\n    \"release_date\": \"2008-03-04\",\n    \"release_date_precision\": \"day\",\n    \"type\": \"album\",\n    \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n  },\n  \"artists\": [{\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n      \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n      \"name\": \"Stephen Malkmus & The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n      \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n      \"name\": \"Stephen Malkmus\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    {\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n      },\n      \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n      \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n      \"name\": \"The Jicks\",\n      \"type\": \"artist\",\n      \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n    }\n  ],\n  \"available_markets\": [\"AR\",\n    \"BO\",\n    \"BR\",\n    \"CA\",\n    \"...\",\n    \"US\",\n    \"UY\",\n    \"VN\"\n  ],\n  \"disc_number\": 1,\n  \"duration_ms\": 608826,\n  \"explicit\": False,\n  \"external_ids\": {\n    \"isrc\": \"USMTD0877204\"\n  },\n  \"external_urls\": {\n    \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n  },\n  \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n  \"id\": \"0BDYBajZydY54OTgQsH940\",\n  \"is_local\": False,\n  \"name\": \"Real Emotional Trash\",\n  \"popularity\": 21,\n  \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n  \"track_number\": 4,\n  \"type\": \"track\",\n  \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n}\n\n\nIn addition to plenty of information about the track, Spotify also includes\ninformation about the album that contains the track. If we were to just use the \ndict.keys()  method to turn this response into a DataFrame, we'd be missing out\non all that extra album information. Well, it would be there, just not readily\naccessible.\n\ntrack_response.keys()\n\n\nOutput:\ndict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n\n\nSo how do we get around this? Well, we could write our own function, but because\npandas is amazing, it already has a built in tool that takes care of this for\nus.\n\nData Normalization\nMeet json_normalize():\n\nimport pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n\n\nOutput:\nalbum.album_type\n album.artists\n album.available_markets\n album.external_urls.spotify\n album.href\n album.id\n album.images\n album.name\n album.release_date\n album.release_date_precision\n ...\n external_urls.spotify\n href\n id\n is_local\n name\n popularity\n preview_url\n track_number\n type\n uri\n 0\n album\n [{'external_urls': {'spotify': 'https://open.s...\n [AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...\n https://open.spotify.com/album/6pWpb4IdPu9vp9m...\n https://api.spotify.com/v1/albums/6pWpb4IdPu9v...\n 6pWpb4IdPu9vp9mOdh5DjY\n [{'height': 640, 'url': 'https://i.scdn.co/ima...\n Real Emotional Trash\n 2008-03-04\n day\n ...\n https://open.spotify.com/track/0BDYBajZydY54OT...\n https://api.spotify.com/v1/tracks/0BDYBajZydY5...\n 0BDYBajZydY54OTgQsH940\n False\n Real Emotional Trash\n 21\n https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...\n 4\n track\n spotify:track:0BDYBajZydY54OTgQsH940\n Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and\nturns it into a DataFrame.\n\nThis makes our life easier when we're dealing with one record, but it really \ncomes in handy when we're dealing with a response that contains multiple\nrecords.\n\ntracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n\n\nOutput:\n{\n  \"tracks\": [{\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 608826,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877204\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0BDYBajZydY54OTgQsH940\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940\",\n      \"id\": \"0BDYBajZydY54OTgQsH940\",\n      \"is_local\": False,\n      \"name\": \"Real Emotional Trash\",\n      \"popularity\": 21,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 4,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0BDYBajZydY54OTgQsH940\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 222706,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877203\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs\",\n      \"id\": \"7fdUqrzb8oCcIoKvFuzMrs\",\n      \"is_local\": False,\n      \"name\": \"Cold Son\",\n      \"popularity\": 25,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 3,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:7fdUqrzb8oCcIoKvFuzMrs\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 416173,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877202\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj\",\n      \"id\": \"0islTY4Fw6lhYbfqi8Qtdj\",\n      \"is_local\": False,\n      \"name\": \"Hopscotch Willie\",\n      \"popularity\": 24,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 2,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:0islTY4Fw6lhYbfqi8Qtdj\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n            \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n            \"name\": \"Stephen Malkmus & The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n            \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n            \"name\": \"Stephen Malkmus\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          {\n            \"external_urls\": {\n              \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n            },\n            \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n            \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n            \"name\": \"The Jicks\",\n            \"type\": \"artist\",\n            \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n          }\n        ],\n        \"available_markets\": [\"AR\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY\",\n        \"id\": \"6pWpb4IdPu9vp9mOdh5DjY\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Real Emotional Trash\",\n        \"release_date\": \"2008-03-04\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:6pWpb4IdPu9vp9mOdh5DjY\"\n      },\n      \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n          \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n          \"name\": \"Stephen Malkmus\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n        },\n        {\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n          \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n          \"name\": \"The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n        }\n      ],\n      \"available_markets\": [\"AR\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 308146,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD0877201\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH\",\n      \"id\": \"3jyFLbljUTKjE13nIWXchH\",\n      \"is_local\": False,\n      \"name\": \"Dragonfly Pie\",\n      \"popularity\": 26,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 1,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:3jyFLbljUTKjE13nIWXchH\"\n    },\n    {\n      \"album\": {\n        \"album_type\": \"album\",\n        \"artists\": [{\n          \"external_urls\": {\n            \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n          },\n          \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n          \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n          \"name\": \"Stephen Malkmus & The Jicks\",\n          \"type\": \"artist\",\n          \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n        }],\n        \"available_markets\": [\"AR\",\n          \"AU\",\n          \"BO\",\n          \"BR\",\n          \"CA\",\n          \"CL\",\n          \"CO\",\n          \"CR\",\n          \"DO\",\n          \"EC\",\n          \"GT\",\n          \"HK\",\n          \"HN\",\n          \"ID\",\n          \"JP\",\n          \"MX\",\n          \"MY\",\n          \"NI\",\n          \"NZ\",\n          \"PA\",\n          \"PE\",\n          \"PH\",\n          \"PY\",\n          \"SG\",\n          \"SV\",\n          \"TH\",\n          \"TW\",\n          \"US\",\n          \"UY\",\n          \"VN\",\n          \"ZA\"\n        ],\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG\"\n        },\n        \"href\": \"https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG\",\n        \"id\": \"5DMvSCwRqfNVlMB5LjHOwG\",\n        \"images\": [{\n            \"height\": 640,\n            \"url\": \"https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb\",\n            \"width\": 640\n          },\n          {\n            \"height\": 300,\n            \"url\": \"https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1\",\n            \"width\": 300\n          },\n          {\n            \"height\": 64,\n            \"url\": \"https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114\",\n            \"width\": 64\n          }\n        ],\n        \"name\": \"Sparkle Hard\",\n        \"release_date\": \"2018-05-18\",\n        \"release_date_precision\": \"day\",\n        \"type\": \"album\",\n        \"uri\": \"spotify:album:5DMvSCwRqfNVlMB5LjHOwG\"\n      },\n      \"artists\": [{\n        \"external_urls\": {\n          \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n        },\n        \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n        \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n        \"name\": \"Stephen Malkmus & The Jicks\",\n        \"type\": \"artist\",\n        \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n      }],\n      \"available_markets\": [\"AR\",\n        \"AU\",\n        \"BO\",\n        \"BR\",\n        \"CA\",\n        \"CL\",\n        \"CO\",\n        \"CR\",\n        \"DO\",\n        \"EC\",\n        \"GT\",\n        \"HK\",\n        \"HN\",\n        \"ID\",\n        \"JP\",\n        \"MX\",\n        \"MY\",\n        \"NI\",\n        \"NZ\",\n        \"PA\",\n        \"PE\",\n        \"PH\",\n        \"PY\",\n        \"SG\",\n        \"SV\",\n        \"TH\",\n        \"TW\",\n        \"US\",\n        \"UY\",\n        \"VN\",\n        \"ZA\"\n      ],\n      \"disc_number\": 1,\n      \"duration_ms\": 423275,\n      \"explicit\": False,\n      \"external_ids\": {\n        \"isrc\": \"USMTD1710380\"\n      },\n      \"external_urls\": {\n        \"spotify\": \"https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs\"\n      },\n      \"href\": \"https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs\",\n      \"id\": \"6dNmC2YWtWbVOFOdTuRDQs\",\n      \"is_local\": False,\n      \"name\": \"Difficulties - Let Them Eat Vowels\",\n      \"popularity\": 35,\n      \"preview_url\": \"https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c\",\n      \"track_number\": 11,\n      \"type\": \"track\",\n      \"uri\": \"spotify:track:6dNmC2YWtWbVOFOdTuRDQs\"\n    }\n  ]\n}\n\n\n\njson_normalise(tracks_response)\n\n\nOutput:\nalbum.album_typealbum.artistsalbum.available_marketsalbum.external_urls.spotify\nalbum.hrefalbum.idalbum.imagesalbum.namealbum.release_date\nalbum.release_date_precision...external_urls.spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsSeparate Ways (Worlds Apart)\nBy default, json_normalize()  uses periods .  to indicate nested levels of the\nJSON object (which is actually converted to a Python dict  by Spotipy). In our\ncase, the album id is found in track['album']['id'], hence the period between\nalbum and id in the DataFrame. This makes things slightly annoying if we want to\ngrab a Series from our new DataFrame. In pandas, we can grab a Series from a\nDataFrame in many ways. To grab the album.id  column, for example:\n\ntracks_df['album.id']\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\nor\n\ntracks_df.loc[:,'album.id']\n\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n\n\npandas also allows us to use dot notation (i.e. dataframe.column_name) to grab a\ncolumn as a Series, but only if our column name doesn't include a period\nalready. Since json_normalize()  uses a period as a separator by default, this\nruins that method. Never fear though – overriding this behavior is as simple as\noverriding the default argument in the function call:\n\ntracks_df = json_normalize(tracks_response['tracks'],sep=\"_\")\ntracks_df\n\n\nOutput:\nalbum_album_typealbum_artistsalbum_available_marketsalbum_external_urls_spotify\nalbum_hrefalbum_idalbum_imagesalbum_namealbum_release_date\nalbum_release_date_precision...external_urls_spotifyhrefidis_localnamepopularity\npreview_urltrack_numbertypeuri\n 0album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0BDYBajZydY54OT...\nhttps://api.spotify.com/v1/tracks/0BDYBajZydY5...0BDYBajZydY54OTgQsH940FALSEReal\nEmotional Trash21https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...4track\nspotify:track:0BDYBajZydY54OTgQsH940\n 1album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/7fdUqrzb8oCcIoK...\nhttps://api.spotify.com/v1/tracks/7fdUqrzb8oCc...7fdUqrzb8oCcIoKvFuzMrsFALSECold\nSon25https://p.scdn.co/mp3-preview/4cf4e21727def470...3track\nspotify:track:7fdUqrzb8oCcIoKvFuzMrs\n 2album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/0islTY4Fw6lhYbf...\nhttps://api.spotify.com/v1/tracks/0islTY4Fw6lh...0islTY4Fw6lhYbfqi8QtdjFALSE\nHopscotch Willie24https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...2track\nspotify:track:0islTY4Fw6lhYbfqi8Qtdj\n 3album[{'external_urls': {'spotify': 'https://open.s...[AR, BO, BR, CA, CL, CO,\nCR, EC, GT, HK, HN, I...https://open.spotify.com/album/6pWpb4IdPu9vp9m...\nhttps://api.spotify.com/v1/albums/6pWpb4IdPu9v...6pWpb4IdPu9vp9mOdh5DjY\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Real Emotional Trash3/4/2008day\n...https://open.spotify.com/track/3jyFLbljUTKjE13...\nhttps://api.spotify.com/v1/tracks/3jyFLbljUTKj...3jyFLbljUTKjE13nIWXchHFALSE\nDragonfly Pie26https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...1track\nspotify:track:3jyFLbljUTKjE13nIWXchH\n 4album[{'external_urls': {'spotify': 'https://open.s...[AR, AU, BO, BR, CA, CL,\nCO, CR, DO, EC, GT, H...https://open.spotify.com/album/5DMvSCwRqfNVlMB...\nhttps://api.spotify.com/v1/albums/5DMvSCwRqfNV...5DMvSCwRqfNVlMB5LjHOwG\n[{'height': 640, 'url': 'https://i.scdn.co/ima...Sparkle Hard5/18/2018day...\nhttps://open.spotify.com/track/6dNmC2YWtWbVOFO...\nhttps://api.spotify.com/v1/tracks/6dNmC2YWtWbV...6dNmC2YWtWbVOFOdTuRDQsFALSE\nDifficulties - Let Them Eat Vowels35\nhttps://p.scdn.co/mp3-preview/787be9d1bbebcd84...11track\nspotify:track:6dNmC2YWtWbVOFOdTuRDQsNow we can go back to using dot notation to\naccess a column as a Series. This saves us some typing every time we want to\ngrab a column, and it looks a bit nicer (to me, at least). I say worth it.\n\ntracks_df.album_id\n\n\nOutput:\n0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n\n\nI Need That Record\nBy including more parameters when we use json_normlize(), we can really extract\njust the data that we want from our API response.\n\nFrom our responses above, we can see that the artist  property contains a list\nof artists that are associated with a track:\n\ntracks_response['tracks'][0]['artists']\n\n\nOutput:\n[{\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe\",\n    \"id\": \"7wyRA7deGRxozTyBc6QXPe\",\n    \"name\": \"Stephen Malkmus & The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7wyRA7deGRxozTyBc6QXPe\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8\",\n    \"id\": \"0WISkx0PwT6lYWdPqKUJY8\",\n    \"name\": \"Stephen Malkmus\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:0WISkx0PwT6lYWdPqKUJY8\"\n  },\n  {\n    \"external_urls\": {\n      \"spotify\": \"https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7\"\n    },\n    \"href\": \"https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7\",\n    \"id\": \"7uStwCeP54Za8gXUFCf5L7\",\n    \"name\": \"The Jicks\",\n    \"type\": \"artist\",\n    \"uri\": \"spotify:artist:7uStwCeP54Za8gXUFCf5L7\"\n  }\n]\n\n\nLet's say I want to load this data into a database later. It would be nice to\nhave a join table that maps each of the artists that are associated with each\ntrack. Luckily, this is possible with json_normalize()'s record_path  and meta \nparameters.\n\nrecord_path  tells json_normalize()  what path of keys leads to each individual\nrecord in the JSON object. In our case, we want to grab every artist id, so our\nfunction call will look like:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],sep=\"_\")\n\n\n\nexternal_urls href id name type uri\n 1 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 1 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 2 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 3 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 4 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 5 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 6 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 7 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 8 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 9 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPe\n 10 {'spotify': 'https://open.spotify.com/artist/0... \nhttps://api.spotify.com/v1/artists/0WISkx0PwT6... 0WISkx0PwT6lYWdPqKUJY8 Stephen\nMalkmus artist spotify:artist:0WISkx0PwT6lYWdPqKUJY8\n 11 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7uStwCeP54Z... 7uStwCeP54Za8gXUFCf5L7 The\nJicks artist spotify:artist:7uStwCeP54Za8gXUFCf5L7\n 12 {'spotify': 'https://open.spotify.com/artist/7... \nhttps://api.spotify.com/v1/artists/7wyRA7deGRx... 7wyRA7deGRxozTyBc6QXPe Stephen\nMalkmus & The Jicks artist spotify:artist:7wyRA7deGRxozTyBc6QXPeCool – we're\nalmost there. Now we want to use the meta  parameter to specify what data we\nwant to include from the rest of the JSON object. In our case, we want to keep\nthe track id and map it to the artist id. If we look back at our API response,\nthe name of the column that included the track is is called, appropriately, id,\nso our full function call should look like this:\n\njson_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n\nOutput:\n-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    <ipython-input-14-77e00a98c3c0> in <module>()\n    ----> 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=\"_\")\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --> 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n\n\nUh oh – an error! What's going on? Well, it turns out that both the album id and\ntrack id were given the key id. pandas doesn't like that, and it gives us a\nhelpful error to tell us so: ValueError: Conflicting metadata name id, need\ndistinguishing prefix.\n\nThere are two more parameters we can use to overcome this error: record_prefix \nand meta_prefix. These are strings we'll add to the beginning of our records and\nmetadata to prevent these naming conflicts. Since we're dealing with Spotify\nartist ids for our records and Spotify track ids as the metadata, I'll use \nsp_artist_  and sp_track_  respectively. When that's done, I'll select only the\ncolumns that we're interested in.\n\nartist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=\"_\"\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n\n\nOutput:\nsp_track_id sp_artist_id\n 00BDYBajZydY54OTgQsH940 7wyRA7deGRxozTyBc6QXPe\n 10BDYBajZydY54OTgQsH940 0WISkx0PwT6lYWdPqKUJY8\n 20BDYBajZydY54OTgQsH940 7uStwCeP54Za8gXUFCf5L7\n 37fdUqrzb8oCcIoKvFuzMrs 7wyRA7deGRxozTyBc6QXPe\n 47fdUqrzb8oCcIoKvFuzMrs 0WISkx0PwT6lYWdPqKUJY8\n 57fdUqrzb8oCcIoKvFuzMrs 7uStwCeP54Za8gXUFCf5L7\n 60islTY4Fw6lhYbfqi8Qtdj 7wyRA7deGRxozTyBc6QXPe\n 70islTY4Fw6lhYbfqi8Qtdj 0WISkx0PwT6lYWdPqKUJY8\n 80islTY4Fw6lhYbfqi8Qtdj 7uStwCeP54Za8gXUFCf5L7\n 93jyFLbljUTKjE13nIWXchH 7wyRA7deGRxozTyBc6QXPe\n 103jyFLbljUTKjE13nIWXchH 0WISkx0PwT6lYWdPqKUJY8\n 113jyFLbljUTKjE13nIWXchH 7uStwCeP54Za8gXUFCf5L7\n 126dNmC2YWtWbVOFOdTuRDQs 7wyRA7deGRxozTyBc6QXPeTL;DR\n * Use pd.io.json.json_normalize()  to automagically flatten a nested JSON\n   object into a DataFrame\n * Make your life slightly easier when it comes to selecting columns by\n   overriding the default sep  parameter\n * Specify what data constitutes a record with the record_path  parameter\n * Include data from outside of the record path with the meta  parameter\n * Fix naming conflicts if they arise with the record_prefix  and meta_prefix \n   parameters","html":"<p>In his post about <a href=\"https://hackersandslackers.com/extracting-massive-datasets-from-apis/\">extracting data from APIs</a>, <a href=\"https://hackersandslackers.com/author/todd/\">Todd</a> demonstrated a nice way to massage JSON into a pandas DataFrame. This method works great when our JSON response is flat, because <code>dict.keys()</code> only gets the keys on the first \"level\" of a dictionary. It gets a little trickier when our JSON starts to become nested though, as I experienced when working with <a href=\"https://developer.spotify.com/documentation/web-api/\">Spotify's API</a> via the <a href=\"https://spotipy.readthedocs.io/en/latest/\">Spotipy</a> library. For example, take a look at a response from their <code>https://api.spotify.com/v1/tracks/{id}</code> endpoint:</p><pre><code class=\"language-python\">import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nspotify_client_id = 'YOUR_ID'\nspotify_client_secret  = 'YOUR_SECRET'\nclient_credentials_manager = SpotifyClientCredentials(client_id=spotify_client_id, client_secret=spotify_client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n</code></pre>\n<pre><code class=\"language-python\">track_response = sp.track('0BDYBajZydY54OTgQsH940')\ntrack_response\n</code></pre>\n<h3 id=\"output-\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;album&quot;: {\n    &quot;album_type&quot;: &quot;album&quot;,\n    &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      {\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n        &quot;name&quot;: &quot;The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n      }\n    ],\n    &quot;available_markets&quot;: [&quot;AR&quot;,\n      &quot;BO&quot;,\n      &quot;BR&quot;,\n      &quot;CA&quot;,\n      &quot;...&quot;,\n      &quot;US&quot;,\n      &quot;UY&quot;,\n      &quot;VN&quot;\n    ],\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n    &quot;images&quot;: [{\n        &quot;height&quot;: 640,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n        &quot;width&quot;: 640\n      },\n      {\n        &quot;height&quot;: 300,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n        &quot;width&quot;: 300\n      },\n      {\n        &quot;height&quot;: 64,\n        &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n        &quot;width&quot;: 64\n      }\n    ],\n    &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n    &quot;release_date&quot;: &quot;2008-03-04&quot;,\n    &quot;release_date_precision&quot;: &quot;day&quot;,\n    &quot;type&quot;: &quot;album&quot;,\n    &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n  },\n  &quot;artists&quot;: [{\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n      &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    {\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n      &quot;name&quot;: &quot;The Jicks&quot;,\n      &quot;type&quot;: &quot;artist&quot;,\n      &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n    }\n  ],\n  &quot;available_markets&quot;: [&quot;AR&quot;,\n    &quot;BO&quot;,\n    &quot;BR&quot;,\n    &quot;CA&quot;,\n    &quot;...&quot;,\n    &quot;US&quot;,\n    &quot;UY&quot;,\n    &quot;VN&quot;\n  ],\n  &quot;disc_number&quot;: 1,\n  &quot;duration_ms&quot;: 608826,\n  &quot;explicit&quot;: False,\n  &quot;external_ids&quot;: {\n    &quot;isrc&quot;: &quot;USMTD0877204&quot;\n  },\n  &quot;external_urls&quot;: {\n    &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n  },\n  &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n  &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n  &quot;is_local&quot;: False,\n  &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n  &quot;popularity&quot;: 21,\n  &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n  &quot;track_number&quot;: 4,\n  &quot;type&quot;: &quot;track&quot;,\n  &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n}\n</code></pre>\n<p>In addition to plenty of information about the track, Spotify also includes information about the album that contains the track. If we were to just use the <code>dict.keys()</code> method to turn this response into a DataFrame, we'd be missing out on all that extra album information. Well, it would be there, just not readily accessible.</p><pre><code class=\"language-python\">track_response.keys()\n</code></pre>\n<h3 id=\"output--1\">Output:</h3><pre><code class=\"language-python\">dict_keys(['album', 'artists', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'external_ids', 'external_urls', 'href', 'id', 'is_local', 'name', 'popularity', 'preview_url', 'track_number', 'type', 'uri'])\n</code></pre>\n<p>So how do we get around this? Well, we could write our own function, but because pandas is amazing, it already has a built in tool that takes care of this for us.</p><h2 id=\"data-normalization\">Data Normalization</h2><p>Meet <code>json_normalize()</code>:</p><pre><code class=\"language-python\">import pandas as pd\nfrom pandas.io.json import json_normalize\njson_normalize(track_response)\n</code></pre>\n<h3 id=\"output--2\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>album.album_type</th>\n      <th>album.artists</th>\n      <th>album.available_markets</th>\n      <th>album.external_urls.spotify</th>\n      <th>album.href</th>\n      <th>album.id</th>\n      <th>album.images</th>\n      <th>album.name</th>\n      <th>album.release_date</th>\n      <th>album.release_date_precision</th>\n      <th>...</th>\n      <th>external_urls.spotify</th>\n      <th>href</th>\n      <th>id</th>\n      <th>is_local</th>\n      <th>name</th>\n      <th>popularity</th>\n      <th>preview_url</th>\n      <th>track_number</th>\n      <th>type</th>\n      <th>uri</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>album</td>\n      <td>[{'external_urls': {'spotify': 'https://open.s...</td>\n      <td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td>\n      <td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td>\n      <td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td>\n      <td>6pWpb4IdPu9vp9mOdh5DjY</td>\n      <td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td>\n      <td>Real Emotional Trash</td>\n      <td>2008-03-04</td>\n      <td>day</td>\n      <td>...</td>\n      <td>https://open.spotify.com/track/0BDYBajZydY54OT...</td>\n      <td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td>\n      <td>0BDYBajZydY54OTgQsH940</td>\n      <td>False</td>\n      <td>Real Emotional Trash</td>\n      <td>21</td>\n      <td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td>\n      <td>4</td>\n      <td>track</td>\n      <td>spotify:track:0BDYBajZydY54OTgQsH940</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Yep – it's that easy. pandas takes our nested JSON object, flattens it out, and turns it into a DataFrame.</p><p>This makes our life easier when we're dealing with one record, but it <em>really</em> comes in handy when we're dealing with a response that contains multiple records.</p><pre><code class=\"language-python\">tracks_response = sp.tracks(\n    ['0BDYBajZydY54OTgQsH940',\n     '7fdUqrzb8oCcIoKvFuzMrs',\n     '0islTY4Fw6lhYbfqi8Qtdj',\n     '3jyFLbljUTKjE13nIWXchH',\n     '6dNmC2YWtWbVOFOdTuRDQs']\n)\ntracks_response\n</code></pre>\n<h3 id=\"output--3\">Output:</h3><pre><code class=\"language-json\">{\n  &quot;tracks&quot;: [{\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 608826,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877204&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0BDYBajZydY54OTgQsH940&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0BDYBajZydY54OTgQsH940&quot;,\n      &quot;id&quot;: &quot;0BDYBajZydY54OTgQsH940&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n      &quot;popularity&quot;: 21,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590d5819849e1aad3eff981dc75?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 4,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0BDYBajZydY54OTgQsH940&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 222706,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877203&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/7fdUqrzb8oCcIoKvFuzMrs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;id&quot;: &quot;7fdUqrzb8oCcIoKvFuzMrs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Cold Son&quot;,\n      &quot;popularity&quot;: 25,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/4cf4e21727def47097e27d30de16ffe9f99b7774?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 3,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:7fdUqrzb8oCcIoKvFuzMrs&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 416173,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877202&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/0islTY4Fw6lhYbfqi8Qtdj&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;id&quot;: &quot;0islTY4Fw6lhYbfqi8Qtdj&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Hopscotch Willie&quot;,\n      &quot;popularity&quot;: 24,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12159db4f90fba8388af034d60?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 2,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:0islTY4Fw6lhYbfqi8Qtdj&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n            &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          {\n            &quot;external_urls&quot;: {\n              &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n            },\n            &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n            &quot;name&quot;: &quot;The Jicks&quot;,\n            &quot;type&quot;: &quot;artist&quot;,\n            &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n          }\n        ],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/6pWpb4IdPu9vp9mOdh5DjY&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;id&quot;: &quot;6pWpb4IdPu9vp9mOdh5DjY&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/918fdb6fdffccf2bd2dd1a1a93136000f8cf9bd3&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/fb39290ebca6fac424d4a40611a7e0d1146c5f88&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/873da4a0a21acd96e4a0036c9ecd0580b62652d4&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Real Emotional Trash&quot;,\n        &quot;release_date&quot;: &quot;2008-03-04&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:6pWpb4IdPu9vp9mOdh5DjY&quot;\n      },\n      &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n        },\n        {\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n          &quot;name&quot;: &quot;The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n        }\n      ],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 308146,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD0877201&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/3jyFLbljUTKjE13nIWXchH&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;id&quot;: &quot;3jyFLbljUTKjE13nIWXchH&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Dragonfly Pie&quot;,\n      &quot;popularity&quot;: 26,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/50f419e7d3e8a6a771515068622250ab06d1cc86?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 1,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:3jyFLbljUTKjE13nIWXchH&quot;\n    },\n    {\n      &quot;album&quot;: {\n        &quot;album_type&quot;: &quot;album&quot;,\n        &quot;artists&quot;: [{\n          &quot;external_urls&quot;: {\n            &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n          },\n          &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n          &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n          &quot;type&quot;: &quot;artist&quot;,\n          &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n        }],\n        &quot;available_markets&quot;: [&quot;AR&quot;,\n          &quot;AU&quot;,\n          &quot;BO&quot;,\n          &quot;BR&quot;,\n          &quot;CA&quot;,\n          &quot;CL&quot;,\n          &quot;CO&quot;,\n          &quot;CR&quot;,\n          &quot;DO&quot;,\n          &quot;EC&quot;,\n          &quot;GT&quot;,\n          &quot;HK&quot;,\n          &quot;HN&quot;,\n          &quot;ID&quot;,\n          &quot;JP&quot;,\n          &quot;MX&quot;,\n          &quot;MY&quot;,\n          &quot;NI&quot;,\n          &quot;NZ&quot;,\n          &quot;PA&quot;,\n          &quot;PE&quot;,\n          &quot;PH&quot;,\n          &quot;PY&quot;,\n          &quot;SG&quot;,\n          &quot;SV&quot;,\n          &quot;TH&quot;,\n          &quot;TW&quot;,\n          &quot;US&quot;,\n          &quot;UY&quot;,\n          &quot;VN&quot;,\n          &quot;ZA&quot;\n        ],\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/album/5DMvSCwRqfNVlMB5LjHOwG&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/albums/5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;id&quot;: &quot;5DMvSCwRqfNVlMB5LjHOwG&quot;,\n        &quot;images&quot;: [{\n            &quot;height&quot;: 640,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/bc96e20fa6b42c765db2fb904d3a70b6ef57b0bb&quot;,\n            &quot;width&quot;: 640\n          },\n          {\n            &quot;height&quot;: 300,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/c7a31ed50b9c704ec066f4aac669cfb9013effb1&quot;,\n            &quot;width&quot;: 300\n          },\n          {\n            &quot;height&quot;: 64,\n            &quot;url&quot;: &quot;https://i.scdn.co/image/8551e108d0950dd62724ff2703e8c13ce7324114&quot;,\n            &quot;width&quot;: 64\n          }\n        ],\n        &quot;name&quot;: &quot;Sparkle Hard&quot;,\n        &quot;release_date&quot;: &quot;2018-05-18&quot;,\n        &quot;release_date_precision&quot;: &quot;day&quot;,\n        &quot;type&quot;: &quot;album&quot;,\n        &quot;uri&quot;: &quot;spotify:album:5DMvSCwRqfNVlMB5LjHOwG&quot;\n      },\n      &quot;artists&quot;: [{\n        &quot;external_urls&quot;: {\n          &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n        },\n        &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n        &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n        &quot;type&quot;: &quot;artist&quot;,\n        &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n      }],\n      &quot;available_markets&quot;: [&quot;AR&quot;,\n        &quot;AU&quot;,\n        &quot;BO&quot;,\n        &quot;BR&quot;,\n        &quot;CA&quot;,\n        &quot;CL&quot;,\n        &quot;CO&quot;,\n        &quot;CR&quot;,\n        &quot;DO&quot;,\n        &quot;EC&quot;,\n        &quot;GT&quot;,\n        &quot;HK&quot;,\n        &quot;HN&quot;,\n        &quot;ID&quot;,\n        &quot;JP&quot;,\n        &quot;MX&quot;,\n        &quot;MY&quot;,\n        &quot;NI&quot;,\n        &quot;NZ&quot;,\n        &quot;PA&quot;,\n        &quot;PE&quot;,\n        &quot;PH&quot;,\n        &quot;PY&quot;,\n        &quot;SG&quot;,\n        &quot;SV&quot;,\n        &quot;TH&quot;,\n        &quot;TW&quot;,\n        &quot;US&quot;,\n        &quot;UY&quot;,\n        &quot;VN&quot;,\n        &quot;ZA&quot;\n      ],\n      &quot;disc_number&quot;: 1,\n      &quot;duration_ms&quot;: 423275,\n      &quot;explicit&quot;: False,\n      &quot;external_ids&quot;: {\n        &quot;isrc&quot;: &quot;USMTD1710380&quot;\n      },\n      &quot;external_urls&quot;: {\n        &quot;spotify&quot;: &quot;https://open.spotify.com/track/6dNmC2YWtWbVOFOdTuRDQs&quot;\n      },\n      &quot;href&quot;: &quot;https://api.spotify.com/v1/tracks/6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;id&quot;: &quot;6dNmC2YWtWbVOFOdTuRDQs&quot;,\n      &quot;is_local&quot;: False,\n      &quot;name&quot;: &quot;Difficulties - Let Them Eat Vowels&quot;,\n      &quot;popularity&quot;: 35,\n      &quot;preview_url&quot;: &quot;https://p.scdn.co/mp3-preview/787be9d1bbebcd845d0793476de843fa0a4fff79?cid=be22fd00039241bc96d161a63876b54c&quot;,\n      &quot;track_number&quot;: 11,\n      &quot;type&quot;: &quot;track&quot;,\n      &quot;uri&quot;: &quot;spotify:track:6dNmC2YWtWbVOFOdTuRDQs&quot;\n    }\n  ]\n}\n\n</code></pre>\n<pre><code class=\"language-python\">json_normalise(tracks_response)\n</code></pre>\n<h3 id=\"output--4\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album.album_type</th><th>album.artists</th><th>album.available_markets</th><th>album.external_urls.spotify</th><th>album.href</th><th>album.id</th><th>album.images</th><th>album.name</th><th>album.release_date</th><th>album.release_date_precision</th><th>...</th><th>external_urls.spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><h2 id=\"separate-ways-worlds-apart-\">Separate Ways (Worlds Apart)</h2><p>By default, <code>json_normalize()</code> uses periods <code>.</code> to indicate nested levels of the JSON object (which is actually converted to a Python <code>dict</code> by Spotipy). In our case, the album id is found in <code>track['album']['id']</code>, hence the period between album and id in the DataFrame. This makes things slightly annoying if we want to grab a Series from our new DataFrame. In pandas, we can grab a Series from a DataFrame in many ways. To grab the <code>album.id</code> column, for example:</p><pre><code class=\"language-python\">tracks_df['album.id']\n</code></pre>\n<h3 id=\"output--5\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>or</p><pre><code class=\"language-python\">tracks_df.loc[:,'album.id']\n\n</code></pre>\n<h3 id=\"output--6\">Output:</h3><pre><code class=\"language-python\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album.id, dtype: object\n</code></pre>\n<p>pandas also allows us to use dot notation (i.e. <code>dataframe.column_name</code>) to grab a column as a Series, but only if our column name doesn't include a period already. Since <code>json_normalize()</code> uses a period as a separator by default, this ruins that method. Never fear though – overriding this behavior is as simple as overriding the default argument in the function call:</p><pre><code class=\"language-python\">tracks_df = json_normalize(tracks_response['tracks'],sep=&quot;_&quot;)\ntracks_df\n</code></pre>\n<h3 id=\"output--7\">Output:</h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>album_album_type</th><th>album_artists</th><th>album_available_markets</th><th>album_external_urls_spotify</th><th>album_href</th><th>album_id</th><th>album_images</th><th>album_name</th><th>album_release_date</th><th>album_release_date_precision</th><th>...</th><th>external_urls_spotify</th><th>href</th><th>id</th><th>is_local</th><th>name</th><th>popularity</th><th>preview_url</th><th>track_number</th><th>type</th><th>uri</th></tr></thead><tbody>\n <tr><td>0</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0BDYBajZydY54OT...</td><td>https://api.spotify.com/v1/tracks/0BDYBajZydY5...</td><td>0BDYBajZydY54OTgQsH940</td><td>FALSE</td><td>Real Emotional Trash</td><td>21</td><td>https://p.scdn.co/mp3-preview/4fcbcd5a99fc7590...</td><td>4</td><td>track</td><td>spotify:track:0BDYBajZydY54OTgQsH940</td></tr>\n <tr><td>1</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/7fdUqrzb8oCcIoK...</td><td>https://api.spotify.com/v1/tracks/7fdUqrzb8oCc...</td><td>7fdUqrzb8oCcIoKvFuzMrs</td><td>FALSE</td><td>Cold Son</td><td>25</td><td>https://p.scdn.co/mp3-preview/4cf4e21727def470...</td><td>3</td><td>track</td><td>spotify:track:7fdUqrzb8oCcIoKvFuzMrs</td></tr>\n <tr><td>2</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/0islTY4Fw6lhYbf...</td><td>https://api.spotify.com/v1/tracks/0islTY4Fw6lh...</td><td>0islTY4Fw6lhYbfqi8Qtdj</td><td>FALSE</td><td>Hopscotch Willie</td><td>24</td><td>https://p.scdn.co/mp3-preview/c7782dc6d7c0bb12...</td><td>2</td><td>track</td><td>spotify:track:0islTY4Fw6lhYbfqi8Qtdj</td></tr>\n <tr><td>3</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, BO, BR, CA, CL, CO, CR, EC, GT, HK, HN, I...</td><td>https://open.spotify.com/album/6pWpb4IdPu9vp9m...</td><td>https://api.spotify.com/v1/albums/6pWpb4IdPu9v...</td><td>6pWpb4IdPu9vp9mOdh5DjY</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Real Emotional Trash</td><td>3/4/2008</td><td>day</td><td>...</td><td>https://open.spotify.com/track/3jyFLbljUTKjE13...</td><td>https://api.spotify.com/v1/tracks/3jyFLbljUTKj...</td><td>3jyFLbljUTKjE13nIWXchH</td><td>FALSE</td><td>Dragonfly Pie</td><td>26</td><td>https://p.scdn.co/mp3-preview/50f419e7d3e8a6a7...</td><td>1</td><td>track</td><td>spotify:track:3jyFLbljUTKjE13nIWXchH</td></tr>\n <tr><td>4</td><td>album</td><td>[{'external_urls': {'spotify': 'https://open.s...</td><td>[AR, AU, BO, BR, CA, CL, CO, CR, DO, EC, GT, H...</td><td>https://open.spotify.com/album/5DMvSCwRqfNVlMB...</td><td>https://api.spotify.com/v1/albums/5DMvSCwRqfNV...</td><td>5DMvSCwRqfNVlMB5LjHOwG</td><td>[{'height': 640, 'url': 'https://i.scdn.co/ima...</td><td>Sparkle Hard</td><td>5/18/2018</td><td>day</td><td>...</td><td>https://open.spotify.com/track/6dNmC2YWtWbVOFO...</td><td>https://api.spotify.com/v1/tracks/6dNmC2YWtWbV...</td><td>6dNmC2YWtWbVOFOdTuRDQs</td><td>FALSE</td><td>Difficulties - Let Them Eat Vowels</td><td>35</td><td>https://p.scdn.co/mp3-preview/787be9d1bbebcd84...</td><td>11</td><td>track</td><td>spotify:track:6dNmC2YWtWbVOFOdTuRDQs</td></tr>\n</tbody></table>\n</div><p>Now we can go back to using dot notation to access a column as a Series. This saves us some typing every time we want to grab a column, and it looks a bit nicer (to me, at least). I say worth it.</p><pre><code class=\"language-python\">tracks_df.album_id\n</code></pre>\n<h3 id=\"output--8\">Output:</h3><pre><code class=\"language-shell\">0    6pWpb4IdPu9vp9mOdh5DjY\n1    6pWpb4IdPu9vp9mOdh5DjY\n2    6pWpb4IdPu9vp9mOdh5DjY\n3    6pWpb4IdPu9vp9mOdh5DjY\n4    5DMvSCwRqfNVlMB5LjHOwG\nName: album_id, dtype: object\n</code></pre>\n<h2 id=\"i-need-that-record\">I Need That Record</h2><p>By including more parameters when we use <code>json_normlize()</code>, we can really extract just the data that we want from our API response.</p><p>From our responses above, we can see that the <code>artist</code> property contains a list of artists that are associated with a track:</p><pre><code class=\"language-python\">tracks_response['tracks'][0]['artists']\n</code></pre>\n<h3 id=\"output--9\">Output:</h3><pre><code class=\"language-json\">[{\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7wyRA7deGRxozTyBc6QXPe&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;id&quot;: &quot;7wyRA7deGRxozTyBc6QXPe&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus &amp; The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7wyRA7deGRxozTyBc6QXPe&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/0WISkx0PwT6lYWdPqKUJY8&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;id&quot;: &quot;0WISkx0PwT6lYWdPqKUJY8&quot;,\n    &quot;name&quot;: &quot;Stephen Malkmus&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:0WISkx0PwT6lYWdPqKUJY8&quot;\n  },\n  {\n    &quot;external_urls&quot;: {\n      &quot;spotify&quot;: &quot;https://open.spotify.com/artist/7uStwCeP54Za8gXUFCf5L7&quot;\n    },\n    &quot;href&quot;: &quot;https://api.spotify.com/v1/artists/7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;id&quot;: &quot;7uStwCeP54Za8gXUFCf5L7&quot;,\n    &quot;name&quot;: &quot;The Jicks&quot;,\n    &quot;type&quot;: &quot;artist&quot;,\n    &quot;uri&quot;: &quot;spotify:artist:7uStwCeP54Za8gXUFCf5L7&quot;\n  }\n]\n</code></pre>\n<p>Let's say I want to load this data into a database later. It would be nice to have a join table that maps each of the artists that are associated with each track. Luckily, this is possible with <code>json_normalize()</code>'s <code>record_path</code> and <code>meta</code> parameters.</p><p><code>record_path</code> tells <code>json_normalize()</code> what path of keys leads to each individual record in the JSON object. In our case, we want to grab every artist id, so our function call will look like:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],sep=&quot;_&quot;)\n</code></pre>\n<h3></h3><div class=\"tableshadow tableContainer\" data-simplebar=\"\">\n<table class=\"responsive-table\">\n<thead><tr class=\"tableizer-firstrow\"><th> </th><th>external_urls </th><th>href </th><th>id </th><th>name </th><th>type </th><th>uri</th></tr></thead><tbody>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10 </td><td>{'spotify': 'https://open.spotify.com/artist/0... </td><td>https://api.spotify.com/v1/artists/0WISkx0PwT6... </td><td>0WISkx0PwT6lYWdPqKUJY8 </td><td>Stephen Malkmus </td><td>artist </td><td>spotify:artist:0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7uStwCeP54Z... </td><td>7uStwCeP54Za8gXUFCf5L7 </td><td>The Jicks </td><td>artist </td><td>spotify:artist:7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12 </td><td>{'spotify': 'https://open.spotify.com/artist/7... </td><td>https://api.spotify.com/v1/artists/7wyRA7deGRx... </td><td>7wyRA7deGRxozTyBc6QXPe </td><td>Stephen Malkmus & The Jicks </td><td>artist </td><td>spotify:artist:7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><p>Cool – we're almost there. Now we want to use the <code>meta</code> parameter to specify what data we want to include from the rest of the JSON object. In our case, we want to keep the track id and map it to the artist id. If we look back at our API response, the name of the column that included the track is is called, appropriately, <code>id</code>, so our full function call should look like this:</p><pre><code class=\"language-python\">json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n</code></pre>\n<h3 id=\"output--10\">Output:</h3><pre><code class=\"language-python\">-----------------------------------------\nValueError                             Traceback (most recent call last)\n\n    &lt;ipython-input-14-77e00a98c3c0&gt; in &lt;module&gt;()\n    ----&gt; 1 json_normalize(tracks_response['tracks'],record_path=['artists'],meta=['id'],sep=&quot;_&quot;)\n\n    ~/anaconda3/envs/music_data/lib/python3.6/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\n        268         if k in result:\n        269             raise ValueError('Conflicting metadata name {name}, '\n    --&gt; 270                              'need distinguishing prefix '.format(name=k))\n        271 \n        272         result[k] = np.array(v).repeat(lengths)\n    \nValueError: Conflicting metadata name id, need distinguishing prefix \n</code></pre>\n<p>Uh oh – an error! What's going on? Well, it turns out that both the album id and track id were given the key <code>id</code>. pandas doesn't like that, and it gives us a helpful error to tell us so: <code>ValueError: Conflicting metadata name id, need distinguishing prefix</code>.</p><p>There are two more parameters we can use to overcome this error: <code>record_prefix</code> and <code>meta_prefix</code>. These are strings we'll add to the beginning of our records and metadata to prevent these naming conflicts. Since we're dealing with Spotify artist ids for our records and Spotify track ids as the metadata, I'll use <code>sp_artist_</code> and <code>sp_track_</code> respectively. When that's done, I'll select only the columns that we're interested in.</p><pre><code class=\"language-python\">artist_and_track = json_normalize(\n    data=tracks_response['tracks'],\n    record_path='artists',\n    meta=['id'],\n    record_prefix='sp_artist_',\n    meta_prefix='sp_track_',\n    sep=&quot;_&quot;\n)\nartist_and_track = artist_and_track[['sp_track_id','sp_artist_id']]\nartist_and_track\n</code></pre>\n<h3 id=\"output--11\">Output:</h3><div class=\"tableshadow tableContainer\">\n<table>\n<thead><tr class=\"tableizer-firstrow\"><th></th><th>sp_track_id </th><th>sp_artist_id</th></tr></thead><tbody>\n <tr><td>0</td><td>0BDYBajZydY54OTgQsH940 </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>1</td><td>0BDYBajZydY54OTgQsH940 </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>2</td><td>0BDYBajZydY54OTgQsH940 </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>3</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>4</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>5</td><td>7fdUqrzb8oCcIoKvFuzMrs </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>6</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>7</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>8</td><td>0islTY4Fw6lhYbfqi8Qtdj </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>9</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n <tr><td>10</td><td>3jyFLbljUTKjE13nIWXchH </td><td>0WISkx0PwT6lYWdPqKUJY8</td></tr>\n <tr><td>11</td><td>3jyFLbljUTKjE13nIWXchH </td><td>7uStwCeP54Za8gXUFCf5L7</td></tr>\n <tr><td>12</td><td>6dNmC2YWtWbVOFOdTuRDQs </td><td>7wyRA7deGRxozTyBc6QXPe</td></tr>\n</tbody></table>\n</div><h2 id=\"tl-dr\">TL;DR</h2><ul><li>Use <code>pd.io.json.json_normalize()</code> to automagically flatten a nested JSON object into a DataFrame</li><li>Make your life slightly easier when it comes to selecting columns by overriding the default <code>sep</code> parameter</li><li>Specify what data constitutes a record with the <code>record_path</code> parameter</li><li>Include data from outside of the record path with the <code>meta</code> parameter</li><li>Fix naming conflicts if they arise with the <code>record_prefix</code> and <code>meta_prefix</code> parameters</li></ul>","url":"https://hackersandslackers.com/json-into-pandas-dataframes/","uuid":"172cef40-4545-4c14-8488-a86a891ef47d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b588eb363c4cc21a000cf51"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ac","title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","slug":"trash-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","excerpt":"(And a way to clean it up with SQLAlchemy).","custom_excerpt":"(And a way to clean it up with SQLAlchemy).","created_at_pretty":"19 July, 2018","published_at_pretty":"23 July, 2018","updated_at_pretty":"17 November, 2018","created_at":"2018-07-18T20:26:25.000-04:00","published_at":"2018-07-23T08:30:00.000-04:00","updated_at":"2018-11-16T20:50:25.000-05:00","meta_title":"(And a way to clean it up with SQLAlchemy) | Hackers And Slackers","meta_description":"Python has an extremely handy little tool called f-strings that make string templating a snap!  ","og_description":"Trash Pandas: Messy, Convenient DB Operations via Pandas","og_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","og_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","twitter_description":"(And a way to clean it up with SQLAlchemy)","twitter_image":"https://hackersandslackers.com/content/images/2018/07/racoon@2x.jpg","twitter_title":"Trash Pandas: Messy, Convenient DB Operations via Pandas","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Let's say you were continuing our task from last week\n[https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/]\n: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a\ndatabase.\n\nLet's say you've been given a new CSV that conflicts with some rows you've\nalready entered, and you're told that these rows are the correct values.\n\nWhy Not Use Pandas' Built-in Method?\nPandas' built-in to_sql  DataFrame method won't be useful here.  Remember, it\nwrites as a block - if you set the if_exists  flag to \"replace\", that'll make it\nreplace the entire DB table with a new one based on the DF you're uploading.\n And if you're doing this piecemeal, you presumably don't want that.\n\nLast week, we just made a new DataFrame out of each row and appended it to the\nDB table.  That won't work here - we need an Update.  Writing each update by\nhand would be annoying, though - luckily we can use code to generate more code!\n Python has an extremely handy little tool called f-strings  that make string\ntemplating a snap!\n\ndef updateStr(row):\n    return (f\"UPDATE books \"\n            f\"\"\"SET author = '{x.author}' \"\"\"\n            f\"\"\"WHERE id = {x.id};\"\"\")\n\n\nLet's walk through that.  It takes a row from a Dataframe - note that we're\nusing dot notation here instead of the bracket notation.  The reason we're doing\nthat is because, instead of using iterrows()  like last week, we'll be using \nitertuples  because the docstring for iterrows()  said I should.  One reason for\nthis is that iterrows()  gives a pandas Series, which will store everything as\nthe same datatype, which will be annoying in some cases.  I think it's supposed\nto be faster too?itertuples()  instead gives us Named Tuples, which is kind of\nlike a dictionary, except we have to use dot notation instead of square\nbrackets.\n\nSooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly\njust a convenient way of formatting strings with variables - any code inside\ncurly parentheses will be evaluated.  They're convenient, flexible, and\nsupposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a\nDataFrame df2  that only contains the rows to be updated...\n\ncnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n\n\nSweet!  Now let's actually execute it.  We'll be using the execute()  function\nin Pandas' io.sql  module.  I get the feeling I'm not supposed to, primarily\nbecause it doesn't have an entry in the official Pandas documentation, and I\nonly found it by poking around the module code.  But hey, it works!  (Warning\nfrom last time applies super-duper-extra-double this time!)\n\nfor x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n\n\nAnd now let's see if it worked...\n\npd.io.sql.read_sql_table(\"books\", cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n\n\nSweet!\n\nNow all that's well and good, but surely we're not the first person to try to\nmake SQL statements by calling Python functions!  How about a slightly less\nerror-prone way of doing this?\n\nSQLAlchemy\nI'll level with you - I've never actually used SQLAlchemy for anything but\nconnecting Pandas to databases before via the create_engine()  function.  But\nthat's why blogging's great - gives you an excuse to finally check out that\nthing you knew was gonna be useful!\n\nSQLAlchemy first needs some information about our table, then it'll let us\ncreate statements doing things to said table automagically.  We can either\ndefine it ourselves (maybe in a future post!) or read an existing table.  I\nfound the default way of doing this a little to\n\"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of\nthe command that worked into a little function.  I encourage you all to do the\nsame!\n\ndef loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n\n\nThere we go!  Now, let's load our books  table...\n\nbooks = loadTable(cnx, \"books\")\n\n\nAnd here's the cool part!  Now that we have our table object, it has a bunch of\nbuilt-in methods for doing SQL things!  We can print an example...\n\nstr(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n\n\nIf we call books.update, it'll do exactly that.  It also has a handy string\nrepresentation, for debugging and sanity checks.\n\nSQLAlchemy wants us to have a Connection  in addition to our Engine.  Well,\nalright then.\n\nconn = cnx.connect()\n\n\nFine, happy now?  Good.\n\nSQLAlchemy lets us build SQL statements by chain methods, which is fantastically\nuseful.  Less error-prone, easier to pass collections.  Our basic pattern would\nbe, based on iterating with itertuples...\n\nfor x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n\n\nSuccess!","html":"<p>Let's say you were continuing our task from <em><a href=\"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/\">last week</a></em>: Taking a bunch of inconsistent Excel files and CSVs, and putting them into a database.</p><p>Let's say you've been given a new CSV that conflicts with some rows you've already entered, and you're told that these rows are the correct values.</p><h2 id=\"why-not-use-pandas-built-in-method\">Why Not Use Pandas' Built-in Method?</h2><p>Pandas' built-in <code>to_sql</code> DataFrame method won't be useful here.  Remember, it writes as a block - if you set the <code>if_exists</code> flag to <code>\"replace\"</code>, that'll make it replace the entire DB table with a new one based on the DF you're uploading.  And if you're doing this piecemeal, you presumably don't want that.</p><p>Last week, we just made a new DataFrame out of each row and appended it to the DB table.  That won't work here - we need an Update.  Writing each update by hand would be annoying, though - luckily we can use code to generate more code!  Python has an extremely handy little tool called <code>f-strings</code> that make string templating a snap!  </p><pre><code class=\"language-python\">def updateStr(row):\n    return (f&quot;UPDATE books &quot;\n            f&quot;&quot;&quot;SET author = '{x.author}' &quot;&quot;&quot;\n            f&quot;&quot;&quot;WHERE id = {x.id};&quot;&quot;&quot;)\n</code></pre>\n<p>Let's walk through that.  It takes a row from a Dataframe - note that we're using dot notation here instead of the bracket notation.  The reason we're doing that is because, instead of using <code>iterrows()</code> like last week, we'll be using <code>itertuples</code> because the docstring for <code>iterrows()</code> said I should.  One reason for this is that <code>iterrows()</code> gives a pandas Series, which will store everything as the same datatype, which will be annoying in some cases.  I think it's supposed to be faster too?  <code>itertuples()</code> instead gives us Named Tuples, which is kind of like a dictionary, except we have to use dot notation instead of square brackets.  </p><p>Sooo, we take a Named Tuple, and then the f-string goes to work.  It's mostly just a convenient way of formatting strings with variables - any code inside curly parentheses will be evaluated.  They're convenient, flexible, and supposedly pretty well-optimized!  Let's give it a spin.  Let's say we have a DataFrame <code>df2</code> that only contains the rows to be updated...</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://root:cracked1@localhost/appointments', echo=False)\nfor x in df2.itertuples(index=False):\n    print(updateStr(x))\nUPDATE books SET author = 'Abby' WHERE id = 3;\nUPDATE books SET author = 'Brian' WHERE id = 7;\nUPDATE books SET author = 'Celestine' WHERE id = 9;\n</code></pre>\n<p>Sweet!  Now let's actually execute it.  We'll be using the <code>execute()</code> function in Pandas' <code>io.sql</code> module.  I get the feeling I'm not supposed to, primarily because it doesn't have an entry in the official Pandas documentation, and I only found it by poking around the module code.  But hey, it works!  (Warning from last time applies super-duper-extra-double this time!)</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    pd.io.sql.execute(updateStr(x), cnx)\n</code></pre>\n<p>And now let's see if it worked...</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx)\n   author copies  id\n     Abby      2   3\n    Brian          7\nCelestine      7   9`\n</code></pre>\n<p>Sweet!</p><p>Now all that's well and good, but surely we're not the first person to try to make SQL statements by calling Python functions!  How about a slightly less error-prone way of doing this?</p><h1 id=\"sqlalchemy\">SQLAlchemy</h1><p>I'll level with you - I've never actually used SQLAlchemy for anything but connecting Pandas to databases before via the <code>create_engine()</code> function.  But that's why blogging's great - gives you an excuse to finally check out that thing you knew was gonna be useful!</p><p>SQLAlchemy first needs some information about our table, then it'll let us create statements doing things to said table automagically.  We can either define it ourselves (maybe in a future post!) or read an existing table.  I found the default way of doing this a little to \"has-a-couple-too-many-steps-and-function-args\"-y, so I packaged the version of the command that worked into a little function.  I encourage you all to do the same!</p><pre><code class=\"language-python\">def loadTable(cnx, tableName):\n    meta = MetaData(bind=cnx) \n    return Table(tableName, meta, autoload=True, autoload_with=cnx)\n\n#Binding it to the Engine will make sure it uses the right SQL dialect\n</code></pre>\n<p>There we go!  Now, let's load our <code>books</code> table...</p><pre><code class=\"language-python\">books = loadTable(cnx, &quot;books&quot;)\n</code></pre>\n<p>And here's the cool part!  Now that we have our table object, it has a bunch of built-in methods for doing SQL things!  We can print an example...</p><pre><code class=\"language-python\">str(books.update())\n'UPDATE books SET index=:index, author=:author, copies=:copies, id=:id'\n</code></pre>\n<p>If we call <code>books.update</code>, it'll do exactly that.  It also has a handy string representation, for debugging and sanity checks.</p><p>SQLAlchemy wants us to have a <code>Connection</code> in addition to our <code>Engine</code>.  Well, alright then.</p><pre><code class=\"language-python\">conn = cnx.connect()\n</code></pre>\n<p>Fine, happy now?  Good.</p><p>SQLAlchemy lets us build SQL statements by chain methods, which is fantastically useful.  Less error-prone, easier to pass collections.  Our basic pattern would be, based on iterating with <code>itertuples</code>...</p><pre><code class=\"language-python\">for x in df2.itertuples(index=False):\n    stmt = (books\n          .update()\n          .where(books.c.id == x.id)\n          .values(author=x.author)\n         )\n    conn.execute(stmt)\n</code></pre>\n<p>Success!</p>","url":"https://hackersandslackers.com/trash-pandas/","uuid":"8a789f92-fbde-48b5-8bdf-01206e340cc1","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b4fdab10dda8433e079043f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a1","title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","excerpt":"Code Snippet Corner ft. Pandas & SQL","custom_excerpt":"Code Snippet Corner ft. Pandas & SQL","created_at_pretty":"12 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"25 November, 2018","created_at":"2018-07-11T21:54:04.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-11-25T12:50:00.000-05:00","meta_title":"Code Snippet Corner: A Dirty Way of Cleaning Data (ft. Pandas & SQL) | Hackers and Slackers","meta_description":"Code Snippet Corner ft. Pandas & SQL","og_description":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","og_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","og_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","twitter_description":"Code Snippet Corner ft. Pandas & SQL","twitter_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","twitter_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Warning  The following is FANTASTICALLY not-secure.  Do not put this in a script\nthat's going to be running unsupervised.  This is for interactive sessions where\nyou're prototyping the data cleaning methods that you're going to use, and/or\njust manually entering stuff.  Especially if there's any chance there could be\nsomething malicious hiding in the data to be uploaded.  We're going to be\nexecuting formatted strings of SQL unsanitized code.  Also, this will lead to\nLOTS of silent failures, which are arguably The Worst Thing - if guaranteed\ncorrectness is a requirement, leave this for the tinkering table.\n Alternatively, if it's a project where \"getting something in there is better\nthan nothing\", this can provide a lot of bang for your buck.  Actually, it's\npurely for entertainment purposes and not for human consumption.\n\nLet's say you were helping someone take a bunch of scattered Excel files and\nCSVs and input them all into a MySQL database.  This is a very iterative, trial\n& error process.  We certainly don't want to be re-entering a bunch of\nboilerplate.  Pandas to the rescue!  We can painlessly load those files into a\nDataFrame, then just export them to the db!\n\nWell, not so fast  First off, loading stuff into a DB is a task all its own -\nPandas and your RDBMS have different kinds of tolerance for mistakes, and differ\nin often-unpredictable ways.  For example, one time I was performing a task\nsimilar to the one described here (taking scattered files and loading them into\na DB) - I was speeding along nicely, but then ran into a speedbump: turns out\nPandas generally doesn't infer that a column is a date unless you tell it\nspecifically, and will generally parse dates as strings.  Now, this was fine\nwhen the dates were present - MySQL is pretty smart about accepting different\nforms of dates & times.  But one thing it doesn't like is accepting an empty\nstring ''  into a date or time column.  Not a huge deal, just had to cast the\ncolumn as a date:\n\ndf['date'] = pd.to_datetime(df['date'])\n\nNow the blank strings are NaT, which MySQL knows how to handle!\n\nThis was simple enough, but there's all kinds of little hiccups that can happen.\n And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing\naffair - if there's one error, that means none of the rows will write.  Which\ncan get pretty annoying if you were trying to write a decent-sized DataFrame,\nespecially if the first error doesn't show up until one of the later rows.\n Waiting sucks.  And it's not just about being impatient - long waiting times\ncan disrupt your flow.\n\nRapid prototyping & highly-interactive development are some of Python's greatest\nstrengths, and they are great strengths indeed!  Paul Graham (one of the guys\nbehind Y Combinator) once made the comparison between REPL-heavy development and\nthe popularizing of oil paints (he was talking about LISP, but it's also quite\ntrue of Python, as Python took a lot of its cues from LISP):\n\nBefore oil paint became popular, painters used a medium, called tempera , that\ncannot be blended or over-painted. The cost of mistakes was high, and this\ntended to make painters conservative. Then came oil paint, and with it a great\nchange in style. Oil \"allows for second thoughts\". This proved a decisive\nadvantage in dealing with difficult subjects like the human figure.The new\nmedium did not just make painters' lives easier. It made possible a new and more\nambitious kind of painting. Janson writes:Without oil, the Flemish\nMasters'conquest of visible reality would have been much more limited. Thus,\nfrom a technical point of view, too, they deserve to be called the \"fathers of\nmodern painting\" , for oil has been the painter's basic medium ever since. As a\nmaterial, tempera is no lesss beautiful than oil. But the flexibility of oil\npaint gives greater scope to the imagination--that was the deciding factor.\nProgramming is now undergoing a similar change...Meanwhile, ideas borrowed from\nLisp increasingly turn up in the mainstream: interactive programming\nenvironments, garbage collection, and run-time typing  to name a few.More\npowerful tools are taking the risk out of exploration. That's good news for\nprogrammers, because it means that we will be able to undertake more ambitious\nprojects. The use of oil paint certainly had this effect. The period immediately\nfollowing its adoption was a golden age for painting. There are signs already\nthat something similar is happening in programming.\n(Emphasis mine)\nFrom here: http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.htmlA\nlittle scenario to demonstrate:\n\nLet's pretend we have a MySQL instance running, and have already created a\ndatabase named items\n\nimport pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(\"\"\"CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)\"\"\", cnx)\n\ndf = pd.DataFrame({\n    \"author\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"copies\": [2, \"\", 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n\n\nSoo, let's tighten this feedback loop, shall we?\n\nWe'll iterate through the DataFrame with the useful iterrows()  method.  This\ngives us essentially an enum  made from our DataFrame - we'll get a bunch of\ntuples giving us the index as the first element and the row as its own Pandas\nSeries as the second.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n\n\nLet's unpack that a bit.\n\nRemember that we're getting a two-element tuple, with the good stuff in the\nsecond element, so\n\nx[1]\n\nNext, we convert the Series to a one-entry DataFrame, because the Series doesn't\nhave the DataFrame's to_sql()  method.\n\npd.DataFrame(x[1])\n\nThe default behavior will assume this is a single column with, each variable\nbeing the address of a different row.  MySQL isn't going to be having it.  Sooo,\nwe transpose!\n\npd.DataFrame(x[1]).transpose()\n\nAnd finally, we use our beloved to_sql  method on that.\n\nLet's check our table now!\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n\n\nIt wrote the first row!  Not much of a difference with this toy example, but\nonce you were writing a few thousand rows and the error didn't pop up until the\n3000th, this would make a pretty noticeable difference in your ability to\nquickly experiment with different cleaning schemes.\n\nNote that this will still short-circuit as soon as we hit the error.  If we\nwanted to make sure we got all the valid input before working on our tough\ncases, we could make a little try/except  block.\n\n\n\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n\n\nThis will try  to write each line, and if it encounters an Exception  it'll \ncontinue  the loop.\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n\n\nAlright, now the bulk of our data's in the db!  Whatever else happens, you've\ndone that much!  Now you can relax a bit, which is useful for stimulating the\ncreativity you'll need for the more complicated edge cases.\n\nSo, we're ready to start testing new cleaning schemes?  Well, not quite yet...\n\nLet's say we went and tried to think up a fix.  We go to test it out and...\n\n#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '1' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '3' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n\n\nThe error we're interested is in there, but what's all this other nonsense\ncrowding it?\n\nWell, one of the handy things about a database is that it'll enforce uniqueness\nbased on the constraints you give it.  It's already got an entry with an id \nvalue of 1, so it's going to complain if you try to put another one.  In\naddition to providing a lot of distraction, this'll also slow us down\nconsiderably - after all, part of the point was to make our experiments with\ndata-cleaning go faster!\n\nLuckily, Pandas' wonderful logical indexing will make it a snap to ensure that\nwe only bother with entries that aren't in the database yet.\n\n#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n\n\nThis will also be shockingly quick - Pandas' logical indexing takes advantage of\nall that magic going on under the hood.  Using it, instead of manually\niteration, can literally bring you from waiting minutes to waiting seconds.\n\nBuuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so\nhow about we just turn it into a function?\n\n#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n    return df[~df.index.isin(usedIDs)]\n\n\nSo, next time we think we've made some progress on an edge case, we just call...\n\n#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nActually, let's clean that up even more - the more keys we hit, the more\nopportunities to make a mistake!  The most bug-free code is the code you don't\nwrite.\n\ndef writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nSo, finally, we can work on our new cleaning scheme, and whenever we think we're\ndone...\n\nwriteNewRows(cleanedDF)\n\nAnd boom!  Instant feedback!","html":"<p><strong>Warning</strong> The following is FANTASTICALLY not-secure.  Do not put this in a script that's going to be running unsupervised.  This is for interactive sessions where you're prototyping the data cleaning methods that you're going to use, and/or just manually entering stuff.  Especially if there's any chance there could be something malicious hiding in the data to be uploaded.  We're going to be executing formatted strings of SQL unsanitized code.  Also, this will lead to LOTS of silent failures, which are arguably The Worst Thing - if guaranteed correctness is a requirement, leave this for the tinkering table.  Alternatively, if it's a project where \"getting something in there is better than nothing\", this can provide a lot of bang for your buck.  Actually, it's purely for entertainment purposes and not for human consumption.</p><p>Let's say you were helping someone take a bunch of scattered Excel files and CSVs and input them all into a MySQL database.  This is a very iterative, trial &amp; error process.  We certainly don't want to be re-entering a bunch of boilerplate.  Pandas to the rescue!  We can painlessly load those files into a DataFrame, then just export them to the db!</p><p>Well, not so fast  First off, loading stuff into a DB is a task all its own - Pandas and your RDBMS have different kinds of tolerance for mistakes, and differ in often-unpredictable ways.  For example, one time I was performing a task similar to the one described here (taking scattered files and loading them into a DB) - I was speeding along nicely, but then ran into a speedbump: turns out Pandas generally doesn't infer that a column is a date unless you tell it specifically, and will generally parse dates as strings.  Now, this was fine when the dates were present - MySQL is pretty smart about accepting different forms of dates &amp; times.  But one thing it doesn't like is accepting an empty string <code>''</code> into a date or time column.  Not a huge deal, just had to cast the column as a date:</p><p><code>df['date'] = pd.to_datetime(df['date'])</code></p><p>Now the blank strings are <code>NaT</code>, which MySQL knows how to handle!</p><p>This was simple enough, but there's all kinds of little hiccups that can happen.  And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing affair - if there's one error, that means none of the rows will write.  Which can get pretty annoying if you were trying to write a decent-sized DataFrame, especially if the first error doesn't show up until one of the later rows.  Waiting sucks.  And it's not just about being impatient - long waiting times can disrupt your flow.</p><p>Rapid prototyping &amp; highly-interactive development are some of Python's greatest strengths, and they are great strengths indeed!  Paul Graham (one of the guys behind Y Combinator) once made the comparison between REPL-heavy development and the popularizing of oil paints (he was talking about LISP, but it's also quite true of Python, as Python took a lot of its cues from LISP):</p><blockquote>Before oil paint became popular, painters used a medium, called tempera , that cannot be blended or over-painted. The cost of mistakes was high, and this tended to make painters conservative. Then came oil paint, and with it a great change in style. Oil \"allows for second thoughts\". This proved a decisive advantage in dealing with difficult subjects like the human figure.The new medium did not just make painters' lives easier. It made possible a new and more ambitious kind of painting. Janson writes:Without oil, the Flemish Masters'conquest of visible reality would have been much more limited. Thus, from a technical point of view, too, they deserve to be called the \"fathers of modern painting\" , for oil has been the painter's basic medium ever since. As a material, tempera is no lesss beautiful than oil. But the flexibility of oil paint gives greater scope to the imagination--that was the deciding factor.<br>Programming is now undergoing a similar change...Meanwhile, ideas borrowed from Lisp increasingly turn up in the mainstream: <strong>interactive programming environments, garbage collection, and run-time typing</strong> to name a few.More powerful tools are taking the risk out of exploration. That's good news for programmers, because it means that we will be able to undertake more ambitious projects. The use of oil paint certainly had this effect. The period immediately following its adoption was a golden age for painting. There are signs already that something similar is happening in programming.<br>(Emphasis mine)<br>From here: <a href=\"http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\">http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html</a></blockquote><p>A little scenario to demonstrate:</p><p>Let's pretend we have a MySQL instance running, and have already created a database named <code>items</code></p><pre><code class=\"language-python\">import pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(&quot;&quot;&quot;CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)&quot;&quot;&quot;, cnx)\n\ndf = pd.DataFrame({\n    &quot;author&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],\n    &quot;copies&quot;: [2, &quot;&quot;, 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n</code></pre>\n<p>Soo, let's tighten this feedback loop, shall we?</p><p>We'll iterate through the DataFrame with the useful <code>iterrows()</code> method.  This gives us essentially an <code>enum</code> made from our DataFrame - we'll get a bunch of tuples giving us the index as the first element and the row as its own Pandas Series as the second.</p><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n</code></pre>\n<p>Let's unpack that a bit.</p><p>Remember that we're getting a two-element tuple, with the good stuff in the second element, so</p><p><code>x[1]</code></p><p>Next, we convert the Series to a one-entry DataFrame, because the Series doesn't have the DataFrame's <code>to_sql()</code> method.</p><p><code>pd.DataFrame(x[1])</code></p><p>The default behavior will assume this is a single column with, each variable being the address of a different row.  MySQL isn't going to be having it.  Sooo, we transpose!</p><p><code>pd.DataFrame(x[1]).transpose()</code></p><p>And finally, we use our beloved <code>to_sql</code> method on that.</p><p>Let's check our table now!</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n</code></pre>\n<p>It wrote the first row!  Not much of a difference with this toy example, but once you were writing a few thousand rows and the error didn't pop up until the 3000th, this would make a pretty noticeable difference in your ability to quickly experiment with different cleaning schemes.</p><p>Note that this will still short-circuit as soon as we hit the error.  If we wanted to make sure we got all the valid input before working on our tough cases, we could make a little <code>try/except</code> block.</p><pre><code class=\"language-python\">\n</code></pre>\n<pre><code>for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n</code></pre><p>This will <code>try</code> to write each line, and if it encounters an <code>Exception</code> it'll <code>continue</code> the loop.</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n</code></pre>\n<p>Alright, now the bulk of our data's in the db!  Whatever else happens, you've done that much!  Now you can relax a bit, which is useful for stimulating the creativity you'll need for the more complicated edge cases.</p><p>So, we're ready to start testing new cleaning schemes?  Well, not quite yet...</p><p>Let's say we went and tried to think up a fix.  We go to test it out and...</p><pre><code class=\"language-python\">#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '1' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '3' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n</code></pre>\n<p>The error we're interested is in there, but what's all this other nonsense crowding it?</p><p>Well, one of the handy things about a database is that it'll enforce uniqueness based on the constraints you give it.  It's already got an entry with an <code>id</code> value of 1, so it's going to complain if you try to put another one.  In addition to providing a lot of distraction, this'll also slow us down considerably - after all, part of the point was to make our experiments with data-cleaning go faster!</p><p>Luckily, Pandas' wonderful logical indexing will make it a snap to ensure that we only bother with entries that aren't in the database yet.</p><pre><code class=\"language-python\">#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n</code></pre>\n<p>This will also be shockingly quick - Pandas' logical indexing takes advantage of all that magic going on under the hood.  Using it, instead of manually iteration, can literally bring you from waiting minutes to waiting seconds.</p><p>Buuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so how about we just turn it into a function?</p><pre><code class=\"language-python\">#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n    return df[~df.index.isin(usedIDs)]\n</code></pre>\n<p>So, next time we think we've made some progress on an edge case, we just call...</p><pre><code class=\"language-python\">#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>Actually, let's clean that up even more - the more keys we hit, the more opportunities to make a mistake!  The most bug-free code is the code you don't write.</p><pre><code class=\"language-python\">def writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>So, finally, we can work on our new cleaning scheme, and whenever we think we're done...</p><p><code>writeNewRows(cleanedDF)</code></p><p>And boom!  Instant feedback!</p>","url":"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/","uuid":"9788b54d-ef44-4a35-9ec6-6a8678038480","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b46b4bcc6a9e951f8a6cc32"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369d","title":"Extract Massive Amounts of Data from APIs in Python","slug":"extracting-massive-datasets-from-apis","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","excerpt":"Abusing REST APIs for all they’re worth.","custom_excerpt":"Abusing REST APIs for all they’re worth.","created_at_pretty":"04 July, 2018","published_at_pretty":"04 July, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-07-04T15:24:18.000-04:00","published_at":"2018-07-04T17:26:00.000-04:00","updated_at":"2019-03-28T07:55:05.000-04:00","meta_title":"Extracting Massive Datasets in Python | Hackers and Slackers","meta_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","og_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","og_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","og_title":"Extracting Massive Datasets in Python","twitter_description":"The data we need and crave is stashed behind APIs all around us. We have the keys to the world, but that power often comes with a few caveats.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/usa@2x.jpg","twitter_title":"Extracting Massive Datasets in Python","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"}],"plaintext":"Taxation without representation. Colonialism. Not letting people eat cake. Human\nbeings rightfully meet atrocities with action in an effort to change the worked\nfor the better. Cruelty by mankind justifies revolution, and it is this writer’s\nopinion that API limitations are one such cruelty.\n\nThe data we need and crave is stashed in readily available APIs all around us.\nIt’s as though we have the keys to the world, but that power often comes with a\nfew caveats:\n\n * Your “key” only lasts a couple of hours, and if you want another one, you’ll\n   have to use some other keys to get another key.\n * You can have the ten thousand records you’re looking for, but you can only\n   pull 50 at a time.\n * You won’t know the exact structure of the data you’re getting, but it’ll\n   probably be a JSON hierarchy designed by an 8-year-old.\n\nAll men may be created equal, but APIs are not. In the spirit of this 4th of\nJuly, let us declare independence from repetitive tasks: One Script, under\nPython, for Liberty and Justice for all.\n\nProject Setup\nWe'll split our project up by separation of concern into just a few files:\n\nmyProject\n├── main.py\n├── config.py\n└── token.py\n\n\nMain.py will unsurprisingly hold the core logic of our script.\n\nConfig.py contains variables such as client secrets and endpoints which we can\neasily swap when applying this script to different APIs. For now we'll just keep\nvariables client_id  and client_secret  in there for now.\n\nToken.py  serves the purpose of Token Generation. Let's start there.\n\nThat's the Token \nSince we're assuming worst case scenarios let's focus on atrocity number one:\nAPIs which require expiring tokens. There are some tyrants in this world who\nbelieve that in order to use their API, it is necessary to to first use a client\nID and client secret to generate a Token which quickly becomes useless hours\nlater. In other words, you need to use an API every time you want to use the\nactual API. Fuck that.\n\nimport requests\nfrom config import client_id, client_secret\n\ntoken_url = 'https://api.fakeapi.com/auth/oauth2/v2/token'\n\ndef generateToken():\n    r = requests.post(token_url, auth=(client_id, client_secret), json={\"grant_type\": \"client_credentials\"})\n    bearer_token = r.json()['access_token']\n    print('new token = ', bearer_token)\n    return bearer_token\n\ntoken = generateToken()\n\n\nWe import client_id  and client_secret  from our config file off the bat: most\nservices will grant these things simply by signing up for their API.\n\nMany APIs have an endpoint which specifically serves the purpose of accepting\nthese variables and spitting out a generated token. token_url  is the variable\nwe use to store this endpoint.\n\nOur token  variable invokes our generateToken()  function which stores the\nresulting Token. With this out of the way, we can now call this function every\ntime we use the API, so we never have to worry about expiring tokens.\n\nPandas to the Rescue\nWe've established that we're looking to pull a large set of data, probably\nsomewhere in the range of thousands of records. While JSON is all fine and\ndandy, it probably isn't very useful for human beings to consume a JSON file\nwith thousands of records. \n\nAgain, we have no idea what the nature of the data coming through will look\nlike. I don't really care to manually map values to fields, and I'm guessing you\ndon't either. Pandas can help us out here: by passing the first page of records\nto Pandas, we can generate the resulting keys into columns in a Dataframe. It's\nalmost like having a database-type schema created for you simply by looking at\nthe data coming through:\n\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nfrom token import token\n\ndef setKeys():\n    headers = {\"Authorization\":\"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    dataframe = pd.DataFrame(columns=r.json()['data'][0].keys())\n    return dataframe\n\nrecords_df = setKeys()\n\nWe can now store all data into records_df  moving forward, allowing us to build\na table of results.\n\nNo Nation for Pagination\nAnd here we are, one of the most obnoxious parts of programming: paginated\nresults. We want thousands of records, but we're only allowed 50 at a time. Joy.\n\nWe've already set records_df  earlier as a global variable, so we're going to\nappend every page of results we get to that Dataframe, starting at page #1. The\nfunction getRecords  is going to pull that first page for us.\n\nbase_url = 'https://api.fakeapi.com/api/1/'\n\ndef getRecords():\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records_df = pd.DataFrame(columns=r.json()['data'][0].keys())\n    if nextpage:\n        getNextPage(nextpage)\n\ngetRecords()\n\n\nLuckily APIs if there are  additional pages of results to a request, most APIs\nwill provide a URL to said page, usually stored in the response as a value. In\nour case, you can see we find this value after making the request: nextpage =\nr.json()['pagination']['next_link']. If this value exists, we make a call to get\nthe next page of results.\n\npage = 1\n\ndef getNextPage(nextpage):\n    global page\n    page = page + 1\n    print('PAGE ', page)\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(nextpage, headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records = r.json()['data']\n    for user in records:\n        s  = pd.Series(user,index=user.keys())\n        global records_df\n        records_df.loc[len(records_df)] = s\n    records_df.to_csv('records.csv')\n    if nextpage:\n        getNextPage(nextpage)\n\nOur function getNextPage  hits that next page of results, and appends them to\nthe pandas DataFrame we created earlier. If another page exists after that, the\nfunction runs again, and our page increments by 1. As long as more pages exist,\nthis function will fire again and again until all innocent records are driven\nout of their comfortable native resting place and forced into our contained\ndataset. There's not much more American than that.\n\nThere's More We Can Do\nThis script is fine, but it can optimized to be even more modular to truly be\none-size-fits-all. For instance, some APIs don't tell you the number of pages \nyou should except, but rather the number of records.  In those cases, we'd have\nto divide total number of records by records per page to know how many pages to\nexpect. As much as I want to go into detail about writing loops on the 4th of\nJuly, I don't. At all.\n\nThere are plenty more examples, but this should be enough to get us thinking how\nwe can replace tedious work with machines. That sounds like a flavor that pairs\nperfectly with Bud Light and hotdogs if you ask me.","html":"<p>Taxation without representation. Colonialism. Not letting people eat cake. Human beings rightfully meet atrocities with action in an effort to change the worked for the better. Cruelty by mankind justifies revolution, and it is this writer’s opinion that API limitations are one such cruelty.</p><p>The data we need and crave is stashed in readily available APIs all around us. It’s as though we have the keys to the world, but that power often comes with a few caveats:</p><ul><li>Your “key” only lasts a couple of hours, and if you want another one, you’ll have to use some other keys to get another key.</li><li>You can have the ten thousand records you’re looking for, but you can only pull 50 at a time.</li><li>You won’t know the exact structure of the data you’re getting, but it’ll probably be a JSON hierarchy designed by an 8-year-old.</li></ul><p>All men may be created equal, but APIs are not. In the spirit of this 4th of July, let us declare independence from repetitive tasks: One Script, under Python, for Liberty and Justice for all.</p><h2 id=\"project-setup\">Project Setup</h2><p>We'll split our project up by separation of concern into just a few files:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">myProject\n├── main.py\n├── config.py\n└── token.py\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Main.py </strong>will unsurprisingly hold the core logic of our script.</p><p><strong>Config.py </strong>contains variables such as client secrets and endpoints which we can easily swap when applying this script to different APIs. For now we'll just keep variables <code>client_id</code> and <code>client_secret</code> in there for now.</p><p><strong>Token.py</strong> serves the purpose of Token Generation. Let's start there.</p><h2 id=\"that-s-the-token\">That's the Token </h2><p>Since we're assuming worst case scenarios let's focus on atrocity number one: APIs which require expiring tokens. There are some tyrants in this world who believe that in order to use their API, it is necessary to to first use a client ID and client secret to generate a Token which quickly becomes useless hours later. In other words, you need to use an API every time you want to use the actual API. Fuck that.</p><!--kg-card-begin: code--><pre><code>import requests\nfrom config import client_id, client_secret\n\ntoken_url = 'https://api.fakeapi.com/auth/oauth2/v2/token'\n\ndef generateToken():\n    r = requests.post(token_url, auth=(client_id, client_secret), json={\"grant_type\": \"client_credentials\"})\n    bearer_token = r.json()['access_token']\n    print('new token = ', bearer_token)\n    return bearer_token\n\ntoken = generateToken()\n</code></pre><!--kg-card-end: code--><p>We import <code>client_id</code> and <code>client_secret</code> from our config file off the bat: most services will grant these things simply by signing up for their API.  </p><p>Many APIs have an endpoint which specifically serves the purpose of accepting these variables and spitting out a generated token. <code>token_url</code> is the variable we use to store this endpoint.</p><p>Our <code>token</code> variable invokes our <code>generateToken()</code> function which stores the resulting Token. With this out of the way, we can now call this function every time we use the API, so we never have to worry about expiring tokens.</p><h2 id=\"pandas-to-the-rescue\">Pandas to the Rescue</h2><p>We've established that we're looking to pull a large set of data, probably somewhere in the range of thousands of records. While JSON is all fine and dandy, it probably isn't very useful for human beings to consume a JSON file with thousands of records. </p><p>Again, we have no idea what the nature of the data coming through will look like. I don't really care to manually map values to fields, and I'm guessing you don't either. Pandas can help us out here: by passing the first page of records to Pandas, we can generate the resulting keys into columns in a Dataframe. It's almost like having a database-type schema created for you simply by looking at the data coming through:</p><!--kg-card-begin: code--><pre><code>import requests\nimport pandas as pd\nimport numpy as np\nimport json\nfrom token import token\n\ndef setKeys():\n    headers = {\"Authorization\":\"Bearer \" + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    dataframe = pd.DataFrame(columns=r.json()['data'][0].keys())\n    return dataframe\n\nrecords_df = setKeys()</code></pre><!--kg-card-end: code--><p>We can now store all data into <code>records_df</code> moving forward, allowing us to build a table of results.</p><h2 id=\"no-nation-for-pagination\">No Nation for Pagination</h2><p>And here we are, one of the most obnoxious parts of programming: paginated results. We want thousands of records, but we're only allowed 50 at a time. Joy.</p><p>We've already set <code>records_df</code> earlier as a global variable, so we're going to append every page of results we get to that Dataframe, starting at page #1. The function <code>getRecords</code> is going to pull that first page for us.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">base_url = 'https://api.fakeapi.com/api/1/'\n\ndef getRecords():\n    headers = {&quot;Authorization&quot;: &quot;Bearer &quot; + token}\n    r = requests.get(base_url + 'users', headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records_df = pd.DataFrame(columns=r.json()['data'][0].keys())\n    if nextpage:\n        getNextPage(nextpage)\n\ngetRecords()\n</code></pre>\n<!--kg-card-end: markdown--><p>Luckily APIs if there <em>are</em> additional pages of results to a request, most APIs will provide a URL to said page, usually stored in the response as a value. In our case, you can see we find this value after making the request: <code>nextpage = r.json()['pagination']['next_link']</code>. If this value exists, we make a call to get the next page of results.</p><!--kg-card-begin: code--><pre><code>page = 1\n\ndef getNextPage(nextpage):\n    global page\n    page = page + 1\n    print('PAGE ', page)\n    headers = {\"Authorization\": \"Bearer \" + token}\n    r = requests.get(nextpage, headers=headers)\n    nextpage = r.json()['pagination']['next_link']\n    records = r.json()['data']\n    for user in records:\n        s  = pd.Series(user,index=user.keys())\n        global records_df\n        records_df.loc[len(records_df)] = s\n    records_df.to_csv('records.csv')\n    if nextpage:\n        getNextPage(nextpage)</code></pre><!--kg-card-end: code--><p>Our function <code>getNextPage</code> hits that next page of results, and <em>appends them to the pandas DataFrame </em>we created earlier. If another page exists after that, the function runs again, and our page increments by 1. As long as more pages exist, this function will fire again and again until all innocent records are driven out of their comfortable native resting place and forced into our contained dataset. There's not much more American than that.</p><h2 id=\"there-s-more-we-can-do\">There's More We Can Do</h2><p>This script is fine, but it can optimized to be even more modular to truly be one-size-fits-all. For instance, some APIs don't tell you the number of <em>pages</em> you should except, but rather the number of <em>records.</em> In those cases, we'd have to divide total number of records by records per page to know how many pages to expect. As much as I want to go into detail about writing loops on the 4th of July, I don't. At all.</p><p>There are plenty more examples, but this should be enough to get us thinking how we can replace tedious work with machines. That sounds like a flavor that pairs perfectly with Bud Light and hotdogs if you ask me.</p>","url":"https://hackersandslackers.com/extracting-massive-datasets-from-apis/","uuid":"39a94407-5d5a-4038-a6b6-04fa228ad0f0","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3d1ee2d0ac8a143588f36e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369a","title":"Using Pandas and SQLAlchemy to Simplify Databases","slug":"using-pandas-to-make-dealing-with-dbs-less-of-a-hassle","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2.jpg","excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","custom_excerpt":"Use SQLAlchemy with PyMySQL to make database connections easy.","created_at_pretty":"02 July, 2018","published_at_pretty":"03 July, 2018","updated_at_pretty":"04 April, 2019","created_at":"2018-07-02T14:54:34.000-04:00","published_at":"2018-07-03T07:00:00.000-04:00","updated_at":"2019-04-04T00:35:09.000-04:00","meta_title":"Using Pandas and SQLAlchemy to Simplify Databases | Hackers and Slackers","meta_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","og_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-2.jpg","og_title":"Using Pandas and SQLAlchemy to Simplify Databases","twitter_description":"One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. ","twitter_image":"https://hackersandslackers.com/content/images/2019/04/pandasdrop2-1.jpg","twitter_title":"Using Pandas and SQLAlchemy to Simplify Databases","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"Manually opening and closing cursors? Iterating through DB output by hand?\nRemembering which function is the actual one that matches the Python data\nstructure you're gonna be using?\n\nThere has to be a better way!\n\nThere totally is.\n\nOne of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an\nExcel file, or a database - Pandas gets you what you want painlessly. In\nfact,I'd say that even if you don't have the spare bandwidth at the moment to\nrewire your brain to learn all the wonderful ways Pandas lets you manipulate\ndata (array-based programming generally isn't the first paradigm people learn),\nthen it's STILL worth using just for the I/O. Grab your data with one of its\ndelicious one-liners, and once it's in a Dataframe there's have a method to\nconvert it to practically any combination of native Python data structures that\nyou want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've\ngot it!).\n\nHere we'll be looking at interacting with a database with Pandas, because that's\nwhat I was finishing up when I saw Todd texted me that a lot of the people who\nwind up on the site are searching for Pandas stuff.\n\nSo, for our purposes, let's say you're doing Python stuff with a MySQL database.\nI'm also assuming you're using Linux (including via Windows Subsystem for Linux,\nwhich is what I use in order to have Ubuntu on my Windows laptop), because I've\nnever done this with Windows.\n\nAnnoyingly, there's more than one potential connector - I generally use PyMySQL,\nfor no particular reason. Installing this involves an extra step - telling pip\nor Conda to install it won't do the whole thing. Annoying, but them's the\nbreaks.\n\nsudo apt-get install python3-pymysql -y\n\n\nShould get you there. I remember fussing with this every time I have to do it on\na new machine.\n\nYou also have to install a mysql-connector, for reasons I won't pretend to\nunderstand.\n\nconda install mysql-connector-python -y\n\n\nI've never actually done this with pip, but I assume it'd be\n\npip install mysql-connector\n\n\nIf that doesn't work, try\n\npip install --allow-external mysql-connector-python mysql-connector-python\n\n\nWE'RE ALMOST DONE INSTALLING THINGS!\nLast, but not least, is a lovely little thing called SQLAlchemy. Remember when I\nsaid I didn't actually know anything about PyMySQL and its relative advantages\nand disadvantages to other MySQL connectors for Python? Part of the reason for\nthat is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap\nPython SQL connectors at a higher level of abstraction and not have to deal with\nthe particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's\nactually a default Anaconda package. BUT, if you're in a fresh environment (or\nnot using Conda)...\n\n(Always remember the little -y   at the end. That ensures that you can walk away\nwhile it installs and it'll be done when you get back, instead of finishing the\nlong process of whatever's happening when you first tell it to install and then\nwaiting to start on the next long process that takes place after you hit y.)\n\nOR\n\npip install sqlalchemy   (that IS one of the nice things about pip! Doesn't need\nyou to confirm).\n\nSo, to review, before we start Pythoning...\n\n$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n\n\nOR\n\n$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n\n\nCool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!\n We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what\nPandas uses to connect to databases.\n\nAaaand one (possible) last step. If you're doing this with a locally-installed\ndb, you might have to sudo service mysql start.\n\n(Like, every time you do dev work on it. At least on my machine. Your mileage\nmay vary)\n\nOn to the Actual Python\nSo, what does all this do for us? Well, first off, it wasn't THAT much - and you\nonly have to do it once (per machine (really per environment per machine (you\nare  taking advantage of Conda envs or virtualenvs, right?))). But what it\nreally lets us skip all the implementation  details and focus on the actual\nlogic of what we're doing. That's really what both SQL and Pandas are all about-\nthey wed short, declarative  bits of code to highly optimized libraries\nunderneath the hood. Relational Database Management Systems sit on top of code\nthat's been optimized for decades, turning your short queries into highly\nefficient compiled code. Pandas takes advantage of scientific computing\nlibraries for doing matrix operations that have been around since before there\nwas an internet. Drill down deep enough in Pandas and you'll actually find\nFORTRAN code that was originally written to simulate nuclear explosions. Our\nwhole goal is to just focus on our data manipulation via SQL and Python's\nscientific computing family - and we don't want to add a bunch of additional\ncognitive load from worrying about cursors.\n\nOkay, I Lied - NOW on to the Actual Python\nimport pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n\n\nFrom all the database connector stuff we installed, we actually just need ONE\nfunction - and then it's pure blissful operations on data from here on out.\ncreate_engine()  will create a connection for us, and then we'll use that as an\nargument to Pandas functions that talk to the DB. You might have to reopen it\nsometimes. Whatever.\n\nFirst, let's make a string that describes our database. Note that normally you\nshouldn't actually store this directly in your code (IT will totally be mad at\nyou), but for our purposes let's include it.\n\nThe format is:\n\ndialect+driver://username:password@host:port/database\n\n\nSo, since we're using mysql, and the PyMySQL package, we'd start with:\n\nmysql+pymysql://username:password@host:port/database\n\n\nThe rest is fairly straightforward. So, if we've got the user \"analyst\"  with\nthe password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty\nsure you don't have to specify a port if it's localhost, but bear with me), and\nthe database itself is named terrible_purchases, it'd be:\n\nmysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n\n\nAnd now let's actually make our connection!\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n\n\nDone! Now Pandas can read and write from our database.\nIf you don't wanna have to see a whole bunch of extraneous stuff, I'd also add\nthe flag echo=False  (create_engine  has a ton of useful little flags, letting\nyou do stuff like automatically strip unicode that your database might not be\nable to read).\nBehold! We can write an SQL query and then get a Pandas Dataframe with those\nexact parameters. We can rapidly prototype without worrying that the reason our\nquery isn't working is that we forgot some fussy intermediary stage. We can get\nstuff wrong and experiment!\n\nReading a table is easy as:\n\ndf = pd.io.sql.sql_query(\"SELECT * FROM cursed_items\", cnx)\n\n\nDone in one line!\n\nWriting is just as easy (with a teeeensy little asterisk, that I'll probably\ntalk about in the future). Let's say we dusted off an old CSV with more cursed\nitems that aren't in our database. After we've loaded it (and maybe done some\ncleaning)\n\nnewCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n\n\nThe if_exists  flag adds a ton of flexibility. You can create a whole new DB\ntable from a Dataframe, and that's actually the default behavior. For our case,\nwe appended - but there's other times when we might want to replace, for\ninstance.\n\nFinal Addendum\nSo, this is a super useful workflow for doing interactive analytics work with\nstuff that's on a database. It's also quite useful for quick-and-dirty scripts,\nor for prototyping the logic of something that'll be expanded upon later.  And,\nah, bigger stuff too - especially if there's gonna be really complex\ntransformations that. As long as the data can fit in memory (RAM), Pandas is\npretty awesome.\n\n\nThat being said, there are  times when you'd actually want to deal with the\nlevels we bypassed here. Everything has tradeoffs. One example would be that all\nthose compiled Linear Algebra libraries that Pandas sit on top of have a massive\nmemory footprint, and AWS Lambda charges you according to memory usage. I'm\npretty sure there are also use cases where this method of handling the\nlower-level nitty-gritty of database interactions (ie, by mostly not  handling\nit) will cause problems. But at that point, we're dealing with the difference\nbetween Data Science & Data Engineering.","html":"<p>Manually opening and closing cursors? Iterating through DB output by hand? Remembering which function is the actual one that matches the Python data structure you're gonna be using?</p><p>There has to be a better way!</p><p>There totally is.</p><p>One of Pandas' most useful abilities is easy I/O. Whether it's a CSV, JSON, an Excel file, or a database - Pandas gets you what you want painlessly. In fact,I'd say that even if you don't have the spare bandwidth at the moment to rewire your brain to learn all the wonderful ways Pandas lets you manipulate data (array-based programming generally isn't the first paradigm people learn), then it's STILL worth using just for the I/O. Grab your data with one of its delicious one-liners, and once it's in a Dataframe there's have a method to convert it to practically any combination of native Python data structures that you want (lists of lists? Lists of dictionaries? Lots of other stuff?  You've got it!).</p><p>Here we'll be looking at interacting with a database with Pandas, because that's what I was finishing up when I saw Todd texted me that a lot of the people who wind up on the site are searching for Pandas stuff.</p><p>So, for our purposes, let's say you're doing Python stuff with a MySQL database. I'm also assuming you're using Linux (including via Windows Subsystem for Linux, which is what I use in order to have Ubuntu on my Windows laptop), because I've never done this with Windows.</p><p>Annoyingly, there's more than one potential connector - I generally use PyMySQL, for no particular reason. Installing this involves an extra step - telling pip or Conda to install it won't do the whole thing. Annoying, but them's the breaks.</p><pre><code class=\"language-bash\">sudo apt-get install python3-pymysql -y\n</code></pre>\n<p>Should get you there. I remember fussing with this every time I have to do it on a new machine.</p><p>You also have to install a mysql-connector, for reasons I won't pretend to understand.</p><pre><code class=\"language-bash\">conda install mysql-connector-python -y\n</code></pre>\n<p>I've never actually done this with pip, but I assume it'd be</p><pre><code class=\"language-bash\">pip install mysql-connector\n</code></pre>\n<p>If that doesn't work, try</p><pre><code class=\"language-bash\">pip install --allow-external mysql-connector-python mysql-connector-python\n</code></pre>\n<h3 id=\"we-re-almost-done-installing-things-\">WE'RE ALMOST DONE INSTALLING THINGS!</h3><p>Last, but not least, is a lovely little thing called SQLAlchemy. Remember when I said I didn't actually know anything about PyMySQL and its relative advantages and disadvantages to other MySQL connectors for Python? Part of the reason for that is that SQLAlchemy, as the name implies, allows you to MAGICALLY wrap Python SQL connectors at a higher level of abstraction and not have to deal with the particulars. So, grab a little chunk of Philosopher's Stone, and - oh, it's actually a default Anaconda package. BUT, if you're in a fresh environment (or not using Conda)...</p><p>(Always remember the little <code>-y</code>  at the end. That ensures that you can walk away while it installs and it'll be done when you get back, instead of finishing the long process of whatever's happening when you first tell it to install and then waiting to start on the next long process that takes place after you hit y.)</p><p>OR</p><p><code>pip install sqlalchemy</code>  (that IS one of the nice things about pip! Doesn't need you to confirm).</p><p>So, to review, before we start Pythoning...</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ conda install mysql-connector-python -y\n$ conda install -c anaconda sqlalchemy -y\n</code></pre>\n<p>OR</p><pre><code class=\"language-bash\">$ sudo apt-get install python3-pymysql -y\n$ pip install mysql-connector\n$ pip install sqlalchemy\n</code></pre>\n<p>Cool! Ready to learn about SQLAlchemy? Well, you won't - this is about Pandas!  We're going YET ANOTHER LEVEL OF ABSTRACTION HIGHER. SQLAlchemy is just what Pandas uses to connect to databases.</p><p>Aaaand one (possible) last step. If you're doing this with a locally-installed db, you might have to <code>sudo service mysql start</code>.</p><p>(Like, every time you do dev work on it. At least on my machine. Your mileage may vary)</p><h2 id=\"on-to-the-actual-python\">On to the Actual Python</h2><p>So, what does all this do for us? Well, first off, it wasn't THAT much - and you only have to do it once (per machine (really per environment per machine (you are  taking advantage of Conda envs or virtualenvs, right?))). But what it really lets us skip all the implementation  details and focus on the actual logic of what we're doing. That's really what both SQL and Pandas are all about- they wed short, declarative  bits of code to highly optimized libraries underneath the hood. Relational Database Management Systems sit on top of code that's been optimized for decades, turning your short queries into highly efficient compiled code. Pandas takes advantage of scientific computing libraries for doing matrix operations that have been around since before there was an internet. Drill down deep enough in Pandas and you'll actually find FORTRAN code that was originally written to simulate nuclear explosions. Our whole goal is to just focus on our data manipulation via SQL and Python's scientific computing family - and we don't want to add a bunch of additional cognitive load from worrying about cursors.</p><h2 id=\"okay-i-lied-now-on-to-the-actual-python\">Okay, I Lied - NOW on to the Actual Python</h2><pre><code class=\"language-python\">import pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\n</code></pre>\n<p>From all the database connector stuff we installed, we actually just need ONE function - and then it's pure blissful operations on data from here on out. create_engine()  will create a connection for us, and then we'll use that as an argument to Pandas functions that talk to the DB. You might have to reopen it sometimes. Whatever.</p><p>First, let's make a string that describes our database. Note that normally you shouldn't actually store this directly in your code (IT will totally be mad at you), but for our purposes let's include it.</p><p>The format is:</p><pre><code class=\"language-bash\">dialect+driver://username:password@host:port/database\n</code></pre>\n<p>So, since we're using mysql, and the PyMySQL package, we'd start with:</p><pre><code class=\"language-bash\">mysql+pymysql://username:password@host:port/database\n</code></pre>\n<p>The rest is fairly straightforward. So, if we've got the user \"analyst\"  with the password \"badsecuritykills\" with a local MySQL running on port 3306 (pretty sure you don't have to specify a port if it's localhost, but bear with me), and the database itself is named terrible_purchases, it'd be:</p><pre><code class=\"language-bash\">mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases\n</code></pre>\n<p>And now let's actually make our connection!</p><pre><code class=\"language-python\">cnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/terrible_purchases)\n</code></pre>\n<p>Done! Now Pandas can read and write from our database.<br>If you don't wanna have to see a whole bunch of extraneous stuff, I'd also add the flag echo=False  (create_engine  has a ton of useful little flags, letting you do stuff like automatically strip unicode that your database might not be able to read).<br>Behold! We can write an SQL query and then get a Pandas Dataframe with those exact parameters. We can rapidly prototype without worrying that the reason our query isn't working is that we forgot some fussy intermediary stage. We can get stuff wrong and experiment!</p><p>Reading a table is easy as:</p><pre><code class=\"language-python\">df = pd.io.sql.sql_query(&quot;SELECT * FROM cursed_items&quot;, cnx)\n</code></pre>\n<p>Done in one line!</p><p>Writing is just as easy (with a teeeensy little asterisk, that I'll probably talk about in the future). Let's say we dusted off an old CSV with more cursed items that aren't in our database. After we've loaded it (and maybe done some cleaning)</p><pre><code class=\"language-python\">newCursedItemsDF.to_sql(name='cursed_items', con=cnx, if_exists='append')\n</code></pre>\n<p>The <code>if_exists</code> flag adds a ton of flexibility. You can create a whole new DB table from a Dataframe, and that's actually the default behavior. For our case, we appended - but there's other times when we might want to replace, for instance.</p><h2 id=\"final-addendum\">Final Addendum</h2><p>So, this is a super useful workflow for doing interactive analytics work with stuff that's on a database. It's also quite useful for quick-and-dirty scripts, or for prototyping the logic of something that'll be expanded upon later.  And, ah, bigger stuff too - especially if there's gonna be really complex transformations that. As long as the data can fit in memory (RAM), Pandas is pretty awesome.</p><p><br>That being said, there are  times when you'd actually want to deal with the levels we bypassed here. Everything has tradeoffs. One example would be that all those compiled Linear Algebra libraries that Pandas sit on top of have a massive memory footprint, and AWS Lambda charges you according to memory usage. I'm pretty sure there are also use cases where this method of handling the lower-level nitty-gritty of database interactions (ie, by mostly not  handling it) will cause problems. But at that point, we're dealing with the difference between Data Science &amp; Data Engineering.</p>","url":"https://hackersandslackers.com/using-pandas-to-make-dealing-with-dbs-less-of-a-hassle/","uuid":"859fc3cb-daff-4c4b-9b10-ee206c25bb9f","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3a74ead0ac8a143588f35a"}}]}},"pageContext":{"slug":"pandas","limit":12,"skip":0,"numberOfPages":2,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/pandas/page/2/"}}