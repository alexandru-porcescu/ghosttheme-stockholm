{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c8","title":"Lazy Pandas and Dask","slug":"cutting-a-file-down-to-size-with-dask","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","excerpt":"Increase the performance of Pandas with Dask.","custom_excerpt":"Increase the performance of Pandas with Dask.","created_at_pretty":"05 August, 2018","published_at_pretty":"06 August, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-08-05T17:09:51.000-04:00","published_at":"2018-08-06T07:30:00.000-04:00","updated_at":"2019-02-19T05:19:53.000-05:00","meta_title":"Picking Low-Hanging Fruit With Dask | Hackers And Slackers","meta_description":"Dask is library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core. ","og_description":"Lazy Pandas and Dask","og_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","og_title":"Lazy Pandas and Dask","twitter_description":"Picking Low-Hanging Fruit With Dask","twitter_image":"https://hackersandslackers.com/content/images/2018/08/dask@2x.jpg","twitter_title":"Lazy Pandas and Dask","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Ah, laziness.  You love it, I love it, everyone agrees it's just better.\n\nFlesh-and-blood are famously lazy.  Pandas the package, however, uses Eager\nEvaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging\nout on the street corner and being fierce to the style choices of people walking\nby?  Well, yes, but that's not the most relevant sense in which I mean it here.\n\nEager evaluation means that once you call pd.read_csv(), Pandas immediately\njumps to read the whole CSV into memory.\n\n\"Wait!\" I hear you ask.\"Isn't that what we want?  Why would I call the function\nif I didn't want it to happen?\"\n\nEventually, yes that is what you want.  But sometimes you want some time in\nbetween when you give the command and when the computer hops to it.  That's why\nit's Lazy and not Inactive - it will get to the job at some point, it'll just\nprocrastinate a bit.\n\nFor example, last week I was tasked with searching through the output of a\ngovernment API.  It had records since the 90s, and was about 300MB.  Now, this\nisn't actually outside the realm of what Pandas can handle - it's quite\noptimized, and as long as the file can fit into memory, Pandas can mess with.\n However, it won't necessarily be fast.  Furthermore, my laptop is old and I\ndidn't feel like offloading what I was doing to a remote machine.\n\nFurthermore, I knew I actually only needed a subset of the file.  Here's where\nLaziness comes in handy.  With Eager evaluation, Pandas would have to load the\nwhole thing into memory, then filter based on my criteria.\n\nEnter Dask:  Dask is a very cool little library that seamlessly allows you to\nparallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed\nto only work on one core.  Dask, on the other hand, lets you split the work\nbetween different cores - both on a single machine, or on a distributed system.\n It doesn't implement every single thing you can do with Pandas, though, so only\nuse it when you have to.\n\n  I probably should have titled this post \"Parallel Pandas\", but hey, too late\nnow - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.\n Anyway, Laziness is  part of the process.  Because Dask is lazy by default\n(much like your humble narrator), we can define our fileout loading it, like so:\n\nimport dask.dataframe as dd\n\ndf = dd.read_csv(\"giantThing.csv\")\n\n\nPandas was taking a long time to parse the file.  What's more is that this file\nhad a few quirks - I'd figured out that it needed a special text encoding, and I\nwasn't sure if there was other weirdness going on.  Was my computer just taking\na long time to nom the file, or was I going to wait there for a few minutes to\nfind an error message?  It's a catch-22 - I needed to figure out how to cut the\nfile down to size, but in order to do that I would have needed to be able to\nload it into memory.  Dask to the rescue!\n\nThis file wasn't terribly well-documented - I had an inkling as to what column\nwould tell me which rows I wanted, but I wasn't sure.  So, first thing I did was\ncheck out the first few rows.  Remember, in order to see these first 5 rows in\nPandas, I'd have to load the whole thing into memory (which might or might not\neven work!).\n\ndf.head()\n\nWith that, I was able to do a simple spot-check and see if there were any weird\ncolumns that might get in the way of parsing.  Furthermore, I confirmed that the\nID columns I was looking at contained something vaguely like what I was looking\nfor.  Even MORE interestingly, I found that it was formatted slightly\nirregularly.  Even more use for laziness!  Let's load just that one column into\nmemory (you could do this with a loop, sure - but selecting a single column is a\nlot clumsier)\n\ndf[\"ORG_NAME\"].compute()\n\nNote the .compute()  method at the end.  That's necessary because of the Lazy\nEvaluation - just calling a column name doesn't make Dask think you necessarily\nwant the thing now.  I'm not sure why I didn't have to call it with df.head(),\nthough (that's the Hackers & Slackers Codeblogging vérité style!).\n\nSo, now that I've seen the formatting, I found out that I'm going to have to\nfilter it with a call of a str.contains()  method instead of an exact value.\n Let's poke around a teensy bit more.\n\norgDF = df[\"ORG_NAME\"]\norgFiltered = corp[corp.str.contains(\"baseName\", na=False)].compute().shape\n\n\nTurns out it was only about 800 rows!So, let's filter that and make a regular\nPandas Dataframe (and probably a new CSV for later!)\n\ndf = dd.read_csv(\"giantThing.csv\")\n\norgFiltered = df[df[\"ORG_NAME\"].str.contains(\"baseName\", na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(\"filteredThing.csv\")\n\n\nNote that I actually could have done this with base Pandas, through use of the\niterator flag.  However, I didn't realize that it's only wind up being so few\nrows.  It also would have been slower - and the speed difference makes a huge\ndifference in terms of how fluidly you can explore.\n\nFor instance, the na=False  flag was something I discovered would be needed\nbecause of a quirk in the file - again, this sort of thing becomes a lot easier\ndo diagnose when you can iterate quickly, and you know you're not going to just\ntimeout from running out of memory.\n\nFor comparison's sake, here's the code for filtering on the fly and loading into\nPandas:\n\niter_csv = pd.read_csv(\"giantThing.csv\",\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[\"ORG_NAME\"].str.contains(\"baseName\", na=False)] \n                for chunk in iter_csv])\n\n\nOn my computer, that took a little over 3 minutes.  While the Dask code took\nabout a minute.","html":"<p>Ah, laziness.  You love it, I love it, everyone agrees it's just better.</p><p>Flesh-and-blood are famously lazy.  Pandas the package, however, uses Eager Evaluation.  What's Eager Evaluation, you ask?  Is Pandas really judgey, hanging out on the street corner and being fierce to the style choices of people walking by?  Well, yes, but that's not the most relevant sense in which I mean it here.  </p><p>Eager evaluation means that once you call <code>pd.read_csv()</code>, Pandas immediately jumps to read the whole CSV into memory.</p><p><strong>\"Wait!\" </strong>I hear you ask.  <strong>\"Isn't that what we want?  Why would I call the function if I didn't want it to happen?\"</strong></p><p><em>Eventually</em>, yes that is what you want.  But sometimes you want some time in between when you give the command and when the computer hops to it.  That's why it's Lazy and not Inactive - it will get to the job at some point, it'll just procrastinate a bit.</p><p>For example, last week I was tasked with searching through the output of a government API.  It had records since the 90s, and was about 300MB.  Now, this isn't actually outside the realm of what Pandas can handle - it's quite optimized, and as long as the file can fit into memory, Pandas can mess with.  However, it won't necessarily be fast.  Furthermore, my laptop is old and I didn't feel like offloading what I was doing to a remote machine.  </p><p>Furthermore, I knew I actually only needed a subset of the file.  Here's where Laziness comes in handy.  With Eager evaluation, Pandas would have to load the whole thing into memory, then filter based on my criteria.</p><p>Enter Dask:  Dask is a very cool little library that seamlessly allows you to parallelize Pandas. Pandas by itself is pretty well-optimized, but it's designed to only work on one core.  Dask, on the other hand, lets you split the work between different cores - both on a single machine, or on a distributed system.  It doesn't implement every single thing you can do with Pandas, though, so only use it when you have to.</p><p> I probably should have titled this post \"Parallel Pandas\", but hey, too late now - plus \"Lazy Pandas\" more easily lends itself to a nice visual metaphor.  Anyway, Laziness <em>is</em> part of the process.  Because Dask is lazy by default (much like your humble narrator), we can define our fileout loading it, like so:</p><pre><code class=\"language-python\">import dask.dataframe as dd\n\ndf = dd.read_csv(&quot;giantThing.csv&quot;)\n</code></pre>\n<p>Pandas was taking a long time to parse the file.  What's more is that this file had a few quirks - I'd figured out that it needed a special text encoding, and I wasn't sure if there was other weirdness going on.  Was my computer just taking a long time to nom the file, or was I going to wait there for a few minutes to find an error message?  It's a catch-22 - I needed to figure out how to cut the file down to size, but in order to do that I would have needed to be able to load it into memory.  Dask to the rescue!</p><p>This file wasn't terribly well-documented - I had an inkling as to what column would tell me which rows I wanted, but I wasn't sure.  So, first thing I did was check out the first few rows.  Remember, in order to see these first 5 rows in Pandas, I'd have to load the whole thing into memory (which might or might not even work!).</p><p><code>df.head()</code></p><p>With that, I was able to do a simple spot-check and see if there were any weird columns that might get in the way of parsing.  Furthermore, I confirmed that the ID columns I was looking at contained something vaguely like what I was looking for.  Even MORE interestingly, I found that it was formatted slightly irregularly.  Even more use for laziness!  Let's load just that one column into memory (you could do this with a loop, sure - but selecting a single column is a lot clumsier)</p><p><code>df[\"ORG_NAME\"].compute()</code></p><p>Note the <code>.compute()</code> method at the end.  That's necessary because of the Lazy Evaluation - just calling a column name doesn't make Dask think you necessarily want the thing now.  I'm not sure why I didn't have to call it with <code>df.head()</code>, though (that's the Hackers &amp; Slackers Codeblogging vérité style!).</p><p>So, now that I've seen the formatting, I found out that I'm going to have to filter it with a call of a <code>str.contains()</code> method instead of an exact value.  Let's poke around a teensy bit more.</p><pre><code class=\"language-python\">orgDF = df[&quot;ORG_NAME&quot;]\norgFiltered = corp[corp.str.contains(&quot;baseName&quot;, na=False)].compute().shape\n</code></pre>\n<blockquote>Turns out it was only about 800 rows!</blockquote><p>So, let's filter that and make a regular Pandas Dataframe (and probably a new CSV for later!)</p><pre><code class=\"language-python\">df = dd.read_csv(&quot;giantThing.csv&quot;)\n\norgFiltered = df[df[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)].compute()\n\ndf2 = pd.DataFrame(orgFiltered)\ndf2.to_csv(&quot;filteredThing.csv&quot;)\n</code></pre>\n<p>Note that I actually could have done this with base Pandas, through use of the iterator flag.  However, I didn't realize that it's only wind up being so few rows.  It also would have been slower - and the speed difference makes a huge difference in terms of how fluidly you can explore.</p><p>For instance, the <code>na=False</code> flag was something I discovered would be needed because of a quirk in the file - again, this sort of thing becomes a lot easier do diagnose when you can iterate quickly, and you know you're not going to just timeout from running out of memory.</p><p>For comparison's sake, here's the code for filtering on the fly and loading into Pandas:</p><pre><code class=\"language-python\">iter_csv = pd.read_csv(&quot;giantThing.csv&quot;,\n                iterator=True, \n                       chunksize=1000)\n\ndf = pd.concat([chunk[chunk[&quot;ORG_NAME&quot;].str.contains(&quot;baseName&quot;, na=False)] \n                for chunk in iter_csv])\n</code></pre>\n<p>On my computer, that took a little over 3 minutes.  While the Dask code took about a minute.</p>","url":"https://hackersandslackers.com/cutting-a-file-down-to-size-with-dask/","uuid":"d4270325-d03f-46fd-a0ae-1b3cbfe1b527","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b67679f17f6083e60a44c5d"}},"pageContext":{"slug":"cutting-a-file-down-to-size-with-dask"}}