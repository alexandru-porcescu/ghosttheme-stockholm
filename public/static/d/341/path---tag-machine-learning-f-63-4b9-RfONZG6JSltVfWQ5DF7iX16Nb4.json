{"data":{"ghostTag":{"slug":"machine-learning","name":"Machine Learning","visibility":"public","feature_image":null,"description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","meta_description":"The latest developments in machine learning tools and technology available to data scientists."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673709","title":"Using Random Forests for Feature Selection with Categorical Features","slug":"random-forests-for-feature-selection","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","custom_excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","created_at_pretty":"24 September, 2018","published_at_pretty":"24 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-23T20:23:32.000-04:00","published_at":"2018-09-24T07:30:00.000-04:00","updated_at":"2019-02-19T03:48:04.000-05:00","meta_title":"Using Random Forests for Feature Selection | Hackers and Slackers","meta_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","og_title":"Using Random Forests for Feature Selection","twitter_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","twitter_title":"Using Random Forests for Feature Selection","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Notebook here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb]\n.  Helper functions here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py]\n.\n\nOne of the best features of Random Forests is that it has built-in Feature\nSelection.  Explicability is one of the things we often lose when we go from\ntraditional statistics to Machine Learning, but Random Forests lets us actually\nget some insight into our dataset instead of just having to treat our model as a\nblack box.\n\nOne problem, though - it doesn't work that well for categorical features.  Since\nyou'll generally have to One-Hot Encode a categorical feature (for instance,\nturn something with 7 categories into 7 variables that are a \"True/False\"),\nyou'll wind up with a bunch of small features.  This gets tough to read,\nespecially if you're dealing with a lot of categories.  It also makes that\nfeature look less important than it is - rather than appearing near the top,\nyou'll maybe have 17 weak-seeming features near the bottom - which gets worse if\nyou're filtering it so that you only see features above a certain threshold.\n\nSoo, here's some helper functions for adding up their importance and displaying\nthem as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my\nmy own One-Hot function, rather than using Scikit's builtin one.\n\nFirst, let's grab a dataset.  I'm using this\n[https://www.kaggle.com/c/avazu-ctr-prediction]  Kaggle dataset because it has a\ngood number of categorical predictors.  I'm also only using the first 500 rows\nbecause the whole dataset is like ~ 1 GB.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)\n\nLet's just use the Categorical variables as our predictors because that's what\nwe're focusing on, but in actual usage you don't have to make them the same.\n\npredVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)\n\nLet's use log_loss  as our metric, because I saw this\n[https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512] \n blog post that used it for this dataset.\n\nfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 \n\nVariable\n Importance\n 0\n device_model\n 0.843122\n 1\n site_category\n 0.083392\n 2\n app_category\n 0.037216\n 3\n device_type\n 0.025057\n 4\n device_conn_type\n 0.011213","html":"<p><em>Notebook <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb\">here</a>.  Helper functions <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py\">here</a>.</em></p><p>One of the best features of Random Forests is that it has built-in Feature Selection.  Explicability is one of the things we often lose when we go from traditional statistics to Machine Learning, but Random Forests lets us actually get some insight into our dataset instead of just having to treat our model as a black box.</p><p>One problem, though - it doesn't work that well for categorical features.  Since you'll generally have to One-Hot Encode a categorical feature (for instance, turn something with 7 categories into 7 variables that are a \"True/False\"), you'll wind up with a bunch of small features.  This gets tough to read, especially if you're dealing with a lot of categories.  It also makes that feature look less important than it is - rather than appearing near the top, you'll maybe have 17 weak-seeming features near the bottom - which gets worse if you're filtering it so that you only see features above a certain threshold.</p><p>Soo, here's some helper functions for adding up their importance and displaying them as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my my own One-Hot function, rather than using Scikit's builtin one.</p><p>First, let's grab a dataset.  I'm using <a href=\"https://www.kaggle.com/c/avazu-ctr-prediction\">this</a> Kaggle dataset because it has a good number of categorical predictors.  I'm also only using the first 500 rows because the whole dataset is like ~ 1 GB.</p><pre><code>import pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)</code></pre><p>Let's just use the Categorical variables as our predictors because that's what we're focusing on, but in actual usage you don't have to make them the same.</p><pre><code>predVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)</code></pre><p>Let's use <code>log_loss</code> as our metric, because I saw <a href=\"https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\">this</a> blog post that used it for this dataset.</p><pre><code>from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 </code></pre><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>device_model</td>\n      <td>0.843122</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>site_category</td>\n      <td>0.083392</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>app_category</td>\n      <td>0.037216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>device_type</td>\n      <td>0.025057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>device_conn_type</td>\n      <td>0.011213</td>\n    </tr>\n  </tbody>\n</table>","url":"https://hackersandslackers.com/random-forests-for-feature-selection/","uuid":"26ebccb3-ab41-44cf-8d57-bf995100b088","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ba82e84a1cf0b13cf2e9886"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673700","title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","slug":"random-forests-hyperparameters-min_samples_leaf","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","custom_excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","created_at_pretty":"17 September, 2018","published_at_pretty":"17 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-16T21:33:48.000-04:00","published_at":"2018-09-17T07:30:00.000-04:00","updated_at":"2019-02-19T03:44:33.000-05:00","meta_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf | Hackers and Slackers","meta_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","og_description":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","og_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","twitter_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","twitter_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Part 1 (n_estimators) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\nPart 2 (max_depth) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/]\nNotebook here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb]\n\n\n--------------------------------------------------------------------------------\n\nAnother parameter, another set of quirks!\n\nmin_samples_leaf  is sort of similar to max_depth.  It helps us avoid\noverfitting.  It's also non-obvious what you should use as your upper and lower\nlimits to search between.  Let's do what we did last week - build a forest with\nno parameters, see what it does, and use the upper and lower limits!\n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\n\nLet's use the handy function from here\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html]  to\ncrawl the number of samples in a tree's leaf nodes: \n\ndef leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n\n\nLast week we made a function to grab them for a whole forest - since this is the\nsecond time we're doing this, and we may do it again, let's make a modular\nlittle function that takes a crawler function as an argument!\n\ndef getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {\"min\": params.min(),\n           \"max\": params.max()}\n\n\nLet's see it in action!\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#> {'max': 199, 'min': 1}\n\n\nAlmost ready to start optimizing!  Since part of what we get out of optimizing \nmin_samples_leaf  is regularization (and because it's just good practice!),\nlet's make a metric with some cross-validation.  Luckily, Scikit  has a builtin \ncross_val_score  function.  We'll just need to do a teensy bit of tweaking to\nmake it use the area under a precision_recall_curve.\n\nfrom sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n\n\nAwesome, now we have a metric that can be fed into our binary search.\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.402102\n 199\n 0.506455\n 1.416349\n 100\n 0.506455\n 1.401090\n 51\n 0.506455\n 1.394548\n 26\n 0.975894\n 1.396503\n 14\n 0.982954\n 1.398522\n 7\n 0.979888\n 1.398929\n 10\n 0.984789\n 1.404815\n 12\n 0.986302\n 1.391171\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.473848\n 0.082938\n 199\n 0.002084\n 1.039718\n 0.000000\n 100\n 0.002084\n 0.433676\n 0.000111\n 51\n 0.002084\n 0.173824\n 0.000396\n 26\n 0.980393\n 0.251484\n 0.154448\n 14\n 0.995105\n 0.331692\n 0.118839\n 7\n 0.988716\n 0.347858\n 0.112585\n 10\n 0.998930\n 0.581632\n 0.067998\n 12\n 1.002084\n 0.039718\n 1.000000\n Looks like the action's between 1 and 51.  More than that, and the score goes\nwhile simultaneously increasing the runtime - the opposite of what we want!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.389387\n 51\n 0.506455\n 1.403807\n 26\n 0.975894\n 1.404517\n 14\n 0.982954\n 1.385420\n 7\n 0.979888\n 1.398840\n 10\n 0.984789\n 1.393863\n 12\n 0.986302\n 1.411774\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n Big drop-off after 26, it seems!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.407957\n 26\n 0.975894\n 1.398042\n 14\n 0.982954\n 1.396782\n 7\n 0.979888\n 1.396096\n 10\n 0.984789\n 1.402322\n 12\n 0.986302\n 1.401080\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.650270\n 1.084306\n 0.040144\n 26\n 0.096077\n 0.248406\n 0.000000\n 14\n 0.774346\n 0.142157\n 0.954016\n 7\n 0.479788\n 0.084306\n 1.000000\n 10\n 0.950677\n 0.609184\n 0.221294\n 12\n 1.096077\n 0.504512\n 0.336668\n One more with 14 as our upper limit!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.401341\n 14\n 0.982954\n 1.400361\n 7\n 0.979888\n 1.402408\n 4\n 0.981121\n 1.401396\n 3\n 0.983580\n 1.401332\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n 3 it is!I suppose when it gets this small we could use a regular Grid Search,\nbut...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs \nGridSearchCV  and/or RandomizedSearchCV.  Who knows what the future holds?","html":"<p>Part 1 (n_estimators) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a><br>Part 2 (max_depth) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/\">here</a><br>Notebook <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb\">here</a></p><hr><p>Another parameter, another set of quirks!</p><p><code>min_samples_leaf</code> is sort of similar to <code>max_depth</code>.  It helps us avoid overfitting.  It's also non-obvious what you should use as your upper and lower limits to search between.  Let's do what we did last week - build a forest with no parameters, see what it does, and use the upper and lower limits!</p><pre><code class=\"language-python\">import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n</code></pre>\n<p>Let's use the handy function from <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">here</a> to crawl the number of samples in a tree's leaf nodes: </p><pre><code class=\"language-python\">def leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n</code></pre>\n<p>Last week we made a function to grab them for a whole forest - since this is the second time we're doing this, and we may do it again, let's make a modular little function that takes a crawler function as an argument!</p><pre><code class=\"language-python\">def getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {&quot;min&quot;: params.min(),\n           &quot;max&quot;: params.max()}\n</code></pre>\n<p>Let's see it in action!</p><pre><code class=\"language-python\">data = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#&gt; {'max': 199, 'min': 1}\n</code></pre>\n<p>Almost ready to start optimizing!  Since part of what we get out of optimizing <code>min_samples_leaf</code> is regularization (and because it's just good practice!), let's make a metric with some cross-validation.  Luckily, <strong>Scikit</strong> has a builtin <code>cross_val_score</code> function.  We'll just need to do a teensy bit of tweaking to make it use the area under a <code>precision_recall_curve</code>.</p><pre><code class=\"language-python\">from sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n</code></pre>\n<p>Awesome, now we have a metric that can be fed into our binary search.</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.402102</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.506455</td>\n      <td>1.416349</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.506455</td>\n      <td>1.401090</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.394548</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.396503</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.398522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398929</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.404815</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.391171</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.473848</td>\n      <td>0.082938</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.002084</td>\n      <td>1.039718</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.002084</td>\n      <td>0.433676</td>\n      <td>0.000111</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.173824</td>\n      <td>0.000396</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.251484</td>\n      <td>0.154448</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.331692</td>\n      <td>0.118839</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.347858</td>\n      <td>0.112585</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.581632</td>\n      <td>0.067998</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>0.039718</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf1.png\" class=\"kg-image\"></figure><p>Looks like the action's between 1 and 51.  More than that, and the score goes while simultaneously increasing the runtime - the opposite of what we want!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.389387</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.403807</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.404517</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.385420</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398840</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.393863</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.411774</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf2.png\" class=\"kg-image\"></figure><p>Big drop-off after 26, it seems!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.407957</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.398042</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.396782</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.396096</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.402322</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.401080</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.650270</td>\n      <td>1.084306</td>\n      <td>0.040144</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.096077</td>\n      <td>0.248406</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.774346</td>\n      <td>0.142157</td>\n      <td>0.954016</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.479788</td>\n      <td>0.084306</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.950677</td>\n      <td>0.609184</td>\n      <td>0.221294</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.096077</td>\n      <td>0.504512</td>\n      <td>0.336668</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf3.png\" class=\"kg-image\"></figure><p>One more with 14 as our upper limit!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.401341</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.400361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.402408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.981121</td>\n      <td>1.401396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.983580</td>\n      <td>1.401332</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf4.png\" class=\"kg-image\"><figcaption>3 it is!</figcaption></figure><p>I suppose when it gets this small we could use a regular Grid Search, but...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs <code>GridSearchCV</code> and/or <code>RandomizedSearchCV</code>.  Who knows what the future holds?</p>","url":"https://hackersandslackers.com/random-forests-hyperparameters-min_samples_leaf/","uuid":"766a3eb8-aacc-47c6-91a9-744b84613626","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b9f047cab64c97c60f7be90"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736fd","title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","slug":"tuning-random-forests-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","custom_excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","created_at_pretty":"09 September, 2018","published_at_pretty":"10 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-09T19:14:32.000-04:00","published_at":"2018-09-10T07:30:00.000-04:00","updated_at":"2019-02-19T03:46:39.000-05:00","meta_title":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","og_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","twitter_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","twitter_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Continued from here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\n\nNotebook for this post is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb]\n\nBinary search code itself is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nmax_depth  is an interesting parameter.  While n_estimators  has a tradeoff\nbetween speed & score, max_depth  has the possibility of improving both.  By\nlimiting the depth of your trees, you can reduce overfitting.\n\nUnfortunately, deciding on upper & lower bounds is less than straightforward.\n It'll depend on your dataset.  Luckily, I found a post on StackOverflow that\nhad a link to a blog post that had a promising methodology\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html].\n\nFirst, we build a tree with default arguments and fit it to our data. \n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\nNow, let's see how deep the trees get when we don't impose any sort of max_depth\n. We'll use the code from that wonderful blog post to crawl our Random Forest,\nand get the height of every tree.\n\n#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#> 2\nnp.hstack(allDepths).max()\n#> 9\n\nWe'll be searching between 2 and 9!\n\nLet's bring back our old make a helper function to easily return scores.\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\nmax_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)\n\nmax_depth\n score\n time\n 2\n 0.987707\n 0.145360\n 9\n 0.987029\n 0.147563\n 6\n 0.986247\n 0.140514\n 4\n 0.968316\n 0.140164\n \nmax_depth\n score\n time\n scoreTimeRatio\n 2\n 1.051571\n 0.837377\n 0.175986\n 9\n 1.016649\n 1.135158\n 0.103478\n 6\n 0.976311\n 0.182516\n 1.000000\n 4\n 0.051571\n 0.135158\n 0.000000\n So, for our purposes, 9 will function as our baseline since that was the\nbiggest depth that it built with default arguments.\n\nLooks like a max_depth  of 2 has a slightly higher score than 9, and is slightly\nfaster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that\nis.","html":"<p>Continued from <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a></p><p>Notebook for this post is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb\">here</a></p><p>Binary search code itself is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p><code>max_depth</code> is an interesting parameter.  While <code>n_estimators</code> has a tradeoff between speed &amp; score, <code>max_depth</code> has the possibility of improving both.  By limiting the depth of your trees, you can reduce overfitting.</p><p>Unfortunately, deciding on upper &amp; lower bounds is less than straightforward.  It'll depend on your dataset.  Luckily, I found a post on StackOverflow that had a link to a blog post that had a promising <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">methodology</a>.  </p><p>First, we build a tree with default arguments and fit it to our data. </p><pre><code>import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)</code></pre><p>Now, let's see how deep the trees get when we don't impose any sort of <code>max_depth</code>. We'll use the code from that wonderful blog post to crawl our Random Forest, and get the height of every tree.</p><pre><code>#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#&gt; 2\nnp.hstack(allDepths).max()\n#&gt; 9</code></pre><p>We'll be searching between 2 and 9!  </p><p>Let's bring back our old make a helper function to easily return scores.</p><pre><code>def getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)</code></pre><pre><code>max_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/09/max_depth.png\" class=\"kg-image\"></figure><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>0.987707</td>\n      <td>0.145360</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.987029</td>\n      <td>0.147563</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.986247</td>\n      <td>0.140514</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.968316</td>\n      <td>0.140164</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>1.051571</td>\n      <td>0.837377</td>\n      <td>0.175986</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.016649</td>\n      <td>1.135158</td>\n      <td>0.103478</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.976311</td>\n      <td>0.182516</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.051571</td>\n      <td>0.135158</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>So, for our purposes, 9 will function as our baseline since that was the biggest depth that it built with default arguments.  </p><p>Looks like a <code>max_depth</code> of 2 has a slightly higher score than 9, and is slightly faster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that is.</p>","url":"https://hackersandslackers.com/tuning-random-forests-hyperparameters-with-binary-search/","uuid":"3c92aed0-61ed-4c1a-b7d5-cc47c709764b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b95a9581fc1fc7d92b5c51f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ec","title":"Tuning Machine Learning Hyperparameters with Binary Search","slug":"tuning-machine-learning-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","custom_excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","created_at_pretty":"30 August, 2018","published_at_pretty":"03 September, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-08-29T21:35:41.000-04:00","published_at":"2018-09-03T07:30:00.000-04:00","updated_at":"2019-02-13T22:50:35.000-05:00","meta_title":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"RandomizedSearchCV goes noticeably faster than a full GridSearchCV but it still takes a while - which can be rough.","og_description":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","og_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","og_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","twitter_description":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python","twitter_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","twitter_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Ah, hyperparameter tuning.  Time & compute-intensive.  Frequently containing\nweird non-linearities in how changing a parameter changes the score and/or the\ntime it takes to train the model.\n\nRandomizedSearchCV  goes noticeably faster than a full GridSearchCV  but it\nstill takes a while - which can be rough, because in my experience you do still\nneed to be iterative with it and experiment with different distributions.  Plus,\nthen you've got hyper-hyperparameters to tune - how many iterations SHOULD you\nrun it for, anyway?\n\nI've been experimenting with using the trusty old Binary Search to tune\nhyperparameters.  I'm finding it has two advantages.\n\n 1. It's blazing fast\n 2. The performance is competitive with a Randomized Search\n 3. It gives you a rough sketch of \"the lay of the land\".  An initial binary\n    search can then provide parameters for future searches, including with Grid\n    or Randomized Searches.\n\nCode is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nNotebook summary is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb]\n\nLet's see it in action!\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nWe'll be using a Random Forest classifier, because, as with all my code posts,\nit's what I've been using recently.\n\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n\nWe'll be using scikit-learn's breast cancer dataset, because I remembered that\nthese packages I'm posting about have built-in demo datasets that I should be\nusing for posts.\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"oob_score\": True}\n\n\nLet's set our random_state  for better reproducibility.\nWe'll set n_jobs=-1  because obviously we want to use all our cores, we are not\npatient people.\n\n\nWe'll have class_weight=\"balanced\"  because that'll compensate for the fact that\nthe breast cancer dataset (like most medical datasets) has unbalanced classes.\nWe'll use oob_score  because we like being lazy, part of the appeal of Random\nForests is the opportunity to be extra lazy (no need to normalize features!),\nand oob  lets us be even lazier  by giving some built-in cross-validation.\n\nNow let's define a function that'll take all this, and spit out a score.  I\nwrote the binary search function to take a function like this as an argument -\nscikit-learn is usually pretty consistent when it comes to the interface it\nprovides you, but sometimes different algorithms need to work a little\ndifferently.  For instance, since we'll be using Area Under \nprecision_recall_curve  as our metric (a good choice for classifiers with\nunbalanced classes!), it takes a teensy bit of extra fiddling to get it to play\nnicely with our oob_decision_function_.\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\n\nWe'll try to optimize the n_estimators  parameter first.  For two reasons:\n\n 1. Finding a good mix between speed and accuracy here will make it easier to\n    tune subsequent parameters.\n 2. It's the most straightforward to decide upper and lower bounds for.  Other\n    ones (like, say, max_depth) require a little work to figure the potential\n    range to search in.\n\nOkay!  So, let's put our lower limit as 32 and our upper limit as 128, because I\nread in a StackOverflow post that there's a paper that says to search within\nthat range.\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nPlotting score, time, and the ratio between them - we're not just optimizing for\nthe best score right now, we're looking for tipping points that give us good\ntradeoffs.  Scores and times are normalized for a more-meaningful ratio between\nthem.\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 32\n 1.073532\n 0.002459\n 1.000000\n 1\n 128\n 1.867858\n 1.002459\n 0.000000\n 2\n 80\n 2.052255\n 0.440060\n 0.006443\n 3\n 56\n 1.605447\n 0.075185\n 0.044843\n 4\n 68\n 1.910411\n 0.107187\n 0.036721\n 5\n 74\n 2.066440\n 0.377136\n 0.008320\n 6\n 77\n 2.066440\n 0.388378\n 0.007955\n 7\n 75\n 2.073532\n 0.457481\n 0.006141\n n_estimators\n score\n time\n 0\n 32\n 0.988663\n 0.180521\n 1\n 128\n 0.989403\n 0.587113\n 2\n 80\n 0.989575\n 0.358446\n 3\n 56\n 0.989159\n 0.210091\n 4\n 68\n 0.989443\n 0.223102\n 5\n 74\n 0.989588\n 0.332861\n 6\n 77\n 0.989588\n 0.337432\n 7\n 75\n 0.989595\n 0.365529\n Hrm, looks like the score starts getting somewhere interesting around 68, and\ntime starts shooting up at about 80.  Let's do another with those as our bounds!\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 68\n 6.390333\n 0.333407\n 0.135692\n 1\n 80\n 7.223667\n 1.064343\n 0.000000\n 2\n 74\n 7.307000\n 0.404471\n 0.123622\n 3\n 71\n 6.307000\n 0.064343\n 1.000000\n 4\n 72\n 6.390333\n 0.175190\n 0.325419\n n_estimators\n score\n time\n 0\n 68\n 0.989443\n 0.344220\n 1\n 80\n 0.989575\n 0.355580\n 2\n 74\n 0.989588\n 0.345324\n 3\n 71\n 0.989430\n 0.340038\n 4\n 72\n 0.989443\n 0.341761\n 71 looks like our winner!  Or close enough for our purposes while we then go\noptimize other things.  And we only had to train our model 13 times - as opposed\nto the 96 we would have with a brute-force grid search.\n\nHopefully this will become a series on using this to tune other RF\nhyperparameters - other ones have some interesting quirks that I'd like to\nexpound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!","html":"<p>Ah, hyperparameter tuning.  Time &amp; compute-intensive.  Frequently containing weird non-linearities in how changing a parameter changes the score and/or the time it takes to train the model.</p><p><code>RandomizedSearchCV</code> goes noticeably faster than a full <code>GridSearchCV</code> but it still takes a while - which can be rough, because in my experience you do still need to be iterative with it and experiment with different distributions.  Plus, then you've got hyper-hyperparameters to tune - how many iterations SHOULD you run it for, anyway?</p><p>I've been experimenting with using the trusty old Binary Search to tune hyperparameters.  I'm finding it has two advantages.</p><ol><li>It's blazing fast</li><li>The performance is competitive with a Randomized Search</li><li>It gives you a rough sketch of \"the lay of the land\".  An initial binary search can then provide parameters for future searches, including with Grid or Randomized Searches.</li></ol><p>Code is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p>Notebook summary is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb\">here</a></p><p>Let's see it in action!</p><pre><code class=\"language-python\">from sklearn.ensemble import RandomForestClassifier\n</code></pre>\n<p>We'll be using a Random Forest classifier, because, as with all my code posts, it's what I've been using recently.</p><pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n</code></pre>\n<p>We'll be using scikit-learn's breast cancer dataset, because I remembered that these packages I'm posting about have built-in demo datasets that I should be using for posts.</p><pre><code class=\"language-python\">rfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;oob_score&quot;: True}\n</code></pre>\n<p>Let's set our <code>random_state</code> for better reproducibility.<br>We'll set <code>n_jobs=-1</code> because obviously we want to use all our cores, we are not patient people.</p><p><br>We'll have <code>class_weight=\"balanced\"</code> because that'll compensate for the fact that the breast cancer dataset (like most medical datasets) has unbalanced classes.<br>We'll use <code>oob_score</code> because we like being lazy, part of the appeal of Random Forests is the opportunity to be extra lazy (no need to normalize features!), and <code>oob</code> lets us be <em>even lazier</em> by giving some built-in cross-validation.</p><p>Now let's define a function that'll take all this, and spit out a score.  I wrote the binary search function to take a function like this as an argument - scikit-learn is usually pretty consistent when it comes to the interface it provides you, but sometimes different algorithms need to work a little differently.  For instance, since we'll be using Area Under <code>precision_recall_curve</code> as our metric (a good choice for classifiers with unbalanced classes!), it takes a teensy bit of extra fiddling to get it to play nicely with our <code>oob_decision_function_</code>.</p><pre><code class=\"language-python\">from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n</code></pre>\n<p>We'll try to optimize the <code>n_estimators</code> parameter first.  For two reasons:</p><ol><li>Finding a good mix between speed and accuracy here will make it easier to tune subsequent parameters.</li><li>It's the most straightforward to decide upper and lower bounds for.  Other ones (like, say, <code>max_depth</code>) require a little work to figure the potential range to search in.</li></ol><p>Okay!  So, let's put our lower limit as 32 and our upper limit as 128, because I read in a StackOverflow post that there's a paper that says to search within that range.</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<p>Plotting score, time, and the ratio between them - we're not just optimizing for the best score right now, we're looking for tipping points that give us good tradeoffs.  Scores and times are normalized for a more-meaningful ratio between them.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--2-.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>1.073532</td>\n      <td>0.002459</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>1.867858</td>\n      <td>1.002459</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>2.052255</td>\n      <td>0.440060</td>\n      <td>0.006443</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1.605447</td>\n      <td>0.075185</td>\n      <td>0.044843</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>1.910411</td>\n      <td>0.107187</td>\n      <td>0.036721</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>2.066440</td>\n      <td>0.377136</td>\n      <td>0.008320</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>2.066440</td>\n      <td>0.388378</td>\n      <td>0.007955</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>2.073532</td>\n      <td>0.457481</td>\n      <td>0.006141</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>0.988663</td>\n      <td>0.180521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>0.989403</td>\n      <td>0.587113</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.358446</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>0.989159</td>\n      <td>0.210091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.223102</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.332861</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>0.989588</td>\n      <td>0.337432</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>0.989595</td>\n      <td>0.365529</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Hrm, looks like the score starts getting somewhere interesting around 68, and time starts shooting up at about 80.  Let's do another with those as our bounds!</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/max_depth.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>6.390333</td>\n      <td>0.333407</td>\n      <td>0.135692</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>7.223667</td>\n      <td>1.064343</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>7.307000</td>\n      <td>0.404471</td>\n      <td>0.123622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>6.307000</td>\n      <td>0.064343</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>6.390333</td>\n      <td>0.175190</td>\n      <td>0.325419</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.344220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.355580</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.345324</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>0.989430</td>\n      <td>0.340038</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>0.989443</td>\n      <td>0.341761</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>71 looks like our winner!  Or close enough for our purposes while we then go optimize other things.  And we only had to train our model 13 times - as opposed to the 96 we would have with a brute-force grid search.</p><p>Hopefully this will become a series on using this to tune other RF hyperparameters - other ones have some interesting quirks that I'd like to expound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!</p>","url":"https://hackersandslackers.com/tuning-machine-learning-hyperparameters-with-binary-search/","uuid":"ca7241c3-52cd-4910-86dc-0bb5474d07af","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b8749ed4b98380b152292ea"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a8","title":"Lynx Roundup, July 20th","slug":"lynx-roundup-july-20th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","excerpt":"Kafka!  New brain cell!  Computers made of liquid!","custom_excerpt":"Kafka!  New brain cell!  Computers made of liquid!","created_at_pretty":"13 July, 2018","published_at_pretty":"20 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:34:04.000-04:00","published_at":"2018-07-20T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 20th | Hackers and Slackers","meta_description":"Kafka!  New brain cell!  Computers made of liquid!","og_description":"Kafka!  New brain cell!  Computers made of liquid!","og_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","og_title":"Lynx Roundup, July 20th","twitter_description":"Kafka!  New brain cell!  Computers made of liquid!","twitter_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","twitter_title":"Lynx Roundup, July 20th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Science News","slug":"science-news","description":"Breakthroughs in general science.","feature_image":null,"meta_description":"Breakthroughs in general science.","meta_title":"Science News | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"http://www.admintome.com/blog/kafka-python-and-google-analytics/\n\n\n\nhttps://neurosciencenews.com/learning-neurons-9516/\n\n\n\nhttp://cs231n.github.io/understanding-cnn/\n\n\n\nhttps://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/\n\n\n\nhttps://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing","html":"<p></p><p><a href=\"http://www.admintome.com/blog/kafka-python-and-google-analytics/\">http://www.admintome.com/blog/kafka-python-and-google-analytics/</a></p><p></p><p><a href=\"https://neurosciencenews.com/learning-neurons-9516/\">https://neurosciencenews.com/learning-neurons-9516/</a></p><p></p><p><a href=\"http://cs231n.github.io/understanding-cnn/\">http://cs231n.github.io/understanding-cnn/</a></p><p></p><p><a href=\"https://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/\">https://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/</a></p><p></p><p><a href=\"https://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing\">https://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-20th/","uuid":"556f0ff4-3929-47ac-87a7-fe061c056c38","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b4863fcc6a9e951f8a6cc67"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a7","title":"Lynx Roundup, July 19th","slug":"lynx-roundup-july-19th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","custom_excerpt":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","created_at_pretty":"13 July, 2018","published_at_pretty":"19 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:32:02.000-04:00","published_at":"2018-07-19T07:00:00.000-04:00","updated_at":"2018-07-24T22:43:44.000-04:00","meta_title":"Lynx Roundup, July 19th | Hackers and Slackers","meta_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","og_title":"Lynx Roundup, July 19th","twitter_description":"Finding the chorus in a song!  A drone that can find you!  An algo that can find out why.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx10@2x.jpg","twitter_title":"Lynx Roundup, July 19th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\n\n\n\nhttps://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\n\n\n\nhttps://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\n\n\n\nhttps://github.com/Microsoft/dowhy\n\n\n\nhttps://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/","html":"<p></p><p><a href=\"https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428\">https://towardsdatascience.com/embedding-machine-learning-models-to-web-apps-part-1-6ab7b55ee428</a></p><p></p><p><a href=\"https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e\">https://medium.com/nanonets/how-i-built-a-self-flying-drone-to-track-people-in-under-50-lines-of-code-7485de7f828e</a></p><p></p><p><a href=\"https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113\">https://medium.com/@vivjay30/finding-choruses-in-songs-with-python-ee96054b0113</a></p><p></p><p><a href=\"https://github.com/Microsoft/dowhy\">https://github.com/Microsoft/dowhy</a></p><p></p><p><a href=\"https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/\">https://thenextweb.com/artificial-intelligence/2018/07/05/scientists-created-an-artificial-neural-network-out-of-dna/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-19th/","uuid":"a9739b8c-a429-4948-8b54-9ba44dea7cee","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486382c6a9e951f8a6cc63"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a4","title":"Lynx Roundup, July 16th","slug":"lynx-roundup-july-16th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","custom_excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","created_at_pretty":"13 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"28 July, 2018","created_at":"2018-07-13T04:18:17.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-07-28T16:26:42.000-04:00","meta_title":"Lynx Roundup, July 16th | Hackers and Slackers","meta_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","og_title":"Lynx Roundup, July 16th","twitter_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","twitter_title":"Lynx Roundup, July 16th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\n\n\n\nhttp://mattturck.com/bigdata2018\n[http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&utm_medium=email&utm_source=Data_Elixir_189]\n\n\n\nOne of the best-written tutorials I've ever seen:\nhttps://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\n\n\n\nhttps://robertheaton.com/2018/06/25/how-to-read/\n\n\n\nhttp://explained.ai/matrix-calculus/index.html","html":"<p><a href=\"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\">https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is</a></p><p></p><p><a href=\"http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&amp;utm_medium=email&amp;utm_source=Data_Elixir_189\">http://mattturck.com/bigdata2018</a></p><p></p><p>One of the best-written tutorials I've ever seen:<br><a href=\"https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\">https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</a></p><p></p><p><a href=\"https://robertheaton.com/2018/06/25/how-to-read/\">https://robertheaton.com/2018/06/25/how-to-read/</a></p><p></p><p><a href=\"http://explained.ai/matrix-calculus/index.html\">http://explained.ai/matrix-calculus/index.html</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-16th/","uuid":"f886dfa1-1d10-423e-a926-2f5c8bbe7085","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486049c6a9e951f8a6cc54"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673699","title":"Lynx Roundup, July 13th","slug":"lynx-roundup-july-13th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","excerpt":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff.","custom_excerpt":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff.","created_at_pretty":"02 July, 2018","published_at_pretty":"13 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:20:31.000-04:00","published_at":"2018-07-13T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 13th | Hackers and Slackers","meta_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","og_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","og_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","og_title":"Lynx Roundup, July 13th","twitter_description":"DevOps for Data, Interview with Keras Creator, Esoteric Deep Learning stuff","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx26@2x.jpg","twitter_title":"Lynx Roundup, July 13th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Way of thinking of regexes in a more modular, chunked way.  In Clojure - someone\nshould make a Python version with Toolz.  Maybe me.\nhttps://github.com/fhur/regie/blob/master/README.md\n\n\n\nhttps://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/\n\n\n\nhttps://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de\n\n\n\nhttps://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/\n\n\n\nhttps://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato","html":"<p></p><p>Way of thinking of regexes in a more modular, chunked way.  In Clojure - someone should make a Python version with Toolz.  Maybe me.  <a href=\"https://github.com/fhur/regie/blob/master/README.md\">https://github.com/fhur/regie/blob/master/README.md</a></p><p></p><p><a href=\"https://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/\">https://www.reddit.com/r/explainlikeIAmA/comments/8v0l6w/explain_javascript_like_i_am_a_stubborn_c3p0/</a></p><p></p><p><a href=\"https://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de\">https://towardsdatascience.com/devops-for-data-scientists-taming-the-unicorn-6410843990de</a></p><p></p><p><a href=\"https://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/\">https://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/</a></p><p></p><p><a href=\"https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato\">https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study/answer/Jonathan-Uesato</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-13th/","uuid":"4d664425-8ee2-4308-afc2-3dc55a1856b9","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c42fd0ac8a143588f355"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673698","title":"Lynx Roundup, July 12th","slug":"lynx-roundup-july-12th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/12/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/12/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"12 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:11:17.000-04:00","published_at":"2018-07-12T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 12th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","og_title":"Lynx Roundup, July 12th","twitter_description":"Daily roundup of Data Science news around the industry, 7/12/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx22@2x.jpg","twitter_title":"Lynx Roundup, July 12th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/\n\n\n\nhttps://www.interviewcake.com/data-structures-reference\n\n\n\nhttps://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6\n\n\n\nhttps://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/\n\n\n\nhttps://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values","html":"<p></p><p><a href=\"https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/\">https://www.microsoft.com/en-us/research/blog/probabilistic-predicates-to-accelerate-inference-queries/</a></p><p></p><p><a href=\"https://www.interviewcake.com/data-structures-reference\">https://www.interviewcake.com/data-structures-reference</a></p><p></p><p><a href=\"https://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6\">https://medium.com/@plotlygraphs/introducing-plotly-py-3-0-0-7bb1333f69c6</a></p><p></p><p><a href=\"https://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/\">https://qz.com/1316050/tech-companies-just-woke-up-to-a-big-problem-with-their-ai/</a></p><p></p><p><a href=\"https://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values\">https://www.quora.com/Why-do-many-data-scientists-complain-that-Kaggles-data-is-clean-even-though-its-usually-in-5-6-separate-files-with-tons-of-features-missing-a-lot-of-values</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-12th/","uuid":"347c148f-b80f-4e98-aec1-42557c7ea37b","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c205d0ac8a143588f34f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673697","title":"Lynx Roundup, July 11th","slug":"lynx-roundup-july-11th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/11/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/11/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"11 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:09:25.000-04:00","published_at":"2018-07-11T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 11th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","og_title":"Lynx Roundup, July 11th","twitter_description":"Daily roundup of Data Science news around the industry, 7/11/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx7@2x.jpg","twitter_title":"Lynx Roundup, July 11th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/\n\n\n\nhttps://www.algorithm-archive.org/\n\n\n\nhttps://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/\n\n\n\nhttps://news.ycombinator.com/item?id=17372497\n\n\n\nhttps://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3","html":"<p></p><p><a href=\"https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/\">https://crate.io/a/lab-notes-how-we-made-joins-23-thousand-times-faster-part-one/</a></p><p></p><p><a href=\"https://www.algorithm-archive.org/\">https://www.algorithm-archive.org/</a></p><p></p><p><a href=\"https://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/\">https://lemire.me/blog/2018/06/26/data-processing-on-modern-hardware/</a></p><p></p><p><a href=\"https://news.ycombinator.com/item?id=17372497\">https://news.ycombinator.com/item?id=17372497</a></p><p></p><p><a href=\"https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3\">https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-11th/","uuid":"6169f835-54d7-4ec1-812e-d990e7f87d9a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39c195d0ac8a143588f34a"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673696","title":"Lynx Roundup, July 10th","slug":"lynx-roundup-july-10th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/lynx69-3.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/10/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/10/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"10 July, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-07-02T02:04:39.000-04:00","published_at":"2018-07-10T07:00:00.000-04:00","updated_at":"2019-02-28T02:16:24.000-05:00","meta_title":"Lynx Roundup, July 10th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/10/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/10/2018.","og_image":"https://hackersandslackers.com/content/images/2019/02/lynx69-3.jpg","og_title":"Lynx Roundup, July 10th","twitter_description":"Daily roundup of Data Science news around the industry, 7/10/2018.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/lynx69-3.jpg","twitter_title":"Lynx Roundup, July 10th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"}],"plaintext":"https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/\n\n\n\nHey, I interviewed with comet.ml!  And they didn't hire me!  Cool company\nthough.\nhttps://medium.com/comet-ml/using-fasttext-and-comet-ml-to-classify-relationships-in-knowledge-graphs-e73d27b40d67\n\n\n\nhttps://medium.com/carwow-product-engineering/sql-vs-pandas-how-to-balance-tasks-between-server-and-client-side-9e2f6c95677\n\n\n\nhttps://medium.com/@marcelogdeandrade/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df\n\n\n\nhttps://medium.com/@trekhleb/permutations-combinations-algorithms-cheat-sheet-68c14879aba5","html":"<p></p><p><a href=\"https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/\">https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/</a></p><p></p><p>Hey, I interviewed with comet.ml!  And they didn't hire me!  Cool company though.  <a href=\"https://medium.com/comet-ml/using-fasttext-and-comet-ml-to-classify-relationships-in-knowledge-graphs-e73d27b40d67\">https://medium.com/comet-ml/using-fasttext-and-comet-ml-to-classify-relationships-in-knowledge-graphs-e73d27b40d67</a></p><p></p><p><a href=\"https://medium.com/carwow-product-engineering/sql-vs-pandas-how-to-balance-tasks-between-server-and-client-side-9e2f6c95677\">https://medium.com/carwow-product-engineering/sql-vs-pandas-how-to-balance-tasks-between-server-and-client-side-9e2f6c95677</a></p><p></p><p><a href=\"https://medium.com/@marcelogdeandrade/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df\">https://medium.com/@marcelogdeandrade/writing-your-own-programming-language-and-compiler-with-python-a468970ae6df</a></p><p></p><p><a href=\"https://medium.com/@trekhleb/permutations-combinations-algorithms-cheat-sheet-68c14879aba5\">https://medium.com/@trekhleb/permutations-combinations-algorithms-cheat-sheet-68c14879aba5</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-10th/","uuid":"9f8efbe3-ab40-4cfc-b698-f78833276cd3","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b39c077d0ac8a143588f344"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673695","title":"Lynx Roundup, July 9th","slug":"lynx-roundup-july-9th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx74@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 7/9/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 7/9/2018.","created_at_pretty":"02 July, 2018","published_at_pretty":"09 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-02T02:01:38.000-04:00","published_at":"2018-07-09T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 9th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 7/9/2018.","og_description":"Daily roundup of Data Science news around the industry, 7/9/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx74@2x.jpg","og_title":"Lynx Roundup, July 9th","twitter_description":"Daily roundup of Data Science news around the industry, 7/9/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx74@2x.jpg","twitter_title":"Lynx Roundup, July 9th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"}],"plaintext":"https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1\n\n\n\nhttps://www.authorea.com/users/18589/articles/304710-a-short-guide-to-using-python-with-data-in-experimental-physics\n\n\n\nhttps://read.iopipe.com/the-right-way-to-do-serverless-in-python-e99535574454\n\n\n\nhttps://github.com/salesforce/decaNLP\n\n\n\nhttps://www.linkedin.com/pulse/major-minor-classifying-mode-song-alex-smith/","html":"<p></p><p><a href=\"https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1\">https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1</a></p><p></p><p><a href=\"https://www.authorea.com/users/18589/articles/304710-a-short-guide-to-using-python-with-data-in-experimental-physics\">https://www.authorea.com/users/18589/articles/304710-a-short-guide-to-using-python-with-data-in-experimental-physics</a></p><p></p><p><a href=\"https://read.iopipe.com/the-right-way-to-do-serverless-in-python-e99535574454\">https://read.iopipe.com/the-right-way-to-do-serverless-in-python-e99535574454</a></p><p></p><p><a href=\"https://github.com/salesforce/decaNLP\">https://github.com/salesforce/decaNLP</a></p><p></p><p><a href=\"https://www.linkedin.com/pulse/major-minor-classifying-mode-song-alex-smith/\">https://www.linkedin.com/pulse/major-minor-classifying-mode-song-alex-smith/</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-9th/","uuid":"f5c028b1-75dd-4881-aa11-31bc4f0d86bb","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b39bfc2d0ac8a143588f340"}}]}},"pageContext":{"slug":"machine-learning","limit":12,"skip":0,"numberOfPages":3,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/machine-learning/page/2/"}}