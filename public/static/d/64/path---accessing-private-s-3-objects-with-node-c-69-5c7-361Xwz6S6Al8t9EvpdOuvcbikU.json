{"data":{"ghostPost":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673683","title":"Read and Write to S3 Buckets via NodeJS","slug":"accessing-private-s3-objects-with-node","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","custom_excerpt":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","created_at_pretty":"21 June, 2018","published_at_pretty":"22 June, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-06-21T19:26:44.000-04:00","published_at":"2018-06-22T07:30:00.000-04:00","updated_at":"2019-03-28T05:25:16.000-04:00","meta_title":"Accessing Private S3 Objects with Node | Hackers and Slackers","meta_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","og_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","og_title":"Accessing Private S3 Objects with Node","twitter_description":"Node’s most popular package interacting with the most popular file store on the world’s most popular cloud.","twitter_image":"https://hackersandslackers.com/content/images/2018/06/nodes3@2x.jpg","twitter_title":"Accessing Private S3 Objects with Node","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"ExpressJS","slug":"expressjs","description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch as a simplistic Express app can evolve into a beautiful monstrosity.","feature_image":null,"meta_description":"A safespace for NodeJS newbies to learn Javascript’s most popular backend framework. Watch a simplistic Express app can evolve into a beautiful monstrosity.","meta_title":"ExpressJS Framework | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"}],"plaintext":"We here at H+S are dedicated to one simple cause: creating posts about oddly\nspecific programming scenarios. Somewhere in the world as sad soul is looking to\nprogrammatically access files from an S3 server while keeping their bucket\nprivate. To that person: we heard you.\n\nThere are plenty of reasons you'd want to access files in S3. For example, let's\nsay you read that post\n[https://hackersandslackers.com/using-pandas-with-aws-lambda/]  about using\nPandas in a Lambda function. Since you're already familiar with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], you may hypothetically be in a\nposition to export data from a DB query to a CSV saved in S3. I bet you can\nguess what I've been doing lately.\n\nConfigure the AWS CLI on your VPS\nThe easiest and safest way to interact with other AWS services on your EC2\ninstance (or VPS of choice) is via the AWS CLI. This is easily installed as a\nglobal Python3 library:\n\n$ pip3 install awscli\n\n\nWith the CLI installed we'll be able to do something truly magical: set our AWS\nconfiguration globally. This means that any time we use interact with a\nmicroservice  (such as S3), the boto3  library will always look to the files\nstored in ~/.aws/  for our keys and secrets, without us specifying.  This\ncritical from a security perspective as it removes all  mentions of credentials\nfrom our codebase: including the location of said secrets.\n\nUse $ aws configure  to kickstart the process:\n\n$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n\n\nThis creates a couple config files for us. If we never need to modify these\nfiles, they can be found here:\n\n$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n\n\nNode Time\nWe'll assume you have an app set up with some basic routing, such as the\nbarebones ExpressJS set up.\n\nIn your app we'll need to add 2 dependencies:\n\n$ npm install --save aws-sdk\n$ npm install --save aws-config\n\n\nNow we'll create a route.\n\nvar awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n\n\nNotice the empty curly brackets in new AWS.S3({}). If we had decided to\nbarbarically hardcode our credentials into our source code, normally those\nvalues would live between those brackets as an object. When the brackets are\nempty, the AWS library automagically knows to look to our AWS credentials file\nfor our access and secret keys. \n\nThis is how you'd do things the wrong way, just in case you wanted to be\nentertained:\n\nvar s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n\n\nYeah, that totally won't get committed somewhere by accident. Shake-my-head fam.\n\nThat's pretty much it: this route will prompt a download of the target file upon\nhitting the route. As much as I'm sure we'd all love to sit here and go through\nmore complicated use cases, let's just avoid Callback Hell altogether and enjoy\nthe rest of our day.\n\nHell will have to wait until next time.","html":"<p>We here at H+S are dedicated to one simple cause: creating posts about oddly specific programming scenarios. Somewhere in the world as sad soul is looking to programmatically access files from an S3 server while keeping their bucket private. To that person: we heard you.</p><p>There are plenty of reasons you'd want to access files in S3. For example, let's say you read <a href=\"https://hackersandslackers.com/using-pandas-with-aws-lambda/\">that post</a> about using Pandas in a Lambda function. Since you're already familiar with <a href=\"https://hackersandslackers.com/using-pymysql/\">PyMySQL</a>, you may hypothetically be in a position to export data from a DB query to a CSV saved in S3. I bet you can guess what I've been doing lately.</p><h2 id=\"configure-the-aws-cli-on-your-vps\">Configure the AWS CLI on your VPS</h2><p>The easiest and safest way to interact with other AWS services on your EC2 instance (or VPS of choice) is via the AWS CLI. This is easily installed as a global Python3 library:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ pip3 install awscli\n</code></pre>\n<!--kg-card-end: markdown--><p>With the CLI installed we'll be able to do something truly magical: set our AWS configuration globally. This means that any time we use interact with a microservice  (such as S3), the <strong>boto3</strong> library will always look to the files stored in <code>~/.aws/</code> for our keys and secrets, without us specifying.  This critical from a security perspective as it removes <em>all</em> mentions of credentials from our codebase: including the location of said secrets.</p><p>Use <code>$ aws configure</code> to kickstart the process:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ aws configure\n$ AWS Access Key ID [None]: YOURACCESSKEY\n$ AWS Secret Access Key [None]: YOURSECRETKEY\n$ Default region name [None]: us-east-2\n$ Default output format [None]: json\n</code></pre>\n<!--kg-card-end: markdown--><p>This creates a couple config files for us. If we never need to modify these files, they can be found here:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim ~/.aws/credentials\n$ vim ~/.aws/config\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"node-time\">Node Time</h2><p>We'll assume you have an app set up with some basic routing, such as the barebones ExpressJS set up.</p><p>In your app we'll need to add 2 dependencies:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install --save aws-sdk\n$ npm install --save aws-config\n</code></pre>\n<!--kg-card-end: markdown--><p>Now we'll create a route.</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var awsConfig = require('aws-config');\nvar AWS = require('aws-sdk');\n\nrouter.get('/export', function(req, res, next) {\n    var file = 'df.csv';\n    console.log('Trying to download file', fileKey);\n\n    var s3 = new AWS.S3({});\n\n    var options = {\n        Bucket: 'your-bucket-name',\n        Key: file,\n    };\n\n    s3.getObject(options, function(err, data) {\n      res.attachment(file);\n      res.send(data.Body);\n  });\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Notice the empty curly brackets in <code>new AWS.S3({})</code>. If we had decided to barbarically hardcode our credentials into our source code, normally those values would live between those brackets as an object. When the brackets are empty, the AWS library automagically knows to look to our AWS credentials file for our access and secret keys. </p><p>This is how you'd do things the <strong><em>wrong </em></strong>way, just in case you wanted to be entertained:</p><!--kg-card-begin: markdown--><pre><code class=\"language-javascript\">var s3 = new AWS.S3({\n    'AccessKeyID': 'YOURACCESSKEY', \n    'SecretAccessKey': 'YOURSECRETACCESSKEY', \n    'Region': 'YOUR REGION'\n});\n</code></pre>\n<!--kg-card-end: markdown--><p>Yeah, that totally won't get committed somewhere by accident. Shake-my-head fam.</p><p>That's pretty much it: this route will prompt a download of the target file upon hitting the route. As much as I'm sure we'd all love to sit here and go through more complicated use cases, let's just avoid Callback Hell altogether and enjoy the rest of our day.</p><p>Hell will have to wait until next time.</p>","url":"https://hackersandslackers.com/accessing-private-s3-objects-with-node/","uuid":"210f5e64-7599-43d0-a148-d68373a9d3c4","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b2c34345f0bc81011d7cfc6"}},"pageContext":{"slug":"accessing-private-s3-objects-with-node"}}