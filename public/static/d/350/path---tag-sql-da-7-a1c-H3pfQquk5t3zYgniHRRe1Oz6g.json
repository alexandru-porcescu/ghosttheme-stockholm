{"data":{"ghostTag":{"slug":"sql","name":"SQL","visibility":"public","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c654f34eab17b74dbf2d2c0","title":"Welcome to SQL 4: Aggregate Functions","slug":"welcome-to-sql-4-aggregate-functions","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","custom_excerpt":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","created_at_pretty":"14 February, 2019","published_at_pretty":"14 March, 2019","updated_at_pretty":"17 March, 2019","created_at":"2019-02-14T06:21:24.000-05:00","published_at":"2019-03-14T03:10:00.000-04:00","updated_at":"2019-03-17T17:25:34.000-04:00","meta_title":"Welcome to SQL 4: Aggregate Functions | Hackers and Slackers","meta_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","og_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","og_title":"Welcome to SQL 4: Aggregate Functions","twitter_description":"Become more intimate with your data- use SQL's aggregate functions to explore the traits which make your data unique and beautiful.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/welcometosql4.jpg","twitter_title":"Welcome to SQL 4: Aggregate Functions","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Aggregate functions in SQL are super dope. When combining these functions with\nclauses such as GROUP BY  and HAVING, we discover ways to view our data from\ncompletely new perspectives. Instead of looking at the same old endless flat\ntable, we can use these functions to give us entirely new insights; aggregate\nfunctions help us to understand bigger-picture things.  Those things might\ninclude finding outliers in datasets, or simply figuring out which employee with\na family to feed should be terminated, based on some arbitrary metric such as\nsales numbers.\n\nWith the basics of JOINs under our belts, this is when SQL starts feel really,\nreally powerful. Our plain two-dimensional tables suddenly gain this power to be\ncombined, aggregated, folded on to themselves, expand infinitely outward as the\nuniverse itself, and even transcend into the fourth dimension.*\n\n*Needs citationOur Base Aggregation Functions\nFirst up, let's see what we mean by \"aggregate functions\" anyway. These simple\nfunctions provide us with a way to mathematically quantify what exactly is in\nour database. Aggregate functions are performed on table columns to give us the\nmake-up of said column. On their own, they seem quite simple:\n\n * AVG: The average of a set of values in a column.\n * COUNT: Number of rows a column contains in a specified table or view.\n * MIN: The minimum value in a set of values.\n * MAX: The maximum value in a set of values.\n * SUM: The sum of values.\n\nDISTINCT Aggregations\nA particularly useful way of using aggregate functions on their own is when we'd\nlike to know the number of DISTINCT  values. While aggregate values take all\nrecords into account, using DISTINCT  limits the data returned to specifically\nrefer to unique values. COUNT(column_name)  will return the number of all\nrecords in a column, where COUNT(DISTINCT column_name)  will ignore counting\nrecords where the value in the counted column is repeated.\n\nUsing GROUP BY\nThe GROUP BY  statement is often used with aggregate functions (COUNT, MAX, MIN,\nSUM, AVG) to group the result-set by one or more columns.\n\nTo demonstrate how aggregate functions work moving forward, I'll be using a\nfamiliar database: the database which contains all the content for this very\nblog. Let's look at how we can use aggregate functions to find which authors\nhave been posting most frequently:\n\nSELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n\n\nAnd the result:\n\nCount\n author_id\n 102\n 1\n 280\n 5c12c3821345c22dced9f591\n 17\n 5c12c3821345c22dced9f592\n 5\n 5c12c3821345c22dced9f593\n 2\n 5c12c3821345c22dced9f594\n 2\n 5c12c3821345c22dced9f595\n Oh look, a real-life data problem to solve! It seems like authors are\nrepresented in Ghost's posts  table simply by their IDs. This isn't very useful.\nLuckily, we've already learned enough about JOINs\n[https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/] \n to know we can fill in the missing information from the users  table!\n\nSELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n\n\nLet's see the results this time around:\n\nCount\n author_id\n 280\n Matthew Alhonte\n 102\n Todd Birchard\n 17\n Max Mileaf\n 5\n Ryan Rosado\n 2\n Graham Beckley\n 2\n David Aquino\n Now that's more like it! Matt is crushing the game with his Lynx Roundup \nseries, with myself in second place. Max had respectable numbers for a moment\nbut has presumably moved on to other hobbies, such as living his life.\n\nFor the remainder, well, I've got nothing to say other than we're hiring. We\ndon't pay though. In fact, there's probably zero benefits to joining us.\n\nConditional Grouping With \"HAVING\"\nHAVING  is like the WHERE  of aggregations. We can't use WHERE  on aggregate\nvalues, so that's why HAVING  exists. HAVING  can't accept any conditional\nvalue, but instead it must accept a numerical conditional derived from a GROUP\nBY. Perhaps this would be easier to visualize in a query:\n\nSELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n\n\nIn this scenario, we want to see which tags on our blog have the highest number\nof associated posts. The query is very similar to the one we made previously,\nonly this time we have a special guest:\n\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) > 10\n\n\nThis usage of HAVING  only gives us tags which have ten posts or more. This\nshould clean up our report by letting Darwinism takes its course. Here's how it\nworked out:\n\ntag\n Count\n Roundup\n 263\n Python\n 80\n Machine Learning\n 29\n DevOps\n 28\n Data Science\n 28\n Software Development\n 27\n Data Engineering\n 23\n Excel\n 19\n SQL\n 18\n Architecture\n 18\n REST APIs\n 16\n #Adventures in Excel\n 16\n Pandas\n 15\n Flask\n 14\n Data Analysis\n 12\n JavaScript\n 12\n AWS\n 11\n MySQL\n 11\n As expected, Matt's roundup posts take the lead (and if we compare this to\nprevious data, we can see Matt has made a total of 17  non-Lynx posts: meaning\nMax and Matt are officially TIED).\n\nIf we hadn't included our HAVING  statement, this list would be much longer,\nfilled with tags nobody cares about. Thanks to explicit omission, now we don't\nneed to experience the dark depression that comes when confronting those sad\npathetic tags. Out of sight, out of mind.\n\nGet Creative\nAggregate functions aren't just about counting values. Especially in Data\nScience, these functions are critical  to drawing any statistical conclusions\nfrom data. That said, attention spans only last so long, and I'm not a\nscientist. Perhaps that can be your job.","html":"<p>Aggregate functions in SQL are super dope. When combining these functions with clauses such as <code>GROUP BY</code> and <code>HAVING</code>, we discover ways to view our data from completely new perspectives. Instead of looking at the same old endless flat table, we can use these functions to give us entirely new insights; aggregate functions help us to understand bigger-picture things.<em> </em>Those things might include finding outliers in datasets, or simply figuring out which employee with a family to feed should be terminated, based on some arbitrary metric such as sales numbers.</p><p>With the basics of <code>JOIN</code>s under our belts, this is when SQL starts feel really, really powerful. Our plain two-dimensional tables suddenly gain this power to be combined, aggregated, folded on to themselves, expand infinitely outward as the universe itself, and even transcend into the fourth dimension.*</p><!--kg-card-begin: html--><div style=\"color:grey; text-align: right; font-style: italic;\">\n    *Needs citation\n</div><!--kg-card-end: html--><h2 id=\"our-base-aggregation-functions\">Our Base Aggregation Functions</h2><p>First up, let's see what we mean by \"aggregate functions\" anyway. These simple functions provide us with a way to mathematically quantify what exactly is in our database. Aggregate functions are performed on table columns to give us the make-up of said column. On their own, they seem quite simple:</p><ul><li><code>AVG</code>: The average of a set of values in a column.</li><li><code>COUNT</code>: Number of rows a column contains in a specified table or view.</li><li><code>MIN</code>: The minimum value in a set of values.</li><li><code>MAX</code>: The maximum value in a set of values.</li><li><code>SUM</code>: The sum of values.</li></ul><h3 id=\"distinct-aggregations\">DISTINCT Aggregations</h3><p>A particularly useful way of using aggregate functions on their own is when we'd like to know the number of <code>DISTINCT</code> values. While aggregate values take all records into account, using <code>DISTINCT</code> limits the data returned to specifically refer to unique values. <code>COUNT(column_name)</code> will return the number of all records in a column, where <code>COUNT(DISTINCT column_name)</code> will ignore counting records where the value in the counted column is repeated.</p><h2 id=\"using-group-by\">Using GROUP BY</h2><p>The <code>GROUP BY</code> statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns.</p><p>To demonstrate how aggregate functions work moving forward, I'll be using a familiar database: the database which contains all the content for this very blog. Let's look at how we can use aggregate functions to find which authors have been posting most frequently:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(title), author_id\nFROM\n  posts\nGROUP BY author_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>And the result:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n  <table>\n    <thead>\n      <tr>\n        <th>Count</th>\n        <th>author_id</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>102</td>\n        <td>1</td>\n      </tr>\n      <tr>\n        <td>280</td>\n        <td>5c12c3821345c22dced9f591</td>\n      </tr>\n      <tr>\n        <td>17</td>\n        <td>5c12c3821345c22dced9f592</td>\n      </tr>\n      <tr>\n        <td>5</td>\n        <td>5c12c3821345c22dced9f593</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f594</td>\n      </tr>\n      <tr>\n        <td>2</td>\n        <td>5c12c3821345c22dced9f595</td>\n      </tr>\n    </tbody>\n  </table>\n</div><!--kg-card-end: html--><p>Oh look, a real-life data problem to solve! It seems like authors are represented in Ghost's <strong><em>posts</em></strong> table simply by their IDs. This isn't very useful. Luckily, we've <a href=\"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/\">already learned enough about JOINs</a> to know we can fill in the missing information from the <strong><em>users</em></strong> table!</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  COUNT(posts.title),\n  users.name\nFROM\n  posts\nLEFT JOIN users\nON \n  (posts.author_id = users.id)\nGROUP BY users.id\nORDER BY COUNT(posts.title) DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see the results this time around:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>Count</th>\n      <th>author_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>280</td>\n      <td>Matthew Alhonte</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>Todd Birchard</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>Max Mileaf</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Ryan Rosado</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Graham Beckley</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>David Aquino</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Now that's more like it! Matt is crushing the game with his <strong>Lynx Roundup</strong> series, with myself in second place. Max had respectable numbers for a moment but has presumably moved on to other hobbies, such as living his life.</p><p>For the remainder, well, I've got nothing to say other than we're hiring. We don't pay though. In fact, there's probably zero benefits to joining us.</p><h3 id=\"conditional-grouping-with-having\">Conditional Grouping With \"HAVING\"</h3><p><code>HAVING</code> is like the <code>WHERE</code> of aggregations. We can't use <code>WHERE</code> on aggregate values, so that's why <code>HAVING</code> exists. <code>HAVING</code> can't accept any conditional value, but instead it <em>must </em>accept a numerical conditional derived from a <code>GROUP BY</code>. Perhaps this would be easier to visualize in a query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT\n  tags.name,\n  COUNT(DISTINCT posts_tags.post_id)\nFROM posts_tags \n  LEFT JOIN tags ON tags.id = posts_tags.tag_id\n  LEFT JOIN posts ON posts.id = posts_tags.post_id\nGROUP BY\n  tags.id\nHAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\nORDER BY\n  COUNT(DISTINCT posts_tags.post_id)\n  DESC;\n</code></pre>\n<!--kg-card-end: markdown--><p>In this scenario, we want to see which tags on our blog have the highest number of associated posts. The query is very similar to the one we made previously, only this time we have a special guest:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">HAVING \n  COUNT(DISTINCT posts_tags.post_id) &gt; 10\n</code></pre>\n<!--kg-card-end: markdown--><p>This usage of <code>HAVING</code> only gives us tags which have ten posts or more. This should clean up our report by letting Darwinism takes its course. Here's how it worked out:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table>\n  <thead>\n    <tr>\n      <th>tag</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Roundup</td>\n      <td>263</td>\n    </tr>\n    <tr>\n      <td>Python</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <td>Machine Learning</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <td>DevOps</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Data Science</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <td>Software Development</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <td>Data Engineering</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <td>Excel</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <td>SQL</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>Architecture</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <td>REST APIs</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>#Adventures in Excel</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <td>Pandas</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <td>Flask</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <td>Data Analysis</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>JavaScript</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <td>AWS</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <td>MySQL</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>As expected, Matt's roundup posts take the lead (and if we compare this to previous data, we can see Matt has made a total of <strong>17</strong> non-Lynx posts: meaning Max and Matt are officially TIED).</p><p>If we hadn't included our <code>HAVING</code> statement, this list would be much longer, filled with tags nobody cares about. Thanks to explicit omission, now we don't need to experience the dark depression that comes when confronting those sad pathetic tags. Out of sight, out of mind.</p><h3 id=\"get-creative\">Get Creative</h3><p>Aggregate functions aren't just about counting values. Especially in Data Science, these functions are <em>critical</em> to drawing any statistical conclusions from data. That said, attention spans only last so long, and I'm not a scientist. Perhaps that can be your job.</p>","url":"https://hackersandslackers.com/welcome-to-sql-4-aggregate-functions/","uuid":"f45c0ccc-3efc-4963-a236-a23db74c2e96","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654f34eab17b74dbf2d2c0"}},{"node":{"id":"Ghost__Post__5c654ed3eab17b74dbf2d2b0","title":"Welcome to SQL 3: Building Relations and Combining Data Sets","slug":"welcome-to-sql-3-building-relationships-and-combining-data","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","custom_excerpt":"This week we look at the fun side of SQL where we JOIN tables and create UNIONs.","created_at_pretty":"14 February, 2019","published_at_pretty":"26 February, 2019","updated_at_pretty":"10 April, 2019","created_at":"2019-02-14T06:19:47.000-05:00","published_at":"2019-02-25T20:11:28.000-05:00","updated_at":"2019-04-10T10:16:10.000-04:00","meta_title":"Relationships and Combining Data in SQL | Hackers and Slackers","meta_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","og_title":"Welcome to SQL 3: Building Relationships and Combining Data","twitter_description":"This week we look at the fun side of SQL. Get the low-down on how to JOIN tables and create UNIONs.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt3@2x.jpg","twitter_title":"Welcome to SQL 3: Building Relationships and Combining Data","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"If you've felt a bit distance or estranged from SQL so far in the series, never\nfear: we're about to discover the magic of what makes relational databases so...\n relational.  Turn down the lights and put on your favorite Marvin Gaye track;\nwe're about to make connections on a whole other level.\n\nI find that existing attempts to explain Database relations (JOINs in\nparticular) have been an utter failure in illustrating these concepts. The Venn\nDiagrams we're all accustomed to seeing mean nothing to somebody who has never\nseen a JOIN occur, and even then, do they really  describe what's happening? I'd\nlove to toss together some quick animations as an alternative, but chances are\nI'll settle for something mediocre like the rest of us.\n\nRelational Databases in Action\nAs much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no,\nnot that  talk; I meant the obligatory \nexample-of-how-two-tables-might-relate-to-one-another  talk. This talk is a bit\nless awkward, but it definitely won't prepare you for the finer things in life.\nJust kidding, data is  the finer part of life. Or at least it is in mine. Let's\nnot linger on that too long.\n\nLet's look at the most common scenario used to illustrate data relationships:\nthe customers  vs. orders  predicament. Let's say we decided to open up an \nOrganic Vegan Paleo Keto Kale Voltron 5000  health-food marketplace to cater to\na high-end clientele: pretentious rich assholes. It just so happens that the\n\"rich asshole\" market is very receptive to best practices in customer relations,\nso we start a CRM to track our best customers. This record-keeping helps us\npretend to remember the names and personalities of our clientele:\n\nCustomers Table\nid\n first_name\n last_name\n email\n gender\n state\n phone\n 653466635\n Timothea\n Crat\n tcrat0@bandcamp.com\n Female\n Washington\n 206-220-3752\n 418540868\n Kettie\n Fuggle\n kfuggle1@cafepress.com\n Female\n California\n 661-793-1372\n 857532654\n Boonie\n Sommerland\n bsommerland2@soundcloud.com\n Male\n North Carolina\n 919-299-0715\n 563295938-4\n Red\n Seldon\n rseldon3@addthis.com\n Male\n Indiana\n 765-880-7420\n 024844147\n Marika\n Gallatly\n mgallatly4@loc.gov\n Female\n New York\n 718-126-1462\n 900992907\n Sharlene\n McMaster\n smcmaster5@gmpg.org\n Female\n Nevada\n 775-376-0931\n 329211747-X\n Grover\n Okey\n gokey6@weather.com\n Male\n Texas\n 915-913-0625\n 656608031\n Farly\n Pluck\n fpluck7@buzzfeed.com\n Male\n Texas\n 432-670-8809\n 906380018\n Sumner\n Pickerell\n spickerellb@bloglovin.com\n Male\n Colorado\n 719-239-5042\n On the other hand, we need to keep track of inventory and items sold. Since\nwe're already swiping credit cards and getting all this personal customer data,\nwhy not associate purchases to loyal customers? Thus, we have a list of\ntransactions which looks something as such:\n\nOrders Table\nitem_id\n customer_id\n item_purchased\n first_name\n last_name\n amount\n date_purchased\n 82565290-530d-4272-9c8b-38dc0bc7426a\n 653466635\n Creme De Menthe Green\n Timothea\n Crat\n $8.57\n 5/13/18\n 9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0\n 653466635\n Veal Inside - Provimi\n Timothea\n Crat\n $5.77\n 3/3/18\n 5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11\n 656608031\n Arizona - Plum Green Tea\n Grover\n Okey\n $1.72\n 9/6/18\n b4813421-12e8-479b-a3b6-3d1c4c539625\n 656608031\n Beer - Fruli\n Grover\n Okey\n $4.05\n 10/1/18\n 4e7c8548-340f-4e89-a7f1-95173dcc6e53\n 656608031\n Boogies\n Grover\n Okey\n $1.97\n 12/17/18\n 65261e94-494d-48cc-8d5a-642ae6921600\n 656608031\n Cup - 3.5oz; Foam\n Grover\n Okey\n $1.84\n 11/28/18\n 1bfdca0f-d54a-4845-bbf5-982813ab4a65\n 656608031\n Arizona - Green Tea\n Grover\n Gauford\n $0.22\n 5/23/18\n d20d7add-bad4-4559-8896-d4f6d05aa3dd\n 906380018\n Lemonade - Strawberry; 591 Ml\n Sumner\n Tortoishell\n $7.98\n 10/11/18\n 12134510-bc6c-4bd7-b733-b549a61edaa3\n 906380018\n Pasta - Cappellini; Dry\n Sumner\n Wash\n $0.31\n 11/13/18\n 80f1957c-df4d-40dc-b9c4-2c3939dd0865\n 906380018\n Remy Red Berry Infusion\n Sumner\n Pisculli\n $1.25\n 12/31/18\n a75f7593-3312-43e4-a604-43405f02efdd\n 906380018\n Veal - Slab Bacon\n Sumner\n Janaszewski\n $9.80\n 3/9/18\n c6ef1f55-f35d-4618-8de7-36f59ea6653a\n 906380018-5\n Beans - Black Bean; Dry\n Sumner\n Piegrome\n $1.36\n 12/11/18\n c5b87ee3-da94-41b1-973a-ef544a3ffb6f\n 906380018\n Calypso - Strawberry Lemonade\n Sumner\n Piegrome\n $7.71\n 2/21/19\n e383c58b-d8da-40ac-afd6-7ee629dc95c6\n 656608031\n Basil - Primerba; Paste\n Mohammed\n Reed\n $2.77\n 10/21/18\n d88ccd5b-0acb-4144-aceb-c4b4b46d3b17\n 656608031\n Cheese - Fontina\n Mohammed\n Reed\n $4.24\n 7/14/18\n 659df773-719c-447e-a1a9-4577dc9c6885\n 656608031\n Cotton Wet Mop 16 Oz\n Jock\n Skittles\n $8.44\n 1/24/19\n ff52e91e-4a49-4a52-b9a5-ddc0b9316429\n 656608031\n Pastry - Trippleberry Muffin - Mini\n Jock\n Skittles\n $9.77\n 11/17/18\n 86f8ad6a-c04c-4714-8f39-01c28dcbb3cb\n 656608031\n Bread - Olive\n Jock\n Skittles\n $4.51\n 1/10/19\n e7a66b71-86ff-4700-ac57-71291e6997b0\n 656608031\n Wine - White; Riesling; Semi - Dry\n Farly\n Pluck\n $4.23\n 4/15/18\n c448db87-1246-494a-bae4-dceb8ee8a7ae\n 656608031\n Melon - Honey Dew\n Farly\n Pluck\n $1.00\n 9/10/18\n 725c171a-452d-45ef-9f23-73ef20109b90\n 656608031\n Sugar - Invert\n Farly\n Pluck\n $9.04\n 3/24/18\n 849f9140-1469-4e23-a1de-83533af5fb88\n 656608031\n Yokaline\n Farly\n Pluck\n $3.21\n 12/31/18\n 2ea79a6b-bfec-4a08-9457-04128f3b37a9\n 656608031\n Cake - Bande Of Fruit\n Farly\n Pluck\n $1.57\n 5/20/18\n Naturally, customers buy more than one item; they buy a lot. Especially that \nFarly Pluck guy at the bottom- quite the unfortunate auto-generated name.\n\nAs standalone tables, the customers  and orders  tables each serve at least one\nstraightforward purpose on their own. The Customers  table helps us with\nconsumer demographic analysis, whereas the Orders  table makes sure we’re making\nmoney and aren't getting robbed. While important, neither of the functions are\nparticularly revolutionary: this basic level of record keeping has been at the\ncore of nearly every business since the 70s. \n\nThe ability to combine data enables us to gain far more significant insights. We\ncan reward loyal customers, cater to the needs of individuals based on their\npreferences, and perhaps even sell the personal data of where and when Mr. Pluck\nhas been every Tuesday and Thursday for the past 4 months to the highest bidding\ndata broker (hint: he's at our store).\n\nThanks to relational databases, we are neither limited to single monolithic\ntables nor are we shackled by the constraints of the tables we set up front.\nAssociating data is trivial, as long as we have a means by which to associate it\nby. Below is a visualization of matching a foreign key  in our orders table to a\n primary key  in our Customers  table:\n\nAn Order's Foreign Key References a customer's IDThe above illustrates what\nwe've already brushed on a bit: Foreign Key association. Primary and foreign\nkeys are essential to describing relations between the tables, and in performing\nSQL joins. Without further adieu, let's join some data.\n\nJoining Sets of Data\nTo “join” multiple sets of data is to consolidate multiple tables into one. \n\nThe manner of this consolidation is determined by which of the four methods of\njoining tables we use: inner joins, right joins, left joins, and outer joins \n(left and right joins are kind of the same, but whatever). Regardless of the\ntype of join, all joins have the following in common:\n\n * Row comparison: we look for rows where the values of a column in Table A \n   match the values of a column in Table B.\n * Consolidation of columns: The purpose of any join is to come away with a\n   table containing columns from both  tables. \n\nLEFT & RIGHT JOINs\nLEFT  and RIGHT  joins cover a myriad of use cases. With a bit of creativity,\nleft/right joins can help solve problems we may not have expected. The terms \"\nleft\" and \"right\" refer to the table we'd like to join on when reading from\nleft-to-right. When joining tables via LEFT JOIN, the first  table in our query\nwill be the \"left\" table. Alternatively, a RIGHT JOIN  refers to the last \ntable. \n\nWhen we say \"table to join on,\" we're specifying which table's key values will\nbe the \"authority\" for our merge. In a LEFT MERGE, all  of the records in Table\nA will survive the merge:\n\n * For rows which have a match in Table B, these rows will be 'extended' to\n   include the data in Table B. This means the new columns being added to Table\n   A  from  Table B  will contain data for all rows where an association has\n   been made.\n * For rows which exist in Table A  but do NOT have a match in Table B, these\n   rows are unaffected: they will contain the same data as before the join, with\n   values in the new columns left blank.\n * Keys which exist in Table B  but do NOT exist in Table A  will be discarded.\n   The purpose of these joins is to enrich the data of the primary table.\n\nBelow is an example of an actual left join I use to power the Kanban board\nmodule on our \"Projects\" page. The left table is a table of JIRA issues, and the\nright table is a collection of issue-based customizations, such as custom icons\nand colors for issue types. Take a look at how this data is associated, and what\nmakes it into the final table:\n\nKeys on the left table determine which rows stay or go.The structure of a LEFT JOIN  query looks as such:\n\nSELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n\n\nHere's an example with actual values:\n\nSELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n\n\nCompare this to a RIGHT JOIN:\n\nSELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n\n\nINNER JOIN (or CROSS JOIN)\nInner joins are the most conservative method for joining sets of data. Unlike \nLEFT  or RIGHT  joins, there is no authoritative table in an inner join:  only\nrows which contain a match in all  tables will survive the join. All other rows\nwill be ignored:\n\nSELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nBecause inner joins will only act on rows which match in all affected tables, an\ninner join will typically contain the most \"complete\" data set (highest number\nof columns satisfied with values), but will contain the fewest number of rows. \n\nOUTER JOINs\nOuter joins  actually come in a few different flavors. Generally speaking, outer\njoins maximize the amount of data which will survive after the join is\nperformed. \n\nLEFT (OR RIGHT) OUTER JOIN\nAt first glance, you might look at the results of a left/right outer  join and\nmistake them to exactly the same as their pure left/right join counterparts.\nWell, you actually wouldn't be mistaken at all! That's right, I was lying:\nthere's essentially no difference between types of joins (thus our time\nmentioning them has been worthless).\n\nFULL OUTER JOIN\nIn a full outer join, all  columns and rows will be joined into the resulting\noutput, regardless of whether or not the rows matched on our specified key. Why\ndo we specify a key at all, you ask? Matching rows on a key still  combines rows\nwhich are similar to all involved tables (if there are truly no rows with common\nground during a merge, you should ask yourself why you're merging two unrelated\nsets of data in the first place).\n\nThe result is kind of a mess. I'm going to borrow an illustration from the \nPandas  documentation here:\n\nSource: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\nWhile Column B appears to be left somewhat intact, take a look at what's\nhappening around it: columns labeled A_x and A_y  have been generated as a\nresult of the join. The outer join has created a table where every possible\ncombination of values for the keys in column B exists. Thus, the number of rows\nin our new table is effectively length of Table A  *  length of Table B.\n\nI personally rarely use outer joins, but that's just me.\n\nSELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n\n\nScenario: Create a New Table from Multiple JOINs\nSo far we've only looked at examples of two tables being joined at once. In\nfact, we can merge as many tables as we want, all at once! Going back to the\nJIRA example, here is the actual query I use to create the final table which\npowers a custom Kanban board:\n\nCREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\n\nIf you're using PostgreSQL, views are a great way to save the results of a join\nwithout adding additional tables. Instead of using CREATE TABLE, try using \nCREATE VIEW:CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n\nUnions & Union All\nA good way to think about JOINs is extending our dataset horizontally. A UNION,\nthen, is a way of combining data vertically. Unions  combine data sets with the\nsame structure: they simply create a table with rows from both tables. UNION \noperators can combine the result-set of two or more SELECT statements, as long\nas:\n\n * Each SELECT statement within UNION must have the same number of columns.\n * The columns must also have similar data types.\n * The columns in each SELECT statement must also be in the same order.\n\nUNION\nSELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n\n\nUNION (with WHERE)\nWe can also add logic to unions via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nUNION ALL\nAn interesting distinction is the presence of UNION  versus UNION ALL. Of the\ntwo, UNION  is the more \"intelligent\" operation: if identical rows exist in both\nSELECT queries, a UNION  will know to only give us one row to avoid duplicates.\nOn the other hand, UNION ALL  does  return duplicates: this results in a faster\nquery and could be useful for those who want to know what is in both SELECT \nstatements:\n\nSELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n\n\nUNION ALL (with WHERE)\nJust like UNION, we can add logic to union all via where  statements:\n\nSELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n\n\nMore SQL Ahead\nI hope that visualizing the way which JOINs  and UNIONs  work can help to reduce\nfriction for SQL new-comers. I find it difficult to believe that human beings\ncan fully grasp these concepts without seeing them happen first-hand, which begs\nthe question: why would anybody explore something so poorly explained, without\nknowing the benefits?\n\nIf you find these guides useful, feel welcome to holler at me to keep them\ncoming. We still have more SQL ahead in our series: stay tuned for when we\nexplore aggregate values and more!","html":"<p>If you've felt a bit distance or estranged from SQL so far in the series, never fear: we're about to discover the magic of what makes relational databases so... <em>relational.</em> Turn down the lights and put on your favorite Marvin Gaye track; we're about to make connections on a whole other level.</p><p>I find that existing attempts to explain Database relations (JOINs in particular) have been an utter failure in illustrating these concepts. The Venn Diagrams we're all accustomed to seeing mean nothing to somebody who has never seen a JOIN occur, and even then, do they <em>really</em> describe what's happening? I'd love to toss together some quick animations as an alternative, but chances are I'll settle for something mediocre like the rest of us.</p><h2 id=\"relational-databases-in-action\">Relational Databases in Action</h2><p>As much as we've covered SQL so far, we still haven't had \"the talk.\" Oh God no, not <em>that</em> talk; I meant the obligatory <em>example-of-how-two-tables-might-relate-to-one-another</em> talk. This talk is a bit less awkward, but it definitely won't prepare you for the finer things in life. Just kidding, data <em>is</em> the finer part of life. Or at least it is in mine. Let's not linger on that too long.</p><p>Let's look at the most common scenario used to illustrate data relationships: the <strong>customers</strong> vs. <strong>orders</strong> predicament. Let's say we decided to open up an <strong>Organic Vegan Paleo Keto Kale Voltron 5000</strong> health-food marketplace to cater to a high-end clientele: pretentious rich assholes. It just so happens that the \"rich asshole\" market is very receptive to best practices in customer relations, so we start a CRM to track our best customers. This record-keeping helps us pretend to remember the names and personalities of our clientele:</p><h3 id=\"customers-table\">Customers Table</h3><!--kg-card-begin: html--><style>\n    .table1 td {\n        padding: 15px;\n    display: table-cell;\n    text-align: left;\n    vertical-align: middle;\n    font-size: 0.8em;\n    text-align: center;\n    line-height: 1.2;\n    font-size: .75em;\n    }\n    \n    \n    .table1 td:nth-of-type(4) {\n        max-width: 80px;\n    }\n    .table1 td:last-of-type{\n        min-width: 100px;\n    }\n    \n        \n</style>\n\n<div class=\"tableContainer\">\n<table class=\"table1\">\n\t<thead>\n       <tr>\n              <th>id</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>email</th>\n              <th>gender</th>\n              <th>state</th>\n              <th>phone</th>\n          </tr>\n    </thead>\n    <tbody>\n       <tr>\n              <td>653466635</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>tcrat0@bandcamp.com</td>\n              <td>Female</td>\n              <td>Washington</td>\n              <td>206-220-3752</td>\n          </tr>\n       <tr>\n              <td>418540868</td>\n              <td>Kettie</td>\n              <td>Fuggle</td>\n              <td>kfuggle1@cafepress.com</td>\n              <td>Female</td>\n              <td>California</td>\n              <td>661-793-1372</td>\n          </tr>\n       <tr>\n              <td>857532654</td>\n              <td>Boonie</td>\n              <td>Sommerland</td>\n              <td>bsommerland2@soundcloud.com</td>\n              <td>Male</td>\n              <td>North Carolina</td>\n              <td>919-299-0715</td>\n          </tr>\n       <tr>\n              <td>563295938-4</td>\n              <td>Red</td>\n              <td>Seldon</td>\n              <td>rseldon3@addthis.com</td>\n              <td>Male</td>\n              <td>Indiana</td>\n              <td>765-880-7420</td>\n          </tr>\n       <tr>\n              <td>024844147</td>\n              <td>Marika</td>\n              <td>Gallatly</td>\n              <td>mgallatly4@loc.gov</td>\n              <td>Female</td>\n              <td>New York</td>\n              <td>718-126-1462</td>\n          </tr>\n       <tr>\n              <td>900992907</td>\n              <td>Sharlene</td>\n              <td>McMaster</td>\n              <td>smcmaster5@gmpg.org</td>\n              <td>Female</td>\n              <td>Nevada</td>\n              <td>775-376-0931</td>\n          </tr>\n       <tr>\n              <td>329211747-X</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>gokey6@weather.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>915-913-0625</td>\n          </tr>\n       <tr>\n              <td>656608031</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>fpluck7@buzzfeed.com</td>\n              <td>Male</td>\n              <td>Texas</td>\n              <td>432-670-8809</td>\n          </tr>\n        <tr>\n              <td>906380018</td>\n              <td>Sumner</td>\n              <td>Pickerell</td>\n              <td>spickerellb@bloglovin.com</td>\n              <td>Male</td>\n              <td>Colorado</td>\n              <td>719-239-5042</td>\n          </tr>\n    </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>On the other hand, we need to keep track of inventory and items sold. Since we're already swiping credit cards and getting all this personal customer data, why not associate purchases to loyal customers? Thus, we have a list of transactions which looks something as such:</p><h3 id=\"orders-table\">Orders Table</h3><!--kg-card-begin: html--><style>\n    .table2 td {\n      padding: 10px 15px;\n    }\n    \n    .table2 tr td:first-of-type {\n        min-width: 100px !important;\n    white-space: nowrap;\n    text-overflow: ellipsis;\n    max-width: 100px;\n    }\n    \n    .table2 tr td:nth-of-type(2) {\n        min-width: 85px !important;\n    }\n</style>\n\n\n<div class=\"tableContainer\">\n   <table class=\"table2\">\n          <thead>\n       <tr>\n              <th>item_id</th>\n              <th>customer_id</th>\n              <th>item_purchased</th>\n              <th>first_name</th>\n              <th>last_name</th>\n              <th>amount</th>\n              <th>date_purchased</th>\n          </tr>\n       </thead>\n       <tbody>\n       <tr>\n              <td>82565290-530d-4272-9c8b-38dc0bc7426a</td>\n              <td>653466635</td>\n              <td>Creme De Menthe Green</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$8.57</td>\n              <td>5/13/18</td>\n          </tr>\n       <tr>\n              <td>9cfa5f5c-6a9c-4400-8f0f-f8262a787cd0</td>\n              <td>653466635</td>\n              <td>Veal Inside - Provimi</td>\n              <td>Timothea</td>\n              <td>Crat</td>\n              <td>$5.77</td>\n              <td>3/3/18</td>\n          </tr>\n       <tr>\n              <td>5dea0cce-c6be-4f35-91f6-0c6a1a8b8f11</td>\n              <td>656608031</td>\n              <td>Arizona - Plum Green Tea</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.72</td>\n              <td>9/6/18</td>\n          </tr>\n       <tr>\n              <td>b4813421-12e8-479b-a3b6-3d1c4c539625</td>\n              <td>656608031</td>\n              <td>Beer - Fruli</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$4.05</td>\n              <td>10/1/18</td>\n          </tr>\n       <tr>\n              <td>4e7c8548-340f-4e89-a7f1-95173dcc6e53</td>\n              <td>656608031</td>\n              <td>Boogies</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.97</td>\n              <td>12/17/18</td>\n          </tr>\n       <tr>\n              <td>65261e94-494d-48cc-8d5a-642ae6921600</td>\n              <td>656608031</td>\n              <td>Cup - 3.5oz; Foam</td>\n              <td>Grover</td>\n              <td>Okey</td>\n              <td>$1.84</td>\n              <td>11/28/18</td>\n          </tr>\n       <tr>\n              <td>1bfdca0f-d54a-4845-bbf5-982813ab4a65</td>\n              <td>656608031</td>\n              <td>Arizona - Green Tea</td>\n              <td>Grover</td>\n              <td>Gauford</td>\n              <td>$0.22</td>\n              <td>5/23/18</td>\n          </tr>\n       <tr>\n              <td>d20d7add-bad4-4559-8896-d4f6d05aa3dd</td>\n              <td>906380018</td>\n              <td>Lemonade - Strawberry; 591 Ml</td>\n              <td>Sumner</td>\n              <td>Tortoishell</td>\n              <td>$7.98</td>\n              <td>10/11/18</td>\n          </tr>\n       <tr>\n              <td>12134510-bc6c-4bd7-b733-b549a61edaa3</td>\n              <td>906380018</td>\n              <td>Pasta - Cappellini; Dry</td>\n              <td>Sumner</td>\n              <td>Wash</td>\n              <td>$0.31</td>\n              <td>11/13/18</td>\n          </tr>\n       <tr>\n              <td>80f1957c-df4d-40dc-b9c4-2c3939dd0865</td>\n              <td>906380018</td>\n              <td>Remy Red Berry Infusion</td>\n              <td>Sumner</td>\n              <td>Pisculli</td>\n              <td>$1.25</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>a75f7593-3312-43e4-a604-43405f02efdd</td>\n              <td>906380018</td>\n              <td>Veal - Slab Bacon</td>\n              <td>Sumner</td>\n              <td>Janaszewski</td>\n              <td>$9.80</td>\n              <td>3/9/18</td>\n          </tr>\n       <tr>\n              <td>c6ef1f55-f35d-4618-8de7-36f59ea6653a</td>\n              <td>906380018-5</td>\n              <td>Beans - Black Bean; Dry</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$1.36</td>\n              <td>12/11/18</td>\n          </tr>\n       <tr>\n              <td>c5b87ee3-da94-41b1-973a-ef544a3ffb6f</td>\n              <td>906380018</td>\n              <td>Calypso - Strawberry Lemonade</td>\n              <td>Sumner</td>\n              <td>Piegrome</td>\n              <td>$7.71</td>\n              <td>2/21/19</td>\n          </tr>\n       <tr>\n              <td>e383c58b-d8da-40ac-afd6-7ee629dc95c6</td>\n              <td>656608031</td>\n              <td>Basil - Primerba; Paste</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$2.77</td>\n              <td>10/21/18</td>\n          </tr>\n       <tr>\n              <td>d88ccd5b-0acb-4144-aceb-c4b4b46d3b17</td>\n              <td>656608031</td>\n              <td>Cheese - Fontina</td>\n              <td>Mohammed</td>\n              <td>Reed</td>\n              <td>$4.24</td>\n              <td>7/14/18</td>\n          </tr>\n       <tr>\n              <td>659df773-719c-447e-a1a9-4577dc9c6885</td>\n              <td>656608031</td>\n              <td>Cotton Wet Mop 16 Oz</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$8.44</td>\n              <td>1/24/19</td>\n          </tr>\n       <tr>\n              <td>ff52e91e-4a49-4a52-b9a5-ddc0b9316429</td>\n              <td>656608031</td>\n              <td>Pastry - Trippleberry Muffin - Mini</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$9.77</td>\n              <td>11/17/18</td>\n          </tr>\n       <tr>\n              <td>86f8ad6a-c04c-4714-8f39-01c28dcbb3cb</td>\n              <td>656608031</td>\n              <td>Bread - Olive</td>\n              <td>Jock</td>\n              <td>Skittles</td>\n              <td>$4.51</td>\n              <td>1/10/19</td>\n          </tr>\n\t\t\t<tr>\n\n              <td>e7a66b71-86ff-4700-ac57-71291e6997b0</td>\n              <td>656608031</td>\n              <td>Wine - White; Riesling; Semi - Dry</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$4.23</td>\n              <td>4/15/18</td>\n          </tr>\n       <tr>\n              <td>c448db87-1246-494a-bae4-dceb8ee8a7ae</td>\n              <td>656608031</td>\n              <td>Melon - Honey Dew</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.00</td>\n              <td>9/10/18</td>\n          </tr>\n       <tr>\n              <td>725c171a-452d-45ef-9f23-73ef20109b90</td>\n              <td>656608031</td>\n              <td>Sugar - Invert</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$9.04</td>\n              <td>3/24/18</td>\n          </tr>\n       <tr>\n              <td>849f9140-1469-4e23-a1de-83533af5fb88</td>\n              <td>656608031</td>\n              <td>Yokaline</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$3.21</td>\n              <td>12/31/18</td>\n          </tr>\n       <tr>\n              <td>2ea79a6b-bfec-4a08-9457-04128f3b37a9</td>\n              <td>656608031</td>\n              <td>Cake - Bande Of Fruit</td>\n              <td>Farly</td>\n              <td>Pluck</td>\n              <td>$1.57</td>\n              <td>5/20/18</td>\n          </tr>\n       </tbody>\n   </table>\n</div><!--kg-card-end: html--><p>Naturally, customers buy more than one item; they buy a <em>lot. </em>Especially that <strong>Farly Pluck </strong>guy at the bottom- quite the unfortunate auto-generated name.</p><p>As standalone tables, the <strong>customers</strong> and <strong>orders</strong> tables each serve at least one straightforward purpose on their own. The <strong>Customers</strong> table helps us with consumer demographic analysis, whereas the <strong>Orders</strong> table makes sure we’re making money and aren't getting robbed. While important, neither of the functions are particularly revolutionary: this basic level of record keeping has been at the core of nearly every business since the 70s. </p><p>The ability to combine data enables us to gain far more significant insights. We can reward loyal customers, cater to the needs of individuals based on their preferences, and perhaps even sell the personal data of where and when Mr. Pluck has been every Tuesday and Thursday for the past 4 months to the highest bidding data broker (hint: he's at our store).</p><p>Thanks to relational databases, we are neither limited to single monolithic tables nor are we shackled by the constraints of the tables we set up front. Associating data is trivial, as long as we have a <em>means by which to associate it by</em>. Below is a visualization of matching a <strong>foreign key</strong> in our orders table to a <strong>primary key</strong> in our <strong>Customers</strong> table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/orders4.gif\" class=\"kg-image\"><figcaption>An Order's Foreign Key References a customer's ID</figcaption></figure><!--kg-card-end: image--><p>The above illustrates what we've already brushed on a bit: Foreign Key association. Primary and foreign keys are essential to describing relations between the tables, and in performing SQL joins. Without further adieu, let's join some data.</p><h2 id=\"joining-sets-of-data\">Joining Sets of Data</h2><p>To “join” multiple sets of data is to consolidate multiple tables into one. </p><p>The manner of this consolidation is determined by which of the four methods of joining tables we use: <strong>inner joins</strong>, <strong>right joins</strong>, <strong>left joins</strong>, and <strong>outer joins</strong> (left and right joins are kind of the same, but whatever). Regardless of the type of <em>join</em>, all joins have the following in common:</p><ul><li>Row comparison: we look for rows where the values of a column in <strong>Table A</strong> match the values of a column in <strong>Table B</strong>.</li><li>Consolidation of columns: The purpose of any join is to come away with a table containing columns from <em>both</em> tables. </li></ul><h3 id=\"left-right-joins\">LEFT &amp; RIGHT JOINs</h3><p><code>LEFT</code> and <code>RIGHT</code> joins cover a myriad of use cases. With a bit of creativity, left/right joins can help solve problems we may not have expected. The terms \"<strong>left</strong>\" and \"<strong>right</strong>\" refer to the table we'd like to join on when reading from left-to-right. When joining tables via <code>LEFT JOIN</code>, the <em>first</em> table in our query will be the \"left\" table. Alternatively, a <code>RIGHT JOIN</code> refers to the <em>last</em> table. </p><p>When we say \"table to join on,\" we're specifying which table's key values will be the \"authority\" for our merge. In a <code>LEFT MERGE</code>, <em>all</em> of the records in <strong>Table A </strong>will survive the merge:</p><ul><li>For rows which have a match in <strong>Table B</strong>, these rows will be 'extended' to include the data in <strong>Table B</strong>. This means the new columns being added to <strong>Table A</strong> from<strong> Table B</strong> will contain data for all rows where an association has been made.</li><li>For rows which exist in <strong>Table A</strong> but do NOT have a match in <strong>Table B</strong>, these rows are unaffected: they will contain the same data as before the join, with values in the new columns left blank.</li><li>Keys which exist in <strong>Table B</strong> but do NOT exist in <strong>Table A</strong> will be discarded. The purpose of these joins is to enrich the data of the primary table.</li></ul><p>Below is an example of an actual left join I use to power the Kanban board module on our \"Projects\" page. The left table is a table of JIRA issues, and the right table is a collection of issue-based customizations, such as custom icons and colors for issue types. Take a look at how this data is associated, and what makes it into the final table:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/tables15.gif\" class=\"kg-image\"><figcaption>Keys on the left table determine which rows stay or go.</figcaption></figure><!--kg-card-end: image--><p>The structure of a <code>LEFT JOIN</code> query looks as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT \n  table_1.*, table_2.*\nFROM\n  t1\n    LEFT JOIN\n  t2 ON t1.column_name = t2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Here's an example with actual values:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount\nFROM customers c\nLEFT JOIN orders o\nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><p>Compare this to a <strong>RIGHT JOIN:</strong></p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT first_name, last_name, order_date, order_amount \nFROM customers c RIGHT JOIN orders o \nON c.customer_id = o.customer_id;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"inner-join-or-cross-join-\">INNER JOIN (or CROSS JOIN)</h3><p>Inner joins are the most conservative method for joining sets of data. Unlike <code>LEFT</code> or <code>RIGHT</code> joins, there is no authoritative table in an <strong>inner join:</strong> only rows which contain a match in <em>all</em> tables will survive the join. All other rows will be ignored:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT table_1.column_name(s), table_2.column_name(s), \nFROM table1\nINNER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><p>Because inner joins will only act on rows which match in all affected tables, an inner join will typically contain the most \"complete\" data set (highest number of columns satisfied with values), but will contain the fewest number of rows. </p><h2 id=\"outer-joins\">OUTER JOINs</h2><p><strong>Outer joins</strong> actually come in a few different flavors. Generally speaking, outer joins maximize the amount of data which will survive after the join is performed. </p><h3 id=\"left-or-right-outer-join\">LEFT (OR RIGHT) OUTER JOIN</h3><p>At first glance, you might look at the results of a left/right <em>outer</em> join and mistake them to exactly the same as their pure left/right join counterparts. Well, you actually wouldn't be mistaken at all! That's right, I was lying: there's essentially no difference between types of joins (thus our time mentioning them has been worthless).</p><h3 id=\"full-outer-join\">FULL OUTER JOIN</h3><p>In a <strong>full outer join</strong>, <em>all</em> columns and rows will be joined into the resulting output, regardless of whether or not the rows matched on our specified key. Why do we specify a key at all, you ask? Matching rows on a key <em>still</em> combines rows which are similar to all involved tables (if there are truly no rows with common ground during a merge, you should ask yourself why you're merging two unrelated sets of data in the first place).</p><p>The result is kind of a mess. I'm going to borrow an illustration from the <strong>Pandas</strong> documentation here:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.digitaloceanspaces.com/posts/2019/02/merging_merge_on_key_dup.png\" class=\"kg-image\"><figcaption>Source: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html</figcaption></figure><!--kg-card-end: image--><p>While Column B appears to be left somewhat intact, take a look at what's happening around it: columns labeled <strong>A_x </strong>and <strong>A_y</strong> have been generated as a result of the join. The outer join has created a table where every possible combination of values for the keys in column B exists. Thus, the number of rows in our new table is effectively <strong><em>length of Table A</em> </strong>*<strong> <em>length of Table B</em>.</strong></p><p>I personally rarely use <strong>outer joins</strong>, but that's just me.</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2\nON table1.column_name = table2.column_name;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"scenario-create-a-new-table-from-multiple-joins\">Scenario: Create a New Table from Multiple JOINs</h2><p>So far we've only looked at examples of two tables being joined at once. In fact, we can merge as many tables as we want, all at once! Going back to the JIRA example, here is the actual query I use to create the final table which powers a custom Kanban board:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">CREATE TABLE jira\nAS\nSELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><div class=\"protip\">\n    If you're using PostgreSQL, views are a great way to save the results of a join without adding additional tables. Instead of using <code>CREATE TABLE</code>, try using <code>CREATE VIEW</code>:\n<pre><code>CREATE VIEW jira\nAS SELECT\n  jira_issues.*,\n  jira_issuetypes.issuetype_url,\n  jira_issuetypes.issuetype_color,\n  jira_epiccolors.epic_color\nFROM\n  jira_issues\n  LEFT JOIN jira_issuetypes ON jira_issues.issuetype = jira_issuetypes.issuetype\n  LEFT JOIN jira_epiccolors ON jira_issues.epic_name = jira_epiccolors.epic_name;</code></pre>\n</div><!--kg-card-end: markdown--><h2 id=\"unions-union-all\">Unions &amp; Union All</h2><p>A good way to think about <code>JOIN</code>s is extending our dataset <em>horizontally</em>. A <code>UNION</code>, then, is a way of combining data <em>vertically. </em><strong>Unions</strong><em> </em>combine data sets with the same structure: they simply create a table with rows from both tables. <code>UNION</code> operators can combine the result-set of two or more SELECT statements, as long as:</p><ul><li>Each SELECT statement within UNION must have the same number of columns.</li><li>The columns must also have similar data types.</li><li>The columns in each SELECT statement must also be in the same order.</li></ul><h3 id=\"union\">UNION</h3><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-with-where-\">UNION (with WHERE)</h3><p>We can also add logic to <strong>unions </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all\">UNION ALL</h3><p>An interesting distinction is the presence of <code>UNION</code> versus <code>UNION ALL</code>. Of the two, <code>UNION</code> is the more \"intelligent\" operation: if identical rows exist in both SELECT <code>queries</code>, a <code>UNION</code> will know to only give us one row to avoid duplicates. On the other hand, <code>UNION ALL</code> <em>does</em> return duplicates: this results in a faster query and could be useful for those who want to know what is in both <code>SELECT</code> statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT column_name(s) FROM table1\nUNION ALL\nSELECT column_name(s) FROM table2;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"union-all-with-where-\">UNION ALL (with WHERE)</h3><p>Just like <code>UNION</code>, we can add logic to <strong>union all </strong>via <strong>where</strong><em> </em>statements:</p><!--kg-card-begin: markdown--><pre><code class=\"language-sql\">SELECT City, Country FROM Customers\nWHERE Country='Germany'\nUNION ALL\nSELECT City, Country FROM Suppliers\nWHERE Country='Germany'\nORDER BY City;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"more-sql-ahead\">More SQL Ahead</h2><p>I hope that visualizing the way which <strong>JOINs</strong> and <strong>UNIONs</strong> work can help to reduce friction for SQL new-comers. I find it difficult to believe that human beings can fully grasp these concepts without seeing them happen first-hand, which begs the question: why would anybody explore something so poorly explained, without knowing the benefits?</p><p>If you find these guides useful, feel welcome to holler at me to keep them coming. We still have more SQL ahead in our series: stay tuned for when we explore aggregate values and more!</p>","url":"https://hackersandslackers.com/welcome-to-sql-3-building-relationships-and-combining-data/","uuid":"5e222417-19b5-49a7-aa64-fbe042891f00","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654ed3eab17b74dbf2d2b0"}},{"node":{"id":"Ghost__Post__5c654e9aeab17b74dbf2d2a3","title":"Welcome to SQL 2: Working With Data Values","slug":"welcome-to-sql-2-working-with-data-values","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","custom_excerpt":"Explore the many flavors of SQL data manipulation in part 2 of our series.","created_at_pretty":"14 February, 2019","published_at_pretty":"22 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T06:18:50.000-05:00","published_at":"2019-02-21T21:56:50.000-05:00","updated_at":"2019-02-27T22:52:38.000-05:00","meta_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","meta_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","og_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","twitter_description":"Explore the many flavors of SQL data manipulation in part 2 of our series.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt2-1.jpg","twitter_title":"Welcome to SQL 2: Working With Data Values | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"Now that we've gotten the fundamentals of creating databases and tables\n[https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/] \nout of the way, we can start getting into the meat and potatoes of SQL\ninteractions: selecting, updating, and deleting  data.\n\nWe'll start with the basic structure of these queries and then break into the\npowerful operations with enough detail to make you dangerous.\n\nSelecting Data From a Table\nAs mentioned previously, SQL operations have a rather strict order of operations\nwhich clauses have to respect in order to make a valid query. We'll begin by\ndissecting a common SELECT statement:\n\nSELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\";\n\n\nThis is perhaps the most common structure of SELECT queries. First, we list the\nnames of the columns we'd like to select separated by commas. To receive all \ncolumns, we can simply say SELECT *.\n\nThese columns need to come from somewhere, so we specify the table we're\nreferring to next. This either takes a form of FROM table_name \n(non-PostgreSQL), or FROM schema_name.table_name  (PostgreSQL). In theory, a\nsemicolon here would result in a valid query, but we usually want to select rows\nthat meet certain criteria.\n\nThis is where the WHERE  clause comes in: only rows which return \"true\"  for our\n WHERE  conditional will be returned. In the above example, we're validating\nthat a string matches exactly \"Value\". \n\nSelecting only Distinct Values\nSomething that often comes in handy is selecting distinct values in a column. In\nother words, if a value exists in the same column in 100 rows, running DISTINCT \nquery will only show us that value once. This is a good way of seeing the unique\ncontent of a column without yet diving into the distribution of said value. The\neffect is similar to the United States Senate, or the Electoral College: forget\nthe masses, and prop up Wyoming 2020:\n\nSELECT DISTINCT column_name \nFROM table_name;\n\n\nOffsetting and Limiting Results in our Queries\nWhen selecting data, the combination of OFFSET  and LIMIT  are critical at\ntimes. If we're selecting from a database with hundreds of thousands of rows, we\nwould be wasting an obscene amount of system resources to fetch all rows at\nonce; instead, we can have our application or API paginate the results.\n\nLIMIT  is followed by an integer, which in essence says \"return no more than X\nresults.\" \n\nOFFSET  is also followed by an integer, which denotes a numerical starting point\nfor returned results, aka: \"return all results which occur after the Xth\nresult:\"\n\nSELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n\n\nThe above returns the first 50 results. If we wanted to build paginated results\non the application side, we could construct our query like this:\n\nfrom SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n\n\nSuch an application could increment page_number  by 1 each time the user clicks\non to the next page, which would then appropriately modify our query to return\nthe next page of results.\n\nAnother use for OFFSET  could be to pick up where a failed script left off. If\nwe were to write an entire database to a CSV and experience a failure. We could\npick up where the script left off by setting OFFSET  equal to the number of rows\nin the CSV, to avoid running the entire script all over again.\n\nSorting Results\nLast to consider for now is sorting our results by using the ORDER BY  clause.\nWe can sort our results by any specified column, and state whether we'd like the\nresults to be ascending (ASC) or descending (DESC):\n\nSELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = \"Value\"\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n\n\nSophisticated SELECT Statements\nOf course, we can select rows with WHERE  logic that goes much deeper than an\nexact match. One of the most versatile of these operations is LIKE.\n\nUsing Regex with LIKE\nLIKE  is perhaps the most powerful way to select columns with string values.\nWith LIKE, we can leverage regular expressions to build highly complex logic.\nLet's start with some of my favorites:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE \"%Wade%\";\n\n\nPassing a string to LIKE  with percentage signs on both sides is essentially a \"\ncontains\" statement. %  is equivalent to a wildcard, thus placing %  on either\nside of our string will return true whether the person's first name, middle\nname, or last name is Wade. Check out other useful combinations for %:\n\n * a%: Finds any values that start with \"a\".\n * %a: Finds any values that end with \"a\".\n * %or%: Finds any values that have \"or\" in any position.\n *   _r%: Finds any values that have \"r\" in the second position.\n * a_%_%:  Finds any values that start with \"a\" and are at least 3 characters in\n   length.\n * a%o:  Finds any values that start with \"a\" and ends with \"o\".\n\nFinding Values which are NOT LIKE\nThe opposite of LIKE  is of course NOT LIKE, which runs the same conditional,\nbut returns the opposite true/false value of LIKE:\n\nSELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE \"%Wade%\";\n\n\nConditionals With DateTime Columns\nDateTime columns are extremely useful for selecting data. Unlike plain strings,\nwe can easily extract numerical values for month, day, and year from a DateTime\nby using MONTH(column_name), DAY(column_name), and YEAR(column_name) \nrespectively. For example, using MONTH()  on a column that contains a DateTime\nof 2019-01-26 05:42:34  would return 1, aka January. Because the values come\nback as integers, it is then trivial to find results within a date range:\n\nSELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) < 2018;\n\n\nFinding Rows with NULL Values\nNULL  is a special datatype which essentially denotes the \"absence of\nsomething,\" therefore no conditional will never equal  NULL. Instead, we find\nrows where a value IS NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n\n\nThis should not come as a surprise to anybody familiar with validating\ndatatypes.\n\nThe reverse of this, of course, is NOT NULL:\n\nSELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n\n\nInserting Data\nAn INSERT  query creates a new row, and is rather straightforward: we state the\ncolumns we'd like to insert data into, followed by the values to insert into\nsaid columns:\n\nINSERT INTO table_name (column_1, column_2, column_3)\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nMany things could result in a failed insert. For one, the number of values must\nmatch the number of columns we specify; if we don't we've either provided too\nfew or too many values.\n\nSecond, vales must respect a column's data type. If we try to insert an integer\ninto a DateTime  column, we'll receive an error.\n\nFinally, we must consider the keys and constraints of the table. If keys exist\nthat specify certain columns must not be empty, or must be unique, those keys\nmust too be respected.\n\nAs a shorthand trick, if we're inserting values into all  of a table's columns,\nwe can skip the part where we explicitly list the column names:\n\nINSERT INTO table_name\nVALUES (\"value1\", \"value2\", \"value3\");\n\n\nHere's a quick example of an insert query with real data:\n\nINSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n\n\nUPDATE Records: The Basics\nUpdating rows is where things get interesting. There's so much we can do here,\nso let's work our way up:\n\nUPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n\n\nThat's as simple as it gets: the value of a column, in a row that matches our\nconditional. Note that SET  always comes before WHERE. Here's the same query\nwith real data:\n\nUPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n\n\nUPDATE Records: Useful Logic\nJoining Strings Using CONCAT\nYou will find that it's common practice to update rows based on data which\nalready exists in said rows: in other words, sanitizing or modifying data. A\ngreat string operator is CONCAT(). CONCAT(\"string_1\", \"string_2\")  will join all\nthe strings passed to a single string.\n\nBelow is a real-world example of using CONCAT()  in conjunction with NOT LIKE \nto determine which post excerpts don't end in punctuation. If the excerpt does\nnot end with a punctuation mark, we add a period to the end:\n\nUPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n\n\nUsing REPLACE\nREPLACE()  works in SQL as it does in nearly every programming language. We pass\n REPLACE()  three values: \n\n 1. The string to be modified. \n 2. The substring within the string which will be replaced. \n 3. The value of the replacement. \n\nWe can do plenty of clever things with REPLACE(). This is an example that\nchanges the featured image of blog posts to contain the “retina image” suffix: \n\nUPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n\n\nScenario: Folder Structure Based on Date\nI across a fun exercise the other day when dealing with a nightmare situation\ninvolving changing CDNs. It touches on everything we’ve reviewed thus far and\nserves a great illustration of what can be achieved in SQL alone. \n\nThe challenge in moving hundreds of images for hundreds of posts came in the\nform of a file structure. Ghost likes to save images in a dated folder\nstructure, like 2019/02/image.jpg. Our previous CDN did not abide by this at\nall, so had a dump of all images in a single folder. Not ideal. \n\nThankfully, we can leverage the metadata of our posts to discern this file\nstructure. Because images are added to posts when posts are created, we can use\nthe created_at  column from our posts table to figure out the right dated\nfolder: \n\nUPDATE\n  posts\nSET\n  feature_image = CONCAT(\"https://cdn.example.com/posts/\", \n\tYEAR(created_at),\n\t\"/\", \n\tLPAD(MONTH(created_at), 2, '0'), \n\t\"/\",\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n\n\nLet's break down the contents in our CONCAT:\n\n * https://cdn.example.com/posts/: The base URL of our new CDN.\n * YEAR(created_at): Extracting the year from our post creation date\n   (corresponds to a folder).\n * LPAD(MONTH(created_at), 2, '0'): Using MONTH(created_at)  returns a single\n   digit for early months, but our folder structure wants to always have months\n   a double-digits (ie: 2018/01/ as opposed to 2018/1/). We can use LPAD()  here\n   to 'pad' our dates so that months are always two digits long, and shorter\n   dates will be padded with the number 0.\n * SUBSTRING_INDEX(feature_image, '/', - 1): We're getting the filename of each\n   post's image by finding everything that comes after the last slash in our\n   existing image URL. \n\nThe result for every image will now look like this:\n\nhttps://cdn.example.com/posts/2018/02/image.jpg\n\n\nDELETE Records\nLet's wrap up for today with our last type of query, deleting rows:\n\nDELETE FROM celebs \nWHERE twitter_handle IS NULL;","html":"<p>Now that we've gotten the fundamentals of <a href=\"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/\">creating databases and tables</a> out of the way, we can start getting into the meat and potatoes of SQL interactions: <strong>selecting</strong>, <strong>updating</strong>, and <strong>deleting</strong> data.</p><p>We'll start with the basic structure of these queries and then break into the powerful operations with enough detail to make you dangerous.</p><h2 id=\"selecting-data-from-a-table\">Selecting Data From a Table</h2><p>As mentioned previously, SQL operations have a rather strict order of operations which clauses have to respect in order to make a valid query. We'll begin by dissecting a common SELECT statement:</p><pre><code class=\"language-sql\">SELECT\n  column_name_1,\n  column_name_2\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;;\n</code></pre>\n<p>This is perhaps the most common structure of SELECT queries. First, we list the names of the columns we'd like to select separated by commas. To receive <em>all</em> columns, we can simply say <code>SELECT *</code>.</p><p>These columns need to come from somewhere, so we specify the table we're referring to next. This either takes a form of <code>FROM table_name</code> (non-PostgreSQL), or <code>FROM schema_name.table_name</code> (PostgreSQL). In theory, a semicolon here would result in a valid query, but we usually want to select rows that meet certain criteria.</p><p>This is where the <code>WHERE</code> clause comes in: only rows which return <strong>\"true\"</strong> for our <code>WHERE</code> conditional will be returned. In the above example, we're validating that a string matches exactly <code>\"Value\"</code>. </p><h3 id=\"selecting-only-distinct-values\">Selecting only Distinct Values</h3><p>Something that often comes in handy is selecting distinct values in a column. In other words, if a value exists in the same column in 100 rows, running <code>DISTINCT</code> query will only show us that value once. This is a good way of seeing the unique content of a column without yet diving into the distribution of said value. The effect is similar to the United States Senate, or the Electoral College: forget the masses, and prop up Wyoming 2020:</p><pre><code class=\"language-sql\">SELECT DISTINCT column_name \nFROM table_name;\n</code></pre>\n<h3 id=\"offsetting-and-limiting-results-in-our-queries\">Offsetting and Limiting Results in our Queries</h3><p>When selecting data, the combination of <code>OFFSET</code> and <code>LIMIT</code> are critical at times. If we're selecting from a database with hundreds of thousands of rows, we would be wasting an obscene amount of system resources to fetch all rows at once; instead, we can have our application or API paginate the results.</p><p><code>LIMIT</code> is followed by an integer, which in essence says \"return no more than X results.\" </p><p><code>OFFSET</code> is also followed by an integer, which denotes a numerical starting point for returned results, aka: \"return all results which occur after the Xth result:\"</p><pre><code class=\"language-sql\">SELECT\n *\nFROM\n table_name\nLIMIT 50 OFFSET 0;\n</code></pre>\n<p>The above returns the first 50 results. If we wanted to build paginated results on the application side, we could construct our query like this:</p><pre><code class=\"language-python\">from SQLAlchemy import engine, session\n\n# Set up a SQLAlchemy session\nSession = sessionmaker()\nengine = create_engine('sqlite:///example.db')\nSession.configure(bind=engine)\nsess = Session()\n\n# Appication variables\npage_number = 3\npage_size = 50\nresults_subset = page_number * results limit\n\n# Query\nsession.query(TableName).limit(page_size).offset(results_subset)\n</code></pre>\n<p>Such an application could increment <code>page_number</code> by 1 each time the user clicks on to the next page, which would then appropriately modify our query to return the next page of results.</p><p>Another use for <code>OFFSET</code> could be to pick up where a failed script left off. If we were to write an entire database to a CSV and experience a failure. We could pick up where the script left off by setting <code>OFFSET</code> equal to the number of rows in the CSV, to avoid running the entire script all over again.</p><h3 id=\"sorting-results\">Sorting Results</h3><p>Last to consider for now is sorting our results by using the <code>ORDER BY</code> clause. We can sort our results by any specified column, and state whether we'd like the results to be ascending (<code>ASC</code>) or descending (<code>DESC</code>):</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  schema_name.table_name\nWHERE\n  column_name_1 = &quot;Value&quot;\nORDER BY\n  updated_date DESC\nLIMIT 50 OFFSET 10;\n</code></pre>\n<h2 id=\"sophisticated-select-statements\">Sophisticated SELECT Statements</h2><p>Of course, we can select rows with <code>WHERE</code> logic that goes much deeper than an exact match. One of the most versatile of these operations is <code>LIKE</code>.</p><h3 id=\"using-regex-with-like\">Using Regex with LIKE</h3><p><code>LIKE</code> is perhaps the most powerful way to select columns with string values. With <code>LIKE</code>, we can leverage regular expressions to build highly complex logic. Let's start with some of my favorites:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name LIKE &quot;%Wade%&quot;;\n</code></pre>\n<p>Passing a string to <code>LIKE</code> with percentage signs on both sides is essentially a \"<strong>contains</strong>\" statement. <code>%</code> is equivalent to a wildcard, thus placing <code>%</code> on either side of our string will return true whether the person's first name, middle name, or last name is <strong>Wade</strong>. Check out other useful combinations for <code>%</code>:</p><ul><li><code>a%</code>: Finds any values that start with \"a\".</li><li><code>%a</code>: Finds any values that end with \"a\".</li><li><code>%or%</code>: Finds any values that have \"or\" in any position.</li><li> <code>_r%</code>: Finds any values that have \"r\" in the second position.</li><li><code>a_%_%</code><strong>:</strong> Finds any values that start with \"a\" and are at least 3 characters in length.</li><li><code>a%o</code>:<strong> </strong>Finds any values that start with \"a\" and ends with \"o\".</li></ul><h3 id=\"finding-values-which-are-not-like\">Finding Values which are NOT LIKE</h3><p>The opposite of <code>LIKE</code> is of course <code>NOT LIKE</code>, which runs the same conditional, but returns the opposite true/false value of <code>LIKE</code>:</p><pre><code class=\"language-sql\">SELECT\n  *\nFROM\n  people\nWHERE\n  name NOT LIKE &quot;%Wade%&quot;;\n</code></pre>\n<h3 id=\"conditionals-with-datetime-columns\">Conditionals With DateTime Columns</h3><p>DateTime columns are extremely useful for selecting data. Unlike plain strings, we can easily extract numerical values for month, day, and year from a DateTime by using <code>MONTH(column_name)</code>, <code>DAY(column_name)</code>, and <code>YEAR(column_name)</code> respectively. For example, using <code>MONTH()</code> on a column that contains a DateTime of <code>2019-01-26 05:42:34</code> would return <code>1</code>, aka January. Because the values come back as integers, it is then trivial to find results within a date range:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE YEAR(created_at) &lt; 2018;\n</code></pre>\n<h3 id=\"finding-rows-with-null-values\">Finding Rows with NULL Values</h3><p><code>NULL</code> is a special datatype which essentially denotes the \"absence of something,\" therefore no conditional will never <em>equal</em> <code>NULL</code>. Instead, we find rows where a value <code>IS NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NULL;\n</code></pre>\n<p>This should not come as a surprise to anybody familiar with validating datatypes.</p><p>The reverse of this, of course, is <code>NOT NULL</code>:</p><pre><code class=\"language-sql\">SELECT \n  * \nFROM \n  posts \nWHERE author IS NOT NULL;\n</code></pre>\n<h2 id=\"inserting-data\">Inserting Data</h2><p>An <code>INSERT</code> query creates a new row, and is rather straightforward: we state the columns we'd like to insert data into, followed by the values to insert into said columns:</p><pre><code class=\"language-sql\">INSERT INTO table_name (column_1, column_2, column_3)\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Many things could result in a failed insert. For one, the number of values must match the number of columns we specify; if we don't we've either provided too few or too many values.</p><p>Second, vales must respect a column's data type. If we try to insert an integer into a <strong>DateTime</strong> column, we'll receive an error.</p><p>Finally, we must consider the keys and constraints of the table. If keys exist that specify certain columns must not be empty, or must be unique, those keys must too be respected.</p><p>As a shorthand trick, if we're inserting values into <em>all</em> of a table's columns, we can skip the part where we explicitly list the column names:</p><pre><code class=\"language-sql\">INSERT INTO table_name\nVALUES (&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;);\n</code></pre>\n<p>Here's a quick example of an insert query with real data:</p><pre><code class=\"language-sql\">INSERT INTO friends (id, name, birthday) \nVALUES (1, 'Jane Doe', '1990-05-30');\n</code></pre>\n<h2 id=\"update-records-the-basics\">UPDATE Records: The Basics</h2><p>Updating rows is where things get interesting. There's so much we can do here, so let's work our way up:</p><pre><code class=\"language-sql\">UPDATE table_name \nSET column_name_1 = 'value' \nWHERE column_name_2 = 'value';\n</code></pre>\n<p>That's as simple as it gets: the value of a column, in a row that matches our conditional. Note that <code>SET</code> always comes before <code>WHERE</code>. Here's the same query with real data:</p><pre><code class=\"language-sql\">UPDATE celebs \nSET twitter_handle = '@taylorswift13' \nWHERE id = 4;\n</code></pre>\n<h2 id=\"update-records-useful-logic\">UPDATE Records: Useful Logic</h2><h3 id=\"joining-strings-using-concat\">Joining Strings Using CONCAT</h3><p>You will find that it's common practice to update rows based on data which already exists in said rows: in other words, sanitizing or modifying data. A great string operator is <code>CONCAT()</code>. <code>CONCAT(\"string_1\", \"string_2\")</code> will join all the strings passed to a single string.</p><p>Below is a real-world example of using <code>CONCAT()</code> in conjunction with <code>NOT LIKE</code> to determine which post excerpts don't end in punctuation. If the excerpt does not end with a punctuation mark, we add a period to the end:</p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET \n  custom_excerpt = CONCAT(custom_excerpt, '.')\nWHERE\n  custom_excerpt NOT LIKE '%.'\n  AND custom_excerpt NOT LIKE '%!'\n  AND custom_excerpt NOT LIKE '%?';\n</code></pre>\n<h3 id=\"using-replace\">Using REPLACE</h3><p><code>REPLACE()</code> works in SQL as it does in nearly every programming language. We pass <code>REPLACE()</code> three values: </p><ol><li>The string to be modified. </li><li>The substring within the string which will be replaced. </li><li>The value of the replacement. </li></ol><p>We can do plenty of clever things with <code>REPLACE()</code>. This is an example that changes the featured image of blog posts to contain the “retina image” suffix: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = REPLACE(feature_image, '.jpg', '@2x.jpg');\n</code></pre>\n<h3 id=\"scenario-folder-structure-based-on-date\">Scenario: Folder Structure Based on Date</h3><p>I across a fun exercise the other day when dealing with a nightmare situation involving changing CDNs. It touches on everything we’ve reviewed thus far and serves a great illustration of what can be achieved in SQL alone. </p><p>The challenge in moving hundreds of images for hundreds of posts came in the form of a file structure. Ghost likes to save images in a dated folder structure, like <strong>2019/02/image.jpg</strong>. Our previous CDN did not abide by this at all, so had a dump of all images in a single folder. Not ideal. </p><p>Thankfully, we can leverage the metadata of our posts to discern this file structure. Because images are added to posts when posts are created, we can use the <strong>created_at</strong> column from our posts table to figure out the right dated folder: </p><pre><code class=\"language-sql\">UPDATE\n  posts\nSET\n  feature_image = CONCAT(&quot;https://cdn.example.com/posts/&quot;, \n\tYEAR(created_at),\n\t&quot;/&quot;, \n\tLPAD(MONTH(created_at), 2, '0'), \n\t&quot;/&quot;,\n\tSUBSTRING_INDEX(feature_image, '/', - 1)\n  );\n</code></pre>\n<p>Let's break down the contents in our <code>CONCAT</code>:</p><ul><li><code>https://cdn.example.com/posts/</code>: The base URL of our new CDN.</li><li><code>YEAR(created_at)</code>: Extracting the year from our post creation date (corresponds to a folder).</li><li><code>LPAD(MONTH(created_at), 2, '0')</code>: Using <strong>MONTH(created_at)</strong> returns a single digit for early months, but our folder structure wants to always have months a double-digits (ie: <strong>2018/01/ </strong>as opposed to <strong>2018/1/</strong>). We can use <code>LPAD()</code> here to 'pad' our dates so that months are always two digits long, and shorter dates will be padded with the number 0.</li><li><code>SUBSTRING_INDEX(feature_image, '/', - 1)</code>: We're getting the filename of each post's image by finding everything that comes after the last slash in our existing image URL. </li></ul><p>The result for every image will now look like this:</p><pre><code>https://cdn.example.com/posts/2018/02/image.jpg\n</code></pre>\n<h2 id=\"delete-records\">DELETE Records</h2><p>Let's wrap up for today with our last type of query, deleting rows:</p><pre><code class=\"language-sql\">DELETE FROM celebs \nWHERE twitter_handle IS NULL;\n</code></pre>\n","url":"https://hackersandslackers.com/welcome-to-sql-2-working-with-data-values/","uuid":"e051cdc6-eb17-425f-bb83-2f70a75e85c5","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c654e9aeab17b74dbf2d2a3"}},{"node":{"id":"Ghost__Post__5c5bb0ec7999ff33f06876e1","title":"Welcome to SQL: Modifying Databases and Tables","slug":"welcome-to-sql-modifying-databases-and-tables","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","custom_excerpt":"Brush up on SQL fundamentals such as creating tables, schemas, and views.","created_at_pretty":"07 February, 2019","published_at_pretty":"19 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-06T23:15:40.000-05:00","published_at":"2019-02-19T18:28:00.000-05:00","updated_at":"2019-02-27T23:16:44.000-05:00","meta_title":"Welcome to SQL: Modifying Databases and Tables | Hackers and Slackers","meta_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","og_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","og_title":"Welcome to SQL: Modifying Databases and Tables","twitter_description":"Brush up on SQL fundamentals such as creating tables, schemas, and views. Part of a multi-part series on learning SQL.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/SQLpt1-3.jpg","twitter_title":"Welcome to SQL: Modifying Databases and Tables","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"#Welcome to SQL","slug":"welcome-to-sql","description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/welcometosqlseries.jpg","meta_description":"If you feel like you’re late to the data party, we all are. The party has been going strong since the 70s: brush up on SQL syntax the old-fashioned way.","meta_title":"Welcome to SQL","visibility":"internal"}],"plaintext":"SQL: we all pretend to be experts at it, and mostly get away with it thanks to\nStackOverflow. Paired with our vast experience of learning how to code in the\n90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go\nahead and chalk up a win for your resume.\n\nSQL has been around longer than our careers have, so why start a series on it \nnow?  Surely there’s sufficient enough documentation that we can Google the\nspecifics whenever the time comes for us to write a query? That, my friends, is\nprecisely the problem. Regardless of what tools we have at our disposable, some\nskills are better learned and practiced by heart. SQL is one of those skills.\n\nSure, SQLAlchemy or similar ORMs might protect us here-and-there from writing\nraw queries. Considering SQL is just one of many query languages we'll use\nregularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert\nreally that critical? In short, yes: relational databases are not only here to\nstay, but thinking  in queries as a second language solidifies one's\nunderstanding of the fine details of data. Marc Laforet\n[https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032] \n recently published a Medium post which drives home just how important leaning\non SQL is:\n\n> What’s even more interesting is that when these transformation scripts were\napplied to the 6.5 GB dataset, python completely failed. Out of 3 attempts,\npython crashed 2 times and my computer completely froze the 3rd time… while SQL\ntook 226 seconds.\n\n\nKeeping logic out of our apps and pipelines and in SQL results in exponentially\nfaster execution, while also being more readable and universally understood than\nwhatever we’d write in our language of choice. The lower down we can push\napplication logic in our stack, the better. This is why I’d much prefer to see\nthe datasphere saturated with SQL tutorials as opposed to Pandas tutorials.\n\nRelational Database Terminology\nI hate it when informational material kicks off with covering obvious\nterminology definitions. Under normal circumstances, I find this to be cliche,\nunhelpful, and damaging to an author's credibility; but these aren't normal\ncircumstances. In SQL, vocabulary commonly has multiple meanings depending on\ncontext, or even which flavor database you're using. Given this fact, it's\nentirely possible (and common) for individuals to rack up experience with\nrelational databases while completely misinterpreting fundamental concepts.\nLet's make sure that doesn't happen:\n\n * Databases: Every Database instance is separated at the highest level into \n   databases. Yes, a database is a collection of databases - we're already off\n   to a great start.\n * Schemas: In PostgreSQL (and other databases), a schema  is a grouping of\n   tables and other objects, including views, relations, etc. A schema is a way\n   of organizing data. Schemas imply that all the data belonging to it is at\n   some form related, even if only by concept. Note that the term schema  is\n   sometimes used to describe other concepts depending on the context.\n * Tables: The meat and potatos of relational databases. Tables consist of rows\n   and columns which hold our sweet, sweet data. Columns are best thought of as\n   'attributes', whereas rows are entries which consist of values for said\n   attributes. All values in a column must share the same data type. * Keys: Keys are used to help us organize and optimize data, as well as\n      place certain constraints on data coming in (for example, email addresses\n      of user accounts must be unique). Keys can also help us keep count of our\n      entries, ensure automatically unique values, and provide a bridge to link\n      multiple tables of data. * Primary keys: Identification tags for each row of data. The primary key\n         is different for every record in the relational database; values must\n         be provided, and they must be unique between rows.\n       * Foreign keys: Enable data searches and manipulation between the primary\n         database table and other related databases.\n      \n      \n   \n   \n * Objects: A blanket term for anything (including relations) that exist in a\n   schema (somewhat PostgreSQL-specific). * Views (PostgreSQL): Views display data in a fashion similar to tables,\n      with the difference that views do not store  data. Views are a snapshot of\n      data pulled from other tables in the form of a query; a good way to think\n      about views is to consider them to be 'virtual tables.'\n    * Functions (PostgreSQL): Logic for interacting with data saved for the\n      purpose of being reused.\n   \n   \n\nIn MySQL, a schema  is synonymous with a database. These keywords can even be\nswapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using CREATE\nSCHEMA  acheives the same effect as instead of CREATE DATABASE.Navigating and\nCreating Databases\nWe've got to start somewhere, so it might as well be with database management.\nAdmittedly, this will be the most useless of the things we'll cover. The act of\nnavigating databases is best suited for a GUI.\n\nShow Databases\nIf you access your database via command line shell (for some reason), the first\nlogical thing to do is to list the available databases:\n\nSHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\n\nUSE Database\nNow that we've listed the possible databases we can connect to, we can explore\nwhat each of these contains. To do this, we have to specify which database we\nwant to connect to, AKA \"use.\" \n\ndb> USE database_name;\nDatabase changed\n\n\nCreate Database\nCreating databases is straightforward. Be sure to pay attention to the character\nset  when creating a database: this will determine which types of characters\nyour database will be able to accept. For example, if we try to insert special\nencoded characters into a simple UTF-8 database, those characters won’t turn out\nas we’d expect.\n\nCREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n\n\nBonus: here's the shorthand for creating a database and then showing the result:\n\nSHOW CREATE DATABASE database_name;\n\n\nCreating and Modifying Tables\nCreating tables via SQL syntax can be critical when automating data imports.\nWhen creating a table, we also set the column names, types, and keys:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];\n\nWe can specify IF NOT EXISTS  when creating our table if we'd like to include\nvalidation in our query. When present, the table will only be created if a table\nof the specified name does not exist.\n\nWhen creating each of our columns, there are a number of things we can specify\nper-column:\n\n * Data Type (required):  The data which can be saved to cells of this column\n   (such as INTEGER, TEXT, etc).\n * Key Type:  Creates a key for the column.\n * Key Attributes:  Any key-related attributes, such as auto-incrementing.\n * Default:  If rows are created in the table without values passed to the\n   current column, the value specified as DEFAULT  \n * Primary Key:  Allows any of the previous specified columns to be set as the\n   table's primary key.\n\nMySQL tables can have a 'storage engine' specified via ENGINE=[engine_type],\nwhich determines the core logic of how the table will interpret data. Leaving\nthis blank defaults to InnoDB and is almost certainly fine to be left alone. In\ncase you're interested, you can find more about MySQL engines here\n[https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html].\n\nHere's an example of what an actual CREATE TABLE  query would look like:\n\nCREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nManaging Keys for Existing Tables\nIf we don't specify our keys at table creation time, we can always do so after\nthe fact. SQL tables can accept the following key types:\n\n * Primary Key:  One or more fields/columns that uniquely identify a record in\n   the table. It can not accept null, duplicate values.\n * Candidate Key:  Candidate keys are kind of like groups of non-committed\n   Primary Keys; these keys only accept unique values, and could potentially  be\n   used in the place of a Primary Key if need be, but are not actual Primary\n   Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.\n * Alternate Key:  Refers to a single Candidate Key (an alternative which can\n   satisfy the duty of a Primary Key id need be).\n * Composite/Compound Key:  Defined by combing the values of multiple columns;\n   the sum of which will always produce a unique value. There can be multiple\n   Candidate Keys in one table. Each Candidate Key can work as Primary Key.\n * Unique Key:  A set of one or more fields/columns of a table that uniquely\n   identify a record in a database table. Similar to Primary key, but it can\n   accept only one null value, and it can not have duplicate values.\n * Foreign Key: Foreign keys denote fields that serve as another table's \n   Primary key. Foreign keys are useful for building relationships between\n   tables. While a foreign key is required in the parent table where they are\n   primary, foreign keys can be null or empty in the tables intended to relate\n   to the other table.\n\nLet's look at an example query where we add a key to a table and dissect the\npieces:\n\nALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n\n\nALTER TABLE  is used to make any changes to a table's structure, whether that be\nmodifying columns or keys.\n\nIn this example, we ADD  a key which happens to be a FOREIGN KEY. While keys\nalways refer to columns, keys themselves must have names of their own to\ndistinguish the column's data and a key's conceptual logic. We name our key \nforeign_key_name  and specify which column the key will act on with \n(column_name). Because this is a foreign key, we need to specify which table's \nprimary key  we want this to be associated with. REFERENCES\nparent_table(primary_key_column)  is stating that the foreign key in this table\ncorresponds to values held in a column named primary_key_column, in a table\nnamed parent_table.\n\nThe statements ON DELETE  and ON UPDATE  are actions which take place if the\nparent table's primary key is deleted or updated, respectively. ON DELETE\nCASCADE  would result in our tables foreign key being deleted if the\ncorresponding primary key were to disappear.\n\nAdding Columns\nAdding columns follows the same syntax we used when creating tables. An\ninteresting additional feature is the ability to place the new column before or\nafter preexisting columns:\n\nALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n\n\nWhen referencing tables in PostgreSQL databases, we must specify the schema\nbelongs to. Thus, ALTER TABLE table_name  becomes ALTER TABLE\nschema_name.table_name. This applies to any time we reference tables, including\nwhen we create and delete tables.Pop Quiz\nThe below statement uses elements of everything we've learned about modifying\nand creating table structures thus far. Can you discern what is happening here?\n\nCREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n\n\nDropping Data\nDANGER ZONE: this is where we can start to mess things up. Dropping columns or\ntables results in a complete loss of data: whenever you see the word \"drop,\" be\nscared.\n\nIf you're sure you know what you're doing and would like to remove a table\ncolumn, this can be done as such:\n\nALTER TABLE table\nDROP column;\n\n\nDropping a table destroys the table structure as well as all data within it:\n\nDROP TABLE table_name;\n\n\nTruncating a table, on the other hand, will purge the table of data but retain\nthe table itself:\n\nTRUNCATE TABLE table_name;\n\n\nDrop Foreign Key\nLike tables and columns, we can drop keys as well:\n\nALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n\n\nThis can also be handed by dropping CONSTRAINT:\n\nALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n\n\nWorking with Views (Specific to PostgreSQL)\nLastly, let's explore the act of creating views. There are three types of views\nPostgreSQL can handle:\n\n * Simple Views: Virtual tables which represent data of underlying tables.\n   Simple views are automatically updatable: the system will allow INSERT,\n   UPDATE and DELETE statements to be used on the view in the same way as on a\n   regular table.\n * Materialized Views: PostgreSQL extends the view concept to a next level that\n   allows views to store data 'physically', and we call those views are\n   materialized views. A materialized view caches the result of a complex query\n   and then allow you to refresh the result periodically.\n * Recursive Views: Recursive views are a bit difficult to explain without\n   delving deep into the complicated (but cool!) functionality of recursive\n   reporting. I won't get into the details, but these views are able to\n   represent relationships which go multiple layers deep. Here's a quick taste,\n   if you;re curious:\n\nSample RECURSIVE  query:\n\nWITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' > ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n\n\nOutput:\n\n employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North > Megan Berry\n           3 | Michael North > Sarah Berry\n           4 | Michael North > Zoe Black\n           5 | Michael North > Tim James\n           6 | Michael North > Megan Berry > Bella Tucker\n           7 | Michael North > Megan Berry > Ryan Metcalfe\n           8 | Michael North > Megan Berry > Max Mills\n           9 | Michael North > Megan Berry > Benjamin Glover\n          10 | Michael North > Sarah Berry > Carolyn Henderson\n          11 | Michael North > Sarah Berry > Nicola Kelly\n          12 | Michael North > Sarah Berry > Alexandra Climo\n          13 | Michael North > Sarah Berry > Dominic King\n          14 | Michael North > Zoe Black > Leonard Gray\n          15 | Michael North > Zoe Black > Eric Rampling\n          16 | Michael North > Megan Berry > Ryan Metcalfe > Piers Paige\n          17 | Michael North > Megan Berry > Ryan Metcalfe > Ryan Henderson\n          18 | Michael North > Megan Berry > Max Mills > Frank Tucker\n          19 | Michael North > Megan Berry > Max Mills > Nathan Ferguson\n          20 | Michael North > Megan Berry > Max Mills > Kevin Rampling\n(20 rows)\n\n\nCreating a View\nCreating a simple view is as simple as writing a standard query! All that is\nrequired is the addition of CREATE VIEW view_name AS  before the query, and this\nwill create a saved place for us to always come back and reference the results\nof this query:\n\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n\n\nGet Out There and Start SQLing\nI highly encourage anybody to get in the habit of always writing SQL queries by\nhand. With the right GUI, autocompletion can be your best friend.\n\nExplicitly forcing one's self to write queries instead of copy & pasting\nanything forces us to come to realizations, such as SQL's order of operations.\nIndeed, this query holds the correct syntax...\n\nSELECT *\nFROM table_name\nWHERE column_name = 'Value';\n\n\n...Whereas this one does not:\n\nSELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n\n\nGrasping the subtleties of SQL is the difference between being blazing fast and\nmostly clueless. The good news is, you’ll start to find that these concepts\naren’t nearly as daunting as they may have once seemed, so the track from ‘bad\ndata engineer’ to ‘expert’ is an easy win that would be foolish not to take.\n\nStick around for next time where we actually work with data in SQL: The Sequel,\nrated PG-13.","html":"<p>SQL: we all pretend to be experts at it, and mostly get away with it thanks to StackOverflow. Paired with our vast experience of learning how to code in the 90s, our field work of PHPMyAdmin and LAMP stacks basically makes us experts. Go ahead and chalk up a win for your resume.</p><p>SQL has been around longer than our careers have, so why start a series on it <em>now?</em> Surely there’s sufficient enough documentation that we can Google the specifics whenever the time comes for us to write a query? That, my friends, is precisely the problem. Regardless of what tools we have at our disposable, some skills are better learned and practiced by heart. SQL is one of those skills.</p><p>Sure, SQLAlchemy or similar ORMs might protect us here-and-there from writing raw queries. Considering SQL is just one of many query languages we'll use regularly (in addition to NoSQL, GraphQL, JQL, etc.), is becoming a SQL expert really that critical? In short, yes: relational databases are not only here to stay, but <em>thinking</em> in queries as a second language solidifies one's understanding of the fine details of data. <a href=\"https://towardsdatascience.com/python-vs-sql-comparison-for-data-pipelines-8ca727b34032\">Marc Laforet</a> recently published a Medium post which drives home just how important leaning on SQL is:</p><blockquote>\n<p>What’s even more interesting is that when these transformation scripts were applied to the 6.5 GB dataset, python completely failed. Out of 3 attempts, python crashed 2 times and my computer completely froze the 3rd time… while SQL took 226 seconds.</p>\n</blockquote>\n<p>Keeping logic out of our apps and pipelines and in SQL results in exponentially faster execution, while also being more readable and universally understood than whatever we’d write in our language of choice. The lower down we can push application logic in our stack, the better. This is why I’d much prefer to see the datasphere saturated with SQL tutorials as opposed to Pandas tutorials.</p><h2 id=\"relational-database-terminology\">Relational Database Terminology</h2><p>I hate it when informational material kicks off with covering obvious terminology definitions. Under normal circumstances, I find this to be cliche, unhelpful, and damaging to an author's credibility; but these aren't normal circumstances. In SQL, vocabulary commonly has multiple meanings depending on context, or even which flavor database you're using. Given this fact, it's entirely possible (and common) for individuals to rack up experience with relational databases while completely misinterpreting fundamental concepts. Let's make sure that doesn't happen:</p><ul>\n<li><strong>Databases</strong>: Every Database instance is separated at the highest level into <em>databases</em>. Yes, a database is a collection of databases - we're already off to a great start.</li>\n<li><strong>Schemas</strong>: In PostgreSQL (and other databases), a <em>schema</em> is a grouping of tables and other objects, including views, relations, etc. A schema is a way of organizing data. Schemas imply that all the data belonging to it is at some form related, even if only by concept. Note that the term <em>schema</em> is sometimes used to describe other concepts depending on the context.</li>\n<li><strong>Tables</strong>: The meat and potatos of relational databases. Tables consist of rows and columns which hold our sweet, sweet data. Columns are best thought of as 'attributes', whereas rows are entries which consist of values for said attributes. All values in a column must share the same data type.\n<ul>\n<li><strong>Keys</strong>: Keys are used to help us organize and optimize data, as well as place certain constraints on data coming in (for example, email addresses of user accounts must be <em>unique</em>). Keys can also help us keep count of our entries, ensure automatically unique values, and provide a bridge to link multiple tables of data.\n<ul>\n<li><strong>Primary keys</strong>:  Identification tags for each row of data. The primary key is different for every record in the relational database; values must be provided, and they must be unique between rows.</li>\n<li><strong>Foreign keys</strong>: Enable data searches and manipulation between the primary database table and other related databases.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objects</strong>: A blanket term for anything (including relations) that exist in a schema (somewhat PostgreSQL-specific).\n<ul>\n<li><strong>Views (PostgreSQL)</strong>: Views display data in a fashion similar to tables, with the difference that views do not <em>store</em> data. Views are a snapshot of data pulled from other tables in the form of a query; a good way to think about views is to consider them to be 'virtual tables.'</li>\n<li><strong>Functions  (PostgreSQL)</strong>: Logic for interacting with data saved for the purpose of being reused.</li>\n</ul>\n</li>\n</ul>\n<div class=\"protip\">\nIn MySQL, a <strong>schema</strong> is synonymous with a <strong>database</strong>. These keywords can even be swapped to use SCHEMA and DATABASE interchangably in MySQL. Thus, using <code>CREATE SCHEMA</code> acheives the same effect as instead of <code>CREATE DATABASE</code>.   \n</div><h2 id=\"navigating-and-creating-databases\">Navigating and Creating Databases</h2><p>We've got to start somewhere, so it might as well be with database management. Admittedly, this will be the most useless of the things we'll cover. The act of navigating databases is best suited for a GUI.</p><h3 id=\"show-databases\">Show Databases</h3><p>If you access your database via command line shell (for some reason), the first logical thing to do is to list the available databases:</p><pre><code class=\"language-sql\">SHOW DATABASES;\n \n+--------------------+\n| Database           |\n+--------------------+\n| classicmodels      |\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n</code></pre>\n<h3 id=\"use-database\">USE Database</h3><p>Now that we've listed the possible databases we can connect to, we can explore what each of these contains. To do this, we have to specify which database we want to connect to, AKA \"use.\" </p><pre><code class=\"language-sql\">db&gt; USE database_name;\nDatabase changed\n</code></pre>\n<h3 id=\"create-database\">Create Database</h3><p>Creating databases is straightforward. Be sure to pay attention to the <em>character set</em> when creating a database: this will determine which types of characters your database will be able to accept. For example, if we try to insert special encoded characters into a simple UTF-8 database, those characters won’t turn out as we’d expect.</p><pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS database_name\nCHARACTER SET utf-8\n[COLLATE collation_name]\n</code></pre>\n<p>Bonus: here's the shorthand for creating a database and then showing the result:</p><pre><code class=\"language-sql\">SHOW CREATE DATABASE database_name;\n</code></pre>\n<h2 id=\"creating-and-modifying-tables\">Creating and Modifying Tables</h2><p>Creating tables via SQL syntax can be critical when automating data imports. When creating a table, we also set the column names, types, and keys:</p><pre><code>CREATE TABLE [IF NOT EXISTS] table_name (\n   column_name_1 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   column_name_2 [COLUMN_DATA_TYPE] [KEY_TYPE] [KEY_ATTRIBUTES] DEFAULT [DEFAULT_VALUE],\n   PRIMARY KEY (column_name_1)\n) ENGINE=[ENGINE_TYPE];</code></pre><p>We can specify <code>IF NOT EXISTS</code> when creating our table if we'd like to include validation in our query. When present, the table will only be created if a table of the specified name does not exist.</p><p>When creating each of our columns, there are a number of things we can specify per-column:</p><ul><li><strong>Data Type (required):</strong> The data which can be saved to cells of this column (such as INTEGER, TEXT, etc).</li><li><strong>Key Type:</strong> Creates a key for the column.</li><li><strong>Key Attributes:</strong> Any key-related attributes, such as auto-incrementing.</li><li><strong>Default:</strong> If rows are created in the table without values passed to the current column, the value specified as <code>DEFAULT</code> </li><li><strong>Primary Key:</strong> Allows any of the previous specified columns to be set as the table's primary key.</li></ul><p>MySQL tables can have a 'storage engine' specified via <code>ENGINE=[engine_type]</code>, which determines the core logic of how the table will interpret data. Leaving this blank defaults to InnoDB and is almost certainly fine to be left alone. In case you're interested, you can find more about MySQL engines <a href=\"https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html\">here</a>.</p><p>Here's an example of what an actual <code>CREATE TABLE</code> query would look like:</p><pre><code>CREATE TABLE IF NOT EXISTS awards (\n   id INTEGER PRIMARY KEY AUTO_INCREMENT,\n   recipient TEXT NOT NULL,\n   award_name TEXT DEFAULT 'Grammy',\n   PRIMARY KEY (id)\n) ENGINE=INNODB;</code></pre><h3 id=\"managing-keys-for-existing-tables\">Managing Keys for Existing Tables</h3><p>If we don't specify our keys at table creation time, we can always do so after the fact. SQL tables can accept the following key types:</p><ul><li><strong>Primary Key:</strong> One or more fields/columns that uniquely identify a record in the table. It can not accept null, duplicate values.</li><li><strong>Candidate Key:</strong> Candidate keys are kind of like groups of non-committed Primary Keys; these keys only accept unique values, and <em>could potentially</em> be used in the place of a Primary Key if need be, but are not actual Primary Keys. Unlike Primary Keys, multiple Candidate Keys may exist per table.</li><li><strong>Alternate Key:</strong> Refers to a single Candidate Key (an alternative which can satisfy the duty of a Primary Key id need be).</li><li><strong>Composite/Compound Key:</strong> Defined by combing the values of multiple columns; the sum of which will always produce a unique value. There can be multiple Candidate Keys in one table. Each Candidate Key can work as Primary Key.</li><li><strong>Unique Key:</strong> A set of one or more fields/columns of a table that uniquely identify a record in a database table. Similar to Primary key, but it can accept only one null value, and it can not have duplicate values.</li><li><strong>Foreign Key: </strong>Foreign keys denote fields that serve as <em>another table's</em> Primary key. Foreign keys are useful for building relationships between tables. While a foreign key is required in the parent table where they are primary, foreign keys can be null or empty in the tables intended to relate to the other table.</li></ul><p>Let's look at an example query where we add a key to a table and dissect the pieces:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD FOREIGN KEY foreign_key_name (column_name)\nREFERENCES parent_table(columns)\nON DELETE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\nON UPDATE { NO ACTION | CASCADE | SET NULL | SET DEFAULT }\n</code></pre>\n<p><code>ALTER TABLE</code> is used to make any changes to a table's structure, whether that be modifying columns or keys.</p><p>In this example, we <code>ADD</code> a key which happens to be a <code>FOREIGN KEY</code>. While keys always refer to columns, keys themselves must have names of their own to distinguish the column's data and a key's conceptual logic. We name our key <code>foreign_key_name</code> and specify which column the key will act on with <code>(column_name)</code>. Because this is a foreign key, we need to specify which table's <em>primary key</em> we want this to be associated with. <code>REFERENCES parent_table(primary_key_column)</code> is stating that the foreign key in this table corresponds to values held in a column named <code>primary_key_column</code>, in a table named <code>parent_table</code>.</p><p>The statements <code>ON DELETE</code> and <code>ON UPDATE</code> are actions which take place if the parent table's primary key is deleted or updated, respectively. <code>ON DELETE CASCADE</code> would result in our tables foreign key being deleted if the corresponding primary key were to disappear.</p><h3 id=\"adding-columns\">Adding Columns</h3><p>Adding columns follows the same syntax we used when creating tables. An interesting additional feature is the ability to place the new column before or after preexisting columns:</p><pre><code class=\"language-sql\">ALTER TABLE table_name\nADD COLUMN column_name [DATA_TYPE] [FIRST|AFTER existing_column];\n</code></pre>\n<div class=\"protip\">\nWhen referencing tables in PostgreSQL databases, we must specify the schema belongs to. Thus, <code>ALTER TABLE table_name</code> becomes <code>ALTER TABLE schema_name.table_name</code>. This applies to any time we reference tables, including when we create and delete tables.\n</div><h3 id=\"pop-quiz\">Pop Quiz</h3><p>The below statement uses elements of everything we've learned about modifying and creating table structures thus far. Can you discern what is happening here?</p><pre><code class=\"language-sql\">CREATE TABLE vendors(\n    vdr_id int not null auto_increment primary key,\n    vdr_name varchar(255)\n)ENGINE=InnoDB;\n \nALTER TABLE products \nADD COLUMN vdr_id int not null AFTER cat_id;\n\nALTER TABLE products\nADD FOREIGN KEY fk_vendor(vdr_id)\nREFERENCES vendors(vdr_id)\nON DELETE NO ACTION\nON UPDATE CASCADE;\n</code></pre>\n<h2 id=\"dropping-data\">Dropping Data</h2><p>DANGER ZONE: this is where we can start to mess things up. Dropping columns or tables results in a complete loss of data: whenever you see the word \"drop,\" be scared.</p><p>If you're sure you know what you're doing and would like to remove a table column, this can be done as such:</p><pre><code class=\"language-sql\">ALTER TABLE table\nDROP column;\n</code></pre>\n<p>Dropping a table destroys the table structure as well as all data within it:</p><pre><code class=\"language-sql\">DROP TABLE table_name;\n</code></pre>\n<p>Truncating a table, on the other hand, will purge the table of data but retain the table itself:</p><pre><code class=\"language-sql\">TRUNCATE TABLE table_name;\n</code></pre>\n<h3 id=\"drop-foreign-key\">Drop Foreign Key</h3><p>Like tables and columns, we can drop keys as well:</p><pre><code class=\"language-sql\">ALTER TABLE table_name \nDROP FOREIGN KEY constraint_name;\n</code></pre>\n<p>This can also be handed by dropping CONSTRAINT:</p><pre><code class=\"language-sql\">ALTER TABLE public.jira_epiccolors\nDROP CONSTRAINT jira_epiccolors_pkey;\n</code></pre>\n<h2 id=\"working-with-views-specific-to-postgresql-\">Working with Views (Specific to PostgreSQL)</h2><p>Lastly, let's explore the act of creating views. There are three types of views PostgreSQL can handle:</p><ul>\n<li><strong>Simple Views</strong>: Virtual tables which represent data of underlying tables. Simple views are automatically updatable: the system will allow INSERT, UPDATE and DELETE statements to be used on the view in the same way as on a regular table.</li>\n<li><strong>Materialized Views</strong>: PostgreSQL extends the view concept to a next level that allows views to store data 'physically', and we call those views are materialized views. A materialized view caches the result of a complex query and then allow you to refresh the result periodically.</li>\n<li><strong>Recursive Views</strong>: Recursive views are a bit difficult to explain without delving deep into the complicated (but cool!) functionality of recursive reporting. I won't get into the details, but these views are able to represent relationships which go multiple layers deep. Here's a quick taste, if you;re curious:</li>\n</ul>\n<p><strong>Sample </strong><code>RECURSIVE</code> <strong>query:</strong></p><pre><code class=\"language-sql\">WITH RECURSIVE reporting_line AS (\n SELECT\n employee_id,\n full_name AS subordinates\n FROM\n employees\n WHERE\n manager_id IS NULL\n UNION ALL\n SELECT\n e.employee_id,\n (\n rl.subordinates || ' &gt; ' || e.full_name\n ) AS subordinates\n FROM\n employees e\n INNER JOIN reporting_line rl ON e.manager_id = rl.employee_id\n) SELECT\n employee_id,\n subordinates\nFROM\n reporting_line\nORDER BY\n employee_id;\n</code></pre>\n<p><strong>Output:</strong></p><pre><code class=\"language-shell\"> employee_id |                         subordinates\n-------------+--------------------------------------------------------------\n           1 | Michael North\n           2 | Michael North &gt; Megan Berry\n           3 | Michael North &gt; Sarah Berry\n           4 | Michael North &gt; Zoe Black\n           5 | Michael North &gt; Tim James\n           6 | Michael North &gt; Megan Berry &gt; Bella Tucker\n           7 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe\n           8 | Michael North &gt; Megan Berry &gt; Max Mills\n           9 | Michael North &gt; Megan Berry &gt; Benjamin Glover\n          10 | Michael North &gt; Sarah Berry &gt; Carolyn Henderson\n          11 | Michael North &gt; Sarah Berry &gt; Nicola Kelly\n          12 | Michael North &gt; Sarah Berry &gt; Alexandra Climo\n          13 | Michael North &gt; Sarah Berry &gt; Dominic King\n          14 | Michael North &gt; Zoe Black &gt; Leonard Gray\n          15 | Michael North &gt; Zoe Black &gt; Eric Rampling\n          16 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Piers Paige\n          17 | Michael North &gt; Megan Berry &gt; Ryan Metcalfe &gt; Ryan Henderson\n          18 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Frank Tucker\n          19 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Nathan Ferguson\n          20 | Michael North &gt; Megan Berry &gt; Max Mills &gt; Kevin Rampling\n(20 rows)\n</code></pre>\n<h3 id=\"creating-a-view\">Creating a View</h3><p>Creating a simple view is as simple as writing a standard query! All that is required is the addition of <code>CREATE VIEW view_name AS</code> before the query, and this will create a saved place for us to always come back and reference the results of this query:</p><pre><code class=\"language-sql\">CREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = 'Comedy';\n</code></pre>\n<h2 id=\"get-out-there-and-start-sqling\">Get Out There and Start SQLing</h2><p>I highly encourage anybody to get in the habit of <em>always </em>writing SQL queries by hand. With the right GUI, autocompletion can be your best friend.</p><p>Explicitly forcing one's self to write queries instead of copy &amp; pasting anything forces us to come to realizations, such as SQL's order of operations. Indeed, this query holds the correct syntax...</p><pre><code class=\"language-sql\">SELECT *\nFROM table_name\nWHERE column_name = 'Value';\n</code></pre>\n<p>...Whereas this one does not:</p><pre><code class=\"language-sql\">SELECT *\nWHERE column_name = 'Value'\nFROM table_name;\n</code></pre>\n<p>Grasping the subtleties of SQL is the difference between being blazing fast and mostly clueless. The good news is, you’ll start to find that these concepts aren’t nearly as daunting as they may have once seemed, so the track from ‘bad data engineer’ to ‘expert’ is an easy win that would be foolish not to take.</p><p>Stick around for next time where we actually work with data in <strong>SQL: The Sequel</strong>, rated PG-13.</p>","url":"https://hackersandslackers.com/welcome-to-sql-modifying-databases-and-tables/","uuid":"fe99e822-f21a-432c-8bbf-4d399e575570","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5bb0ec7999ff33f06876e1"}},{"node":{"id":"Ghost__Post__5c5a3e362c71af62216fd45e","title":"Manage Database Models with Flask-SQLAlchemy","slug":"manage-database-models-with-flask-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","custom_excerpt":"Connect your Flask app to a database using Flask-SQLAlchemy.","created_at_pretty":"06 February, 2019","published_at_pretty":"06 February, 2019","updated_at_pretty":"03 April, 2019","created_at":"2019-02-05T20:53:58.000-05:00","published_at":"2019-02-06T08:00:00.000-05:00","updated_at":"2019-04-03T11:38:02.000-04:00","meta_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","meta_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","og_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","og_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","twitter_description":"Connect your Flask application to a database using the Flask-SQLAlchemy library. The most important Flask library you'll ever use.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/flask-sqlalchemy2.jpg","twitter_title":"Manage Database Models with Flask-SQLAlchemy | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Python’s fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Python’s fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"By now you're surely familiar with the benefits of Python's core SQLAlchemy\nlibrary\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/]:\nthe all-in-one solution for basically anything database related. Like most major\nPython libraries, SQLAlchemy has been ported into a version specifically\ncompatible with Flask, aptly named Flask-SQLAlchemy.\n\nSimilar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us\nto modify application data by easily creating defined models. Regardless of what\nyour database of choice might be, Flask-SQLAlchemy will ensure that the models\nwe create in Python will translate to the syntax of our chosen database. Given\nthe ease-of-use and one-size-fits-all  nature of Flask-SQLAlchemy, it's no\nwonder that the library has been the de facto database library of choice for\nFlask since the very beginning (seriously, is there even another option?)\n\nConfiguring Flask-SQLAlchemy For Your Application\nThere are a few essential configuration variables we need to set upfront before\ninteracting with our database. As is standard, we'll be using a class defined in\n config.py  to handle our Flask config:\n\nimport os\n\n\nclass Config:\n    \"\"\"Set Flask configuration vars from .env file.\"\"\"\n    \n    # General\n    TESTING = os.environ[\"TESTING\"]\n    FLASK_DEBUG = os.environ[\"FLASK_DEBUG\"]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(\"SQLALCHEMY_DATABASE_URI\")\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(\"SQLALCHEMY_TRACK_MODIFICATIONS\")\n\n\nLet's break these down:\n\n * SQLALCHEMY_DATABASE_URI: the connection string we need to connect to our\n   database. This follows the standard convention: \n   [db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]\n * SQLALCHEMY_ECHO: When set to 'True', Flask-SQLAlchemy will log all database\n   activity to Python's stderr for debugging purposes.\n * SQLALCHEMY_TRACK_MODIFICATIONS: Honestly, I just always set this to 'False,'\n   otherwise an obnoxious warning appears every time you run your app reminding\n   you that this option takes a lot of system resources.\n\nThose are the big ones we should worry about. If you're into some next-level\ndatabase shit, there are a few other pro-mode configuration variables which you\ncan find here [http://flask-sqlalchemy.pocoo.org/2.3/config/].\n\nBy using the exact naming conventions for the variables above, simply having\nthem in our config file will automatically configure our database connections\nfor us. We will never have to create engines, sessions, or connections.\nInitiating Flask-SQLAlchemy With Our App\nAs always, we're going to use the Flask Application Factory method\n[https://hackersandslackers.com/structuring-your-flask-app/]  for initiating our\napp. If you're unfamiliar with the term, you're going to find this tutorial to\nbe confusing and pretty much useless.\n\nThe most basic __init__.py  file for Flask applications using Flask-SQLAlchemy\nshould look like this:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n\n\nNote the presence of db  and its location: this our database object being set as\na global  variable outside of create_app(). Inside of create_app(), on the other\nhand, contains the line db.init_app(app). Even though we've set our db object\nglobally, this means nothing until we initialize it after creating our\napplication. We accomplish this by calling init_app()  within create_app(), and\npassing our app as the parameter. Within the actual 'application context' is\nwhere we'll call create_all(), which we'll cover in a bit.\n\nIf that last paragraph sounded like total gibberish to you, you are not alone. \nThe Flask Application Factory is perhaps one of the most odd and poorly\nexplained concepts in Python software development- my best advice is to not\nbecome frustrated, take the copy + paste code above, and blindly accept the\nspoon-fed nonsense enough times until it becomes second nature. That's what I\ndid, and even as I worked through this tutorial, I still  came across obnoxious\nquirks that caught me off-guard.\n\nTake note of import we make inside of the application context called routes.\nThis is one of two files we haven't written just yet: once we create them, our\napplication file structure will look something like this:\n\nmy-app\n├── /application\n│   ├── __init__.py\n│   ├── routes.py\n│   ├── models.py\n├── .env\n├── config.py\n└── wsgi.py\n\n\nCreating Database Models\nCreate a models.py  file in our application directory. Here we'll import the db \nobject that we created in __init__.py. Now we can create database models by\ndefining classes in this file.\n\nA common example would be to start with a User  model. The first variable we\ncreate is __tablename__, which will correspond to the name of the SQL table new\nusers will be saved. Each additional variable we create within this model class\nwill correspond a column in the database:\n\nfrom . import db\n\n\nclass User(db.Model):\n    \"\"\"Model for user accounts.\"\"\"\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n\nEach \"column\" accepts the following attributes:\n\n * Data Type:  Accepts one of the following: String(size), Text, DateTime, Float\n   , Boolean, PickleType, or LargeBinary.\n * primary_key: Whether or not the column should serve as the primary key.\n * unique: Whether or not to enforce unique values for the column.\n * nullable: Denotes required fields.\n\nWith our first model created, you're already way closer to interacting with your\ndatabase than you might think.\n\nCreating Our First Entry\nLet's create a user in our routes.py  file.\n\nfrom flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(\"User created!\")\n\n\nCheck out how easy this is! All it takes to create a user is create an instance\nof the User  class from models.py, add it to our session via \ndb.session.add(new_user), and commit the changes with db.session.commit()! Let's\nsee what happens when we run this app:\n\nUser Created!\n\n\nThat's what we like to see! If we access our database at this point, we can see\nthat this exact record was created in our users  table.\n\nQuerying Our New Data\nCreating information is dope, but how can we confirm it exists? I've added a few\nthings to routes.py  to show us what's up:\n\nfrom flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    \"\"\"Endpoint to create a user.\"\"\"\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=\"In West Philadelphia born and raised, on the playground is where I spent most of my days\",\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=\"Show Users\")\n\n\n  The statement User.query.all()  will return all instances of User  in our\ndatabase. I created a Jinja template to show us all records nicely:\n\n{% extends \"layout.html\" %}\n\n{% block content %}\n  {% for user in users %}\n    <ul id=\"user.username\">\n      <li>Username: {{ user.username }}</li>\n      <li>Email: {{ user.email }}</li>\n      <li>Created: {{ user.created }}</li>\n      <li>Bio: {{ user.bio }}</li>\n      <li>Admin: {{ user.admin }}</li>\n    </ul>\n  {% endfor %}\n{% endblock %}\n\n\nThus, our app gives us:\n\nWe have liftoff!So we can get a single user, but what about a whole table full\nof users? Well, all we need to do is keep changing the username and email\naddress (our unique keys, to avoid a clash) when firing up the app, and each\ntime it runs, it'll create a new user. Here's what comes back after running the\napp a few times with different values:\n\nI Can't Believe It's Not Error Messages.™Here, Take All My Stuff\nSure, Flask-SQLAlchemy is great once you get going, but as we've already seen\n\"getting set up\" isn't always a walk in the park. This is one of those things\nthat always seems to be wrong no matter how many times you've done it from\nmemory.\n\nAs a parting gift, I've put the source for this tutorial up on Github\n[https://github.com/toddbirchard/flasksqlalchemy-tutorial]  for you to treasure\nand enjoy. No seriously, take it. Get it away from me. I'm done with Flask for\ntoday. I need to go play some Rocket League.\n\nPS: Add me on PSN","html":"<p>By now you're surely familiar with the benefits of Python's <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">core SQLAlchemy library</a>: the all-in-one solution for basically anything database related. Like most major Python libraries, SQLAlchemy has been ported into a version specifically compatible with Flask, aptly named <strong>Flask-SQLAlchemy</strong>.</p><p>Similar to the core SQLAlchemy package, Flask-SQLAlchemy provides an ORM for us to modify application data by easily creating defined models. Regardless of what your database of choice might be, Flask-SQLAlchemy will ensure that the models we create in Python will translate to the syntax of our chosen database. Given the ease-of-use and one-size-fits-all  nature of Flask-SQLAlchemy, it's no wonder that the library has been the de facto database library of choice for Flask since the very beginning (seriously, is there even another option?)</p><h2 id=\"configuring-flask-sqlalchemy-for-your-application\">Configuring Flask-SQLAlchemy For Your Application</h2><p>There are a few essential configuration variables we need to set upfront before interacting with our database. As is standard, we'll be using a class defined in <code>config.py</code> to handle our Flask config:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\n\n\nclass Config:\n    &quot;&quot;&quot;Set Flask configuration vars from .env file.&quot;&quot;&quot;\n    \n    # General\n    TESTING = os.environ[&quot;TESTING&quot;]\n    FLASK_DEBUG = os.environ[&quot;FLASK_DEBUG&quot;]\n\n    # Database\n    SQLALCHEMY_DATABASE_URI = os.environ.get(&quot;SQLALCHEMY_DATABASE_URI&quot;)\n    SQLALCHEMY_TRACK_MODIFICATIONS = os.environ.get(&quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's break these down:</p><ul><li><code>SQLALCHEMY_DATABASE_URI</code>: the connection string we need to connect to our database. This follows the standard convention: <code>[db_type]+[db_connector]://[username]:[password]@[host]:[port]/[db_name]</code></li><li><code>SQLALCHEMY_ECHO</code>: When set to 'True', Flask-SQLAlchemy will log all database activity to Python's stderr for debugging purposes.</li><li><code>SQLALCHEMY_TRACK_MODIFICATIONS</code>: Honestly, I just always set this to 'False,' otherwise an obnoxious warning appears every time you run your app reminding you that this option takes a lot of system resources.</li></ul><p>Those are the big ones we should worry about. If you're into some next-level database shit, there are a few other pro-mode configuration variables which you can find <a href=\"http://flask-sqlalchemy.pocoo.org/2.3/config/\">here</a>.</p><!--kg-card-begin: html--><div class=\"protip\">By using the exact naming conventions for the variables above, simply having them in our config file will automatically configure our database connections for us. We will never have to create engines, sessions, or connections.</div><!--kg-card-end: html--><h2 id=\"initiating-flask-sqlalchemy-with-our-app\">Initiating Flask-SQLAlchemy With Our App</h2><p>As always, we're going to use the <a href=\"https://hackersandslackers.com/structuring-your-flask-app/\">Flask Application Factory method</a> for initiating our app. If you're unfamiliar with the term, you're going to find this tutorial to be confusing and pretty much useless.</p><p>The most basic <code>__init__.py</code> file for Flask applications using Flask-SQLAlchemy should look like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    db.init_app(app)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Imports\n        from . import routes\n        \n        # Create tables for our models\n        db.create_all()\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>Note the presence of <code>db</code> and its location: this our database object being set as a <em>global</em> variable outside of <code>create_app()</code>. Inside of <code>create_app()</code>, on the other hand, contains the line <code>db.init_app(app)</code>. Even though we've set our db object globally, this means nothing until we initialize it after creating our application. We accomplish this by calling <code>init_app()</code> within <code>create_app()</code>, and passing our app as the parameter. Within the actual 'application context' is where we'll call <code>create_all()</code>, which we'll cover in a bit.</p><p>If that last paragraph sounded like total gibberish to you, <em>you are not alone.</em> The Flask Application Factory is perhaps one of the most odd and poorly explained concepts in Python software development- my best advice is to not become frustrated, take the copy + paste code above, and blindly accept the spoon-fed nonsense enough times until it becomes second nature. That's what I did, and even as I worked through this tutorial, I <em>still</em> came across obnoxious quirks that caught me off-guard.</p><p>Take note of import we make inside of the application context called <strong>routes</strong>. This is one of two files we haven't written just yet: once we create them, our application file structure will look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-app\n├── /application\n│   ├── __init__.py\n│   ├── routes.py\n│   ├── models.py\n├── .env\n├── config.py\n└── wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-database-models\">Creating Database Models</h2><p>Create a <code>models.py</code> file in our application directory. Here we'll import the <code>db</code> object that we created in <code>__init__.py</code>. Now we can create database models by defining classes in this file.</p><p>A common example would be to start with a <strong>User</strong> model. The first variable we create is <code>__tablename__</code>, which will correspond to the name of the SQL table new users will be saved. Each additional variable we create within this model class will correspond a column in the database:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from . import db\n\n\nclass User(db.Model):\n    &quot;&quot;&quot;Model for user accounts.&quot;&quot;&quot;\n\n    __tablename__ = 'users'\n    id = db.Column(db.Integer,\n                   primary_key=True\n                   )\n    username = db.Column(db.String(64),\n                         index=False,\n                         unique=True,\n                         nullable=False\n                         )\n    email = db.Column(db.String(80),\n                      index=True,\n                      unique=True,\n                      nullable=False\n                      )\n    created = db.Column(db.DateTime,\n                        index=False,\n                        unique=False,\n                        nullable=False\n                        )\n    bio = db.Column(db.Text,\n                    index=False,\n                    unique=False,\n                    nullable=True\n                    )\n    admin = db.Column(db.Boolean,\n                      index=False,\n                      unique=False,\n                      nullable=False\n                      )\n    \n    def __repr__(self):\n        return '&lt;User {}&gt;'.format(self.username)\n</code></pre>\n<!--kg-card-end: markdown--><p>Each \"column\" accepts the following attributes:</p><ul><li><strong>Data Type:</strong> Accepts one of the following: <code>String(size)</code>, <code>Text</code>, <code>DateTime</code>, <code>Float</code>, <code>Boolean</code>, <code>PickleType</code>, or <code>LargeBinary</code>.</li><li><strong>primary_key</strong>: Whether or not the column should serve as the primary key.</li><li><strong>unique</strong>: Whether or not to enforce unique values for the column.</li><li><strong>nullable: </strong>Denotes required fields.</li></ul><p>With our first model created, you're already way closer to interacting with your database than you might think.</p><h2 id=\"creating-our-first-entry\">Creating Our First Entry</h2><p>Let's create a user in our <code>routes.py</code> file.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template, make_response\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    return make_response(&quot;User created!&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out how easy this is! All it takes to create a user is create an instance of the <code>User</code> class from <code>models.py</code>, add it to our session via <code>db.session.add(new_user)</code>, and commit the changes with <code>db.session.commit()</code>! Let's see what happens when we run this app:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">User Created!\n</code></pre>\n<!--kg-card-end: markdown--><p>That's what we like to see! If we access our database at this point, we can see that this exact record was created in our <em>users</em> table.</p><h2 id=\"querying-our-new-data\">Querying Our New Data</h2><p>Creating information is dope, but how can we confirm it exists? I've added a few things to <code>routes.py</code> to show us what's up:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import request, render_template\nfrom datetime import datetime as dt\nfrom flask import current_app as app\nfrom .models import db, User\n\n\n@app.route('/', methods=['GET'])\ndef entry():\n    &quot;&quot;&quot;Endpoint to create a user.&quot;&quot;&quot;\n    new_user = User(username='myuser',\n                    email='myuser@example.com',\n                    created=dt.now(),\n                    bio=&quot;In West Philadelphia born and raised, on the playground is where I spent most of my days&quot;,\n                    admin=False\n                    )\n    db.session.add(new_user)\n    db.session.commit()\n    users = User.query.all()\n    return render_template('users.html', users=users, title=&quot;Show Users&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p> The statement <code>User.query.all()</code> will return all instances of <code>User</code> in our database. I created a Jinja template to show us all records nicely:</p><!--kg-card-begin: markdown--><pre><code class=\"language-html\">{% extends &quot;layout.html&quot; %}\n\n{% block content %}\n  {% for user in users %}\n    &lt;ul id=&quot;user.username&quot;&gt;\n      &lt;li&gt;Username: {{ user.username }}&lt;/li&gt;\n      &lt;li&gt;Email: {{ user.email }}&lt;/li&gt;\n      &lt;li&gt;Created: {{ user.created }}&lt;/li&gt;\n      &lt;li&gt;Bio: {{ user.bio }}&lt;/li&gt;\n      &lt;li&gt;Admin: {{ user.admin }}&lt;/li&gt;\n    &lt;/ul&gt;\n  {% endfor %}\n{% endblock %}\n</code></pre>\n<!--kg-card-end: markdown--><p>Thus, our app gives us:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/flasksqlalchemy-test.png\" class=\"kg-image\"><figcaption>We have liftoff!</figcaption></figure><!--kg-card-end: image--><p>So we can get a single user, but what about a whole table full of users? Well, all we need to do is keep changing the username and email address (our unique keys, to avoid a clash) when firing up the app, and each time it runs, it'll create a new user. Here's what comes back after running the app a few times with different values:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-06-at-12.39.07-AM.png\" class=\"kg-image\"><figcaption>I Can't Believe It's Not Error Messages.<b>™</b></figcaption></figure><!--kg-card-end: image--><h3 id=\"here-take-all-my-stuff\">Here, Take All My Stuff</h3><p>Sure, Flask-SQLAlchemy is great once you get going, but as we've already seen \"getting set up\" isn't always a walk in the park. This is one of those things that always seems to be wrong no matter how many times you've done it from memory.</p><p>As a parting gift, I've put the source for this tutorial up <a href=\"https://github.com/toddbirchard/flasksqlalchemy-tutorial\">on Github</a> for you to treasure and enjoy. No seriously, take it. Get it away from me. I'm done with Flask for today. I need to go play some Rocket League.</p><!--kg-card-begin: html--><span style=\"color: #a7a7a7;font-style: italic;font-size:.9em;\">PS: Add me on PSN</span><!--kg-card-end: html-->","url":"https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/","uuid":"e9fdcb15-3289-472f-8892-2e01cdaced9d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c5a3e362c71af62216fd45e"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apache’s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c3d0b441719dc6b38ee53b6","title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","slug":"psycopg2-postgres-python-the-old-fashioned-way","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","custom_excerpt":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","created_at_pretty":"14 January, 2019","published_at_pretty":"15 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-14T17:20:52.000-05:00","published_at":"2019-01-15T15:57:34.000-05:00","updated_at":"2019-03-28T14:46:14.000-04:00","meta_title":"Psycopg2: PostgreSQL & Python the Old Way | Hackers and Slackers","meta_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","og_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","og_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","twitter_description":"Manage PostgreSQL database interactions in Python with the Psycopg2 library.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/psycopg2.jpg","twitter_title":"Psycopg2: PostgreSQL & Python the Old Fashioned Way","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Last time we met, we joyfully shared a little tirade\n[https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/] \nabout missing out on functionality provided to us by libraries such as \nSQLAlchemy, and the advantages of interacting with databases where ORMs are\ninvolved. I stand by that sentiment, but I’ll now directly contradict myself by\nsharing some tips on using vanilla Psycopg2  to interact with databases. \n\nWe never know when we’ll be stranded on a desert island without access to\nSQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps\nyou’re part of a development team stuck in a certain way of doing things which\ndoesn't include utilize SQLAlchemy. Whatever the situation may be, we’re here\nfor you. \n\nThe Quintessential Boilerplate\nNo matter the type of database or the library, the boilerplate code for\nconnecting to databases remains mostly the same. To some extent, this even holds\ntrue across programming languages. Let's look at a barebones example while\nignoring the library at hand:\n\nimport SomeDatabaseLibrary\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nIn the above example, we could swap SomeDatabaseLibrary  with either Psycopg2 \nor PyMySQL  just the same. If we compare this to our example with PyMySQL\n[https://hackersandslackers.com/using-pymysql/], it's easy to see that the\nbasics of utilizing connections, cursors, and the methods to close them\ntranscend libraries. If you know the basics of one, you know them all.\n\nIf you'd like to keep your connection logic separate (as I do), we can cleanly\nbreak the logic of handling connections out to a separate function. This time,\nwe'll replace SomeDatabaseLibrary  with Psycopg2  to produce some working code:\n\nimport psycopg2\n\nclass Database:\n    \"\"\"A Generic Database class.\"\"\"\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        \"\"\"Encapsulated connection management logic.\"\"\"\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(\"ERROR: Could not connect to Postgres.\")\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n\n\nPsycopg2 Extras\nPsycopg2 has many useful features via a library called psycopg2.extras\n[http://initd.org/psycopg/docs/extras.html]. My personal favorite of these\nextras is the DictCursor, which renders the rows being returned by our query as\nPython dictionaries  as opposed to lists. \n\nUsing DictCursor to Return More Useful Results\nWhen using a DictCursor, the key  is always the column name, and the value is\nthe value of that column in that particular row.\n\nTo use extras, we import psycopg2.extras.\n\nThen, we turn our attention to the following line:\n\nself.conn.cursor() as cur:\n\n\nWithin cursor, we can pass an attribute named cursor_factory   and set it as\nsuch:\n\nconn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n\n\nWhile our cursor is open, all rows returned by the query will be returned as\ndictionaries. For example, the row  in the above example will be returned as a\ndict. To demonstrate, here's what a query on this exact post  you're reading now\nlooks like when returned as a Dict:\n\n{\n    title: \"Psycopg2: Postgres & Python the Old Fashioned Way\",\n    slug: \"psycopg2-postgres-python-the-old-fashioned-way\",\n    feature_image: \"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n    status: \"draft\",\n    created_at: \"2019-01-14 22:20:52\",\n    custom_excerpt: \"Managing Postgres Database connections with Psycopg2\"\n}\n\n\nCompare this to what we would've seen had we not used DictCursor:\n\n[\"Psycopg2: Postgres & Python the Old Fashioned Way\",\n\"psycopg2-postgres-python-the-old-fashioned-way\",\n\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg\",\n\"draft\",\n\"2019-01-14 22:20:52\",\n\"Managing Postgres Database connections with Psycopg2\"]\n\n\nYes, it's a list, and thereby much less useful. Even from a readability\nstandpoint, I (the human user) have no idea what these values represent unless\ncomparing them to the table schema. Even worse would be compiling CSVs or even\nPandas Dataframes this way. When building a table made of lists, you set your\nheaders and hope that every row to come matches the number of header columns\none-to-one. Otherwise, it's entirely unclear as to which value belongs to which\ncolumn.\n\nOther Psycopg2 Extras\nThere are plenty more Psycopg2 extras where that came from; it's mostly up to\nyou to decide which are worth your while.\n\nFor example, another extra which might be of interest could be \npsycopg2.extras.LoggingConnection, useful for debugging connection statuses and\nerrors as you work through your program.\n\nThere's even a JSON Adaptation  extra, which provides support for leveraging\nJSON data in building queries:\n\ncur.execute(\"insert into mytable (jsondata) values (%s)\",\n    [Json({'a': 100})])\n\n\nI don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike\nextras I'm missing, feel free to call them out in the COMMENTS BELOW!  (Hah!\nI've always wanted to say that).\n\nA Few More Fundamental Useful Things\nSomething worth visiting is the ability to upload CSVs into Postgres to create\ntables. We can accomplish this via the built-in method copy_expert.\n\nFrom CSV to Postgres Table\nTo save a CSV to Postgres table, we need to begin with a basic SQL query saved\nin our project as a variable:\n\nCOPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n\n\nAs should be familiar, %s  represents a value we can pass in later. With this\nraw query, we're only missing two more values:\n\n * The path of our CSV file to be uploaded\n * The name of the table we'd like to upload to in Postgres\n\nCheck out how we use copy_expert  here to put it all together:\n\nsql = \"COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','\"\nfile = open('files/myfile.csv', \"r\")\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(\"truncate \" + table + \";\")\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n\n\nNotice that I opt to truncate the existing table before uploading the new data,\nas seen by cur.execute(\"truncate \" + table + \";\"). Without doing this, we would\nbe uploading the same CSV to the same table forever, creating duplicate rows\nover and over.\n\nWhat if The Table Doesn't Exist?\nUgh, of course  this would come up. The truth is (to the best of my knowledge),\nthere aren't many native things Psycopg2 has to offer to make this process easy.\n \n\nRecall that creating a table has a syntax similar to this:\n\nCREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n\n\nIt's not impossible to build this string yourself in Python. It just entails a\nlot of iterating over whichever dynamic data structure you have coming through,\ndetermining the correct data type per column, and then the unavoidable task of\nsetting your Primary  and Unique  keys if applicable. This is where my patience\nends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey,\nit's possible! I just don't feel like writing about it. :).\n\nGodspeed to You, Brave Warrior\nFor those about to Psycopg2, we salute you. That is unless the choice is\nself-inflicted. In that case, perhaps it's best we don't work together any time\nsoon.","html":"<p>Last time we met, we joyfully <a href=\"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/\">shared a little tirade</a> about missing out on functionality provided to us by libraries such as <strong>SQLAlchemy</strong>, and the advantages of interacting with databases where ORMs are involved. I stand by that sentiment, but I’ll now directly contradict myself by sharing some tips on using vanilla <strong>Psycopg2</strong> to interact with databases. </p><p>We never know when we’ll be stranded on a desert island without access to SQLAlchemy, but a lonesome Psycopg2 washes up on shore. Either that or perhaps you’re part of a development team stuck in a certain way of doing things which doesn't include utilize SQLAlchemy. Whatever the situation may be, we’re here for you. </p><h2 id=\"the-quintessential-boilerplate\">The Quintessential Boilerplate</h2><p>No matter the type of database or the library, the boilerplate code for connecting to databases remains mostly the same. To some extent, this even holds true across programming languages. Let's look at a barebones example while ignoring the library at hand:</p><pre><code class=\"language-python\">import SomeDatabaseLibrary\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n\n    def run_query(self, query):\n            conn = None\n            records = []\n            try:\n                conn = SomeDatabaseLibrary.connect(host=self.host, \n                                                user=self.username, \n                                                password=self.password,\n                                                port=self.port, \n                                                dbname=self.db)\n                with conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, SomeDatabaseLibrary.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<p>In the above example, we could swap <code>SomeDatabaseLibrary</code> with either <code>Psycopg2</code> or <code>PyMySQL</code> just the same. If we compare this to <a href=\"https://hackersandslackers.com/using-pymysql/\">our example with PyMySQL</a>, it's easy to see that the basics of utilizing <strong>connections</strong>, <strong>cursors</strong>, and the methods to close them transcend libraries. If you know the basics of one, you know them all.</p><p>If you'd like to keep your connection logic separate (as I do), we can cleanly break the logic of handling connections out to a separate function. This time, we'll replace <code>SomeDatabaseLibrary</code> with <code>Psycopg2</code> to produce some working code:</p><pre><code class=\"language-python\">import psycopg2\n\nclass Database:\n    &quot;&quot;&quot;A Generic Database class.&quot;&quot;&quot;\n\n    def __init__(self, config):\n        self.username = config.database('USERNAME')\n        self.password = config.database('PASSWORD')\n        self.host = config.database('HOST')\n        self.port = config.database('PORT')\n        self.db = config.database('DB')\n        self.conn = None\n        \n    def open_connection():\n        &quot;&quot;&quot;Encapsulated connection management logic.&quot;&quot;&quot;\n        try:\n            if(self.conn is None):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)\n            elif (not conn.open):\n                self.conn = psycopg2.connect(host=self.host, \n                                       user=self.username, \n                                       password=self.password,\n                                       port=self.port, \n                                       dbname=self.db)  \n        except:\n            logger.error(&quot;ERROR: Could not connect to Postgres.&quot;)\n            sys.exit()\n\n    def run_query(self, query):\n            records = []\n            try:\n                open_connection()\n                with self.conn.cursor() as cur:\n                    cur.execute(query)\n                    result = cur.fetchall()\n                    for row in result:\n                        records.append(row)\n                    cur.close()\n                    return records\n            except (Exception, psycopg2.DatabaseError) as error:\n                print(error)\n            finally:\n                if conn is not None:\n                    conn.close()\n                    print('Database connection closed.')\n</code></pre>\n<h2 id=\"psycopg2-extras\">Psycopg2 Extras</h2><p>Psycopg2 has many useful features via a library called <a href=\"http://initd.org/psycopg/docs/extras.html\">psycopg2.extras</a>. My personal favorite of these extras is the <code>DictCursor</code>, which renders the rows being returned by our query as Python <em>dictionaries</em> as opposed to <em>lists. </em></p><h3 id=\"using-dictcursor-to-return-more-useful-results\">Using DictCursor to Return More Useful Results</h3><p>When using a DictCursor, the <em>key</em> is always the column name, and the <em>value </em>is the value of that column in that particular row.</p><p>To use extras, we <code>import psycopg2.extras</code>.</p><p>Then, we turn our attention to the following line:</p><pre><code class=\"language-python\">self.conn.cursor() as cur:\n</code></pre>\n<p>Within <code>cursor</code>, we can pass an attribute named <code>cursor_factory</code>  and set it as such:</p><pre><code class=\"language-python\">conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n</code></pre>\n<p>While our cursor is open, all rows returned by the query will be returned as dictionaries. For example, the <strong>row</strong> in the above example will be returned as a dict. To demonstrate, here's what a query on this <em>exact post</em> you're reading now looks like when returned as a Dict:</p><pre><code class=\"language-python\">{\n    title: &quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n    slug: &quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n    feature_image: &quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n    status: &quot;draft&quot;,\n    created_at: &quot;2019-01-14 22:20:52&quot;,\n    custom_excerpt: &quot;Managing Postgres Database connections with Psycopg2&quot;\n}\n</code></pre>\n<p>Compare this to what we would've seen had we not used <code>DictCursor</code>:</p><pre><code class=\"language-python\">[&quot;Psycopg2: Postgres &amp; Python the Old Fashioned Way&quot;,\n&quot;psycopg2-postgres-python-the-old-fashioned-way&quot;,\n&quot;https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/psycopg2.jpg&quot;,\n&quot;draft&quot;,\n&quot;2019-01-14 22:20:52&quot;,\n&quot;Managing Postgres Database connections with Psycopg2&quot;]\n</code></pre>\n<p>Yes, it's a list, and thereby much less useful. Even from a readability standpoint, I (the human user) have no idea what these values represent unless comparing them to the table schema. Even worse would be compiling CSVs or even Pandas Dataframes this way. When building a table made of lists, you set your headers and hope that every row to come matches the number of header columns one-to-one. Otherwise, it's entirely unclear as to which value belongs to which column.</p><h3 id=\"other-psycopg2-extras\">Other Psycopg2 Extras</h3><p>There are plenty more Psycopg2 extras where that came from; it's mostly up to you to decide which are worth your while.</p><p>For example, another extra which might be of interest could be <code>psycopg2.extras.LoggingConnection</code>, useful for debugging connection statuses and errors as you work through your program.</p><p>There's even a <strong>JSON Adaptation</strong> extra, which provides support for leveraging JSON data in building queries:</p><pre><code class=\"language-python\">cur.execute(&quot;insert into mytable (jsondata) values (%s)&quot;,\n    [Json({'a': 100})])\n</code></pre>\n<p>I don't dwell too deep in Psycopg2 extras myself, but if you see any Godlike extras I'm missing, feel free to call them out in the <strong><em>COMMENTS BELOW!</em></strong> (Hah! I've always wanted to say that).</p><h2 id=\"a-few-more-fundamental-useful-things\">A Few More Fundamental Useful Things</h2><p>Something worth visiting is the ability to upload CSVs into Postgres to create tables. We can accomplish this via the built-in method <code>copy_expert</code>.</p><h3 id=\"from-csv-to-postgres-table\">From CSV to Postgres Table</h3><p>To save a CSV to Postgres table, we need to begin with a basic SQL query saved in our project as a variable:</p><pre><code class=\"language-sql\">COPY %s FROM STDIN WITH\n                    CSV\n                    HEADER\n                    DELIMITER AS ','\n</code></pre>\n<p>As should be familiar, <code>%s</code> represents a value we can pass in later. With this raw query, we're only missing two more values:</p><ul><li>The path of our CSV file to be uploaded</li><li>The name of the table we'd like to upload to in Postgres</li></ul><p>Check out how we use <code>copy_expert</code> here to put it all together:</p><pre><code class=\"language-python\">sql = &quot;COPY %s FROM STDIN WITH CSVHEADER DELIMITER AS ','&quot;\nfile = open('files/myfile.csv', &quot;r&quot;)\ntable = 'my_postgres_table'\nwith conn.cursor() as cur:\n    cur.execute(&quot;truncate &quot; + table + &quot;;&quot;)\n    cur.copy_expert(sql=sql % table, file=file)\n    conn.commit()\n    cur.close()\n    conn.close()\n</code></pre>\n<p>Notice that I opt to truncate the existing table before uploading the new data, as seen by <code>cur.execute(\"truncate \" + table + \";\")</code>. Without doing this, we would be uploading the same CSV to the same table forever, creating duplicate rows over and over.</p><h3 id=\"what-if-the-table-doesn-t-exist\">What if The Table Doesn't Exist?</h3><p>Ugh, of <em>course</em> this would come up. The truth is (to the best of my knowledge), there aren't many native things Psycopg2 has to offer to make this process easy. </p><p>Recall that creating a table has a syntax similar to this:</p><pre><code class=\"language-sql\">CREATE TABLE `recommended_reads` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `title` varchar(150) NOT NULL,\n  `content` text,\n  `url` varchar(150) NOT NULL,\n  `created` int(11) NOT NULL,\n  `unique_ID` int(11) NOT NULL,\n  `image` varchar(150) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `id` (`id`),\n  UNIQUE KEY `uniqueid` (`unique_ID`) USING BTREE\n)\n</code></pre>\n<p>It's not impossible to build this string yourself in Python. It just entails a lot of iterating over whichever dynamic data structure you have coming through, determining the correct data type per column, and then the unavoidable task of setting your <strong>Primary</strong> and <strong>Unique</strong> keys if applicable. This is where my patience ends and knee-jerk reaction of \"would be easier in SQLAlchemy\" kicks in. Hey, it's possible! I just don't feel like writing about it. :).</p><h2 id=\"godspeed-to-you-brave-warrior\">Godspeed to You, Brave Warrior</h2><p>For those about to Psycopg2, we salute you. That is unless the choice is self-inflicted. In that case, perhaps it's best we don't work together any time soon.</p>","url":"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/","uuid":"f07736c5-c167-4fe9-b932-1b6b4d95e3ff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3d0b441719dc6b38ee53b6"}},{"node":{"id":"Ghost__Post__5c3409a094d3e847951adf44","title":"Pythonic Database Management with SQLAlchemy","slug":"pythonic-database-management-with-sqlalchemy","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","excerpt":"The iconic Python library for handling any conceivable database interaction.","custom_excerpt":"The iconic Python library for handling any conceivable database interaction.","created_at_pretty":"08 January, 2019","published_at_pretty":"09 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-07T21:23:28.000-05:00","published_at":"2019-01-09T08:00:00.000-05:00","updated_at":"2019-03-28T11:17:45.000-04:00","meta_title":"Pythonic Database Management with SQLAlchemy | Hackers and Slackers","meta_description":"The iconic Python library for handling any conceivable database interaction.","og_description":"The iconic Python library for handling any conceivable database interaction.","og_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-2.jpg","og_title":"Pythonic Database Management with SQLAlchemy","twitter_description":"The iconic Python library for handling any conceivable database interaction.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/sqlalchemy2-1-1.jpg","twitter_title":"Pythonic Database Management with SQLAlchemy","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Something we've taken for granted thus far on Hackers and Slackers is a library\nmost data professionals have accepted as an undisputed standard: SQLAlchemy\n[https://www.sqlalchemy.org/].\n\nIn the past, we've covered database connection management and querying using\nlibraries such as PyMySQL [https://hackersandslackers.com/using-pymysql/]  and \nPsycopg2\n[https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/]\n, both of which do an excellent job of interacting with databases just as we'd\nexpect them to. The nature of opening/closing DB connections and working with\ncursors hasn't changed much in the past few decades (nearly the lifespan of\nrelational databases themselves). While boilerplate is boring, at least it has\nremained consistent, one might figure. That may  have been the case, but the\nphilosophical boom of MVC frameworks nearly a decade ago sparked the emergence\nof popularity for ORMs. While the world was singing praises of object-oriented\nprogramming, containing database-oriented functionality within objects must have\nbeen a wet dream.\n\nThe only thing shocking about SQLAlchemy's popularity is its flip side: the\ncontingency of those functioning without  SQLAlchemy as a part of their regular\nstack. Whether this stems from unawareness or active reluctance to change, data\nteams using Python without a proper ORM are surprisingly prevalent. It's easy to\nforget the reality of the workforce when our interactions with other\nprofessionals come mostly from blogs published by those at the top of their\nfield.\n\nI realize the \"this is how we've always done it\" attitude is a cliché with no\nshortage of commentary. Tales of adopting new (relatively speaking) practices\ndominate Silicon Valley blogs every day- it's the manner in which this is\nmanifested, however, that catches me off guard. In this case, resistance to a\nsingle Python library can shed light on a frightening mental model that has\nimplications up and down a corporation's stack.\n\nPutting The 'M' In MVC\nFrameworks which enforce a Model-View-Controller have held undisputed consensus\nfor long enough: none of us need to recap why creating apps this way is\nunequivocally correct. To understand why side-stepping an ORM is so significant,\nlet's recall what ORM stands for:\n\n> Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a\ntechnique that connects the rich objects of an application to tables in a\nrelational database management system. Using ORM, the properties and\nrelationships of the objects in an application can be easily stored and\nretrieved from a database without writing SQL statements directly and with less\noverall database access code. - Active Record\n[https://guides.rubyonrails.org/active_record_basics.html]\nORMs allow us to interact with databases simply by modifying objects in code\n(such as classes) as opposed to generating SQL queries by hand for each database\ninteraction. Bouncing from application code to SQL is a major context switch,\nand the more interactions we introduce, the more out of control our app becomes.\n \n\nTo illustrate the alternative to this using models, I'll use an example offered\nby Flask-SQLAlchemy. Let's say we have a table of users which contains columns\nfor id, username,  and email. A model for such a table would look as such:\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '<User %r>' % self.username\n\n\nThe 'model' is an object representing the structure of a single entry in our\ntable. Once our model exists, this is all it takes to create an entry:\n\nnewuser = User(username='admin', email='admin@example.com')\n\n\nThat's a single readable line of code without writing a single line of SQL.\nCompare this to the alternative, which would be to use Psycopg2:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\ndef query_function(query):\n  \"\"\"Runs a database query.\"\"\"\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n\n\nSure, query_function()  only needs to be set once, but compare the readability\nof using a model to the following:\n\nquery = \"INSERT INTO users VALUES username='admin', email='admin@example.com';\"\n\nquery_function(query)\n\n\nDespite achieving the same effect, the latter is much less readable or\nmaintainable by human beings. Building an application around raw string queries\ncan quickly become a nightmare.\n\nIntegration With Other Data Libraries\nWhen it comes to golden standards of Python libraries, there is none more\nquintessential to data analysis than Pandas. The pairing of Pandas and\nSQLAlchemy is standard to the point where Pandas has built-in integrations to\ninteract with data from SQLAlchemy. Here's what it takes to turn a database\ntable into a Pandas dataframe with SQLAlchemy as our connector:\n\ndf = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n\n\nOnce again, a single line of Python code!\n\nWriting Queries Purely in Python\nSo far by using SQLAlchemy, we haven't needed to write a single line of SQL: how\nfar could we take this? As far as we want, in fact. SQLAlchemy contains what\nthey've dubbed as function-based query construction, which is to say we can\nconstruct nearly any conceivable SQL query purely in Python by using the methods\noffered to us. For example, here's an update query:\n\nstmt = users.update().values(fullname=\"Fullname: \" + users.c.name)\nconn.execute(stmt)\n\n\nCheck the  full reference to see what I mean\n[https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates].\nEvery query you've ever needed to write: it's all there. All of it.\n\nSimple Connection Management\nSeeing as how we all now agree that SQLAlchemy is beneficial to our workflow,\nlet's visit square one and see how simple it is to manage connections. The two\nkey words to remember here are engines  and sessions.\n\nThe Engine\nAn engine in SQLAlchemy is merely a bare-bones object representing our database.\nMaking SQLAlchemy aware of our database is as simple as these two lines:\n\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\nThe Engine can interact with our database by accepting a simple URI. Once engine \n exists, we could in theory use engine exclusively via functions such as \nengine.connect()  and engine.execute().\n\nSessions\nTo interact with our database in a Pythonic manner via the ORM, we'll need to\ncreate a session  from the engine we just declared. Thus our code expands:\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n\n\nThat's all it takes! Now just as before, we can use SQLAlchemy's ORM and\nbuilt-in functions to make simple interacts:\n\nnew_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n\n\nTakeaway Goodies\nIt's worth mentioning that SQLAlchemy works with nearly every type of database,\nand does so by leveraging the base Python library for the respective type of\ndatabase. For example, it probably seems to the outsider that we've spent some\ntime shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a\nPostgres database, it is using the Psycopg2 library under the hood to manage the\nboilerplate for us. The same goes for every other type of relational database\n[https://docs.sqlalchemy.org/en/latest/core/engines.html]  along with their\nstandard libraries.\n\nThere are plenty of more reasons [https://www.sqlalchemy.org/features.html]  why\nSQLAlchemy is beneficial to the point where it is arguably critical to data\nanalysis workflows. The critical point to be made here is that leaving\nSQLAlchemy out of any data workflow only hurts the person writing the code, or\nmore importantly, all those who come after.","html":"<p>Something we've taken for granted thus far on Hackers and Slackers is a library most data professionals have accepted as an undisputed standard: <strong><a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a></strong>.</p><p>In the past, we've covered database connection management and querying using libraries such as <a href=\"https://hackersandslackers.com/using-pymysql/\"><strong>PyMySQL</strong></a> and <strong><a href=\"https://hackersandslackers.com/psycopg2-postgres-python-the-old-fashioned-way/\">Psycopg2</a></strong>, both of which do an excellent job of interacting with databases just as we'd expect them to. The nature of opening/closing DB connections and working with cursors hasn't changed much in the past few decades (nearly the lifespan of relational databases themselves). While boilerplate is boring, at least it has remained consistent, one might figure. That <strong><em>may</em></strong> have been the case, but the philosophical boom of MVC frameworks nearly a decade ago sparked the emergence of popularity for ORMs. While the world was singing praises of object-oriented programming, containing database-oriented functionality within objects must have been a wet dream.</p><p>The only thing shocking about SQLAlchemy's popularity is its flip side: the contingency of those functioning <em>without</em> SQLAlchemy as a part of their regular stack. Whether this stems from unawareness or active reluctance to change, data teams using Python without a proper ORM are surprisingly prevalent. It's easy to forget the reality of the workforce when our interactions with other professionals come mostly from blogs published by those at the top of their field.</p><p>I realize the \"this is how we've always done it\" attitude is a cliché with no shortage of commentary. Tales of adopting new (relatively speaking) practices dominate Silicon Valley blogs every day- it's the manner in which this is manifested, however, that catches me off guard. In this case, resistance to a single Python library can shed light on a frightening mental model that has implications up and down a corporation's stack.</p><h2 id=\"putting-the-m-in-mvc\">Putting The 'M' In MVC</h2><p>Frameworks which enforce a Model-View-Controller have held undisputed consensus for long enough: none of us need to recap why creating apps this way is unequivocally correct. To understand why side-stepping an ORM is so significant, let's recall what ORM stands for:</p><blockquote><em>Object-Relational Mapping, commonly referred to as its abbreviation ORM, is a technique that connects the rich objects of an application to tables in a relational database management system. Using ORM, the properties and relationships of the objects in an application can be easily stored and retrieved from a database without writing SQL statements directly and with less overall database access code. <strong>- <a href=\"https://guides.rubyonrails.org/active_record_basics.html\">Active Record</a></strong></em></blockquote><p>ORMs allow us to interact with databases simply by modifying objects in code (such as classes) as opposed to generating SQL queries by hand for each database interaction. Bouncing from application code to SQL is a <em>major context switch</em>, and the more interactions we introduce, the more out of control our app becomes. </p><p>To illustrate the alternative to this using models, I'll use an example offered by <strong>Flask-SQLAlchemy</strong>. Let's say we have a table of users which contains columns for <strong>id, username,</strong> and <strong>email. </strong>A model for such a table would look as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">class User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n\n    def __repr__(self):\n        return '&lt;User %r&gt;' % self.username\n</code></pre>\n<!--kg-card-end: markdown--><p>The 'model' is an object representing the structure of a single entry in our table. Once our model exists, this is all it takes to create an entry:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">newuser = User(username='admin', email='admin@example.com')\n</code></pre>\n<!--kg-card-end: markdown--><p>That's a single readable line of code without writing a single line of SQL. Compare this to the alternative, which would be to use Psycopg2:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\ndef query_function(query):\n  &quot;&quot;&quot;Runs a database query.&quot;&quot;&quot;\n  try:\n    conn = psycopg2.connect(\n      user = config.username,\n      password = config.password,\n      host = config.host,\n      port = config.port,\n      database = config.database)\n      with conn.cursor() as cur:\n         cur.execute(query)\n           cur.close()\n           conn.close()\n  except Exception as e:\n      print(e)\n        \nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Sure, <code>query_function()</code> only needs to be set once, but compare the readability of using a model to the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">query = &quot;INSERT INTO users VALUES username='admin', email='admin@example.com';&quot;\n\nquery_function(query)\n</code></pre>\n<!--kg-card-end: markdown--><p>Despite achieving the same effect, the latter is much less readable or maintainable by human beings. Building an application around raw string queries can quickly become a nightmare.</p><h2 id=\"integration-with-other-data-libraries\">Integration With Other Data Libraries</h2><p>When it comes to golden standards of Python libraries, there is none more quintessential to data analysis than Pandas. The pairing of Pandas and SQLAlchemy is standard to the point where Pandas has built-in integrations to interact with data from SQLAlchemy. Here's what it takes to turn a database table into a Pandas dataframe with SQLAlchemy as our connector:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df = pd.read_sql(session.query(Table).filter(User.id == 2).statement,session.bind)\n</code></pre>\n<!--kg-card-end: markdown--><p>Once again, a single line of Python code!</p><h2 id=\"writing-queries-purely-in-python\">Writing Queries Purely in Python</h2><p>So far by using SQLAlchemy, we haven't needed to write a single line of SQL: how far could we take this? As far as we want, in fact. SQLAlchemy contains what they've dubbed as <strong>function-based query construction, </strong>which is to say we can construct nearly any conceivable SQL query purely in Python by using the methods offered to us. For example, here's an update query:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">stmt = users.update().values(fullname=&quot;Fullname: &quot; + users.c.name)\nconn.execute(stmt)\n</code></pre>\n<!--kg-card-end: markdown--><p>Check the<a href=\"https://docs.sqlalchemy.org/en/latest/core/tutorial.html#inserts-and-updates\"> full reference to see what I mean</a>. Every query you've ever needed to write: it's all there. All of it.</p><h2 id=\"simple-connection-management\">Simple Connection Management</h2><p>Seeing as how we all now agree that SQLAlchemy is beneficial to our workflow, let's visit square one and see how simple it is to manage connections. The two key words to remember here are <strong>engines</strong> and <strong>sessions</strong>.</p><h3 id=\"the-engine\">The Engine</h3><p>An engine in SQLAlchemy is merely a bare-bones object representing our database. Making SQLAlchemy aware of our database is as simple as these two lines:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:', echo=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>The Engine can interact with our database by accepting a simple URI. Once <code>engine</code> exists, we could in theory use engine exclusively via functions such as <code>engine.connect()</code> and <code>engine.execute()</code>.</p><h3 id=\"sessions\">Sessions</h3><p>To interact with our database in a Pythonic manner via the ORM, we'll need to create a <strong>session</strong> from the <strong>engine </strong>we just declared. Thus our code expands:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nengine = create_engine('sqlite:///:memory:', echo=True)\nSession = sessionmaker(bind=engine)\n</code></pre>\n<!--kg-card-end: markdown--><p>That's all it takes! Now just as before, we can use SQLAlchemy's ORM and built-in functions to make simple interacts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">new_user = User(name='todd', fullname='Todd Hacker', password='toddspassword')\nsession.add(new_user)\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"takeaway-goodies\">Takeaway Goodies</h2><p>It's worth mentioning that SQLAlchemy works with nearly every type of database, and does so by leveraging the base Python library for the respective type of database. For example, it probably seems to the outsider that we've spent some time shitting on Psycopg2. On the contrary, when SQLAlchemy connects to a Postgres database, it is using the Psycopg2 library under the hood to manage the boilerplate for us. The same goes for <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\">every other type of relational database</a> along with their standard libraries.</p><p>There are <a href=\"https://www.sqlalchemy.org/features.html\">plenty of more reasons</a> why SQLAlchemy is beneficial to the point where it is arguably critical to data analysis workflows. The critical point to be made here is that leaving SQLAlchemy out of any data workflow only hurts the person writing the code, or more importantly, all those who come after.</p>","url":"https://hackersandslackers.com/pythonic-database-management-with-sqlalchemy/","uuid":"7246d9db-39cb-44aa-9da8-cf87df00eeff","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c3409a094d3e847951adf44"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ca","title":"PostgreSQL Cloud Database on Google Cloud","slug":"cloud-sql-postgres-on-gcp","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","excerpt":"Deep Dive into Cloud SQL and its out-of-the-box API.","custom_excerpt":"Deep Dive into Cloud SQL and its out-of-the-box API.","created_at_pretty":"09 August, 2018","published_at_pretty":"10 August, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-08-09T12:49:05.000-04:00","published_at":"2018-08-10T05:52:00.000-04:00","updated_at":"2019-02-27T23:37:34.000-05:00","meta_title":"Cloud-Hosted Postgres on Google Cloud | Hackers and Slackers","meta_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.","og_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.\n\n#Postgres #GoogleCloud #Databases","og_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","og_title":"Cloud SQL Postgres on GCP","twitter_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.\n\n#Postgres #GoogleCloud #Databases\n\n","twitter_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","twitter_title":"Cloud SQL Postgres on GCP","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"Well folks, I have a confession to make. I've been maintaining an affair with\ntwo lovers. That's right; they're none other than PostgreSQL, and Google Cloud.\nWhile such polygamy may be shunned by the masses, I believe that somehow, some\nway, we can just make this ménage à trois work. What entices me about Cloud SQL\nis the existence of the Cloud SQL API\n[https://cloud.google.com/sql/docs/postgres/admin-api/]  , which generates\npredictable REST endpoints for presumably reading and writing to your database.\nPlease allow a moment of silence for the old workflow of API Gateways and Lambda\nfunctions. RIP.\n\nWe’ll get to APIs eventually, but for now we have one glaring obstacle: creating\nour DB, and connecting to it in a way vaguely resembles something secure*.\n\nNote: today may or may not be opposite day.Creating Our Cloud Database\nHit up the Cloud SQL [https://console.cloud.google.com/sql/]  section of your\nconsole to get this party started. Database creation on GCP is surprisingly\neasy.\n\nThat's pretty much it tbh.Databases Gone Wild: Postgres Exposed\nThere are plenty of correct ways to connect to your Postgres database correctly\nand securely. You can set up SSL for an IP\n[https://cloud.google.com/sql/docs/postgres/connect-admin-ip], connect using a\nproxy [https://cloud.google.com/sql/docs/postgres/connect-admin-proxy], or even\nvia internal cloud functions\n[https://cloud.google.com/sql/docs/postgres/connect-cloud-functions]. You may\nwant to consider doing one of those things. I'll be doing this a different way,\nbecause I'd rather get my useless data on a hackable public database than\nrewrite Google tutorials:\n\nDo as I say, not as I do.This is where you can feel free to go ahead and\npopulate data into your DB via whichever GUI you'd prefer. It'll be easier to\nsee which API calls work if there's actual data involved.\n\nPick whichever overpriced client suits you best!Enabling the API\nAs always with GCP, we need to explicitly activate the API for SQL; that way,\nthey can charge us money forever, long after we've forgotten this tutorial. We\ncan do this here\n[https://console.cloud.google.com/flows/enableapi?apiid=sqladmin]. Are you\nstarting to feel excited? I know I am; just think, all those API calls right\naround the corner, coming from a real SQL database. Wow. \n\nIn the overall process, we've made it here: the part where we run into OAuth2:\n\nRefresh tokens? Scopes? Uh oh.I'll admit it took me a good amount of time to\ndecrypt the information which failed to conveyed here. After clicking into every\nrelated link and failing at attempts to hit the API via Postman, the bad vibes\nstarted kicking in. What if this isn't the dream after all? To spare you the\nprocess, let me introduce you to a very useful GCP tool.\n\nGoogle API Explorer\nGoogle's API explorer is a GUI for playing with any API, connected to any of\nyour services. This is a cool way to preview what the exact scope of an API is\nbefore you sign up for it. Better yet, you can use placeholder User_IDs  and \nUser_Secrets  since this is basically just a sandbox.\n\nInteractive API learning tools beat reading documentation any day.After\nselecting an 'endpoint' and specifying some details like your project and\ndatabase instance, you can immediately see (theoretical) results of what the\nlive API can do. This is very useful, but I'm afraid this is where things get\ndark.\n\nHello Darkness My Old Friend\nYou may have noticed a lot of similar words or phrases popping up in these\nendpoints. Words such as \"admin\"  and \"list\", while lacking phrases such as \n\"show me my god damn data\". Google's Cloud SQL API  is NOT, in fact, an API to\ninteract with your data, but rather an admin API which enables you to do things\nprobably better suited for, you know, admin consoles.\n\nAs a big fan of GCP, this is but one of a number of growing pains I've\nexperienced with the platform so far. For instance, this entire blog along with\nits VPC has temporary deleted today, because apparently the phrases \"remove my\nproject from Firebase\"  and \"delete my project along with everything I love\" are\nsentimentally similar enough to leave that language vague and awkward.\n\nWhere Do We Go From Here?\nTo reiterate, the problem we were originally looking to solve was to find a\nservice which could (after what, 30 years?) make relational database reading and\nwriting trivial, especially in the case of apps which are simply themes without\na configurable backend, such as this blog.\n\nMongoDB Atlas  is an organizational mess which can't even describe their own\nproduct. Firebase  has yet to implement an import feature, so unless you feel\nlike writing loops to write to an experimental NoSQL database (I don't), we're\nstill kind of screwed. I know there are guys like Dreamfactory out there, but\nthese services are the sketchy ones who email you every day just for looking at\na trial. Also, anything related to Oracle or running on Oracle products (by\nchoice) sucks. There, I said it. Java developers will probably be too bust with\ngarbage collection and getting sued to argue with me anyway.\n\nAll that said, it feels like the \"Backend as a service\" thing is looming over\nthe horizon. There just doesn't seem to be anybody who's executed this\neffectively yet.\n\nUPDATE:  As it turns out, there is a service out there that accomplishes\neverything we hoped to achieve in Google cloud, and it is called Apisentris\n[https://apisentris.com/]. It's awesome, it's free, and the guy behind it is a\nchill dude.","html":"<p>Well folks, I have a confession to make. I've been maintaining an affair with two lovers. That's right; they're none other than PostgreSQL, and Google Cloud. While such polygamy may be shunned by the masses, I believe that somehow, some way, we can just make this ménage à trois work. What entices me about Cloud SQL is the existence of the <a href=\"https://cloud.google.com/sql/docs/postgres/admin-api/\">Cloud SQL API</a> , which generates predictable REST endpoints for presumably reading and writing to your database. Please allow a moment of silence for the old workflow of API Gateways and Lambda functions. RIP.</p><p>We’ll get to APIs eventually, but for now we have one glaring obstacle: creating our DB, and connecting to it in a way vaguely resembles something secure*.</p><span style=\"color: #669ab5; font-style: italic; font-size: 15px; float: right;\">Note: today may or may not be opposite day.</span><h2 id=\"creating-our-cloud-database\">Creating Our Cloud Database</h2><p>Hit up the <a href=\"https://console.cloud.google.com/sql/\">Cloud SQL</a> section of your console to get this party started. Database creation on GCP is surprisingly easy.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/setuppostgres.png\" class=\"kg-image\"><figcaption>That's pretty much it tbh.</figcaption></figure><h2 id=\"databases-gone-wild-postgres-exposed\">Databases Gone Wild: Postgres Exposed  </h2><p>There are plenty of <em>correct </em>ways to connect to your Postgres database correctly and securely. You can <a href=\"https://cloud.google.com/sql/docs/postgres/connect-admin-ip\">set up SSL for an IP</a>, connect <a href=\"https://cloud.google.com/sql/docs/postgres/connect-admin-proxy\">using a proxy</a>, or even via internal <a href=\"https://cloud.google.com/sql/docs/postgres/connect-cloud-functions\">cloud functions</a>. You may want to consider doing one of those things. I'll be doing this a different way, because I'd rather get my useless data on a hackable public database than rewrite Google tutorials:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/public.png\" class=\"kg-image\"><figcaption>Do as I say, not as I do.</figcaption></figure><p>This is where you can feel free to go ahead and populate data into your DB via whichever GUI you'd prefer. It'll be easier to see which API calls work if there's actual data involved.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-08-10-at-4.15.22-AM.png\" class=\"kg-image\"><figcaption>Pick whichever overpriced client suits you best!</figcaption></figure><h2 id=\"enabling-the-api\">Enabling the API</h2><p>As always with GCP, we need to explicitly activate the API for SQL; that way, they can charge us money forever, long after we've forgotten this tutorial. We can do this <a href=\"https://console.cloud.google.com/flows/enableapi?apiid=sqladmin\">here</a>. Are you starting to feel excited? I know I am; just think, all those API calls right around the corner, coming from a real SQL database. Wow. </p><p>In the overall process, we've made it <em>here</em>: the part where we run into OAuth2:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-08-10-at-4.21.55-AM.png\" class=\"kg-image\"><figcaption>Refresh tokens? Scopes? Uh oh.</figcaption></figure><p>I'll admit it took me a good amount of time to decrypt the information which failed to conveyed here. After clicking into every related link and failing at attempts to hit the API via Postman, the bad vibes started kicking in. What if this isn't the dream after all? To spare you the process, let me introduce you to a very useful GCP tool.</p><h2 id=\"google-api-explorer\">Google API Explorer</h2><p>Google's API explorer is a GUI for playing with any API, connected to any of your services. This is a cool way to preview what the exact scope of an API is before you sign up for it. Better yet, you can use placeholder <em>User_IDs</em> and <em>User_Secrets</em> since this is basically just a sandbox.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sandbox.gif\" class=\"kg-image\"><figcaption>Interactive API learning tools beat reading documentation any day.</figcaption></figure><p>After selecting an 'endpoint' and specifying some details like your project and database instance, you can immediately see (theoretical) results of what the live API can do. This is very useful, but I'm afraid this is where things get dark.</p><h3 id=\"hello-darkness-my-old-friend\">Hello Darkness My Old Friend</h3><p>You may have noticed a lot of similar words or phrases popping up in these endpoints. Words such as <em>\"admin\"</em> and \"<em>list\"</em>, while lacking phrases such as <em>\"show me my god damn data\". </em>Google's <strong>Cloud SQL API</strong> is NOT, in fact, an API to interact with your data, but rather an <em>admin </em>API which enables you to do things probably better suited for, you know, admin consoles.</p><p>As a big fan of GCP, this is but one of a number of growing pains I've experienced with the platform so far. For instance, this entire blog along with its VPC has temporary deleted today, because apparently the phrases <em>\"remove my project from Firebase\"</em> and <em>\"delete my project along with everything I love\" </em>are sentimentally similar enough to leave that language vague and awkward.</p><h2 id=\"where-do-we-go-from-here\">Where Do We Go From Here?</h2><p>To reiterate, the problem we were originally looking to solve was to find a service which could (after what, 30 years?) make relational database reading and writing trivial, especially in the case of apps which are simply themes without a configurable backend, such as this blog.</p><p><em>MongoDB Atlas</em> is an organizational mess which can't even describe their own product. <em>Firebase</em> has yet to implement an import feature, so unless you feel like writing loops to write to an experimental NoSQL database (I don't), we're still kind of screwed. I know there are guys like <em>Dreamfactory </em>out there, but these services are the sketchy ones who email you every day just for looking at a trial. Also, anything related to Oracle or running on Oracle products (by choice) sucks. There, I said it. Java developers will probably be too bust with garbage collection and getting sued to argue with me anyway.</p><p>All that said, it feels like the \"Backend as a service\" thing is looming over the horizon. There just doesn't seem to be anybody who's executed this effectively yet.</p><p><strong>UPDATE:</strong> As it turns out, there is a service out there that accomplishes everything we hoped to achieve in Google cloud, and it is called <a href=\"https://apisentris.com/\">Apisentris</a>. It's awesome, it's free, and the guy behind it is a chill dude.</p>","url":"https://hackersandslackers.com/cloud-sql-postgres-on-gcp/","uuid":"76daacf1-55b4-4ede-b21d-29fd727e1d50","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b6c70819dcd9d3270b58635"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a1","title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","excerpt":"Code Snippet Corner ft. Pandas & SQL","custom_excerpt":"Code Snippet Corner ft. Pandas & SQL","created_at_pretty":"12 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"25 November, 2018","created_at":"2018-07-11T21:54:04.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-11-25T12:50:00.000-05:00","meta_title":"Code Snippet Corner: A Dirty Way of Cleaning Data (ft. Pandas & SQL) | Hackers and Slackers","meta_description":"Code Snippet Corner ft. Pandas & SQL","og_description":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","og_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","og_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","twitter_description":"Code Snippet Corner ft. Pandas & SQL","twitter_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","twitter_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Warning  The following is FANTASTICALLY not-secure.  Do not put this in a script\nthat's going to be running unsupervised.  This is for interactive sessions where\nyou're prototyping the data cleaning methods that you're going to use, and/or\njust manually entering stuff.  Especially if there's any chance there could be\nsomething malicious hiding in the data to be uploaded.  We're going to be\nexecuting formatted strings of SQL unsanitized code.  Also, this will lead to\nLOTS of silent failures, which are arguably The Worst Thing - if guaranteed\ncorrectness is a requirement, leave this for the tinkering table.\n Alternatively, if it's a project where \"getting something in there is better\nthan nothing\", this can provide a lot of bang for your buck.  Actually, it's\npurely for entertainment purposes and not for human consumption.\n\nLet's say you were helping someone take a bunch of scattered Excel files and\nCSVs and input them all into a MySQL database.  This is a very iterative, trial\n& error process.  We certainly don't want to be re-entering a bunch of\nboilerplate.  Pandas to the rescue!  We can painlessly load those files into a\nDataFrame, then just export them to the db!\n\nWell, not so fast  First off, loading stuff into a DB is a task all its own -\nPandas and your RDBMS have different kinds of tolerance for mistakes, and differ\nin often-unpredictable ways.  For example, one time I was performing a task\nsimilar to the one described here (taking scattered files and loading them into\na DB) - I was speeding along nicely, but then ran into a speedbump: turns out\nPandas generally doesn't infer that a column is a date unless you tell it\nspecifically, and will generally parse dates as strings.  Now, this was fine\nwhen the dates were present - MySQL is pretty smart about accepting different\nforms of dates & times.  But one thing it doesn't like is accepting an empty\nstring ''  into a date or time column.  Not a huge deal, just had to cast the\ncolumn as a date:\n\ndf['date'] = pd.to_datetime(df['date'])\n\nNow the blank strings are NaT, which MySQL knows how to handle!\n\nThis was simple enough, but there's all kinds of little hiccups that can happen.\n And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing\naffair - if there's one error, that means none of the rows will write.  Which\ncan get pretty annoying if you were trying to write a decent-sized DataFrame,\nespecially if the first error doesn't show up until one of the later rows.\n Waiting sucks.  And it's not just about being impatient - long waiting times\ncan disrupt your flow.\n\nRapid prototyping & highly-interactive development are some of Python's greatest\nstrengths, and they are great strengths indeed!  Paul Graham (one of the guys\nbehind Y Combinator) once made the comparison between REPL-heavy development and\nthe popularizing of oil paints (he was talking about LISP, but it's also quite\ntrue of Python, as Python took a lot of its cues from LISP):\n\nBefore oil paint became popular, painters used a medium, called tempera , that\ncannot be blended or over-painted. The cost of mistakes was high, and this\ntended to make painters conservative. Then came oil paint, and with it a great\nchange in style. Oil \"allows for second thoughts\". This proved a decisive\nadvantage in dealing with difficult subjects like the human figure.The new\nmedium did not just make painters' lives easier. It made possible a new and more\nambitious kind of painting. Janson writes:Without oil, the Flemish\nMasters'conquest of visible reality would have been much more limited. Thus,\nfrom a technical point of view, too, they deserve to be called the \"fathers of\nmodern painting\" , for oil has been the painter's basic medium ever since. As a\nmaterial, tempera is no lesss beautiful than oil. But the flexibility of oil\npaint gives greater scope to the imagination--that was the deciding factor.\nProgramming is now undergoing a similar change...Meanwhile, ideas borrowed from\nLisp increasingly turn up in the mainstream: interactive programming\nenvironments, garbage collection, and run-time typing  to name a few.More\npowerful tools are taking the risk out of exploration. That's good news for\nprogrammers, because it means that we will be able to undertake more ambitious\nprojects. The use of oil paint certainly had this effect. The period immediately\nfollowing its adoption was a golden age for painting. There are signs already\nthat something similar is happening in programming.\n(Emphasis mine)\nFrom here: http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.htmlA\nlittle scenario to demonstrate:\n\nLet's pretend we have a MySQL instance running, and have already created a\ndatabase named items\n\nimport pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(\"\"\"CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)\"\"\", cnx)\n\ndf = pd.DataFrame({\n    \"author\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"copies\": [2, \"\", 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n\n\nSoo, let's tighten this feedback loop, shall we?\n\nWe'll iterate through the DataFrame with the useful iterrows()  method.  This\ngives us essentially an enum  made from our DataFrame - we'll get a bunch of\ntuples giving us the index as the first element and the row as its own Pandas\nSeries as the second.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n\n\nLet's unpack that a bit.\n\nRemember that we're getting a two-element tuple, with the good stuff in the\nsecond element, so\n\nx[1]\n\nNext, we convert the Series to a one-entry DataFrame, because the Series doesn't\nhave the DataFrame's to_sql()  method.\n\npd.DataFrame(x[1])\n\nThe default behavior will assume this is a single column with, each variable\nbeing the address of a different row.  MySQL isn't going to be having it.  Sooo,\nwe transpose!\n\npd.DataFrame(x[1]).transpose()\n\nAnd finally, we use our beloved to_sql  method on that.\n\nLet's check our table now!\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n\n\nIt wrote the first row!  Not much of a difference with this toy example, but\nonce you were writing a few thousand rows and the error didn't pop up until the\n3000th, this would make a pretty noticeable difference in your ability to\nquickly experiment with different cleaning schemes.\n\nNote that this will still short-circuit as soon as we hit the error.  If we\nwanted to make sure we got all the valid input before working on our tough\ncases, we could make a little try/except  block.\n\n\n\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n\n\nThis will try  to write each line, and if it encounters an Exception  it'll \ncontinue  the loop.\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n\n\nAlright, now the bulk of our data's in the db!  Whatever else happens, you've\ndone that much!  Now you can relax a bit, which is useful for stimulating the\ncreativity you'll need for the more complicated edge cases.\n\nSo, we're ready to start testing new cleaning schemes?  Well, not quite yet...\n\nLet's say we went and tried to think up a fix.  We go to test it out and...\n\n#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '1' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '3' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n\n\nThe error we're interested is in there, but what's all this other nonsense\ncrowding it?\n\nWell, one of the handy things about a database is that it'll enforce uniqueness\nbased on the constraints you give it.  It's already got an entry with an id \nvalue of 1, so it's going to complain if you try to put another one.  In\naddition to providing a lot of distraction, this'll also slow us down\nconsiderably - after all, part of the point was to make our experiments with\ndata-cleaning go faster!\n\nLuckily, Pandas' wonderful logical indexing will make it a snap to ensure that\nwe only bother with entries that aren't in the database yet.\n\n#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n\n\nThis will also be shockingly quick - Pandas' logical indexing takes advantage of\nall that magic going on under the hood.  Using it, instead of manually\niteration, can literally bring you from waiting minutes to waiting seconds.\n\nBuuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so\nhow about we just turn it into a function?\n\n#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n    return df[~df.index.isin(usedIDs)]\n\n\nSo, next time we think we've made some progress on an edge case, we just call...\n\n#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nActually, let's clean that up even more - the more keys we hit, the more\nopportunities to make a mistake!  The most bug-free code is the code you don't\nwrite.\n\ndef writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nSo, finally, we can work on our new cleaning scheme, and whenever we think we're\ndone...\n\nwriteNewRows(cleanedDF)\n\nAnd boom!  Instant feedback!","html":"<p><strong>Warning</strong> The following is FANTASTICALLY not-secure.  Do not put this in a script that's going to be running unsupervised.  This is for interactive sessions where you're prototyping the data cleaning methods that you're going to use, and/or just manually entering stuff.  Especially if there's any chance there could be something malicious hiding in the data to be uploaded.  We're going to be executing formatted strings of SQL unsanitized code.  Also, this will lead to LOTS of silent failures, which are arguably The Worst Thing - if guaranteed correctness is a requirement, leave this for the tinkering table.  Alternatively, if it's a project where \"getting something in there is better than nothing\", this can provide a lot of bang for your buck.  Actually, it's purely for entertainment purposes and not for human consumption.</p><p>Let's say you were helping someone take a bunch of scattered Excel files and CSVs and input them all into a MySQL database.  This is a very iterative, trial &amp; error process.  We certainly don't want to be re-entering a bunch of boilerplate.  Pandas to the rescue!  We can painlessly load those files into a DataFrame, then just export them to the db!</p><p>Well, not so fast  First off, loading stuff into a DB is a task all its own - Pandas and your RDBMS have different kinds of tolerance for mistakes, and differ in often-unpredictable ways.  For example, one time I was performing a task similar to the one described here (taking scattered files and loading them into a DB) - I was speeding along nicely, but then ran into a speedbump: turns out Pandas generally doesn't infer that a column is a date unless you tell it specifically, and will generally parse dates as strings.  Now, this was fine when the dates were present - MySQL is pretty smart about accepting different forms of dates &amp; times.  But one thing it doesn't like is accepting an empty string <code>''</code> into a date or time column.  Not a huge deal, just had to cast the column as a date:</p><p><code>df['date'] = pd.to_datetime(df['date'])</code></p><p>Now the blank strings are <code>NaT</code>, which MySQL knows how to handle!</p><p>This was simple enough, but there's all kinds of little hiccups that can happen.  And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing affair - if there's one error, that means none of the rows will write.  Which can get pretty annoying if you were trying to write a decent-sized DataFrame, especially if the first error doesn't show up until one of the later rows.  Waiting sucks.  And it's not just about being impatient - long waiting times can disrupt your flow.</p><p>Rapid prototyping &amp; highly-interactive development are some of Python's greatest strengths, and they are great strengths indeed!  Paul Graham (one of the guys behind Y Combinator) once made the comparison between REPL-heavy development and the popularizing of oil paints (he was talking about LISP, but it's also quite true of Python, as Python took a lot of its cues from LISP):</p><blockquote>Before oil paint became popular, painters used a medium, called tempera , that cannot be blended or over-painted. The cost of mistakes was high, and this tended to make painters conservative. Then came oil paint, and with it a great change in style. Oil \"allows for second thoughts\". This proved a decisive advantage in dealing with difficult subjects like the human figure.The new medium did not just make painters' lives easier. It made possible a new and more ambitious kind of painting. Janson writes:Without oil, the Flemish Masters'conquest of visible reality would have been much more limited. Thus, from a technical point of view, too, they deserve to be called the \"fathers of modern painting\" , for oil has been the painter's basic medium ever since. As a material, tempera is no lesss beautiful than oil. But the flexibility of oil paint gives greater scope to the imagination--that was the deciding factor.<br>Programming is now undergoing a similar change...Meanwhile, ideas borrowed from Lisp increasingly turn up in the mainstream: <strong>interactive programming environments, garbage collection, and run-time typing</strong> to name a few.More powerful tools are taking the risk out of exploration. That's good news for programmers, because it means that we will be able to undertake more ambitious projects. The use of oil paint certainly had this effect. The period immediately following its adoption was a golden age for painting. There are signs already that something similar is happening in programming.<br>(Emphasis mine)<br>From here: <a href=\"http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\">http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html</a></blockquote><p>A little scenario to demonstrate:</p><p>Let's pretend we have a MySQL instance running, and have already created a database named <code>items</code></p><pre><code class=\"language-python\">import pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(&quot;&quot;&quot;CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)&quot;&quot;&quot;, cnx)\n\ndf = pd.DataFrame({\n    &quot;author&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],\n    &quot;copies&quot;: [2, &quot;&quot;, 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n</code></pre>\n<p>Soo, let's tighten this feedback loop, shall we?</p><p>We'll iterate through the DataFrame with the useful <code>iterrows()</code> method.  This gives us essentially an <code>enum</code> made from our DataFrame - we'll get a bunch of tuples giving us the index as the first element and the row as its own Pandas Series as the second.</p><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n</code></pre>\n<p>Let's unpack that a bit.</p><p>Remember that we're getting a two-element tuple, with the good stuff in the second element, so</p><p><code>x[1]</code></p><p>Next, we convert the Series to a one-entry DataFrame, because the Series doesn't have the DataFrame's <code>to_sql()</code> method.</p><p><code>pd.DataFrame(x[1])</code></p><p>The default behavior will assume this is a single column with, each variable being the address of a different row.  MySQL isn't going to be having it.  Sooo, we transpose!</p><p><code>pd.DataFrame(x[1]).transpose()</code></p><p>And finally, we use our beloved <code>to_sql</code> method on that.</p><p>Let's check our table now!</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n</code></pre>\n<p>It wrote the first row!  Not much of a difference with this toy example, but once you were writing a few thousand rows and the error didn't pop up until the 3000th, this would make a pretty noticeable difference in your ability to quickly experiment with different cleaning schemes.</p><p>Note that this will still short-circuit as soon as we hit the error.  If we wanted to make sure we got all the valid input before working on our tough cases, we could make a little <code>try/except</code> block.</p><pre><code class=\"language-python\">\n</code></pre>\n<pre><code>for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n</code></pre><p>This will <code>try</code> to write each line, and if it encounters an <code>Exception</code> it'll <code>continue</code> the loop.</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n</code></pre>\n<p>Alright, now the bulk of our data's in the db!  Whatever else happens, you've done that much!  Now you can relax a bit, which is useful for stimulating the creativity you'll need for the more complicated edge cases.</p><p>So, we're ready to start testing new cleaning schemes?  Well, not quite yet...</p><p>Let's say we went and tried to think up a fix.  We go to test it out and...</p><pre><code class=\"language-python\">#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '1' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '3' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n</code></pre>\n<p>The error we're interested is in there, but what's all this other nonsense crowding it?</p><p>Well, one of the handy things about a database is that it'll enforce uniqueness based on the constraints you give it.  It's already got an entry with an <code>id</code> value of 1, so it's going to complain if you try to put another one.  In addition to providing a lot of distraction, this'll also slow us down considerably - after all, part of the point was to make our experiments with data-cleaning go faster!</p><p>Luckily, Pandas' wonderful logical indexing will make it a snap to ensure that we only bother with entries that aren't in the database yet.</p><pre><code class=\"language-python\">#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n</code></pre>\n<p>This will also be shockingly quick - Pandas' logical indexing takes advantage of all that magic going on under the hood.  Using it, instead of manually iteration, can literally bring you from waiting minutes to waiting seconds.</p><p>Buuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so how about we just turn it into a function?</p><pre><code class=\"language-python\">#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n    return df[~df.index.isin(usedIDs)]\n</code></pre>\n<p>So, next time we think we've made some progress on an edge case, we just call...</p><pre><code class=\"language-python\">#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>Actually, let's clean that up even more - the more keys we hit, the more opportunities to make a mistake!  The most bug-free code is the code you don't write.</p><pre><code class=\"language-python\">def writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>So, finally, we can work on our new cleaning scheme, and whenever we think we're done...</p><p><code>writeNewRows(cleanedDF)</code></p><p>And boom!  Instant feedback!</p>","url":"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/","uuid":"9788b54d-ef44-4a35-9ec6-6a8678038480","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b46b4bcc6a9e951f8a6cc32"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a2","title":"Lynx Roundup, July 14th","slug":"lynx-roundup-july-14th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","excerpt":"Blockchain algos, SQL Cheat Sheet, Convex Optimization.","custom_excerpt":"Blockchain algos, SQL Cheat Sheet, Convex Optimization.","created_at_pretty":"13 July, 2018","published_at_pretty":"14 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:11:32.000-04:00","published_at":"2018-07-14T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 14th | Hackers and Slackers","meta_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","og_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","og_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","og_title":"Lynx Roundup, July 14th","twitter_description":"Blockchain algos, SQL Cheat Sheet, Convex Optimization","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx41@2x.jpg","twitter_title":"Lynx Roundup, July 14th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"}],"plaintext":"https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f\n\n\n\nhttps://www.kdnuggets.com/2018/07/sql-cheat-sheet.html\n\n\n\nhttps://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization\n\n\n\nhttps://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi\n\n\n\nhttps://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization","html":"<p></p><p><a href=\"https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f\">https://hackernoon.com/consensuspedia-an-encyclopedia-of-29-consensus-algorithms-e9c4b4b7d08f</a></p><p></p><p><a href=\"https://www.kdnuggets.com/2018/07/sql-cheat-sheet.html\">https://www.kdnuggets.com/2018/07/sql-cheat-sheet.html</a></p><p></p><p><a href=\"https://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization\">https://www.quora.com/What-are-the-prerequisites-to-learn-convex-optimization</a></p><p></p><p><a href=\"https://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi\">https://jods.mitpress.mit.edu/pub/issue3-case?__s=wvtwtdzek6uqqfiffffi</a></p><p></p><p><a href=\"https://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization\">https://www.quora.com/What-are-remarkable-algorithms-methods-for-convex-optimization</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-14th/","uuid":"16d7a70b-ae09-4917-8c78-478523e1c93a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b485eb4c6a9e951f8a6cc4b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867367c","title":"Lynx Roundup, June 21st","slug":"lynx-roundup-june-21st","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 6/21/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 6/21/2018.","created_at_pretty":"19 June, 2018","published_at_pretty":"21 June, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-06-19T15:48:26.000-04:00","published_at":"2018-06-21T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:03.000-04:00","meta_title":"Lynx Roundup, June 21st | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","og_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","og_title":"Lynx Roundup, June 21st","twitter_description":"Daily roundup of Data Science news around the industry, 6/21/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx56@2x.jpg","twitter_title":"Lynx Roundup, June 21st","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Science News","slug":"science-news","description":"Breakthroughs in general science.","feature_image":null,"meta_description":"Breakthroughs in general science.","meta_title":"Science News | Hackers and Slackers","visibility":"public"}],"plaintext":"https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/\n\nhttps://github.com/o1lab/xmysql\n\nhttps://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html\n\nKickin' rad MOOC that I'm taking right now, from a kickin' rad organization that\ndoes kickin' rad research. \nhttps://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells\n\nhttp://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html","html":"<p><a href=\"https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/\">https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/</a></p>\n<p><a href=\"https://github.com/o1lab/xmysql\">https://github.com/o1lab/xmysql</a></p>\n<p><a href=\"https://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html\">https://golem.ph.utexas.edu/category/2018/04/dynamical_systems_and_their_st.html</a></p>\n<p>Kickin' rad MOOC that I'm taking right now, from a kickin' rad organization that does kickin' rad research.  <a href=\"https://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells\">https://www.complexityexplorer.org/courses/63-algorithmic-information-dynamics-a-computational-approach-to-causality-and-living-systems-from-networks-to-cells</a></p>\n<p><a href=\"http://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html\">http://highscalability.com/blog/2018/6/18/how-ably-efficiently-implemented-consistent-hashing.html</a></p>\n","url":"https://hackersandslackers.com/lynx-roundup-june-21st/","uuid":"cbf4f4ce-d4b0-4d70-acce-215fc8ce6740","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b295e0aded32f5af8fd6723"}}]}},"pageContext":{"slug":"sql","limit":12,"skip":0,"numberOfPages":2,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/sql/page/2/"}}