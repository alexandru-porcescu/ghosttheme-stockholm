{"data":{"ghostPost":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},"pageContext":{"slug":"manage-s3-assests-with-boto3-python-sdk"}}