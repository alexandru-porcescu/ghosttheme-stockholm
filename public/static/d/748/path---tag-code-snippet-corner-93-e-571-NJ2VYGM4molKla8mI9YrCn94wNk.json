{"data":{"ghostTag":{"slug":"code-snippet-corner","name":"#Code Snippet Corner","visibility":"internal","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c4e57144b23df2da7332b80","title":"Downcast Numerical Data Types with Pandas","slug":"downcast-numerical-columns-python-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","excerpt":"Using an Example Where We Downcast Numerical Columns.","custom_excerpt":"Using an Example Where We Downcast Numerical Columns.","created_at_pretty":"28 January, 2019","published_at_pretty":"28 January, 2019","updated_at_pretty":"14 February, 2019","created_at":"2019-01-27T20:12:52.000-05:00","published_at":"2019-01-28T07:30:00.000-05:00","updated_at":"2019-02-13T22:50:18.000-05:00","meta_title":"Using Pandas' Assign Function on Multiple Columns | Hackers and Slackers","meta_description":"Using Pandas' Assign function on multiple columns via an example: downcasting numerical columns.","og_description":"Using Pandas' Assign by example: downcasting numerical columns.","og_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","og_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","twitter_description":"Using Pandas' Assign by example: downcasting numerical columns.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/codesnippetdatatypes@2x.jpg","twitter_title":"Code Snippet Corner: Using Pandas' Assign Function on Multiple Columns","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Recently, I had to find a way to reduce the memory footprint of a Pandas\nDataFrame in order to actually do operations on it.  Here's a trick that came in\nhandy!\n\nBy default, if you read a DataFrame from a file, it'll cast all the numerical\ncolumns as the float64  type.  This is in keeping with the philosophy behind\nPandas and NumPy - by using strict types (instead of normal Python \"duck\ntyping\"), you can do things a lot faster.  The float64  is the most flexible\nnumerical type - it can handle fractions, as well as turning missing values into\na NaN.  This will let us read it into memory, and then start messing with it.\n The downside is that it consumes a lot of memory.\n\nNow, let's say we want to save memory by manually downcasting our columns into\nthe smallest type that can handle its values?  And let's ALSO say that we want\nto be really, really lazy and don't want to look at a bunch of numbers by hand.\n And let's say we wanna do this via Method Chaining, because of all the\nadvantages outlined here: https://tomaugspurger.github.io/method-chaining\n\nLet's introduce our example DataFrame.  We'll convert all the values to floats\nmanually because that's what the default is when we read from a file.\n\ndf = pd.DataFrame({\n    \"stay_float\": [0.5, 3.7, 7.5],\n    \"to_int\": [-5, 7, 5],\n    \"to_uint\": [1, 100, 200]}).astype(float)\n\n\nFirst, let's introduce the workhorse of this exercise - Pandas's to_numeric \nfunction, and its handy optional argument, downcast.  This will take a numerical\ntype - float, integer  (not int), or unsigned  - and then downcast it to the\nsmallest version available.\n\nNext, let's make a function that checks to see if a column can be downcast from\na float to an integer.\n\ndef float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser\n\nWe're using the try/except pattern here because if we try to make a column with \nNaN  values into an integer column, it'll throw an error.  If it'd otherwise be\na good candidate for turning into an integer, we should figure a value to impute\nfor those missing values - but that'll be different for every column.  Sometimes\nit'd make sense to make it 0, other times the mean or median of the column, or\nsomething else entirely.\n\nI'd also like to direct your attention to Line 4, which has a very useful Pandas\npattern - if (ser == int_ser).all().  When you do operations on Pandas columns\nlike Equals or Greater Than, you get a new column where the operation was\napplied element-by-element.  If you're trying to set up a conditional, the\ninterpreter doesn't know what to do with an array containing [True, False, True] \n - you have to boil it down to a single value.  So, if you wan to check if two\ncolumns are completely equal, you have to call the .all()  method (which has a\nuseful sibling, any()) to make a conditional that can actually be used to\ncontrol execution.\n\nNext, let's make a function that lets us apply a transformation to multiple\ncolumns based on a condition.  The assign  method is pretty awesome, and it'd be\nfun to not have to leave it (or, if we do, to at least replace it with a\nfunction we can pipe as part of a chain of transformations to the DataFrame as a\nwhole).\n\ndef multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n\n\nassign  lets us do multiple assignments, so long as we make a dictionary of\ncolumn names and target values and then unpack it.  Really, it'd actually be\neasier to skip the function and go directly to using this syntax, except that\nI'm not aware of a method of accessing a filterable list of the DF's columns\nwhile still \"in\" the chain.  I think future versions of Pandas' syntax will\ninclude this, as I've read they want to support more Method Chaining.\n Personally, I find the reduction in Cognitive Load is worth it, with having a\nlot of little modular lego-piece transformations chained together.\n\nIt also works as a nice foundation for other little helper functions.  So,\nhere's one to turn as many float columns to integers as we can.\n\ndef all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[\"float\"])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSee the pattern in action!  We decide on a transformation function, we decide on\nwhat conditions we want to apply all these transformations (we could have a\nhundred columns, and who wants to make a note of all that?), and then we pass it\nto the multi-assign  function.\n\n(df\n     .pipe(all_float_to_int)).dtypes\n\n\nstay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n\n\nCool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of\ninteger takes up as many bytes as 64 bytes of float, just like how a hundred\npounds of feathers weighs as much as a hundred pounds of bricks.  What we did do\nis make it easier to downcast those columns later.\n\nNext, let's make a function that takes a subset of the columns, and tries to\ndowncast it to the smallest version that it can.  We've got fairly small values\nhere, so it should get some work done.\n\ndef downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n\n\nSame basic pattern as before!  But now we have two arguments - one is the \ntarget_type, which tells us what types to try to downcast to.  By default, this\nwill be the same as the initial_type, with one exception that we'll grab in a\nsecond!\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n\n\nAlright, now we're getting somewhere!  Wonder if we can do even better, though?\n That last column has a conspicuous name!  And it has no values lower than 0 -\nmaybe we could save space if we store it as an unsigned integer!  Let's add a\npipe to our chain that'll try to downcast certain integers into unsigneds...\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).dtypes\n\n\nstay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n\n\nWhat do ya know, we can!\n\nLet's see how much memory we save by doing this.\n\ndf.info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n\n\nvs\n\n(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, \"float\")\n     .pipe(downcast_all, \"integer\")\n     .pipe(downcast_all,  \n           target_type = \"unsigned\", \n           inital_type = \"integer\")\n).info(memory_usage='deep')\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n\n\n152 down to 98 - we reduced it by more than 1/3rd!","html":"<p>Recently, I had to find a way to reduce the memory footprint of a Pandas DataFrame in order to actually do operations on it.  Here's a trick that came in handy!</p><p>By default, if you read a DataFrame from a file, it'll cast all the numerical columns as the <code>float64</code> type.  This is in keeping with the philosophy behind Pandas and NumPy - by using strict types (instead of normal Python \"duck typing\"), you can do things a lot faster.  The <code>float64</code> is the most flexible numerical type - it can handle fractions, as well as turning missing values into a <code>NaN</code>.  This will let us read it into memory, and then start messing with it.  The downside is that it consumes a lot of memory.</p><p>Now, let's say we want to save memory by manually downcasting our columns into the smallest type that can handle its values?  And let's ALSO say that we want to be really, really lazy and don't want to look at a bunch of numbers by hand.  And let's say we wanna do this via Method Chaining, because of all the advantages outlined here: <a href=\"https://tomaugspurger.github.io/method-chaining\">https://tomaugspurger.github.io/method-chaining</a></p><p>Let's introduce our example DataFrame.  We'll convert all the values to floats manually because that's what the default is when we read from a file.</p><pre><code class=\"language-python\">df = pd.DataFrame({\n    &quot;stay_float&quot;: [0.5, 3.7, 7.5],\n    &quot;to_int&quot;: [-5, 7, 5],\n    &quot;to_uint&quot;: [1, 100, 200]}).astype(float)\n</code></pre>\n<p>First, let's introduce the workhorse of this exercise - Pandas's <code>to_numeric</code> function, and its handy optional argument, <code>downcast</code>.  This will take a numerical type - <code>float</code>, <code>integer</code> (not <code>int</code>), or <code>unsigned</code> - and then downcast it to the smallest version available.</p><p>Next, let's make a function that checks to see if a column can be downcast from a float to an integer.</p><pre><code>def float_to_int(ser):\n    try:\n        int_ser = ser.astype(int)\n        if (ser == int_ser).all():\n            return int_ser\n        else:\n            return ser\n    except ValueError:\n        return ser</code></pre><p>We're using the try/except pattern here because if we try to make a column with <code>NaN</code> values into an integer column, it'll throw an error.  If it'd otherwise be a good candidate for turning into an integer, we should figure a value to impute for those missing values - but that'll be different for every column.  Sometimes it'd make sense to make it 0, other times the mean or median of the column, or something else entirely.</p><p>I'd also like to direct your attention to Line 4, which has a very useful Pandas pattern - <code>if (ser == int_ser).all()</code>.  When you do operations on Pandas columns like Equals or Greater Than, you get a new column where the operation was applied element-by-element.  If you're trying to set up a conditional, the interpreter doesn't know what to do with an array containing <code>[True, False, True]</code> - you have to boil it down to a single value.  So, if you wan to check if two columns are completely equal, you have to call the <code>.all()</code> method (which has a useful sibling, <code>any()</code>) to make a conditional that can actually be used to control execution.</p><p>Next, let's make a function that lets us apply a transformation to multiple columns based on a condition.  The <code>assign</code> method is pretty awesome, and it'd be fun to not have to leave it (or, if we do, to at least replace it with a function we can pipe as part of a chain of transformations to the DataFrame as a whole).</p><pre><code class=\"language-python\">def multi_assign(df, transform_fn, condition):\n    df_to_use = df.copy()\n    \n    return (df_to_use\n        .assign(\n            **{col: transform_fn(df_to_use[col])\n               for col in condition(df_to_use)})\n           )\n</code></pre>\n<p><code>assign</code> lets us do multiple assignments, so long as we make a dictionary of column names and target values and then unpack it.  Really, it'd actually be easier to skip the function and go directly to using this syntax, except that I'm not aware of a method of accessing a filterable list of the DF's columns while still \"in\" the chain.  I think future versions of Pandas' syntax will include this, as I've read they want to support more Method Chaining.  Personally, I find the reduction in Cognitive Load is worth it, with having a lot of little modular lego-piece transformations chained together.  </p><p>It also works as a nice foundation for other little helper functions.  So, here's one to turn as many float columns to integers as we can.</p><pre><code class=\"language-python\">def all_float_to_int(df):\n    df_to_use = df.copy()\n    transform_fn = float_to_int\n    condition = lambda x: list(x\n                    .select_dtypes(include=[&quot;float&quot;])\n                    .columns)    \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>See the pattern in action!  We decide on a transformation function, we decide on what conditions we want to apply all these transformations (we could have a hundred columns, and who wants to make a note of all that?), and then we pass it to the <code>multi-assign</code> function.  </p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float64\nto_int          int64\nto_uint         int64\ndtype: object\n</code></pre>\n<p>Cool!  But we didn't actually decrease the size of our DataFrame - 64 bytes of integer takes up as many bytes as 64 bytes of float, just like how a hundred pounds of feathers weighs as much as a hundred pounds of bricks.  What we did do is make it easier to downcast those columns later.</p><p>Next, let's make a function that takes a subset of the columns, and tries to downcast it to the smallest version that it can.  We've got fairly small values here, so it should get some work done.</p><pre><code class=\"language-python\">def downcast_all(df, target_type, inital_type=None):\n    #Gotta specify floats, unsigned, or integer\n    #If integer, gotta be 'integer', not 'int'\n    #Unsigned should look for Ints\n    if inital_type is None:\n        inital_type = target_type\n    \n    df_to_use = df.copy()\n    \n    transform_fn = lambda x: pd.to_numeric(x, \n                                downcast=target_type)\n    \n    condition = lambda x: list(x\n                    .select_dtypes(include=[inital_type])\n                    .columns) \n    \n    return multi_assign(df_to_use, transform_fn, condition)\n</code></pre>\n<p>Same basic pattern as before!  But now we have two arguments - one is the <code>target_type</code>, which tells us what types to try to downcast to.  By default, this will be the same as the <code>initial_type</code>, with one exception that we'll grab in a second!</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         int16\ndtype: object\n</code></pre>\n<p>Alright, now we're getting somewhere!  Wonder if we can do even better, though?  That last column has a conspicuous name!  And it has no values lower than 0 - maybe we could save space if we store it as an unsigned integer!  Let's add a pipe to our chain that'll try to downcast certain integers into unsigneds...</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).dtypes\n</code></pre>\n<pre><code class=\"language-bash\">stay_float    float32\nto_int           int8\nto_uint         uint8\ndtype: objec\n</code></pre>\n<p>What do ya know, we can!</p><p>Let's see how much memory we save by doing this.</p><pre><code class=\"language-python\">df.info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float64\nto_int        3 non-null float64\nto_uint       3 non-null float64\ndtypes: float64(3)\nmemory usage: 152.0 bytes\n</code></pre>\n<p>vs</p><pre><code class=\"language-python\">(df\n     .pipe(all_float_to_int)\n     .pipe(downcast_all, &quot;float&quot;)\n     .pipe(downcast_all, &quot;integer&quot;)\n     .pipe(downcast_all,  \n           target_type = &quot;unsigned&quot;, \n           inital_type = &quot;integer&quot;)\n).info(memory_usage='deep')\n</code></pre>\n<pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\nstay_float    3 non-null float32\nto_int        3 non-null int8\nto_uint       3 non-null uint8\ndtypes: float32(1), int8(1), uint8(1)\nmemory usage: 98.0 bytes\n</code></pre>\n<p>152 down to 98 - we reduced it by more than 1/3rd!</p>","url":"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/","uuid":"58bbb902-99bb-404d-8a3c-232d56b6e776","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c4e57144b23df2da7332b80"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673709","title":"Using Random Forests for Feature Selection with Categorical Features","slug":"random-forests-for-feature-selection","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","custom_excerpt":"Python helper functions for adding feature importance, and displaying them as a single variable.","created_at_pretty":"24 September, 2018","published_at_pretty":"24 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-23T20:23:32.000-04:00","published_at":"2018-09-24T07:30:00.000-04:00","updated_at":"2019-02-19T03:48:04.000-05:00","meta_title":"Using Random Forests for Feature Selection | Hackers and Slackers","meta_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","og_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","og_title":"Using Random Forests for Feature Selection","twitter_description":"Helper functions in Python to gauge  importance of Categorical Features for Random Forests in Scikit-learn","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codesnippertsomething@2x.jpg","twitter_title":"Using Random Forests for Feature Selection","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Notebook here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb]\n.  Helper functions here\n[https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py]\n.\n\nOne of the best features of Random Forests is that it has built-in Feature\nSelection.  Explicability is one of the things we often lose when we go from\ntraditional statistics to Machine Learning, but Random Forests lets us actually\nget some insight into our dataset instead of just having to treat our model as a\nblack box.\n\nOne problem, though - it doesn't work that well for categorical features.  Since\nyou'll generally have to One-Hot Encode a categorical feature (for instance,\nturn something with 7 categories into 7 variables that are a \"True/False\"),\nyou'll wind up with a bunch of small features.  This gets tough to read,\nespecially if you're dealing with a lot of categories.  It also makes that\nfeature look less important than it is - rather than appearing near the top,\nyou'll maybe have 17 weak-seeming features near the bottom - which gets worse if\nyou're filtering it so that you only see features above a certain threshold.\n\nSoo, here's some helper functions for adding up their importance and displaying\nthem as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my\nmy own One-Hot function, rather than using Scikit's builtin one.\n\nFirst, let's grab a dataset.  I'm using this\n[https://www.kaggle.com/c/avazu-ctr-prediction]  Kaggle dataset because it has a\ngood number of categorical predictors.  I'm also only using the first 500 rows\nbecause the whole dataset is like ~ 1 GB.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)\n\nLet's just use the Categorical variables as our predictors because that's what\nwe're focusing on, but in actual usage you don't have to make them the same.\n\npredVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)\n\nLet's use log_loss  as our metric, because I saw this\n[https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512] \n blog post that used it for this dataset.\n\nfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 \n\nVariable\n Importance\n 0\n device_model\n 0.843122\n 1\n site_category\n 0.083392\n 2\n app_category\n 0.037216\n 3\n device_type\n 0.025057\n 4\n device_conn_type\n 0.011213","html":"<p><em>Notebook <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/Categorical%20Feature%20Importance.ipynb\">here</a>.  Helper functions <a href=\"https://github.com/mattalhonte/random_forest_categorical_feature_imprtance/blob/master/foresthelpers/featureimportance.py\">here</a>.</em></p><p>One of the best features of Random Forests is that it has built-in Feature Selection.  Explicability is one of the things we often lose when we go from traditional statistics to Machine Learning, but Random Forests lets us actually get some insight into our dataset instead of just having to treat our model as a black box.</p><p>One problem, though - it doesn't work that well for categorical features.  Since you'll generally have to One-Hot Encode a categorical feature (for instance, turn something with 7 categories into 7 variables that are a \"True/False\"), you'll wind up with a bunch of small features.  This gets tough to read, especially if you're dealing with a lot of categories.  It also makes that feature look less important than it is - rather than appearing near the top, you'll maybe have 17 weak-seeming features near the bottom - which gets worse if you're filtering it so that you only see features above a certain threshold.</p><p>Soo, here's some helper functions for adding up their importance and displaying them as a single variable.  I did have to \"reinvent the wheel\" a bit and roll my my own One-Hot function, rather than using Scikit's builtin one.</p><p>First, let's grab a dataset.  I'm using <a href=\"https://www.kaggle.com/c/avazu-ctr-prediction\">this</a> Kaggle dataset because it has a good number of categorical predictors.  I'm also only using the first 500 rows because the whole dataset is like ~ 1 GB.</p><pre><code>import pandas as pd\n\ndf = pd.read_csv(\"train.csv\", \n                   nrows=500)</code></pre><p>Let's just use the Categorical variables as our predictors because that's what we're focusing on, but in actual usage you don't have to make them the same.</p><pre><code>predVars = [\n    \"site_category\",\n    \"app_category\",\n    \"device_model\",\n    \"device_type\",\n    \"device_conn_type\",\n]\n\nX = (df\n     .dropna()\n     [predVars]\n     .pipe((fh.oneHotEncodeMultipleVars, \"df\"),\n           varList = predVars) #Change this if you don't have solely categoricals\n    )\n\nlabels = X.columns\n\ny = (df\n     .dropna()\n     [\"click\"]\n     .values)</code></pre><p>Let's use <code>log_loss</code> as our metric, because I saw <a href=\"https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512\">this</a> blog post that used it for this dataset.</p><pre><code>from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import log_loss\nfi.displayFeatureImportances(X,y,labels,log_loss,{\"n_estimators\": 18,\"oob_score\": True},)\nScore is 3.6297600214665064 </code></pre><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>device_model</td>\n      <td>0.843122</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>site_category</td>\n      <td>0.083392</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>app_category</td>\n      <td>0.037216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>device_type</td>\n      <td>0.025057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>device_conn_type</td>\n      <td>0.011213</td>\n    </tr>\n  </tbody>\n</table>","url":"https://hackersandslackers.com/random-forests-for-feature-selection/","uuid":"26ebccb3-ab41-44cf-8d57-bf995100b088","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5ba82e84a1cf0b13cf2e9886"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673700","title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","slug":"random-forests-hyperparameters-min_samples_leaf","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","custom_excerpt":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n.","created_at_pretty":"17 September, 2018","published_at_pretty":"17 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-16T21:33:48.000-04:00","published_at":"2018-09-17T07:30:00.000-04:00","updated_at":"2019-02-19T03:44:33.000-05:00","meta_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf | Hackers and Slackers","meta_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","og_description":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","og_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","twitter_description":"Tune the min_samples_leaf parameter in for a Random Forests classifier in scikit-learn in Python\n","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner2-1-1@2x.jpg","twitter_title":"Tuning Random Forests Hyperparameters with Binary Search Part III: min_samples_leaf","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Part 1 (n_estimators) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\nPart 2 (max_depth) here\n[https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/]\nNotebook here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb]\n\n\n--------------------------------------------------------------------------------\n\nAnother parameter, another set of quirks!\n\nmin_samples_leaf  is sort of similar to max_depth.  It helps us avoid\noverfitting.  It's also non-obvious what you should use as your upper and lower\nlimits to search between.  Let's do what we did last week - build a forest with\nno parameters, see what it does, and use the upper and lower limits!\n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\n\nLet's use the handy function from here\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html]  to\ncrawl the number of samples in a tree's leaf nodes: \n\ndef leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n\n\nLast week we made a function to grab them for a whole forest - since this is the\nsecond time we're doing this, and we may do it again, let's make a modular\nlittle function that takes a crawler function as an argument!\n\ndef getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {\"min\": params.min(),\n           \"max\": params.max()}\n\n\nLet's see it in action!\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#> {'max': 199, 'min': 1}\n\n\nAlmost ready to start optimizing!  Since part of what we get out of optimizing \nmin_samples_leaf  is regularization (and because it's just good practice!),\nlet's make a metric with some cross-validation.  Luckily, Scikit  has a builtin \ncross_val_score  function.  We'll just need to do a teensy bit of tweaking to\nmake it use the area under a precision_recall_curve.\n\nfrom sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n\n\nAwesome, now we have a metric that can be fed into our binary search.\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.402102\n 199\n 0.506455\n 1.416349\n 100\n 0.506455\n 1.401090\n 51\n 0.506455\n 1.394548\n 26\n 0.975894\n 1.396503\n 14\n 0.982954\n 1.398522\n 7\n 0.979888\n 1.398929\n 10\n 0.984789\n 1.404815\n 12\n 0.986302\n 1.391171\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.473848\n 0.082938\n 199\n 0.002084\n 1.039718\n 0.000000\n 100\n 0.002084\n 0.433676\n 0.000111\n 51\n 0.002084\n 0.173824\n 0.000396\n 26\n 0.980393\n 0.251484\n 0.154448\n 14\n 0.995105\n 0.331692\n 0.118839\n 7\n 0.988716\n 0.347858\n 0.112585\n 10\n 0.998930\n 0.581632\n 0.067998\n 12\n 1.002084\n 0.039718\n 1.000000\n Looks like the action's between 1 and 51.  More than that, and the score goes\nwhile simultaneously increasing the runtime - the opposite of what we want!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.389387\n 51\n 0.506455\n 1.403807\n 26\n 0.975894\n 1.404517\n 14\n 0.982954\n 1.385420\n 7\n 0.979888\n 1.398840\n 10\n 0.984789\n 1.393863\n 12\n 0.986302\n 1.411774\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n Big drop-off after 26, it seems!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.407957\n 26\n 0.975894\n 1.398042\n 14\n 0.982954\n 1.396782\n 7\n 0.979888\n 1.396096\n 10\n 0.984789\n 1.402322\n 12\n 0.986302\n 1.401080\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.650270\n 1.084306\n 0.040144\n 26\n 0.096077\n 0.248406\n 0.000000\n 14\n 0.774346\n 0.142157\n 0.954016\n 7\n 0.479788\n 0.084306\n 1.000000\n 10\n 0.950677\n 0.609184\n 0.221294\n 12\n 1.096077\n 0.504512\n 0.336668\n One more with 14 as our upper limit!\n\nmin_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    \"min_samples_leaf\", \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n\n\nmin_samples_leaf\n score\n time\n 1\n 0.981662\n 1.401341\n 14\n 0.982954\n 1.400361\n 7\n 0.979888\n 1.402408\n 4\n 0.981121\n 1.401396\n 3\n 0.983580\n 1.401332\n \nmin_samples_leaf\n score\n time\n scoreTimeRatio\n 1\n 0.992414\n 0.188492\n 0.200671\n 51\n 0.002084\n 0.735618\n 0.000000\n 26\n 0.980393\n 0.762561\n 0.048920\n 14\n 0.995105\n 0.037944\n 1.000000\n 7\n 0.988716\n 0.547179\n 0.068798\n 10\n 0.998930\n 0.358303\n 0.106209\n 12\n 1.002084\n 1.037944\n 0.036709\n 3 it is!I suppose when it gets this small we could use a regular Grid Search,\nbut...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs \nGridSearchCV  and/or RandomizedSearchCV.  Who knows what the future holds?","html":"<p>Part 1 (n_estimators) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a><br>Part 2 (max_depth) <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-random-learning-hyperparameters-with-binary-search/\">here</a><br>Notebook <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Samples).ipynb\">here</a></p><hr><p>Another parameter, another set of quirks!</p><p><code>min_samples_leaf</code> is sort of similar to <code>max_depth</code>.  It helps us avoid overfitting.  It's also non-obvious what you should use as your upper and lower limits to search between.  Let's do what we did last week - build a forest with no parameters, see what it does, and use the upper and lower limits!</p><pre><code class=\"language-python\">import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n</code></pre>\n<p>Let's use the handy function from <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">here</a> to crawl the number of samples in a tree's leaf nodes: </p><pre><code class=\"language-python\">def leaf_samples(tree, node_id = 0):\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n    \n    if left_child == _tree.TREE_LEAF:\n        samples = np.array([tree.n_node_samples[node_id]])\n        \n    else:\n        \n        left_samples = leaf_samples(tree, left_child)\n        right_samples = leaf_samples(tree, right_child)\n        \n        samples = np.append(left_samples, right_samples)\n        \n    return samples\n</code></pre>\n<p>Last week we made a function to grab them for a whole forest - since this is the second time we're doing this, and we may do it again, let's make a modular little function that takes a crawler function as an argument!</p><pre><code class=\"language-python\">def getForestParams(X, y, param, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    params = np.hstack([param(estimator.tree_) \n                 for estimator in clf.estimators_])\n    return {&quot;min&quot;: params.min(),\n           &quot;max&quot;: params.max()}\n</code></pre>\n<p>Let's see it in action!</p><pre><code class=\"language-python\">data = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;n_estimators&quot;: 18,\n         &quot;oob_score&quot;: True}\n\ngetForestParams(X, y, leaf_samples, rfArgs)\n#&gt; {'max': 199, 'min': 1}\n</code></pre>\n<p>Almost ready to start optimizing!  Since part of what we get out of optimizing <code>min_samples_leaf</code> is regularization (and because it's just good practice!), let's make a metric with some cross-validation.  Luckily, <strong>Scikit</strong> has a builtin <code>cross_val_score</code> function.  We'll just need to do a teensy bit of tweaking to make it use the area under a <code>precision_recall_curve</code>.</p><pre><code class=\"language-python\">from sklearn.model_selection import cross_val_score\n\ndef auc_prc(estimator, X, y):\n    estimator.fit(X, y)\n    y_pred = estimator.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\ndef getForestAccuracyCV(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    return np.mean(cross_val_score(clf, X, y, scoring=auc_prc, cv=5))\n</code></pre>\n<p>Awesome, now we have a metric that can be fed into our binary search.</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    199)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.402102</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.506455</td>\n      <td>1.416349</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.506455</td>\n      <td>1.401090</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.394548</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.396503</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.398522</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398929</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.404815</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.391171</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.473848</td>\n      <td>0.082938</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.002084</td>\n      <td>1.039718</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.002084</td>\n      <td>0.433676</td>\n      <td>0.000111</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.173824</td>\n      <td>0.000396</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.251484</td>\n      <td>0.154448</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.331692</td>\n      <td>0.118839</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.347858</td>\n      <td>0.112585</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.581632</td>\n      <td>0.067998</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>0.039718</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf1.png\" class=\"kg-image\"></figure><p>Looks like the action's between 1 and 51.  More than that, and the score goes while simultaneously increasing the runtime - the opposite of what we want!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.389387</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.506455</td>\n      <td>1.403807</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.404517</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.385420</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.398840</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.393863</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.411774</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf2.png\" class=\"kg-image\"></figure><p>Big drop-off after 26, it seems!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    26)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.407957</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.975894</td>\n      <td>1.398042</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.396782</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.396096</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984789</td>\n      <td>1.402322</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.986302</td>\n      <td>1.401080</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.650270</td>\n      <td>1.084306</td>\n      <td>0.040144</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.096077</td>\n      <td>0.248406</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.774346</td>\n      <td>0.142157</td>\n      <td>0.954016</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.479788</td>\n      <td>0.084306</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.950677</td>\n      <td>0.609184</td>\n      <td>0.221294</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.096077</td>\n      <td>0.504512</td>\n      <td>0.336668</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf3.png\" class=\"kg-image\"></figure><p>One more with 14 as our upper limit!</p><pre><code class=\"language-python\">min_samples_leaf = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracyCV,        \n                    rfArgs, \n                    &quot;min_samples_leaf&quot;, \n                    0, \n                    1, \n                    14)\nbgs.showTimeScoreChartAndGraph(min_samples_leaf)\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.981662</td>\n      <td>1.401341</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.982954</td>\n      <td>1.400361</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.979888</td>\n      <td>1.402408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.981121</td>\n      <td>1.401396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.983580</td>\n      <td>1.401332</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>min_samples_leaf</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992414</td>\n      <td>0.188492</td>\n      <td>0.200671</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.002084</td>\n      <td>0.735618</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.980393</td>\n      <td>0.762561</td>\n      <td>0.048920</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.995105</td>\n      <td>0.037944</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.988716</td>\n      <td>0.547179</td>\n      <td>0.068798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.998930</td>\n      <td>0.358303</td>\n      <td>0.106209</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.002084</td>\n      <td>1.037944</td>\n      <td>0.036709</td>\n    </tr>\n  </tbody>\n</table>\n</div><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/min_samples_leaf4.png\" class=\"kg-image\"><figcaption>3 it is!</figcaption></figure><p>I suppose when it gets this small we could use a regular Grid Search, but...maybe next week!  Or maybe another variable!  Or maybe benchmarks vs <code>GridSearchCV</code> and/or <code>RandomizedSearchCV</code>.  Who knows what the future holds?</p>","url":"https://hackersandslackers.com/random-forests-hyperparameters-min_samples_leaf/","uuid":"766a3eb8-aacc-47c6-91a9-744b84613626","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b9f047cab64c97c60f7be90"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736fd","title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","slug":"tuning-random-forests-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","custom_excerpt":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python","created_at_pretty":"09 September, 2018","published_at_pretty":"10 September, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-09-09T19:14:32.000-04:00","published_at":"2018-09-10T07:30:00.000-04:00","updated_at":"2019-02-19T03:46:39.000-05:00","meta_title":"Code snippet corner is back! Tune the max_depth parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","og_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","og_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","twitter_description":"While n_estimators has a tradeoff between speed & score, max_depth can improve both.  By limiting the depth of your trees, you can reduce overfitting.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/codecorner@2x.jpg","twitter_title":"Tuning Random  Forests Hyperparameters with Binary Search Part II: max_depth","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Continued from here\n[https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/]\n\nNotebook for this post is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb]\n\nBinary search code itself is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nmax_depth  is an interesting parameter.  While n_estimators  has a tradeoff\nbetween speed & score, max_depth  has the possibility of improving both.  By\nlimiting the depth of your trees, you can reduce overfitting.\n\nUnfortunately, deciding on upper & lower bounds is less than straightforward.\n It'll depend on your dataset.  Luckily, I found a post on StackOverflow that\nhad a link to a blog post that had a promising methodology\n[https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html].\n\nFirst, we build a tree with default arguments and fit it to our data. \n\nimport pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)\n\nNow, let's see how deep the trees get when we don't impose any sort of max_depth\n. We'll use the code from that wonderful blog post to crawl our Random Forest,\nand get the height of every tree.\n\n#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#> 2\nnp.hstack(allDepths).max()\n#> 9\n\nWe'll be searching between 2 and 9!\n\nLet's bring back our old make a helper function to easily return scores.\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\nmax_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)\n\nmax_depth\n score\n time\n 2\n 0.987707\n 0.145360\n 9\n 0.987029\n 0.147563\n 6\n 0.986247\n 0.140514\n 4\n 0.968316\n 0.140164\n \nmax_depth\n score\n time\n scoreTimeRatio\n 2\n 1.051571\n 0.837377\n 0.175986\n 9\n 1.016649\n 1.135158\n 0.103478\n 6\n 0.976311\n 0.182516\n 1.000000\n 4\n 0.051571\n 0.135158\n 0.000000\n So, for our purposes, 9 will function as our baseline since that was the\nbiggest depth that it built with default arguments.\n\nLooks like a max_depth  of 2 has a slightly higher score than 9, and is slightly\nfaster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that\nis.","html":"<p>Continued from <a href=\"https://hackersandslackers.com/code-snippet-corner-tuning-machine-learning-hyperparameters-with-binary-search/\">here</a></p><p>Notebook for this post is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Introspect%20Trees%20(Depth).ipynb\">here</a></p><p>Binary search code itself is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p><code>max_depth</code> is an interesting parameter.  While <code>n_estimators</code> has a tradeoff between speed &amp; score, <code>max_depth</code> has the possibility of improving both.  By limiting the depth of your trees, you can reduce overfitting.</p><p>Unfortunately, deciding on upper &amp; lower bounds is less than straightforward.  It'll depend on your dataset.  Luckily, I found a post on StackOverflow that had a link to a blog post that had a promising <a href=\"https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\">methodology</a>.  </p><p>First, we build a tree with default arguments and fit it to our data. </p><pre><code>import pandas as pd\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"n_estimators\": 18,\n         \"oob_score\": True}\n\nclf = RandomForestClassifier(**rfArgs)\nclf.fit(X, y)</code></pre><p>Now, let's see how deep the trees get when we don't impose any sort of <code>max_depth</code>. We'll use the code from that wonderful blog post to crawl our Random Forest, and get the height of every tree.</p><pre><code>#From here: https://aysent.github.io/2015/11/08/random-forest-leaf-visualization.html\ndef leaf_depths(tree, node_id = 0):\n    \n    '''\n    tree.children_left and tree.children_right store ids\n    of left and right chidren of a given node\n    '''\n    left_child = tree.children_left[node_id]\n    right_child = tree.children_right[node_id]\n\n    '''\n    If a given node is terminal, \n    both left and right children are set to _tree.TREE_LEAF\n    '''\n    if left_child == _tree.TREE_LEAF:\n        \n        '''\n        Set depth of terminal nodes to 0\n        '''\n        depths = np.array([0])\n    else:\n        '''\n        Get depths of left and right children and\n        increment them by 1\n        '''\n        left_depths = leaf_depths(tree, left_child) + 1\n        right_depths = leaf_depths(tree, right_child) + 1\n \n        depths = np.append(left_depths, right_depths)\n \n    return depths\n\nallDepths = [leaf_depths(estimator.tree_) \n             for estimator in clf.estimators_]\n\nnp.hstack(allDepths).min()\n#&gt; 2\nnp.hstack(allDepths).max()\n#&gt; 9</code></pre><p>We'll be searching between 2 and 9!  </p><p>Let's bring back our old make a helper function to easily return scores.</p><pre><code>def getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)</code></pre><pre><code>max_depth = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"max_depth\", \n                    0, \n                    2, \n                    9)\nbgs.showTimeScoreChartAndGraph(max_depth, html=True)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://hackersandslackers.com/content/images/2018/09/max_depth.png\" class=\"kg-image\"></figure><table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>0.987707</td>\n      <td>0.145360</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.987029</td>\n      <td>0.147563</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.986247</td>\n      <td>0.140514</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.968316</td>\n      <td>0.140164</td>\n    </tr>\n  </tbody>\n</table>\n<br>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>max_depth</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>1.051571</td>\n      <td>0.837377</td>\n      <td>0.175986</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.016649</td>\n      <td>1.135158</td>\n      <td>0.103478</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.976311</td>\n      <td>0.182516</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.051571</td>\n      <td>0.135158</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>So, for our purposes, 9 will function as our baseline since that was the biggest depth that it built with default arguments.  </p><p>Looks like a <code>max_depth</code> of 2 has a slightly higher score than 9, and is slightly faster!  Interestingly, it's slightly slower than  4 or 6.  Not sure why that is.</p>","url":"https://hackersandslackers.com/tuning-random-forests-hyperparameters-with-binary-search/","uuid":"3c92aed0-61ed-4c1a-b7d5-cc47c709764b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b95a9581fc1fc7d92b5c51f"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ec","title":"Tuning Machine Learning Hyperparameters with Binary Search","slug":"tuning-machine-learning-hyperparameters-with-binary-search","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","custom_excerpt":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python.","created_at_pretty":"30 August, 2018","published_at_pretty":"03 September, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-08-29T21:35:41.000-04:00","published_at":"2018-09-03T07:30:00.000-04:00","updated_at":"2019-02-13T22:50:35.000-05:00","meta_title":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python | Hackers And Slackers","meta_description":"RandomizedSearchCV goes noticeably faster than a full GridSearchCV but it still takes a while - which can be rough.","og_description":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","og_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","og_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","twitter_description":"Tune the n_estimators parameter in for a Random Forests classifier in scikit-learn in Python","twitter_image":"https://hackersandslackers.com/content/images/2018/08/ai2@2x.jpg","twitter_title":"Code Snippet Corner: Tuning Machine Learning Hyperparameters with Binary Search","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},"tags":[{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Ah, hyperparameter tuning.  Time & compute-intensive.  Frequently containing\nweird non-linearities in how changing a parameter changes the score and/or the\ntime it takes to train the model.\n\nRandomizedSearchCV  goes noticeably faster than a full GridSearchCV  but it\nstill takes a while - which can be rough, because in my experience you do still\nneed to be iterative with it and experiment with different distributions.  Plus,\nthen you've got hyper-hyperparameters to tune - how many iterations SHOULD you\nrun it for, anyway?\n\nI've been experimenting with using the trusty old Binary Search to tune\nhyperparameters.  I'm finding it has two advantages.\n\n 1. It's blazing fast\n 2. The performance is competitive with a Randomized Search\n 3. It gives you a rough sketch of \"the lay of the land\".  An initial binary\n    search can then provide parameters for future searches, including with Grid\n    or Randomized Searches.\n\nCode is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py]\n\nNotebook summary is here\n[https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb]\n\nLet's see it in action!\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nWe'll be using a Random Forest classifier, because, as with all my code posts,\nit's what I've been using recently.\n\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n\nWe'll be using scikit-learn's breast cancer dataset, because I remembered that\nthese packages I'm posting about have built-in demo datasets that I should be\nusing for posts.\n\nrfArgs = {\"random_state\": 0,\n          \"n_jobs\": -1,\n          \"class_weight\": \"balanced\",\n         \"oob_score\": True}\n\n\nLet's set our random_state  for better reproducibility.\nWe'll set n_jobs=-1  because obviously we want to use all our cores, we are not\npatient people.\n\n\nWe'll have class_weight=\"balanced\"  because that'll compensate for the fact that\nthe breast cancer dataset (like most medical datasets) has unbalanced classes.\nWe'll use oob_score  because we like being lazy, part of the appeal of Random\nForests is the opportunity to be extra lazy (no need to normalize features!),\nand oob  lets us be even lazier  by giving some built-in cross-validation.\n\nNow let's define a function that'll take all this, and spit out a score.  I\nwrote the binary search function to take a function like this as an argument -\nscikit-learn is usually pretty consistent when it comes to the interface it\nprovides you, but sometimes different algorithms need to work a little\ndifferently.  For instance, since we'll be using Area Under \nprecision_recall_curve  as our metric (a good choice for classifiers with\nunbalanced classes!), it takes a teensy bit of extra fiddling to get it to play\nnicely with our oob_decision_function_.\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n\n\nWe'll try to optimize the n_estimators  parameter first.  For two reasons:\n\n 1. Finding a good mix between speed and accuracy here will make it easier to\n    tune subsequent parameters.\n 2. It's the most straightforward to decide upper and lower bounds for.  Other\n    ones (like, say, max_depth) require a little work to figure the potential\n    range to search in.\n\nOkay!  So, let's put our lower limit as 32 and our upper limit as 128, because I\nread in a StackOverflow post that there's a paper that says to search within\nthat range.\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nPlotting score, time, and the ratio between them - we're not just optimizing for\nthe best score right now, we're looking for tipping points that give us good\ntradeoffs.  Scores and times are normalized for a more-meaningful ratio between\nthem.\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 32\n 1.073532\n 0.002459\n 1.000000\n 1\n 128\n 1.867858\n 1.002459\n 0.000000\n 2\n 80\n 2.052255\n 0.440060\n 0.006443\n 3\n 56\n 1.605447\n 0.075185\n 0.044843\n 4\n 68\n 1.910411\n 0.107187\n 0.036721\n 5\n 74\n 2.066440\n 0.377136\n 0.008320\n 6\n 77\n 2.066440\n 0.388378\n 0.007955\n 7\n 75\n 2.073532\n 0.457481\n 0.006141\n n_estimators\n score\n time\n 0\n 32\n 0.988663\n 0.180521\n 1\n 128\n 0.989403\n 0.587113\n 2\n 80\n 0.989575\n 0.358446\n 3\n 56\n 0.989159\n 0.210091\n 4\n 68\n 0.989443\n 0.223102\n 5\n 74\n 0.989588\n 0.332861\n 6\n 77\n 0.989588\n 0.337432\n 7\n 75\n 0.989595\n 0.365529\n Hrm, looks like the score starts getting somewhere interesting around 68, and\ntime starts shooting up at about 80.  Let's do another with those as our bounds!\n\nn_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    \"n_estimators\", \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n\n\nn_estimators\n score\n time\n scoreTimeRatio\n 0\n 68\n 6.390333\n 0.333407\n 0.135692\n 1\n 80\n 7.223667\n 1.064343\n 0.000000\n 2\n 74\n 7.307000\n 0.404471\n 0.123622\n 3\n 71\n 6.307000\n 0.064343\n 1.000000\n 4\n 72\n 6.390333\n 0.175190\n 0.325419\n n_estimators\n score\n time\n 0\n 68\n 0.989443\n 0.344220\n 1\n 80\n 0.989575\n 0.355580\n 2\n 74\n 0.989588\n 0.345324\n 3\n 71\n 0.989430\n 0.340038\n 4\n 72\n 0.989443\n 0.341761\n 71 looks like our winner!  Or close enough for our purposes while we then go\noptimize other things.  And we only had to train our model 13 times - as opposed\nto the 96 we would have with a brute-force grid search.\n\nHopefully this will become a series on using this to tune other RF\nhyperparameters - other ones have some interesting quirks that I'd like to\nexpound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!","html":"<p>Ah, hyperparameter tuning.  Time &amp; compute-intensive.  Frequently containing weird non-linearities in how changing a parameter changes the score and/or the time it takes to train the model.</p><p><code>RandomizedSearchCV</code> goes noticeably faster than a full <code>GridSearchCV</code> but it still takes a while - which can be rough, because in my experience you do still need to be iterative with it and experiment with different distributions.  Plus, then you've got hyper-hyperparameters to tune - how many iterations SHOULD you run it for, anyway?</p><p>I've been experimenting with using the trusty old Binary Search to tune hyperparameters.  I'm finding it has two advantages.</p><ol><li>It's blazing fast</li><li>The performance is competitive with a Randomized Search</li><li>It gives you a rough sketch of \"the lay of the land\".  An initial binary search can then provide parameters for future searches, including with Grid or Randomized Searches.</li></ol><p>Code is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/binarygridsearch/binarygridsearch.py\">here</a></p><p>Notebook summary is <a href=\"https://github.com/mattalhonte/binary-grid-search/blob/master/Binary%20Search%20Interactive%20(n_estimators).ipynb\">here</a></p><p>Let's see it in action!</p><pre><code class=\"language-python\">from sklearn.ensemble import RandomForestClassifier\n</code></pre>\n<p>We'll be using a Random Forest classifier, because, as with all my code posts, it's what I've been using recently.</p><pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX, y = data.data, data.target\n</code></pre>\n<p>We'll be using scikit-learn's breast cancer dataset, because I remembered that these packages I'm posting about have built-in demo datasets that I should be using for posts.</p><pre><code class=\"language-python\">rfArgs = {&quot;random_state&quot;: 0,\n          &quot;n_jobs&quot;: -1,\n          &quot;class_weight&quot;: &quot;balanced&quot;,\n         &quot;oob_score&quot;: True}\n</code></pre>\n<p>Let's set our <code>random_state</code> for better reproducibility.<br>We'll set <code>n_jobs=-1</code> because obviously we want to use all our cores, we are not patient people.</p><p><br>We'll have <code>class_weight=\"balanced\"</code> because that'll compensate for the fact that the breast cancer dataset (like most medical datasets) has unbalanced classes.<br>We'll use <code>oob_score</code> because we like being lazy, part of the appeal of Random Forests is the opportunity to be extra lazy (no need to normalize features!), and <code>oob</code> lets us be <em>even lazier</em> by giving some built-in cross-validation.</p><p>Now let's define a function that'll take all this, and spit out a score.  I wrote the binary search function to take a function like this as an argument - scikit-learn is usually pretty consistent when it comes to the interface it provides you, but sometimes different algorithms need to work a little differently.  For instance, since we'll be using Area Under <code>precision_recall_curve</code> as our metric (a good choice for classifiers with unbalanced classes!), it takes a teensy bit of extra fiddling to get it to play nicely with our <code>oob_decision_function_</code>.</p><pre><code class=\"language-python\">from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\n\ndef getForestAccuracy(X, y, kwargs):\n    clf = RandomForestClassifier(**kwargs)\n    clf.fit(X, y)\n    y_pred = clf.oob_decision_function_[:, 1]\n    precision, recall, _ = precision_recall_curve(y, y_pred)\n    return auc(recall, precision)\n</code></pre>\n<p>We'll try to optimize the <code>n_estimators</code> parameter first.  For two reasons:</p><ol><li>Finding a good mix between speed and accuracy here will make it easier to tune subsequent parameters.</li><li>It's the most straightforward to decide upper and lower bounds for.  Other ones (like, say, <code>max_depth</code>) require a little work to figure the potential range to search in.</li></ol><p>Okay!  So, let's put our lower limit as 32 and our upper limit as 128, because I read in a StackOverflow post that there's a paper that says to search within that range.</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    18, \n                    128)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<p>Plotting score, time, and the ratio between them - we're not just optimizing for the best score right now, we're looking for tipping points that give us good tradeoffs.  Scores and times are normalized for a more-meaningful ratio between them.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--2-.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>1.073532</td>\n      <td>0.002459</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>1.867858</td>\n      <td>1.002459</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>2.052255</td>\n      <td>0.440060</td>\n      <td>0.006443</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1.605447</td>\n      <td>0.075185</td>\n      <td>0.044843</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>1.910411</td>\n      <td>0.107187</td>\n      <td>0.036721</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>2.066440</td>\n      <td>0.377136</td>\n      <td>0.008320</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>2.066440</td>\n      <td>0.388378</td>\n      <td>0.007955</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>2.073532</td>\n      <td>0.457481</td>\n      <td>0.006141</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>0.988663</td>\n      <td>0.180521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>128</td>\n      <td>0.989403</td>\n      <td>0.587113</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.358446</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>0.989159</td>\n      <td>0.210091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.223102</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.332861</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>77</td>\n      <td>0.989588</td>\n      <td>0.337432</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75</td>\n      <td>0.989595</td>\n      <td>0.365529</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Hrm, looks like the score starts getting somewhere interesting around 68, and time starts shooting up at about 80.  Let's do another with those as our bounds!</p><pre><code class=\"language-python\">n_estimators = bgs.compareValsBaseCase(X, \n                    y, \n                    getForestAccuracy,        \n                    rfArgs, \n                    &quot;n_estimators&quot;, \n                    0, \n                    68, \n                    80)\n\nbgs.showTimeScoreChartAndGraph(n_estimators)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/max_depth.png\" class=\"kg-image\"></figure><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n      <th>scoreTimeRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>6.390333</td>\n      <td>0.333407</td>\n      <td>0.135692</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>7.223667</td>\n      <td>1.064343</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>7.307000</td>\n      <td>0.404471</td>\n      <td>0.123622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>6.307000</td>\n      <td>0.064343</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>6.390333</td>\n      <td>0.175190</td>\n      <td>0.325419</td>\n    </tr>\n  </tbody>\n</table>\n</div><div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_estimators</th>\n      <th>score</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>68</td>\n      <td>0.989443</td>\n      <td>0.344220</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>0.989575</td>\n      <td>0.355580</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74</td>\n      <td>0.989588</td>\n      <td>0.345324</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>0.989430</td>\n      <td>0.340038</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>0.989443</td>\n      <td>0.341761</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>71 looks like our winner!  Or close enough for our purposes while we then go optimize other things.  And we only had to train our model 13 times - as opposed to the 96 we would have with a brute-force grid search.</p><p>Hopefully this will become a series on using this to tune other RF hyperparameters - other ones have some interesting quirks that I'd like to expound upon.  Or you could just look at the GitHub repo for spoilers.  Or both!</p>","url":"https://hackersandslackers.com/tuning-machine-learning-hyperparameters-with-binary-search/","uuid":"ca7241c3-52cd-4910-86dc-0bb5474d07af","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b8749ed4b98380b152292ea"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736d6","title":"Importing Excel Datetimes Into Pandas, Part II","slug":"importing-excel-datetimes-into-pandas-part-2","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas.jpg","excerpt":"Pandas and Excel Pt. 2","custom_excerpt":"Pandas and Excel Pt. 2","created_at_pretty":"13 August, 2018","published_at_pretty":"20 August, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-08-12T22:28:46.000-04:00","published_at":"2018-08-20T07:30:00.000-04:00","updated_at":"2019-04-10T00:47:31.000-04:00","meta_title":"Pandas and Excel Pt. 2 | Hackers And Slackers","meta_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","og_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","og_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas-2.jpg","og_title":"Importing Excel Datetimes Into Pandas II","twitter_description":"Read date times from Excel files into a Pandas DataFrame. Utilize Python's Arrow and Toolz libraries to write a quick script for date extraction.","twitter_image":"https://hackersandslackers.com/content/images/2019/04/excelpandas-1.jpg","twitter_title":"Importing Excel Datetimes Into Pandas II","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"What if, like during my data import task a few months back, the dates & times\nare in separate columns?  This gives us a few new issues.  Let's import that\nExcel file!\n\nimport pandas as pd\nimport xlrd\nimport datetime\n\ndf = pd.read_excel(\"hasDatesAndTimes.xlsx\", sheet_name=\"Sheet1\")\n\nbook = xlrd.open_workbook(\"hasDatesAndTimes.xlsx\")\ndatemode = book.datemode\n\n\n\nAnd let's see that time variable!\n\ndf[\"Time\"]\n\n\nIndex\n Time\n 0\n 0.909907\n 1\n 0.909919\n 2\n 0.909931\n 3\n 0.909942\n 4\n 0.909954\n df[\"Time\"].map(lambda x: xlrd.xldate_as_tuple(x, datemode))\n\n\nSo far, so good....\n\ndf[\"Time\"].map(lambda x: datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                              datemode)))\nValueError: year 0 is out of range\n\n\nAgh!  Plain datetime won't let us have 0 as our year.\n\nWe'll want two packages to fix this.  One is an awesome package for handling\ndates called arrow.  In order for arrow  to recognize what we want it to,\nthough, we'll need some more manipulations - I'll be using the pipe  function\nfrom toolz  in order to make that more readable.\n\nIndex\n Time\n 0\n (0, 0, 0, 21, 50, 16)\n 1\n (0, 0, 0, 21, 50, 17)\n 2\n (0, 0, 0, 21, 50, 18)\n 3\n (0, 0, 0, 21, 50, 19)\n 4\n (0, 0, 0, 21, 50, 20)\n Pipe lets us have a nice workflow where we just list the transformations we\nwant, and our value will be \"piped\" sequentially through each one.\n\nfns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n     lambda x: x[3:6],\n      lambda x: map(str, x),\n      lambda x: \"-\".join(x),\n       lambda x: arrow.get(x, \"H-m-s\"),\n       lambda x: x.format('HH:mm:ss')\n      ]\n\n\nLet's see a blow-by-blow of how one of our values gets transformed by that.\n\nfnRanges = [fns[:i+1] for i in range(len(fns))]\n[pipe(0.909907, *x) for x in fnRanges]\n\n[(0, 0, 0, 21, 50, 16),\n (21, 50, 16),\n <map at 0x7f105151af98>,\n '21-50-16',\n <Arrow [0001-01-01T21:50:16+00:00]>,\n '21:50:16']\n\n\nThe first function takes us from an Excel datetime to a datetime tuple.\n\nThe next extracts just the time variables.\n\nWe then map that all to a string (which shows up as nothing because map  is\nevaluated lazily).\n\nThen we put a dash between all those elements so it'll be easier to parse as a\ntime.\n\nThen arrow  consumes the value, with the format we specified.\n\nAnd finally gives us a neatly-formatted time, ready to be consumed by a\ndatabase!\n\nHelper Functions\ndef mapPipe(ser, fns):\n    return ser.map(lambda a: pipe(a, *fns),\n        na_action=\"ignore\" )\n\nmapPipe(df['Time'],\n   fns)\n\n\nIndex\n Time\n 0\n 21:50:16\n 1\n 21:50:17\n 2\n 21:50:18\n 3\n 21:50:19\n 4\n 21:50:20\n Dates are a bit easier - though the pipe  syntax is still helpful!\n\ndateFns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n      lambda x: arrow.get(*x),\n      lambda x: x.format('YYYY-MM-DD')\n      ]\nmapPipe(df['Date'],\n       dateFns)\n\n\nIndex\n Date\n 0\n 2018-08-12\n 1\n 2018-08-12\n 2\n 2018-08-12\n 3\n 2018-08-12\n 4\n 2018-08-12\n Put it all together....\n\n(df.assign(Date = mapPipe(df['Date'],\n       dateFns))\n   .assign(Time = mapPipe(df['Time'],\n       fns)))\n\n\nIndex\n Date\n Time\n 0\n 2018-08-12\n 21:50:16\n 1\n 2018-08-12\n 21:50:17\n 2\n 2018-08-12\n 21:50:18\n 3\n 2018-08-12\n 21:50:19\n 4\n 2018-08-12\n 21:50:20","html":"<p>What if, like during my data import task a few months back, the dates &amp; times are in separate columns?  This gives us a few new issues.  Let's import that Excel file!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import pandas as pd\nimport xlrd\nimport datetime\n\ndf = pd.read_excel(&quot;hasDatesAndTimes.xlsx&quot;, sheet_name=&quot;Sheet1&quot;)\n\nbook = xlrd.open_workbook(&quot;hasDatesAndTimes.xlsx&quot;)\ndatemode = book.datemode\n\n</code></pre>\n<!--kg-card-end: markdown--><p>And let's see that time variable!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;]\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.909907</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.909919</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.909931</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.909942</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.909954</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;].map(lambda x: xlrd.xldate_as_tuple(x, datemode))\n</code></pre>\n<!--kg-card-end: markdown--><p>So far, so good....</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">df[&quot;Time&quot;].map(lambda x: datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                              datemode)))\nValueError: year 0 is out of range\n</code></pre>\n<!--kg-card-end: markdown--><p>Agh!  Plain datetime won't let us have 0 as our year.</p><p>We'll want two packages to fix this.  One is an awesome package for handling dates called <code>arrow</code>.  In order for <code>arrow</code> to recognize what we want it to, though, we'll need some more manipulations - I'll be using the <code>pipe</code> function from <code>toolz</code> in order to make that more readable.</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>(0, 0, 0, 21, 50, 16)</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>(0, 0, 0, 21, 50, 17)</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>(0, 0, 0, 21, 50, 18)</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>(0, 0, 0, 21, 50, 19)</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>(0, 0, 0, 21, 50, 20)</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Pipe lets us have a nice workflow where we just list the transformations we want, and our value will be \"piped\" sequentially through each one.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">fns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n     lambda x: x[3:6],\n      lambda x: map(str, x),\n      lambda x: &quot;-&quot;.join(x),\n       lambda x: arrow.get(x, &quot;H-m-s&quot;),\n       lambda x: x.format('HH:mm:ss')\n      ]\n</code></pre>\n<!--kg-card-end: markdown--><p>Let's see a blow-by-blow of how one of our values gets transformed by that.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">fnRanges = [fns[:i+1] for i in range(len(fns))]\n[pipe(0.909907, *x) for x in fnRanges]\n\n[(0, 0, 0, 21, 50, 16),\n (21, 50, 16),\n &lt;map at 0x7f105151af98&gt;,\n '21-50-16',\n &lt;Arrow [0001-01-01T21:50:16+00:00]&gt;,\n '21:50:16']\n</code></pre>\n<!--kg-card-end: markdown--><p>The first function takes us from an Excel datetime to a datetime tuple.</p><p>The next extracts just the time variables.</p><p>We then map that all to a string (which shows up as nothing because <code>map</code> is evaluated lazily).</p><p>Then we put a dash between all those elements so it'll be easier to parse as a time.</p><p>Then <code>arrow</code> consumes the value, with the format we specified.</p><p>And finally gives us a neatly-formatted time, ready to be consumed by a database!</p><h2 id=\"helper-functions\">Helper Functions</h2><!--kg-card-begin: markdown--><pre><code class=\"language-python\">def mapPipe(ser, fns):\n    return ser.map(lambda a: pipe(a, *fns),\n        na_action=&quot;ignore&quot; )\n\nmapPipe(df['Time'],\n   fns)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>21:50:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>21:50:17</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>21:50:18</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>21:50:19</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Dates are a bit easier - though the <code>pipe</code> syntax is still helpful!</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">dateFns = [lambda x: xlrd.xldate_as_tuple(x, datemode),\n      lambda x: arrow.get(*x),\n      lambda x: x.format('YYYY-MM-DD')\n      ]\nmapPipe(df['Date'],\n       dateFns)\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n    <table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2018-08-12</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2018-08-12</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html--><p>Put it all together....</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">(df.assign(Date = mapPipe(df['Date'],\n       dateFns))\n   .assign(Time = mapPipe(df['Time'],\n       fns)))\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Index</th>\n      <th>Date</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2018-08-12</td>\n      <td>21:50:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2018-08-12</td>\n      <td>21:50:17</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2018-08-12</td>\n      <td>21:50:18</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2018-08-12</td>\n      <td>21:50:19</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2018-08-12</td>\n      <td>21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><!--kg-card-end: html-->","url":"https://hackersandslackers.com/importing-excel-datetimes-into-pandas-part-2/","uuid":"f106291a-af02-4b8a-87b4-7afe333a9548","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b70ecde0230162100a1daa5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736d3","title":"Importing Excel Datetimes Into Pandas, Part I","slug":"importing-excel-dates-times-into-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1.jpg","excerpt":"Pandas & Excel, Part 1.","custom_excerpt":"Pandas & Excel, Part 1.","created_at_pretty":"13 August, 2018","published_at_pretty":"13 August, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-08-12T20:21:40.000-04:00","published_at":"2018-08-13T07:30:00.000-04:00","updated_at":"2019-04-09T23:40:15.000-04:00","meta_title":"Pandas & Excel, Part 1 | Hackers And Slackers","meta_description":"Import dates & times from Excel .xlsx files into Pandas!","og_description":"Import dates & times from Excel .xlsx files into Pandas!","og_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1-2.jpg","og_title":"Importing Excel Datetimes Into Pandas","twitter_description":"Import dates & times from Excel .xlsx files into Pandas!","twitter_image":"https://hackersandslackers.com/content/images/2019/04/excelpandaspart1-1-1.jpg","twitter_title":"Importing Excel Datetimes Into Pandas","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Excel","slug":"excel","description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/excelseries-1.jpg","meta_description":"Pro Excel secrets and magic. The kind of industry knowledge to put junior finance guys out of business.","meta_title":"Adventures in Excel","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Different file formats are different!  For all kinds of reasons!\n\nA few months back, I had to import some Excel files into a database. In this\nprocess I learned so much about the delightfully unique way Excel stores dates &\ntimes!\n\nThe basic datetime will be a decimal number, like 43324.909907407404.  The\nnumber before the decimal is the day, the number afterwards is the time.  So\nfar, so good - this is pretty common for computers.  The date is often the\nnumber of days past a certain date, and the time is the number of seconds.\n\nSo, let's load our excel sheet!  Pandas of course has a painless way of doing\nthis.\n\nimport pandas as pd\n\ndfRaw = pd.read_excel(\"hasDates.xlsx\", sheet_name=\"Sheet1\")\n\ndfRaw[\"dateTimes\"]\n\n\n0\n 0\n 43324.909907\n 1\n 43324.909919\n 2\n 43324.909931\n 3\n 43324.909942\n 4\n 43324.909954\n Sadly, we can't yet convert these.  Different Excel files start at different\ndates, and you'll get a very wrong result if you use the wrong one.  Luckily,\nthere are tools that'll go into the file and extract what we need!  Enter xlrd:\n\nimport xlrd\n\nbook = xlrd.open_workbook(\"hasDates.xlsx\")\ndatemode = book.datemode\n\n\nxlrd  also has a handy function for turning those dates into a datetime  tuple\nthat'll play nicely with Python.\n\ndfRaw[\"dateTimes\"].map(lambda x: \n          xlrd.xldate_as_tuple(x, datemode))\n\n\n0\n 0\n (2018, 8, 12, 21, 50, 16)\n 1\n (2018, 8, 12, 21, 50, 17)\n 2\n (2018, 8, 12, 21, 50, 18)\n 3\n (2018, 8, 12, 21, 50, 19)\n 4\n (2018, 8, 12, 21, 50, 20)\n And once we've got that, simple enough to convert to proper datetimes!\n\nimport datetime\n\ndfRaw[\"dateTimes\"].map(lambda x: \n          datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                                  datemode)))\n\n\n0\n 0\n 2018-08-12 21:50:16\n 1\n 2018-08-12 21:50:17\n 2\n 2018-08-12 21:50:18\n 3\n 2018-08-12 21:50:19\n 4\n 2018-08-12 21:50:20\n Stick around for Part 2, where we look at some messier situations.","html":"<p>Different file formats are different!  For all kinds of reasons!</p><p>A few months back, I had to import some Excel files into a database. In this process I learned so much about the delightfully unique way Excel stores dates &amp; times!  </p><p>The basic datetime will be a decimal number, like <code>43324.909907407404</code>.  The number before the decimal is the day, the number afterwards is the time.  So far, so good - this is pretty common for computers.  The date is often the number of days past a certain date, and the time is the number of seconds.  </p><p>So, let's load our excel sheet!  Pandas of course has a painless way of doing this.</p><pre><code class=\"language-python\">import pandas as pd\n\ndfRaw = pd.read_excel(&quot;hasDates.xlsx&quot;, sheet_name=&quot;Sheet1&quot;)\n\ndfRaw[&quot;dateTimes&quot;]\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43324.909907</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43324.909919</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43324.909931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43324.909942</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>43324.909954</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Sadly, we can't yet convert these.  Different Excel files start at different dates, and you'll get a very wrong result if you use the wrong one.  Luckily, there are tools that'll go into the file and extract what we need!  Enter <code>xlrd</code>:</p><pre><code class=\"language-python\">import xlrd\n\nbook = xlrd.open_workbook(&quot;hasDates.xlsx&quot;)\ndatemode = book.datemode\n</code></pre>\n<p><code>xlrd</code> also has a handy function for turning those dates into a <code>datetime</code> tuple that'll play nicely with Python.</p><pre><code class=\"language-python\">dfRaw[&quot;dateTimes&quot;].map(lambda x: \n          xlrd.xldate_as_tuple(x, datemode))\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(2018, 8, 12, 21, 50, 16)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(2018, 8, 12, 21, 50, 17)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(2018, 8, 12, 21, 50, 18)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(2018, 8, 12, 21, 50, 19)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(2018, 8, 12, 21, 50, 20)</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>And once we've got that, simple enough to convert to proper datetimes!</p><pre><code class=\"language-python\">import datetime\n\ndfRaw[&quot;dateTimes&quot;].map(lambda x: \n          datetime.datetime(*xlrd.xldate_as_tuple(x, \n                                                  datemode)))\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-08-12 21:50:16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-08-12 21:50:17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-08-12 21:50:18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-08-12 21:50:19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-08-12 21:50:20</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Stick around for Part 2, where we look at some messier situations.</p>","url":"https://hackersandslackers.com/importing-excel-dates-times-into-pandas/","uuid":"727f6571-8ca4-4abc-b278-c7517cdaa29b","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b70cf140230162100a1da9b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736c4","title":"All That Is Solid Melts Into Graphs","slug":"all-that-is-solid-melts-into-graphs","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","excerpt":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair.","custom_excerpt":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair.","created_at_pretty":"26 July, 2018","published_at_pretty":"30 July, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-07-25T21:53:23.000-04:00","published_at":"2018-07-30T07:30:00.000-04:00","updated_at":"2019-02-02T04:07:03.000-05:00","meta_title":"All That Is Solid Melts Into Graphs | Hackers and Slackers","meta_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with Altair","og_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with #Altair","og_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","og_title":"All That Is Solid Melts Into Graphs","twitter_description":"Reshaping Pandas dataframes with a real-life example, and graphing it with #Altair","twitter_image":"https://hackersandslackers.com/content/images/2018/07/iceberg@2x.jpg","twitter_title":"All That Is Solid Melts Into Graphs","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Last few Code Snippet Corners were about using Pandas as an easy way to handle\ninput and output between files & databases.  Let's shift gears a little bit!\n Among other reasons, because earlier today I discovered a package that\nexclusively does that, which means I can stop importing the massive Pandas\npackage when all I really wanted to do with it was take advantage of its I/O\nmodules.Check it out [https://github.com/insightindustry/sqlathanor]! \n\nSo, rather than the entrances & exits, let's focus on all the crazy ways you can\nreshape data with Pandas!\n\nOur Data\nFor our demonstration, I'll use a dataset based on something I was once actually\nsent.  It was a big CSV with sensor readings from HVAC systems.  Each line had a\ndifferent house, a different room, a datetime, and readings from a bunch of\ndifferent types of sensors.  Oh, hrm, I probably shouldn't use data I got from a\nclient.  Uh...\n\nBONUS SECTION!\nGENERATING DUMMY TEMPERATURE DATA\n(Feel free to skip to next part if you don't care)\n\nWe want it to fluctuate, but we don't want to just make a bunch of totally\nrandom values - a reading should have some relationship to the reading taken a\nsecond earlier.\n\nLet's use NumPy  for some Randomness, and the accumulate  and repeat  functions\nfrom itertools.  Maybe I'll do an in-depth post on these at some point, but the\ncode I'll be writing with them will be pretty short and hopefully somewhat\nself-demonstrating.  If you wanna go deeper here's some good material: Official\nDocs [https://docs.python.org/3/library/itertools.html], Good article\n[https://realpython.com/python-itertools/]\n\nimport numpy as np\nfrom itertools import accumulate, repeat\n\n\nWe want there to be some random \"noise\", but we also want the occasional\nsubstantive change.  We'll reflect this by having it so that 90% of the time we\nget a small fluctuation, with a 10% chance of a smaller fluctuation. \n\ndef genTempDataPoint(x, *args):\n    if np.random.rand(1) <= 0.9:\n        return x + np.random.uniform(-3,3,1)[0]\n    else:\n        return x + np.random.uniform(-10,10,1)[0]\n\n\n  Now let's see some test points!\n\nlist(accumulate(repeat(70, 5), genTempDataPoint))\n[70,\n 69.00258239202094,\n 59.34919781643355,\n 56.60722073795931,\n 57.265078261782946]\n\n\nSure, fine, why not.  Good enough for our purposes!   Now let's put it all\ntogether so we can just call it with a base temp and the number of points we\nwant.\n\ndef genTempData(base, n):\n    return list(accumulate(repeat(base, n), \n                           genTempDataPoint))\n\n\nTo simulate the dataset, we actually need to mix it up.  Or else what good are\nthe GroupBys gonna be?  So, let's create a problem to fix later!  Here's a\nfunction to create a simplified version of the dataset - each row will have a\nlocation ID, a number corresponding to time (just raw ints, I'm not making\nactual datetimes - I've spent too much time on this part already).  We'll also\ngenerate humidity values, to add another monkey wrench to fix later (we'll still\nuse the genTempData  function).\n\nfrom itertools import chain\n\ndef makeLocation(name, base1, n1, base2, n2):\n    return [(x[0], name, x[1][0], x[1][1]) \n        for x in enumerate(zip(genTempData(base1, n1),\n              genTempData(base2, n2)) )]\n\nbigList = list(chain.from_iterable(makeLocation(str(x), \n                                                70, \n                                                15,\n                                                40, \n                                                15) \n                         for x in range(5)))\nnp.random.shuffle(bigList)\n\ndf = pd.DataFrame(bigList, \n                  columns = [\"Time\", \"Loc\", \"Temp\", \"Hum\"])\n\n\nBack To The Main Plot\nLet's look at some test rows!\n\n# Viewing test rows\n\ndf.iloc[:5]\nTime\tLoc\tTemp     \tHum\n10\t4\t68.396970\t34.169753\n13\t0\t80.288846\t42.076786\n7\t4\t69.923273\t37.967951\n6\t0\t71.781362\t41.186802\n5\t2\t62.678844\t37.321636\n\n\nNow, when I'm getting started with a new dataset, one of the first things I like\nto do is make some graphs.  As of late, my favorite package has been Altair\n[https://altair-viz.github.io/].  Looks very nice by default, is pretty easy to\niterate with, and has nice declarative syntax.\n\nOnly one problem!  It wants date in \"long-form\" - as in, rather than each row\nhaving several variables of interest, each row has one (or more) \"ID\" variables,\none numerical value, and the name of the variable we're measuring.  So for\ninstance, something more like this:\n\nTime\tLoc\tvariable\tvalue\n10\t4\tTemp\t        68.396970\n13\t0\tTemp\t        80.288846\n7\t4\tTemp\t        69.923273\n6\t0\tTemp\t        71.781362\n5\t2\tTemp\t        62.678844\n\n\nNot quite sure why!  Buuut, that's kind of a feature of modern coding - we're\nsitting on an inheritance of libraries that have built up over the years, and so\nmore often than not we're just building the \"plumbing\" between existing stuff.\n It's cool!  And good!  It lets us separate Function from Implementation.  We\ndon't need to know what's going on under the hood - we just need to know thing X\nwill produce an output we want, and that in order to get it we first need to\nreshape what we've already got into an input that it'll accept.  Since that's\nsuch a huge part of coding these days, Pandas' power in that realm is super\nuseful.\n\nSooo, how do we get from here to there?  Shockingly easily!\n\nmelted = pd.melt(df, id_vars=[\"Time\", \"Loc\"])\n\n\nDone!\n\nWell, obviously we're not REALLY done yet.  Half the point of having such terse,\nexpressive code is that we can do MORE things!\n\nLet's say we want to see how humidity & temperature change over the course of\nthe day.  First, we'll have to grab all the readings from a single location.\n Let's say Location 3!\n\nloc3 = melted[melted[\"Loc\"]==\"3\"]\n\n\nAltair's pretty neat.\n\n(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O', #We're encoding time as an Ordinal \n         y='value:Q',\n         color='variable:N'))\n\n\nHrm, lot of room there at the bottom.  If we were in an interactive session, we\ncould make this interactive (zoomable and navigable!) by just adding the \n.interactive()  method to the end, but I don't know how to do that in the blog.\n Regardless, it's pretty easy to rescale if we want a closer look!\n\n(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='variable:N'))\n\n\nLet's try it with just temperature, and color will encode the location!\n\nmeltedJustTemp = pd.melt(df, \n                         id_vars=[\"Time\", \"Loc\"],\n                        value_vars= [\"Temp\"])\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y='value:Q',\n         color='Loc:N'))\n\n\nLet's zoom in again...\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='Loc:N'))\n\n\nAltair also lets us Facet our graphs extremely flexibly & painlessly.\n\nalt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='Loc:N',\n      column=\"variable\")\n\n\nOr how about another way!  Let's see humidity & temp, location by location.\n\nalt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='variable:N',\n      row=\"Loc\")\n\n\nWe could make them nicer (there's a WIDE array of customizations), but I'm\nlooking to simulate Exploratory Data Analysis.  I can't think of another\ngraphing package in Python that has quite this level of \"instant gratification\"\nfor so many different variations.","html":"<p>Last few Code Snippet Corners were about using Pandas as an easy way to handle input and output between files &amp; databases.  Let's shift gears a little bit!  Among other reasons, because earlier today I discovered a package that exclusively does that, which means I can stop importing the massive Pandas package when all I really wanted to do with it was take advantage of its I/O modules.  <a href=\"https://github.com/insightindustry/sqlathanor\">Check it out</a>! </p><p>So, rather than the entrances &amp; exits, let's focus on all the crazy ways you can reshape data with Pandas!</p><h2 id=\"our-data\">Our Data</h2><p>For our demonstration, I'll use a dataset based on something I was once actually sent.  It was a big CSV with sensor readings from HVAC systems.  Each line had a different house, a different room, a datetime, and readings from a bunch of different types of sensors.  Oh, hrm, I probably shouldn't use data I got from a client.  Uh...</p><h2 id=\"bonus-section-\">BONUS SECTION!</h2><h3 id=\"generating-dummy-temperature-data\">GENERATING DUMMY TEMPERATURE DATA</h3><p><strong>(Feel free to skip to next part if you don't care)</strong></p><p>We want it to fluctuate, but we don't want to just make a bunch of totally random values - a reading should have some relationship to the reading taken a second earlier.</p><p>Let's use <code>NumPy</code> for some Randomness, and the <code>accumulate</code> and <code>repeat</code> functions from <code>itertools</code>.  Maybe I'll do an in-depth post on these at some point, but the code I'll be writing with them will be pretty short and hopefully somewhat self-demonstrating.  If you wanna go deeper here's some good material: <a href=\"https://docs.python.org/3/library/itertools.html\">Official Docs</a>, <a href=\"https://realpython.com/python-itertools/\">Good article</a></p><pre><code class=\"language-python\">import numpy as np\nfrom itertools import accumulate, repeat\n</code></pre>\n<p>We want there to be some random \"noise\", but we also want the occasional substantive change.  We'll reflect this by having it so that 90% of the time we get a small fluctuation, with a 10% chance of a smaller fluctuation. </p><pre><code class=\"language-python\">def genTempDataPoint(x, *args):\n    if np.random.rand(1) &lt;= 0.9:\n        return x + np.random.uniform(-3,3,1)[0]\n    else:\n        return x + np.random.uniform(-10,10,1)[0]\n</code></pre>\n<p> Now let's see some test points!</p><pre><code class=\"language-python\">list(accumulate(repeat(70, 5), genTempDataPoint))\n[70,\n 69.00258239202094,\n 59.34919781643355,\n 56.60722073795931,\n 57.265078261782946]\n</code></pre>\n<p>Sure, fine, why not.  Good enough for our purposes!   Now let's put it all together so we can just call it with a base temp and the number of points we want.</p><pre><code class=\"language-python\">def genTempData(base, n):\n    return list(accumulate(repeat(base, n), \n                           genTempDataPoint))\n</code></pre>\n<p>To simulate the dataset, we actually need to mix it up.  Or else what good are the GroupBys gonna be?  So, let's create a problem to fix later!  Here's a function to create a simplified version of the dataset - each row will have a location ID, a number corresponding to time (just raw ints, I'm not making actual datetimes - I've spent too much time on this part already).  We'll also generate humidity values, to add another monkey wrench to fix later (we'll still use the <code>genTempData</code> function).</p><pre><code class=\"language-python\">from itertools import chain\n\ndef makeLocation(name, base1, n1, base2, n2):\n    return [(x[0], name, x[1][0], x[1][1]) \n        for x in enumerate(zip(genTempData(base1, n1),\n              genTempData(base2, n2)) )]\n\nbigList = list(chain.from_iterable(makeLocation(str(x), \n                                                70, \n                                                15,\n                                                40, \n                                                15) \n                         for x in range(5)))\nnp.random.shuffle(bigList)\n\ndf = pd.DataFrame(bigList, \n                  columns = [&quot;Time&quot;, &quot;Loc&quot;, &quot;Temp&quot;, &quot;Hum&quot;])\n</code></pre>\n<h2 id=\"back-to-the-main-plot\">Back To The Main Plot</h2><p>Let's look at some test rows!</p><pre><code class=\"language-python\"># Viewing test rows\n\ndf.iloc[:5]\nTime\tLoc\tTemp     \tHum\n10\t4\t68.396970\t34.169753\n13\t0\t80.288846\t42.076786\n7\t4\t69.923273\t37.967951\n6\t0\t71.781362\t41.186802\n5\t2\t62.678844\t37.321636\n</code></pre>\n<p>Now, when I'm getting started with a new dataset, one of the first things I like to do is make some graphs.  As of late, my favorite package has been <a href=\"https://altair-viz.github.io/\">Altair</a>.  Looks very nice by default, is pretty easy to iterate with, and has nice declarative syntax.</p><p>Only one problem!  It wants date in \"long-form\" - as in, rather than each row having several variables of interest, each row has one (or more) \"ID\" variables, one numerical value, and the name of the variable we're measuring.  So for instance, something more like this:</p><pre><code class=\"language-python\">Time\tLoc\tvariable\tvalue\n10\t4\tTemp\t        68.396970\n13\t0\tTemp\t        80.288846\n7\t4\tTemp\t        69.923273\n6\t0\tTemp\t        71.781362\n5\t2\tTemp\t        62.678844\n</code></pre>\n<p>Not quite sure why!  Buuut, that's kind of a feature of modern coding - we're sitting on an inheritance of libraries that have built up over the years, and so more often than not we're just building the \"plumbing\" between existing stuff.  It's cool!  And good!  It lets us separate Function from Implementation.  We don't need to know what's going on under the hood - we just need to know thing X will produce an output we want, and that in order to get it we first need to reshape what we've already got into an input that it'll accept.  Since that's such a huge part of coding these days, Pandas' power in that realm is super useful.</p><p>Sooo, how do we get from here to there?  Shockingly easily!</p><pre><code class=\"language-python\">melted = pd.melt(df, id_vars=[&quot;Time&quot;, &quot;Loc&quot;])\n</code></pre>\n<p>Done!</p><p>Well, obviously we're not REALLY done yet.  Half the point of having such terse, expressive code is that we can do MORE things!</p><p>Let's say we want to see how humidity &amp; temperature change over the course of the day.  First, we'll have to grab all the readings from a single location.  Let's say Location 3!</p><pre><code class=\"language-python\">loc3 = melted[melted[&quot;Loc&quot;]==&quot;3&quot;]\n</code></pre>\n<p>Altair's pretty neat.</p><pre><code class=\"language-python\">(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O', #We're encoding time as an Ordinal \n         y='value:Q',\n         color='variable:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--2--1.png\" class=\"kg-image\"></figure><p>Hrm, lot of room there at the bottom.  If we were in an interactive session, we could make this interactive (zoomable and navigable!) by just adding the <code>.interactive()</code> method to the end, but I don't know how to do that in the blog.  Regardless, it's pretty easy to rescale if we want a closer look!</p><pre><code class=\"language-python\">(alt.Chart(loc3)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='variable:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--3--2.png\" class=\"kg-image\"></figure><p>Let's try it with just temperature, and color will encode the location!</p><pre><code class=\"language-python\">meltedJustTemp = pd.melt(df, \n                         id_vars=[&quot;Time&quot;, &quot;Loc&quot;],\n                        value_vars= [&quot;Temp&quot;])\n\n(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y='value:Q',\n         color='Loc:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--4--2.png\" class=\"kg-image\"></figure><p>Let's zoom in again...</p><pre><code class=\"language-python\">(alt.Chart(meltedJustTemp)\n .mark_line()\n .encode(x='Time:O',\n         y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n         color='Loc:N'))\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--4--2.png\" class=\"kg-image\"></figure><p>Altair also lets us Facet our graphs extremely flexibly &amp; painlessly.</p><pre><code class=\"language-python\">alt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='Loc:N',\n      column=&quot;variable&quot;)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--5--1.png\" class=\"kg-image\"></figure><p>Or how about another way!  Let's see humidity &amp; temp, location by location.</p><pre><code class=\"language-python\">alt.Chart(melted).mark_line().encode(\n      x='Time:O',\n      y=alt.Y('value:Q', scale=alt.Scale(zero=False)),\n      color='variable:N',\n      row=&quot;Loc&quot;)\n</code></pre>\n<figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/visualization--6--1.png\" class=\"kg-image\"></figure><p>We could make them nicer (there's a WIDE array of customizations), but I'm looking to simulate Exploratory Data Analysis.  I can't think of another graphing package in Python that has quite this level of \"instant gratification\" for so many different variations.</p>","url":"https://hackersandslackers.com/all-that-is-solid-melts-into-graphs/","uuid":"603156b0-ee55-4aaa-b5cd-34950389cd08","page":false,"codeinjection_foot":"<script>\n    hljs.configure({language: ['python']})\n </script>","codeinjection_head":"","comment_id":"5b5929932714bc41b8a370c5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a1","title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","slug":"code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","excerpt":"Code Snippet Corner ft. Pandas & SQL","custom_excerpt":"Code Snippet Corner ft. Pandas & SQL","created_at_pretty":"12 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"25 November, 2018","created_at":"2018-07-11T21:54:04.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-11-25T12:50:00.000-05:00","meta_title":"Code Snippet Corner: A Dirty Way of Cleaning Data (ft. Pandas & SQL) | Hackers and Slackers","meta_description":"Code Snippet Corner ft. Pandas & SQL","og_description":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","og_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","og_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","twitter_description":"Code Snippet Corner ft. Pandas & SQL","twitter_image":"https://hackersandslackers.com/content/images/2018/07/panderz@2x.jpg","twitter_title":"A Dirty Way of Cleaning Data (ft. Pandas & SQL)","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"Warning  The following is FANTASTICALLY not-secure.  Do not put this in a script\nthat's going to be running unsupervised.  This is for interactive sessions where\nyou're prototyping the data cleaning methods that you're going to use, and/or\njust manually entering stuff.  Especially if there's any chance there could be\nsomething malicious hiding in the data to be uploaded.  We're going to be\nexecuting formatted strings of SQL unsanitized code.  Also, this will lead to\nLOTS of silent failures, which are arguably The Worst Thing - if guaranteed\ncorrectness is a requirement, leave this for the tinkering table.\n Alternatively, if it's a project where \"getting something in there is better\nthan nothing\", this can provide a lot of bang for your buck.  Actually, it's\npurely for entertainment purposes and not for human consumption.\n\nLet's say you were helping someone take a bunch of scattered Excel files and\nCSVs and input them all into a MySQL database.  This is a very iterative, trial\n& error process.  We certainly don't want to be re-entering a bunch of\nboilerplate.  Pandas to the rescue!  We can painlessly load those files into a\nDataFrame, then just export them to the db!\n\nWell, not so fast  First off, loading stuff into a DB is a task all its own -\nPandas and your RDBMS have different kinds of tolerance for mistakes, and differ\nin often-unpredictable ways.  For example, one time I was performing a task\nsimilar to the one described here (taking scattered files and loading them into\na DB) - I was speeding along nicely, but then ran into a speedbump: turns out\nPandas generally doesn't infer that a column is a date unless you tell it\nspecifically, and will generally parse dates as strings.  Now, this was fine\nwhen the dates were present - MySQL is pretty smart about accepting different\nforms of dates & times.  But one thing it doesn't like is accepting an empty\nstring ''  into a date or time column.  Not a huge deal, just had to cast the\ncolumn as a date:\n\ndf['date'] = pd.to_datetime(df['date'])\n\nNow the blank strings are NaT, which MySQL knows how to handle!\n\nThis was simple enough, but there's all kinds of little hiccups that can happen.\n And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing\naffair - if there's one error, that means none of the rows will write.  Which\ncan get pretty annoying if you were trying to write a decent-sized DataFrame,\nespecially if the first error doesn't show up until one of the later rows.\n Waiting sucks.  And it's not just about being impatient - long waiting times\ncan disrupt your flow.\n\nRapid prototyping & highly-interactive development are some of Python's greatest\nstrengths, and they are great strengths indeed!  Paul Graham (one of the guys\nbehind Y Combinator) once made the comparison between REPL-heavy development and\nthe popularizing of oil paints (he was talking about LISP, but it's also quite\ntrue of Python, as Python took a lot of its cues from LISP):\n\nBefore oil paint became popular, painters used a medium, called tempera , that\ncannot be blended or over-painted. The cost of mistakes was high, and this\ntended to make painters conservative. Then came oil paint, and with it a great\nchange in style. Oil \"allows for second thoughts\". This proved a decisive\nadvantage in dealing with difficult subjects like the human figure.The new\nmedium did not just make painters' lives easier. It made possible a new and more\nambitious kind of painting. Janson writes:Without oil, the Flemish\nMasters'conquest of visible reality would have been much more limited. Thus,\nfrom a technical point of view, too, they deserve to be called the \"fathers of\nmodern painting\" , for oil has been the painter's basic medium ever since. As a\nmaterial, tempera is no lesss beautiful than oil. But the flexibility of oil\npaint gives greater scope to the imagination--that was the deciding factor.\nProgramming is now undergoing a similar change...Meanwhile, ideas borrowed from\nLisp increasingly turn up in the mainstream: interactive programming\nenvironments, garbage collection, and run-time typing  to name a few.More\npowerful tools are taking the risk out of exploration. That's good news for\nprogrammers, because it means that we will be able to undertake more ambitious\nprojects. The use of oil paint certainly had this effect. The period immediately\nfollowing its adoption was a golden age for painting. There are signs already\nthat something similar is happening in programming.\n(Emphasis mine)\nFrom here: http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.htmlA\nlittle scenario to demonstrate:\n\nLet's pretend we have a MySQL instance running, and have already created a\ndatabase named items\n\nimport pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(\"\"\"CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)\"\"\", cnx)\n\ndf = pd.DataFrame({\n    \"author\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"copies\": [2, \"\", 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n\n\nSoo, let's tighten this feedback loop, shall we?\n\nWe'll iterate through the DataFrame with the useful iterrows()  method.  This\ngives us essentially an enum  made from our DataFrame - we'll get a bunch of\ntuples giving us the index as the first element and the row as its own Pandas\nSeries as the second.\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n\n\nLet's unpack that a bit.\n\nRemember that we're getting a two-element tuple, with the good stuff in the\nsecond element, so\n\nx[1]\n\nNext, we convert the Series to a one-entry DataFrame, because the Series doesn't\nhave the DataFrame's to_sql()  method.\n\npd.DataFrame(x[1])\n\nThe default behavior will assume this is a single column with, each variable\nbeing the address of a different row.  MySQL isn't going to be having it.  Sooo,\nwe transpose!\n\npd.DataFrame(x[1]).transpose()\n\nAnd finally, we use our beloved to_sql  method on that.\n\nLet's check our table now!\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n\n\nIt wrote the first row!  Not much of a difference with this toy example, but\nonce you were writing a few thousand rows and the error didn't pop up until the\n3000th, this would make a pretty noticeable difference in your ability to\nquickly experiment with different cleaning schemes.\n\nNote that this will still short-circuit as soon as we hit the error.  If we\nwanted to make sure we got all the valid input before working on our tough\ncases, we could make a little try/except  block.\n\n\n\n\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n\n\nThis will try  to write each line, and if it encounters an Exception  it'll \ncontinue  the loop.\n\npd.io.sql.read_sql_table(\"books\", cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n\n\nAlright, now the bulk of our data's in the db!  Whatever else happens, you've\ndone that much!  Now you can relax a bit, which is useful for stimulating the\ncreativity you'll need for the more complicated edge cases.\n\nSo, we're ready to start testing new cleaning schemes?  Well, not quite yet...\n\nLet's say we went and tried to think up a fix.  We go to test it out and...\n\n#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '1' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, \"Incorrect integer value: '' for column 'copies' at row 1\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, \"Duplicate entry '3' for key 'PRIMARY'\") [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n\n\nThe error we're interested is in there, but what's all this other nonsense\ncrowding it?\n\nWell, one of the handy things about a database is that it'll enforce uniqueness\nbased on the constraints you give it.  It's already got an entry with an id \nvalue of 1, so it's going to complain if you try to put another one.  In\naddition to providing a lot of distraction, this'll also slow us down\nconsiderably - after all, part of the point was to make our experiments with\ndata-cleaning go faster!\n\nLuckily, Pandas' wonderful logical indexing will make it a snap to ensure that\nwe only bother with entries that aren't in the database yet.\n\n#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n\n\nThis will also be shockingly quick - Pandas' logical indexing takes advantage of\nall that magic going on under the hood.  Using it, instead of manually\niteration, can literally bring you from waiting minutes to waiting seconds.\n\nBuuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so\nhow about we just turn it into a function?\n\n#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(\"books\", cnx, columns=[\"id\"])[\"id\"].values\n    return df[~df.index.isin(usedIDs)]\n\n\nSo, next time we think we've made some progress on an edge case, we just call...\n\n#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nActually, let's clean that up even more - the more keys we hit, the more\nopportunities to make a mistake!  The most bug-free code is the code you don't\nwrite.\n\ndef writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n\n\nSo, finally, we can work on our new cleaning scheme, and whenever we think we're\ndone...\n\nwriteNewRows(cleanedDF)\n\nAnd boom!  Instant feedback!","html":"<p><strong>Warning</strong> The following is FANTASTICALLY not-secure.  Do not put this in a script that's going to be running unsupervised.  This is for interactive sessions where you're prototyping the data cleaning methods that you're going to use, and/or just manually entering stuff.  Especially if there's any chance there could be something malicious hiding in the data to be uploaded.  We're going to be executing formatted strings of SQL unsanitized code.  Also, this will lead to LOTS of silent failures, which are arguably The Worst Thing - if guaranteed correctness is a requirement, leave this for the tinkering table.  Alternatively, if it's a project where \"getting something in there is better than nothing\", this can provide a lot of bang for your buck.  Actually, it's purely for entertainment purposes and not for human consumption.</p><p>Let's say you were helping someone take a bunch of scattered Excel files and CSVs and input them all into a MySQL database.  This is a very iterative, trial &amp; error process.  We certainly don't want to be re-entering a bunch of boilerplate.  Pandas to the rescue!  We can painlessly load those files into a DataFrame, then just export them to the db!</p><p>Well, not so fast  First off, loading stuff into a DB is a task all its own - Pandas and your RDBMS have different kinds of tolerance for mistakes, and differ in often-unpredictable ways.  For example, one time I was performing a task similar to the one described here (taking scattered files and loading them into a DB) - I was speeding along nicely, but then ran into a speedbump: turns out Pandas generally doesn't infer that a column is a date unless you tell it specifically, and will generally parse dates as strings.  Now, this was fine when the dates were present - MySQL is pretty smart about accepting different forms of dates &amp; times.  But one thing it doesn't like is accepting an empty string <code>''</code> into a date or time column.  Not a huge deal, just had to cast the column as a date:</p><p><code>df['date'] = pd.to_datetime(df['date'])</code></p><p>Now the blank strings are <code>NaT</code>, which MySQL knows how to handle!</p><p>This was simple enough, but there's all kinds of little hiccups that can happen.  And, unfortunately, writing a DataFrame to a DB table is an all-or-nothing affair - if there's one error, that means none of the rows will write.  Which can get pretty annoying if you were trying to write a decent-sized DataFrame, especially if the first error doesn't show up until one of the later rows.  Waiting sucks.  And it's not just about being impatient - long waiting times can disrupt your flow.</p><p>Rapid prototyping &amp; highly-interactive development are some of Python's greatest strengths, and they are great strengths indeed!  Paul Graham (one of the guys behind Y Combinator) once made the comparison between REPL-heavy development and the popularizing of oil paints (he was talking about LISP, but it's also quite true of Python, as Python took a lot of its cues from LISP):</p><blockquote>Before oil paint became popular, painters used a medium, called tempera , that cannot be blended or over-painted. The cost of mistakes was high, and this tended to make painters conservative. Then came oil paint, and with it a great change in style. Oil \"allows for second thoughts\". This proved a decisive advantage in dealing with difficult subjects like the human figure.The new medium did not just make painters' lives easier. It made possible a new and more ambitious kind of painting. Janson writes:Without oil, the Flemish Masters'conquest of visible reality would have been much more limited. Thus, from a technical point of view, too, they deserve to be called the \"fathers of modern painting\" , for oil has been the painter's basic medium ever since. As a material, tempera is no lesss beautiful than oil. But the flexibility of oil paint gives greater scope to the imagination--that was the deciding factor.<br>Programming is now undergoing a similar change...Meanwhile, ideas borrowed from Lisp increasingly turn up in the mainstream: <strong>interactive programming environments, garbage collection, and run-time typing</strong> to name a few.More powerful tools are taking the risk out of exploration. That's good news for programmers, because it means that we will be able to undertake more ambitious projects. The use of oil paint certainly had this effect. The period immediately following its adoption was a golden age for painting. There are signs already that something similar is happening in programming.<br>(Emphasis mine)<br>From here: <a href=\"http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html\">http://www.cs.oswego.edu/~blue/xhx/books/ai/ns1/section02/main.html</a></blockquote><p>A little scenario to demonstrate:</p><p>Let's pretend we have a MySQL instance running, and have already created a database named <code>items</code></p><pre><code class=\"language-python\">import pymysql\nfrom sqlalchemy import create_engine\nimport sqlalchemy\nimport pandas as pd\n\ncnx = create_engine('mysql+pymysql://analyst:badsecuritykills@localhost:3306/items)\n\npd.io.sql.execute(&quot;&quot;&quot;CREATE TABLE books( \\\nid                               VARCHAR(40) PRIMARY KEY NOT NULL \\\n,author                          VARCHAR(255) \\\n,copies                          INT)&quot;&quot;&quot;, cnx)\n\ndf = pd.DataFrame({\n    &quot;author&quot;: [&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;],\n    &quot;copies&quot;: [2, &quot;&quot;, 7, ],}, \n    index = [1, 2, 3])\n    #Notice that one of these has the wrong data type!\n    \ndf.to_sql(name='books',con=cnx,if_exists='append',index=False)\n#Yeah, I'm not listing this whole stacktrace.  Fantastic package with some extremely helpful Exceptions, but you've gotta scroll a whole bunch to find em.  Here's the important part:\nInternalError: (pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n</code></pre>\n<p>Soo, let's tighten this feedback loop, shall we?</p><p>We'll iterate through the DataFrame with the useful <code>iterrows()</code> method.  This gives us essentially an <code>enum</code> made from our DataFrame - we'll get a bunch of tuples giving us the index as the first element and the row as its own Pandas Series as the second.</p><pre><code class=\"language-python\">for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except:\n        continue\n</code></pre>\n<p>Let's unpack that a bit.</p><p>Remember that we're getting a two-element tuple, with the good stuff in the second element, so</p><p><code>x[1]</code></p><p>Next, we convert the Series to a one-entry DataFrame, because the Series doesn't have the DataFrame's <code>to_sql()</code> method.</p><p><code>pd.DataFrame(x[1])</code></p><p>The default behavior will assume this is a single column with, each variable being the address of a different row.  MySQL isn't going to be having it.  Sooo, we transpose!</p><p><code>pd.DataFrame(x[1]).transpose()</code></p><p>And finally, we use our beloved <code>to_sql</code> method on that.</p><p>Let's check our table now!</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n  \tauthor\tcopies\nid\n1\tAlice\t2\n</code></pre>\n<p>It wrote the first row!  Not much of a difference with this toy example, but once you were writing a few thousand rows and the error didn't pop up until the 3000th, this would make a pretty noticeable difference in your ability to quickly experiment with different cleaning schemes.</p><p>Note that this will still short-circuit as soon as we hit the error.  If we wanted to make sure we got all the valid input before working on our tough cases, we could make a little <code>try/except</code> block.</p><pre><code class=\"language-python\">\n</code></pre>\n<pre><code>for x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index=False,)\n    except:\n        continue\n</code></pre><p>This will <code>try</code> to write each line, and if it encounters an <code>Exception</code> it'll <code>continue</code> the loop.</p><pre><code class=\"language-python\">pd.io.sql.read_sql_table(&quot;books&quot;, cnx, index_col='id')\n\tauthor\tcopies\nid\t\t\n1\tAlice\t2\n3\tCharlie\t7\n</code></pre>\n<p>Alright, now the bulk of our data's in the db!  Whatever else happens, you've done that much!  Now you can relax a bit, which is useful for stimulating the creativity you'll need for the more complicated edge cases.</p><p>So, we're ready to start testing new cleaning schemes?  Well, not quite yet...</p><p>Let's say we went and tried to think up a fix.  We go to test it out and...</p><pre><code class=\"language-python\">#Note that we want to see our exceptions here, so either do without the the try/except block\nfor x in df.iterrows():\n    pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                              con=cnx,\n                              if_exists='append',\n                             index=False,\n                             )\n\n#OR have it print the exception\nfor x in df.iterrows():\n    try:\n        pd.DataFrame(x[1]).transpose().to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n    except Exception as e:\n        print(e)\n        continue\n        \n#Either way, we get...\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '1' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 1, 'author': 'Alice', 'copies': 2}] (Background on this error at: http://sqlalche.me/e/gkpj)\n(pymysql.err.InternalError) (1366, &quot;Incorrect integer value: '' for column 'copies' at row 1&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 2, 'author': 'Bob', 'copies': ''}] (Background on this error at: http://sqlalche.me/e/2j85)\n(pymysql.err.IntegrityError) (1062, &quot;Duplicate entry '3' for key 'PRIMARY'&quot;) [SQL: 'INSERT INTO books (id, author, copies) VALUES (%(id)s, %(author)s, %(copies)s)'] [parameters: {'id': 3, 'author': 'Charlie', 'copies': 7}] (Background on this error at: http://sqlalche.me/e/gkpj)            \n</code></pre>\n<p>The error we're interested is in there, but what's all this other nonsense crowding it?</p><p>Well, one of the handy things about a database is that it'll enforce uniqueness based on the constraints you give it.  It's already got an entry with an <code>id</code> value of 1, so it's going to complain if you try to put another one.  In addition to providing a lot of distraction, this'll also slow us down considerably - after all, part of the point was to make our experiments with data-cleaning go faster!</p><p>Luckily, Pandas' wonderful logical indexing will make it a snap to ensure that we only bother with entries that aren't in the database yet.</p><pre><code class=\"language-python\">#First, let's get the indices that are in there\nusedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n\ndf[~df.index.isin(usedIDs)]\n    author\tcopies\n2\tBob\t\n#Remember how the logical indexing works: We want every element of the dataframe where the index ISN'T in our array of IDs that are already in the DB\n</code></pre>\n<p>This will also be shockingly quick - Pandas' logical indexing takes advantage of all that magic going on under the hood.  Using it, instead of manually iteration, can literally bring you from waiting minutes to waiting seconds.</p><p>Buuut, that's a lot of stuff to type!  We're going to be doing this A LOT, so how about we just turn it into a function?</p><pre><code class=\"language-python\">#Ideally we'd make a much more modular version, but for this toy example we'll be messy and hardcode some paramaters\ndef filterDFNotInDB(df):\n    usedIDs = pd.read_sql_table(&quot;books&quot;, cnx, columns=[&quot;id&quot;])[&quot;id&quot;].values\n    return df[~df.index.isin(usedIDs)]\n</code></pre>\n<p>So, next time we think we've made some progress on an edge case, we just call...</p><pre><code class=\"language-python\">#Going back to the to_sql method here - we don't want to have to loop through every single failing case, or get spammed with every variety of error message the thing can throw at us.\n\nfilterDFNotInDB(cleanedDF).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>Actually, let's clean that up even more - the more keys we hit, the more opportunities to make a mistake!  The most bug-free code is the code you don't write.</p><pre><code class=\"language-python\">def writeNewRows(df):\n    filterDFNotInDB(df).to_sql(name='books',\n                          con=cnx,\n                          if_exists='append',\n                         index_label='id')\n</code></pre>\n<p>So, finally, we can work on our new cleaning scheme, and whenever we think we're done...</p><p><code>writeNewRows(cleanedDF)</code></p><p>And boom!  Instant feedback!</p>","url":"https://hackersandslackers.com/code-snippet-corner-a-dirty-way-of-cleaning-data-ft-pandas-sql/","uuid":"9788b54d-ef44-4a35-9ec6-6a8678038480","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b46b4bcc6a9e951f8a6cc32"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369e","title":"Getting Conda Envs (And Environment Variables!) To Play Nicely With Cron","slug":"conda-environments-and-cron","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/anaconda@2x.jpg","excerpt":"Code Snippet Corner.","custom_excerpt":"Code Snippet Corner.","created_at_pretty":"05 July, 2018","published_at_pretty":"09 July, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-07-05T19:27:38.000-04:00","published_at":"2018-07-09T07:00:00.000-04:00","updated_at":"2019-02-19T03:53:43.000-05:00","meta_title":"Getting Conda To Play Nicely With Cron | Hackers and Slackers","meta_description":"Setting up multiple Conda environments with separate databases to simulate a development-to-production workflow.","og_description":"Setting up multiple Conda environments with separate databases to simulate a development-to-production workflow.","og_image":"https://hackersandslackers.com/content/images/2018/07/anaconda@2x.jpg","og_title":"Getting Conda Envs (And Environment Variables!) To Play Nicely With Cron","twitter_description":"Setting up multiple Conda environments with separate databases to simulate a development-to-production workflow.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/anaconda@2x.jpg","twitter_title":"Getting Conda Envs (And Environment Variables!) To Play Nicely With Cron","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Code Snippet Corner","slug":"codesnippetcorner","description":"Real-world examples of Python being used to solve complex data problems, primarily using Jupyter notebooks.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o-1.jpg","meta_description":"Real-world examples of Python being used to solve complex data problems.","meta_title":"Python Code Snippet Corner","visibility":"public"},{"name":"#Code Snippet Corner","slug":"code-snippet-corner","description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","feature_image":"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/codecornerseries_o_o.jpg","meta_description":"Your weekly dose of Python tidbits and Jupyter notebooks to get you feeling saucy.","meta_title":"Python Code Snippet Corner","visibility":"internal"}],"plaintext":"This isn't really a tutorial on cron  in general; Better people at Linux have\nwritten way better ones than I could write.  Here's one: \nhttp://mediatemple.net/blog/news/complete-beginners-guide-cron-part-1/   This is\nmore of a code journaling exercise for a problem that I didn't find a\nneat-and-tidy answer to online when I was looking for it, and that I presume at\nleast one person will encounter at some point between now and the heat death of\nthe universe.\n\nLet's say you've got two different conda envs:production  and development.\n Let's say that, in addition to having different packages installed, they each\nuse a seperate database - development  writes to one that you can wipe and\nreconstruct to your heart's content, while production  gets used for stuff that\nactually affects customers.\n\nNot a problem!  Just a teensy bit of fiddling and you're there.  From the\nofficial docs: \nhttps://conda.io/docs/user-guide/tasks/manage-environments.html#macos-and-linux\n\nSo, for our purposes, let's say we want to set the variables from the \ndevelopment  environment.  First we create two folders to hold a simple .sh \nscript toactivate  and deactivate  the relevant environment variables:\n\ncd /home/matt/anaconda3/envs/development\nmkdir -p ./etc/conda/activate.d\nmkdir -p ./etc/conda/deactivate.d\ntouch ./etc/conda/activate.d/env_vars.sh\ntouch ./etc/conda/deactivate.d/env_vars.sh\n\n\nThen let's add the info for the database we want to access.  Edit \n./etc/conda/activate.d/env_vars.sh  to say:\n\n#!/bin/sh\n\nexport db-string='mysql+pymysql://dev:password@localhost/dev-db'\n\n\nNow edit ./etc/conda/deactivate.d/env_vars.sh  to say:\n\n#!/bin/sh\n\nunset db-string\n\n\nPresto!  Now if you source activate development  and run a script it'll all go\ngreat!\n\nCron Craziness\nIf you're running scripts by hand, this is all great.  It wouldn't even have\nwarranted a blog post!  However, a little hiccup occurs if you try to have cron \n(or something that wraps cron  - I was using the cool Ruby gem whenever  because\nthat was what was running stuff on the job server my stuff was running on) run\nthe script.\n\nI naively tried to have cron  activate the env, run it, then deactivate it.\n\nsource activate production; python /home/matta/python_workspace/dbUpdater.py; source deactivate\n\n\nThis did not work.  Research lead me to believe the reason involved rules about\nspawning subshells, a concept I vaguely understand.  What I understood more\nconcretely, however, was that this was not going to work.\n\nRun It With The Packages You Want\nEventually I discovered that I at least had access to the environment's packages\nif, instead of running the script by calling python, I ran it by calling the \npython  in the env itself.  Sooo...\n\n/home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n\n\nSadly, we still have a problem - it's not loading our environment variables!\n This makes sense - Python doesn't know where those variables are, they get\nexported when we call source activate development.\n\nSourcing the Env Variables\nBuuut, since exporting the variables just happens in a .sh  script, we can run\nit directly!\n\nsource /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n\n\nRuns in the terminal!  Should run in the cron, right?\n\nWell, in the moral sense it \"should\" - unfortunately, it does not.\n\nIt turns out that source  is actually an alias for a command whose True Name is \n..  For reasons I will not pretend to understand, cron  does not know this, and\nwill not be taught.  It is we who shall have to adjust to the machine's\npreferences.\n\nPutting It All Together\nThe command we want is:\n\n. /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n\n\nLet's say we want it to run every day at 3AM UTC:\n\n* 3 * * * . /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n\n\nAnd to add it directly to your crontab\n\ncrontab -l | { /bin/cat; /bin/echo \"* 3 * * * . /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\"; } | crontab -","html":"<p>This isn't really a tutorial on <code>cron</code> in general; Better people at Linux have written way better ones than I could write.  Here's one: <a href=\"http://mediatemple.net/blog/news/complete-beginners-guide-cron-part-1/\">http://mediatemple.net/blog/news/complete-beginners-guide-cron-part-1/</a>  This is more of a code journaling exercise for a problem that I didn't find a neat-and-tidy answer to online when I was looking for it, and that I presume at least one person will encounter at some point between now and the heat death of the universe.</p><p>Let's say you've got two different conda envs:  <code>production</code> and <code>development</code>.  Let's say that, in addition to having different packages installed, they each use a seperate database - <code>development</code> writes to one that you can wipe and reconstruct to your heart's content, while <code>production</code> gets used for stuff that actually affects customers.</p><p>Not a problem!  Just a teensy bit of fiddling and you're there.  From the official docs: <a href=\"https://conda.io/docs/user-guide/tasks/manage-environments.html#macos-and-linux\">https://conda.io/docs/user-guide/tasks/manage-environments.html#macos-and-linux</a></p><p>So, for our purposes, let's say we want to set the variables from the <code>development</code> environment.  First we create two folders to hold a simple <code>.sh</code> script to  <code>activate</code> and <code>deactivate</code> the relevant environment variables:</p><pre><code class=\"language-shell\">cd /home/matt/anaconda3/envs/development\nmkdir -p ./etc/conda/activate.d\nmkdir -p ./etc/conda/deactivate.d\ntouch ./etc/conda/activate.d/env_vars.sh\ntouch ./etc/conda/deactivate.d/env_vars.sh\n</code></pre>\n<p>Then let's add the info for the database we want to access.  Edit <code>./etc/conda/activate.d/env_vars.sh</code> to say:</p><pre><code class=\"language-shell\">#!/bin/sh\n\nexport db-string='mysql+pymysql://dev:password@localhost/dev-db'\n</code></pre>\n<p>Now edit <code>./etc/conda/deactivate.d/env_vars.sh</code> to say:</p><pre><code class=\"language-shell\">#!/bin/sh\n\nunset db-string\n</code></pre>\n<p>Presto!  Now if you <code>source activate development</code> and run a script it'll all go great!</p><h2 id=\"cron-craziness\">Cron Craziness</h2><p>If you're running scripts by hand, this is all great.  It wouldn't even have warranted a blog post!  However, a little hiccup occurs if you try to have <code>cron</code> (or something that wraps <code>cron</code> - I was using the cool Ruby gem <code>whenever</code> because that was what was running stuff on the job server my stuff was running on) run the script.</p><p>I naively tried to have <code>cron</code> activate the env, run it, then deactivate it.</p><pre><code class=\"language-shell\">source activate production; python /home/matta/python_workspace/dbUpdater.py; source deactivate\n</code></pre>\n<p>This did not work.  Research lead me to believe the reason involved rules about spawning <code>subshells</code>, a concept I vaguely understand.  What I understood more concretely, however, was that this was not going to work.</p><h2 id=\"run-it-with-the-packages-you-want\">Run It With The Packages You Want</h2><p>Eventually I discovered that I at least had access to the environment's packages if, instead of running the script by calling <code>python</code>, I ran it by calling the <code>python</code> in the env itself.  Sooo...</p><pre><code class=\"language-shell\">/home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n</code></pre>\n<p>Sadly, we still have a problem - it's not loading our environment variables!  This makes sense - Python doesn't know where those variables are, they get exported when we call <code>source activate development</code>.</p><h2 id=\"sourcing-the-env-variables\">Sourcing the Env Variables</h2><p>Buuut, since exporting the variables just happens in a <code>.sh</code> script, we can run it directly!</p><pre><code class=\"language-shell\">source /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n</code></pre>\n<p>Runs in the terminal!  Should run in the <code>cron</code>, right?</p><p>Well, in the moral sense it \"should\" - unfortunately, it does not.</p><p>It turns out that <code>source</code> is actually an alias for a command whose True Name is <code>.</code>.  For reasons I will not pretend to understand, <code>cron</code> does not know this, and will not be taught.  It is we who shall have to adjust to the machine's preferences.</p><h2 id=\"putting-it-all-together\">Putting It All Together</h2><p>The command we want is:</p><pre><code class=\"language-shell\">. /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n</code></pre>\n<p>Let's say we want it to run every day at 3AM UTC:</p><pre><code class=\"language-shell\">* 3 * * * . /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py\n</code></pre>\n<p>And to add it directly to your <code>crontab</code></p><pre><code class=\"language-shell\">crontab -l | { /bin/cat; /bin/echo &quot;* 3 * * * . /home/matt/anaconda3/envs/development/etc/conda/activate.d/env_vars.sh ; /home/matt/anaconda3/envs/development/bin/python /home/matta/python_workspace/dbUpdater.py&quot;; } | crontab -\n</code></pre>\n","url":"https://hackersandslackers.com/conda-environments-and-cron/","uuid":"348836b6-1358-4baf-8dd1-40db61f57f7c","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3ea96ad0ac8a143588f37c"}}]}},"pageContext":{"slug":"code-snippet-corner","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}