{"data":{"ghostAuthor":{"slug":"todd","name":"Todd Birchard","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","cover_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/fox_o_o.jpg","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","location":"New York City","website":"https://toddbirchard.com","twitter":"@ToddRBirchard","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673734","title":"Scraping Data on the Web with BeautifulSoup","slug":"scraping-urls-with-beautifulsoup","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","excerpt":"The honest act of systematically stealing data without permission.","custom_excerpt":"The honest act of systematically stealing data without permission.","created_at_pretty":"11 November, 2018","published_at_pretty":"11 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-11T04:53:44.000-05:00","published_at":"2018-11-11T08:35:09.000-05:00","updated_at":"2019-01-05T13:21:06.000-05:00","meta_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","meta_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","og_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","twitter_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","twitter_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"There are plenty of reliable and open sources of data on the web. Datasets are\nfreely released to the public domain by the likes of Kaggle, Google Cloud, and\nof course local & federal government. Like most things free and open, however,\nfollowing the rules to obtain public data can be a bit... boring. I'm not\nsuggesting we go and blatantly break some grey-area laws by stealing data, but\nthis blog isn't exactly called People Who Play It Safe And Slackers, either. \n\nMy personal Python roots can actually be traced back to an ambitious\nside-project: to aggregate all new music from across the web and deliver it the\nmasses. While that project may have been abandoned (after realizing it already\nexisted), BeautifulSoup  was more-or-less my first ever experience with Python. \n\nThe Tool(s) for the Job(s)\nBefore going any further, we'd be ill-advised to not at least mention Python's\nother web-scraping behemoth, Scrapy [https://scrapy.org/]. BeautifulSoup  and \nScrapy  have two very different agendas. BeautifulSoup is intended to parse or\nextract data one page at a time, with each page being served up via the requests \n library or equivalent. Scrapy,  on the other hand, is for creating crawlers: or\nrather absolute monstrosities unleashed upon the web like a swarm, loosely\nfollowing links and haste-fully grabbing data where data exists to be grabbed.\nTo put this in perspective, Google Cloud functions will not even let you import\nScrapy as a usable library.\n\nThis isn't to say that BeautifulSoup  can't be made into a similar monstrosity\nof its own. For now, we'll focus on a modest task: generating link previews for\nURLs by grabbing their metadata.\n\nStep 1: Stalk Your Prey\nBefore we steal any data, we should take a look at the data we're hoping to\nsteal.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape URLs to generate previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n\n\nThe above is the minimum needed to retrieve the DOM structure of an HTML page. \nBeautifulSoup  accepts the .content  output from a request, from which we can\ninvestigate the contents.\n\nUsing BeauitfulSoup will often result in different results for your scaper than\nyou might see as a human, such as 403 errors or blocked content. An easy way\naround this faking your headers into looking like normal browser agents, as we\ndo here: \nheaders.update({\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101\nFirefox/52.0',\n})`The result of print(soup.prettify())  will predictably output a \"pretty\" printed\nversion of your target DOM structure:\n\n<html class=\"gr__example_com\"><head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta property=\"og:site_name\" content=\"Example dot com\">\n    <meta property=\"og:type\" content=\"website\">\n    <meta property=\"og:title\" content=\"Example\">\n    <meta property=\"og:description\" content=\"An Example website.\">\n    <meta property=\"og:image\" content=\"http://example.com/img/image.jpg\">\n    <meta name=\"twitter:title\" content=\"Hackers and Slackers\">\n    <meta name=\"twitter:description\" content=\"An Example website.\">\n    <meta name=\"twitter:url\" content=\"http://example.com/\">\n    <meta name=\"twitter:image\" content=\"http://example.com/img/image.jpg\">\n</head>\n\n<body data-gr-c-s-loaded=\"true\">\n  <div>\n    <h1>Example Domain</h1>\n      <p>This domain is established to be used for illustrative examples in documents.</p>\n      <p>You may use this domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n  </div>\n</body>\n    \n</html>\n\n\nStep 2: The Extraction\nAfter turning our request content into a BeautifulSoup object, we access items\nin the DOM via dot notation as such:\n\ntitle = soup.title.string\n\n\n.string  gives us the actual content of the tag which is Example Domain, whereas\n soup.title  would return the entirety of the tag as <title>Example\nDomain</title>. \n\nDot notation is fine when pages have predictable hierarchies or structures, but\nbecomes much less useful for extracting patterns we see in the document. soup.a \nwill only return the first instance of a link, and probably isn't what we want.\n\nIf we wanted to extract all  <a>  tags of a page's content while avoiding the\nnoise of nav links etc, we can use CSS selectors to return a list of all\nelements matching the selection. soup.select('body p > a')  retrieves all links\nembedded in paragraph text, limited to the body of the page. \n\nSome other methods of grabbing elements:\n\n * soup.find(id=\"example\"): Useful for when a single element is expected.\n * soup.find_all('a'):  Returns a list of all elements matching the selection\n   after searching the document recursively.\n * .parent and .child: Relative selectors to a currently engaged element.\n\nGet Some Attributes\nChances are we'll almost always want the contents or the attributes of a tag, as\nopposed to the entire <a>  tag's HTML. A common example of going after a tag's\nattributes would be in the cases of img  and a  tags. Chances are we're most\ninterested in the src  and href  attributes of such tags, respectively. \n\nThe .get  method refers specifically to getting the value of attributes on a\ntag. For example, soup.find('.logo').get('href')  would find an element with the\nclass \"logo\", and return the url to that image.\n\nPesky Tags to Deal With\nIn our example of creating link previews, a good first source of information\nwould obviously be the page's meta tags: specifically the og  tags they've\nspecified to openly provide the bite-sized information we're looking for.\nGrabbing these tags are a bit more difficult to deal with:\n\nsoup.find(\"meta\", property=\"og:description\").get('content')\n\n\nOh yeah, now that's some ugly shit right there. Meta tags are especially\ninteresting because they're all uselessly dubbed 'meta', thus we need a second\ndifferentiator in addition to the tag name to specify which meta tag we care\nabout. Only then can we bother to get  the actual content of said tag.\n\nStep 3: Realizing Something Will Always Break\nIf we were to try the above selector on an HTML page which did not contain an \nog:description, our script would break unforgivingly. Not only do we miss this\ndata, but we miss out on everything entirely - this means we always need to\nbuild in a plan B, and at the very least deal with a lack of tag altogether.\n\nIt's best to break out this logic one tag at a time. First, let's look at an\nexample for a base scraper with all the knowledge we have so far:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p > a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n\n\nGreat - there's a base function for snatching all links out of the body of a\npage. Ultimately we'll create a JSON object for each of these links containing\npreview data, link_preview_dict.\n\nTo handle each value of our dict, we have individual functions:\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get a preview image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\nIn case you're wondering:\n\n * getTitle tries to get the <title>  tag, and falls back to the page's first \n   <h1>  tag (surprisingly enough some pages are in fact missing a title).\n * getDescription  looks for the OG description, and falls back to the content\n   of the page's first paragraph.\n * getImage looks for the OG image, and falls back to the page's first image.\n * getSiteName similarly tries to grab the OG attribute, otherwise it does it's\n   best to extract the domain name from the URL string under the assumption that\n   this is the origin's name (look, it ain't perfect).\n\nWhat Did We Just Build?\nBelieve it or not, the above is considered to be enough logic to be a paid\nservice with a monthly fee. Go ahead and Google it; or better yet, just steal my\nsource code entirely:\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p > a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)","html":"<p>There are plenty of reliable and open sources of data on the web. Datasets are freely released to the public domain by the likes of Kaggle, Google Cloud, and of course local &amp; federal government. Like most things free and open, however, following the rules to obtain public data can be a bit... boring. I'm not suggesting we go and blatantly break some grey-area laws by stealing data, but this blog isn't exactly called <strong>People Who Play It Safe And Slackers</strong>, either. </p><p>My personal Python roots can actually be traced back to an ambitious side-project: to aggregate all new music from across the web and deliver it the masses. While that project may have been abandoned (after realizing it already existed), <strong>BeautifulSoup</strong> was more-or-less my first ever experience with Python. </p><h2 id=\"the-tool-s-for-the-job-s-\">The Tool(s) for the Job(s)</h2><p>Before going any further, we'd be ill-advised to not at least mention Python's other web-scraping behemoth, <strong><a href=\"https://scrapy.org/\">Scrapy</a></strong>. <strong>BeautifulSoup</strong> and <strong>Scrapy</strong> have two very different agendas. BeautifulSoup is intended to parse or extract data one page at a time, with each page being served up via the <strong>requests</strong> library or equivalent. <strong>Scrapy,</strong> on the other hand, is for creating crawlers: or rather absolute monstrosities unleashed upon the web like a swarm, loosely following links and haste-fully grabbing data where data exists to be grabbed. To put this in perspective, Google Cloud functions will not even let you import Scrapy as a usable library.</p><p>This isn't to say that <strong>BeautifulSoup</strong> can't be made into a similar monstrosity of its own. For now, we'll focus on a modest task: generating link previews for URLs by grabbing their metadata.</p><h2 id=\"step-1-stalk-your-prey\">Step 1: Stalk Your Prey</h2><p>Before we steal any data, we should take a look at the data we're hoping to steal.</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape URLs to generate previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n</code></pre>\n<p>The above is the minimum needed to retrieve the DOM structure of an HTML page. <strong>BeautifulSoup</strong> accepts the <code>.content</code> output from a request, from which we can investigate the contents.</p><div class=\"protip\">\n    Using BeauitfulSoup will often result in different results for your scaper than you might see as a human, such as 403 errors or blocked content. An easy way around this faking your headers into looking like normal browser agents, as we do here: <br><code>headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })`</code>\n</div><p>The result of <code>print(soup.prettify())</code> will predictably output a \"pretty\" printed version of your target DOM structure:</p><pre><code class=\"language-html\">&lt;html class=&quot;gr__example_com&quot;&gt;&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n    &lt;meta charset=&quot;utf-8&quot;&gt;\n    &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;\n    &lt;meta property=&quot;og:site_name&quot; content=&quot;Example dot com&quot;&gt;\n    &lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n    &lt;meta property=&quot;og:title&quot; content=&quot;Example&quot;&gt;\n    &lt;meta property=&quot;og:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n    &lt;meta name=&quot;twitter:title&quot; content=&quot;Hackers and Slackers&quot;&gt;\n    &lt;meta name=&quot;twitter:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta name=&quot;twitter:url&quot; content=&quot;http://example.com/&quot;&gt;\n    &lt;meta name=&quot;twitter:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n&lt;/head&gt;\n\n&lt;body data-gr-c-s-loaded=&quot;true&quot;&gt;\n  &lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n      &lt;p&gt;This domain is established to be used for illustrative examples in documents.&lt;/p&gt;\n      &lt;p&gt;You may use this domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&quot;http://www.iana.org/domains/example&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n    \n&lt;/html&gt;\n</code></pre>\n<h2 id=\"step-2-the-extraction\">Step 2: The Extraction</h2><p>After turning our request content into a BeautifulSoup object, we access items in the DOM via dot notation as such:</p><pre><code class=\"language-python\">title = soup.title.string\n</code></pre>\n<p><code>.string</code> gives us the actual content of the tag which is <code>Example Domain</code>, whereas <code>soup.title</code> would return the entirety of the tag as <code>&lt;title&gt;Example Domain&lt;/title&gt;</code>. </p><p>Dot notation is fine when pages have predictable hierarchies or structures, but becomes much less useful for extracting patterns we see in the document. <code>soup.a</code> will only return the first instance of a link, and probably isn't what we want.</p><p>If we wanted to extract <em>all</em> <code>&lt;a&gt;</code> tags of a page's content while avoiding the noise of nav links etc, we can use CSS selectors to return a list of all elements matching the selection. <code>soup.select('body p &gt; a')</code> retrieves all links embedded in paragraph text, limited to the body of the page. </p><p>Some other methods of grabbing elements:</p><ul><li><strong>soup.find(id=\"example\")</strong>: Useful for when a single element is expected.</li><li><strong>soup.find_all('a')</strong>:<strong> </strong>Returns a list of all elements matching the selection after searching the document recursively.</li><li><strong>.parent </strong>and <strong>.child</strong>: Relative selectors to a currently engaged element.</li></ul><h3 id=\"get-some-attributes\">Get Some Attributes</h3><p>Chances are we'll almost always want the contents or the attributes of a tag, as opposed to the entire <code>&lt;a&gt;</code> tag's HTML. A common example of going after a tag's attributes would be in the cases of <code>img</code> and <code>a</code> tags. Chances are we're most interested in the <code>src</code> and <code>href</code> attributes of such tags, respectively. </p><p>The <code>.get</code> method refers specifically to getting the value of attributes on a tag. For example, <code>soup.find('.logo').get('href')</code> would find an element with the class \"logo\", and return the url to that image.</p><h3 id=\"pesky-tags-to-deal-with\">Pesky Tags to Deal With</h3><p>In our example of creating link previews, a good first source of information would obviously be the page's meta tags: specifically the <code>og</code> tags they've specified to openly provide the bite-sized information we're looking for. Grabbing these tags are a bit more difficult to deal with:</p><pre><code class=\"language-python\">soup.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n</code></pre>\n<p>Oh yeah, now that's some ugly shit right there. Meta tags are especially interesting because they're all uselessly dubbed 'meta', thus we need a second differentiator in addition to the tag name to specify <em>which </em>meta tag we care about. Only then can we bother to <em>get</em> the actual content of said tag.</p><h2 id=\"step-3-realizing-something-will-always-break\">Step 3: Realizing Something Will Always Break</h2><p>If we were to try the above selector on an HTML page which did not contain an <code>og:description</code>, our script would break unforgivingly. Not only do we miss this data, but we miss out on everything entirely - this means we always need to build in a plan B, and at the very least deal with a lack of tag altogether.</p><p>It's best to break out this logic one tag at a time. First, let's look at an example for a base scraper with all the knowledge we have so far:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p &gt; a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n</code></pre>\n<p>Great - there's a base function for snatching all links out of the body of a page. Ultimately we'll create a JSON object for each of these links containing preview data, <code>link_preview_dict</code>.</p><p>To handle each value of our dict, we have individual functions:</p><pre><code class=\"language-python\">def getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get a preview image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n</code></pre>\n<p>In case you're wondering:</p><ul><li><strong>getTitle </strong>tries to get the <code>&lt;title&gt;</code> tag, and falls back to the page's first <code>&lt;h1&gt;</code> tag (surprisingly enough some pages are in fact missing a title).</li><li><strong>getDescription</strong> looks for the OG description, and falls back to the content of the page's first paragraph.</li><li><strong>getImage </strong>looks for the OG image, and falls back to the page's first image.</li><li><strong>getSiteName </strong>similarly tries to grab the OG attribute, otherwise it does it's best to extract the domain name from the URL string under the assumption that this is the origin's name (look, it ain't perfect).</li></ul><h2 id=\"what-did-we-just-build\">What Did We Just Build?</h2><p>Believe it or not, the above is considered to be enough logic to be a paid service with a monthly fee. Go ahead and Google it; or better yet, just steal my source code entirely:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p &gt; a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)\n</code></pre>\n","url":"https://hackersandslackers.com/scraping-urls-with-beautifulsoup/","uuid":"c933218e-6bbf-44b7-8f01-bfd188c71d89","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be7fc282ec6e0035b4b16bc"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673733","title":"Python-Lambda: The Essential Library for AWS Cloud Functions","slug":"improve-your-aws-lambda-workflow-with-python-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","custom_excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","created_at_pretty":"07 November, 2018","published_at_pretty":"08 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-07T16:01:48.000-05:00","published_at":"2018-11-07T19:13:20.000-05:00","updated_at":"2019-01-05T13:22:03.000-05:00","meta_title":"Simplify Lambda Deployment with python-lambda | Hackers and Slackers","meta_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","og_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","twitter_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","twitter_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our series about building AWS APIs\n[https://hackersandslackers.com/tag/aws-api/], we've covered a lot of ground\naround learning the AWS ecosystem. Now that we're all feeling a bit more\ncomfortable, it may be time to let everybody in on the world's worst-kept\nsecret: Almost nobody builds architecture by interacting with the AWS UI\ndirectly. There are plenty examples of how this is done, with the main example\nbeing HashiCorp:  an entire business model based around the premise that AWS has\na shitty UI, to the point where it's easier to write code to make things which\nwill host your code. What a world.\n\nIn the case of creating Python Lambda functions, the \"official\" (aka: manual)\nworkflow of deploying your code to AWS is something horrible like this:\n\n * You start a project locally and begin development.\n * You opt to use virtualenv, because you're well aware that you're going to\n   need the source for any packages you use available.\n * When you're ready to 'deploy' to AWS, you copy all your dependencies from \n   /site-packages  and move them into your root directory, temporarily creating\n   an abomination of a project structure.\n * With your project fully bloated and confused, you cherry pick the files\n   needed to zip into an archive.\n * Finally, you upload your code via zip either to Lambda directory or to S3,\n   only to run your code, realize its broken, and need to start all over.\n\nThere Must be a Better Way\nIndeed there is, and surprisingly enough the solution is 100% Python (sorry\nHashiCorp, we'll talk another time). This \"better way\" is my personal method of\nleveraging the following:\n\n * The official AWS CLI\n   [https://docs.aws.amazon.com/cli/latest/userguide/installing.html].\n * Pipenv [https://pipenv.readthedocs.io/en/latest/]  as an environment manager.\n * Python's python-lambda [https://github.com/nficano/python-lambda]  package:\n   the magic behind it all.\n\nObligatory \"Installing the CLI\" Recap\nFirst off, make sure you're using a compatible version of Python on your system,\nas AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by\nthe way, Python 2.7 doesn't count as compatible - let it die before your career\ndoes).\n\n$ pip3 install awscli --upgrade --user\n\n\nIf you're working off an EC2 instance, it has come to my attention pip3 does not\ncome preinstalled. Remember to run: * $ apt update\n * $ apt upgrade\n * $ apt install python3-pip\n\nYou may be prompted to run apt install awscli  as well.Awesome, now that we have\nthe CLI installed on the real  version of Python, we need to store your\ncredentials. Your Access Key ID and Secret Access Key can be found in your IAM\npolicy manager.\n\n$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:\n\nOn both Linux and OSX, this should generate files found under cd ~/.aws  which\nwill be referenced by default whenever you use an AWS service moving forward.\n\nSet Up Your Environment\nAs mentioned, we'll use pipenv  for easy environment management. We'll create an\nenvironment using Lambda's preferred Python version:\n\n$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n\n\nSomething you should be aware of at the time of writing: Pip's latest version,\n18.1, is actually a breaking change  for Pipenv. Thus, the first thing we should\ndo is force usage of pip 18.0 (is there even a fix for this yet?). This is\nsolved by typing pip3 install pip==18.0  with the Pipenv shell activated. Now\nlet's get to the easy part.\n\npython-lambda: The Savior of AWS\nSo far we've made our lives easier in two ways: we're keeping our AWS\ncredentials safe and far away from ourselves, and we have what is by far the\nsuperior Python package management solution. But this is all foreplay leading up\nto python-lambda:\n\n$ pip3 install python-lambda\n\n\nThis library alone is about to do you the following favors:\n\n * Initiate your Lambda project structure for you.\n * Isolate Lambda configuration to a  config.yaml  file, covering everything\n   from the name of your entry point, handler function, and even\n   program-specific variables.\n * Allow you to run tests locally, where a test.json  file simulates a request\n   being made to your function locally.\n * Build a production-ready zip file with all dependencies completely separated \n   from your beautiful file structure.\n * The ability to deploy directly  to S3 or Lambda with said zip file from\n   command-line.\n\nCheck out the commands for yourself:\n\nCommands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n\n\nInitiate your project\nRunning lambda init  will generate the following file structure:\n\n.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n\n\nChecking out the entry point: service.py\npython-lambda starts you off with a basic handler as an example of a working\nproject. Feel free to rename service.py  and its handler function to whatever\nyou please, as we can configure that in a bit.\n\n# -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n\n\nEasy configuration via configure.yaml\nThe base config generated by lambda init  looks like this:\n\nregion: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n\n\nLook familiar? These are all the properties you would normally have to set up\nvia the UI. As an added bonus, you can store values (such as S3 bucket names for\nboto3) in this file as well. That's dope.\n\nSetting up event.json\nThe default event.json  is about as simplistic as you can get, and naturally not\nvery helpful at first (it isn't meant to be). These are the contents:\n\n{\n  \"pi\": 3.14,\n  \"e\": 2.718\n}\n\n\nWe can replace this a real test JSON which we can grab from Lambda itself.\nHere's an example of a Cloudwatch event we can use instead:\n\n{\n  \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n  \"detail-type\": \"Scheduled Event\",\n  \"source\": \"aws.events\",\n  \"account\": \"{{account-id}}\",\n  \"time\": \"1970-01-01T00:00:00Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n  ],\n  \"pi\": 3.14,\n  \"e\": 2.718\n  \"detail\": {}\n}\n\n\nRemember that event.json  is what is being passed to our handler as the event \nparameter. Thus, now we can run our Lambda function locally  to see if it works:\n\n$ lambda invoke\n5.8580000000000005\n\n\nPretty cool if you ask me.\n\nDeploy it, Ship it, Roll Credits\nAfter you express your coding genius, remember to output pip freeze >\nrequirements.txt. python-lambda  will use this as a reference for which packages\nneed to be included. This is neat because we can use Pipenv and the benefits of\nthe workflow it provides while still easily outputting what we need to deploy. \n\nBecause we already specified which Lambda we're going to deploy to in \nconfig.yaml, we can deploy to that Lambda immediately. lambda deploy  will use\nthe zip upload method, whereas lambda deploy-s3  will store your source on S3.\n\nIf you'd like to deploy the function yourself, run with lambda build  which will\nzip your source code plus dependencies  neatly into a /dist  directory. Suddenly\nwe never have to compromise our project structure, and now we can easily source\ncontrol our Lambdas by .gitignoring our build folders while hanging on to our\nPipfiles.\n\nHere's to hoping you never need to deploy Lambdas using any other method ever\nagain. Cheers.","html":"<p>In our series about building <a href=\"https://hackersandslackers.com/tag/aws-api/\">AWS APIs</a>, we've covered a lot of ground around learning the AWS ecosystem. Now that we're all feeling a bit more comfortable, it may be time to let everybody in on the world's worst-kept secret: Almost nobody builds architecture by interacting with the AWS UI directly. There are plenty examples of how this is done, with the main example being <strong>HashiCorp:</strong> an entire business model based around the premise that AWS has a shitty UI, to the point where it's easier to write code to make things which will host your code. What a world.</p><p>In the case of creating Python Lambda functions, the \"official\" (aka: manual) workflow of deploying your code to AWS is something horrible like this:</p><ul><li>You start a project locally and begin development.</li><li>You opt to use <strong>virtualenv, </strong>because you're well aware that you're going to need the source for any packages you use available.</li><li>When you're ready to 'deploy' to AWS, you <em>copy all your dependencies from </em><code>/site-packages</code> <em>and move them into your root directory</em>, temporarily creating an abomination of a project structure.</li><li>With your project fully bloated and confused, you cherry pick the files needed to zip into an archive.</li><li>Finally, you upload your code via zip either to Lambda directory or to S3, only to run your code, realize its broken, and need to start all over.</li></ul><h2 id=\"there-must-be-a-better-way\">There Must be a Better Way</h2><p>Indeed there is, and surprisingly enough the solution is 100% Python (sorry HashiCorp, we'll talk another time). This \"better way\" is my personal method of leveraging the following:</p><ul><li>The official <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\">AWS CLI</a>.</li><li><a href=\"https://pipenv.readthedocs.io/en/latest/\">Pipenv</a> as an environment manager.</li><li>Python's <strong><a href=\"https://github.com/nficano/python-lambda\">python-lambda</a></strong> package: the magic behind it all.</li></ul><h3 id=\"obligatory-installing-the-cli-recap\">Obligatory \"Installing the CLI\" Recap</h3><p>First off, make sure you're using a compatible version of Python on your system, as AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by the way, <em>Python 2.7 </em>doesn't count as compatible - let it die before your career does).</p><pre><code class=\"language-python\">$ pip3 install awscli --upgrade --user\n</code></pre>\n<div class=\"protip\">\n    If you're working off an EC2 instance, it has come to my attention pip3 does not come preinstalled. Remember to run:\n<ul>\n    <li><code>$ apt update</code></li>\n    <li><code>$ apt upgrade</code></li>\n    <li><code>$ apt install python3-pip</code></li>\n</ul>\n    \n    You may be prompted to run <code>apt install awscli</code> as well.\n</div><p>Awesome, now that we have the CLI installed on the <em>real</em> version of Python, we need to store your credentials. Your Access Key ID and Secret Access Key can be found in your IAM policy manager.</p><pre><code>$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:</code></pre><p>On both Linux and OSX, this should generate files found under <code>cd ~/.aws</code> which will be referenced by default whenever you use an AWS service moving forward.</p><h2 id=\"set-up-your-environment\">Set Up Your Environment</h2><p>As mentioned, we'll use <code>pipenv</code> for easy environment management. We'll create an environment using Lambda's preferred Python version:</p><pre><code class=\"language-python\">$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n</code></pre>\n<p>Something you should be aware of at the time of writing: Pip's latest version, 18.1, is actually a <em>breaking change</em> for Pipenv. Thus, the first thing we should do is force usage of pip 18.0 (is there even a fix for this yet?). This is solved by typing <code>pip3 install pip==18.0</code> with the Pipenv shell activated. Now let's get to the easy part.</p><h2 id=\"python-lambda-the-savior-of-aws\">python-lambda: The Savior of AWS</h2><p>So far we've made our lives easier in two ways: we're keeping our AWS credentials safe and far away from ourselves, and we have what is by far the superior Python package management solution. But this is all foreplay leading up to <code>python-lambda</code>:</p><pre><code class=\"language-bash\">$ pip3 install python-lambda\n</code></pre>\n<p>This library alone is about to do you the following favors:</p><ul><li>Initiate your Lambda project structure for you.</li><li>Isolate Lambda configuration to a<em> config.yaml</em> file, covering everything from the name of your entry point, handler function, and even program-specific variables.</li><li>Allow you to run tests locally, where a <em>test.json</em> file simulates a request being made to your function locally.</li><li>Build a production-ready zip file with all dependencies <em>completely separated </em>from your beautiful file structure.</li><li>The ability to deploy <em>directly</em> to S3 or Lambda with said zip file from command-line.</li></ul><p>Check out the commands for yourself:</p><pre><code class=\"language-bash\">Commands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n</code></pre>\n<h3 id=\"initiate-your-project\">Initiate your project</h3><p>Running <code>lambda init</code> will generate the following file structure:</p><pre><code class=\"language-bash\">.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n</code></pre>\n<h3 id=\"checking-out-the-entry-point-service-py\">Checking out the entry point: service.py</h3><p>python-lambda starts you off with a basic handler as an example of a working project. Feel free to rename <code>service.py</code> and its handler function to whatever you please, as we can configure that in a bit.</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n</code></pre>\n<h3 id=\"easy-configuration-via-configure-yaml\">Easy configuration via configure.yaml</h3><p>The base config generated by <code>lambda init</code> looks like this:</p><pre><code class=\"language-yaml\">region: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n</code></pre>\n<p>Look familiar? These are all the properties you would normally have to set up via the UI. As an added bonus, you can store values (such as S3 bucket names for boto3) in this file as well. That's dope.</p><h3 id=\"setting-up-event-json\">Setting up event.json</h3><p>The default <code>event.json</code> is about as simplistic as you can get, and naturally not very helpful at first (it isn't meant to be). These are the contents:</p><pre><code class=\"language-json\">{\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n}\n</code></pre>\n<p>We can replace this a real test JSON which we can grab from Lambda itself. Here's an example of a Cloudwatch event we can use instead:</p><pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;cdc73f9d-aea9-11e3-9d5a-835b769c0d9c&quot;,\n  &quot;detail-type&quot;: &quot;Scheduled Event&quot;,\n  &quot;source&quot;: &quot;aws.events&quot;,\n  &quot;account&quot;: &quot;{{account-id}}&quot;,\n  &quot;time&quot;: &quot;1970-01-01T00:00:00Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [\n    &quot;arn:aws:events:us-east-1:123456789012:rule/ExampleRule&quot;\n  ],\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n  &quot;detail&quot;: {}\n}\n</code></pre>\n<p>Remember that <code>event.json</code> is what is being passed to our handler as the <code>event</code> parameter. Thus, now we can run our Lambda function <em>locally</em> to see if it works:</p><pre><code class=\"language-bash\">$ lambda invoke\n5.8580000000000005\n</code></pre>\n<p>Pretty cool if you ask me.</p><h2 id=\"deploy-it-ship-it-roll-credits\">Deploy it, Ship it, Roll Credits</h2><p>After you express your coding genius, remember to output <code>pip freeze &gt; requirements.txt</code>. <strong>python-lambda</strong> will use this as a reference for which packages need to be included. This is neat because we can use Pipenv and the benefits of the workflow it provides while still easily outputting what we need to deploy. </p><p>Because we already specified which Lambda we're going to deploy to in <code>config.yaml</code>, we can deploy to that Lambda immediately. <code>lambda deploy</code> will use the zip upload method, whereas <code>lambda deploy-s3</code> will store your source on S3.</p><p>If you'd like to deploy the function yourself, run with <code>lambda build</code> which will zip your source code <em>plus dependencies</em> neatly into a /<em>dist</em> directory. Suddenly we never have to compromise our project structure, and now we can easily source control our Lambdas by <em>.gitignoring </em>our build folders while hanging on to our Pipfiles.</p><p>Here's to hoping you never need to deploy Lambdas using any other method ever again. Cheers.</p>","url":"https://hackersandslackers.com/improve-your-aws-lambda-workflow-with-python-lambda/","uuid":"08ad7706-8dd7-4475-875e-880c017de8d5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be352bc2aa81b1606ab77a7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673651","title":"Building an API with Amazon's API Gateway","slug":"creating-apis-with-api-gateway","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","custom_excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","created_at_pretty":"13 May, 2018","published_at_pretty":"29 October, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-05-13T17:29:07.000-04:00","published_at":"2018-10-29T19:41:00.000-04:00","updated_at":"2019-01-05T13:28:10.000-05:00","meta_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","meta_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","og_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","twitter_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","twitter_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our last adventure, we ventured off to create our very own cloud database\n[https://hackersandslackers.com/setting-up-mysql-on-aws/]  by using Amazon's RDS \n service. We've also briefly covered\n[https://hackersandslackers.com/building-an-api-using-aws/]  the general concept\nbehind what Lambda functions. In case you've already forgotten, Lambdas are\nbasically just chunks of code in the cloud; think of them as tiny virtual\nservers, which have already been configured (and locked down) to serve one\nspecific purpose. Because that's literally what it is.\n\nThe data being stored in RDS is ultimately what we're targeting, and Lambdas \nserve as the in-between logic to serve up, modify, or add to the proper data.\nThe only piece missing from the picture is API Gateway. \n\nAs the name suggests, API Gateway  is the, uh, gateway  that users or systems\ninteract with to obtain what they're seeking. It is (hopefully) the only part of\nthis VPC structure an external user can interact with:\n\nSimple API to interact with RDS.Serving as a \"gateway\" is obviously what all\nAPIs so, but the term is also true in the sense that API Gateway  is completely\nconfigured via UI, thus engineers of any programming background can safely\nmodify endpoints, methods,  CORs  configurations, or any of the high-level API\nstructure without being locked into a programming language. API Gateway  is\ntherefore very much an enterprise-geared product: it lends itself to large teams\nand scaling. That said, if it were to be compared to building an API via a\nframework designed to do such things (such as Express or Flask) the experience\nis undoubtedly more clunky. The trade-off being made for speed is immediate\nvisibility, assurance, and a higher chance for collaboration.\n\nThe Challenge of Building a \"Well-Designed\" API\nGood APIs are premeditated. A complex API might accept multiple methods per\nendpoint, allow advanced filtering of results, or handle advanced\nAuthentication. Neither of us have the time to attempt covering all of those\nthings in detail, but I will  leave you with the knowledge that all these\nfeatures are very much possible.\n\nThe API Gateway  interface is where you'd get started. Let's blow through the\nworld's most inappropriately fast explanation of building APIs ever ,and check\nout the UI:\n\nIt ain't pretty, but it works. * Your APIs are listed on the left. You can create more than one, if you're\n   some sort of sadist.\n * The Resources pane is the full structure of your API. At the highest level,\n   'resources' refers to Endpoints,  which are the URLs your API will ultimately\n   expose.\n * Every Endpoint  can contain whichever Methods  you choose to associate with\n   them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST\n   method could contain entirely unrelated logic from a PUT method: its your\n   responsibility to make sure your API design makes sense.\n * Finally, each Method has their expected Request and Response  structures\n   defined individually, which what the horribly designed box diagram is\n   attempting to explain on the right. The box on the left labeled CLIENT refers\n   to the requester, where the box on the right represents the triggered action.\n\nThis UI is your bread and butter. I hope you're strapped in, because walking\nthrough this interface is going to be hella boring for all of us.\n\nCreating a Method Request\nThe first step to creating an endpoint (let's say a GET endpoint) is to set the\nexpectation for what the user will send to us:\n\nAwe yea, authorization. 1. Authorization  allows you to restrict users from using your API unless they\n    follow your IAM policy.\n 2. Request Validator  lets you chose if you'd like this validation to happen\n    via the body, query string parameters, headers, or all of the above.\n 3. API Keys  are useful if you're creating an API to sell commercially or\n    enforce limited access. If your business model revolves around selling an\n    API, you can realistically do this.\n 4. Query String Parameters  are... actually forget it, you know this by now.\n 5. See above.\n 6. If preferred, the Request Body can be assigned a model,  which is\n    essentially a JSON schema. If a request is made to your endpoint which does\n    not match the request body model, it is a malformed request. We'll cover \n    models  in the advanced course, once somebody actually starts paying me to\n    write this stuff.\n\nMethod Execution: AKA \"What do we do with this?\"\nSet the game plan. 1. Integration Type  specifies which AWS service will be accepting or affected\n    by this request. The vast majority of the time, this will be Lambda. If\n    you're wondering why other AWS Services aren't present, this has been made\n    intentional over time as just about any AWS service you can interact with\n    will still need logic to do anything useful: you can't just shove a JSON\n    object in a database's face and expect to get results. Unless you're using\n    MongoDB or something.\n 2. Lambda Proxies  are generally a bad idea. They auto-format your Lambda's \n    request  and response  body to follow a very  specific structure, which is\n    presumably intended to help speed up or standardize development. The\n    downside is these structures are bloated and most likely contain useless\n    information. To get an idea of what these structures look like, check them\n    out here.\n 3. The Region  your Lambda hosted lives in.\n 4. Name of the Lamba Function  your request will be directed to.\n 5. Execution role  refers to the IAM role your Lambda policy will be a part of.\n    This is kind of an obnoxious concept, but your function has permissions as\n    though it were a user. This is presumably Amazon's way of thinking ahead to\n    extending human rights to robots.\n 6. Caller Credentials  refers to API keys, assuming you chose to use them. If\n    this is checked, the API will not be usable without an API key, thus making\n    it difficult to test\n 7. Credentials Cache  probably refers to expiring credentials or something, I'm\n    sure you'll figure it out.\n 8. Timeout  can be increased if you're dealing with an API call that takes a\n    lot of time to respond, such as occasions with heavy data sets.\n 9. URL Paths probably do something, I don't know. Who really cares?\n\nINTERMISSION: The Part Where Things Happen\nThe next step in the flow would be where the AWS service we selected to handle\nthe request would do its thing. We'll get into that next time.\n\nResponse Codes and Headers\nKeep it 200 baby. 1. While AWS provides users with standard error codes  and generic errors, you\n    can add your own specific error/success messages. Props to whoever puts in\n    the effort.\n 2. Header Mappings  are the headings returned with the response. For example,\n    this is where you might solve cross-domain issues via the \n    Access-Control-Allow-Origin  header.\n\n3. Mapping Templates  are the Content-Type  of the response returned, most\ncommonly application/json.\n\nMethod Response\nI almost never spend time hereThis step is a continuation of the previous step.\nI'm not entirely sure what the point in splitting this into two screens is, but\nI'm guessing its not important.\n\nReap Your Rewards\nAt long last, this brings us to the end of our journey. This is presumably where\n you've executed a successful AWS test or something. However, there's a final\nstep before you go live; deploying your API:\n\nDeploy your API to a live \"stage\" and retire.Next time we'll cover the logical,\nless boring part of writing actual code behind these endpoints.","html":"<p>In our last adventure, we ventured off to create our very own cloud <a href=\"https://hackersandslackers.com/setting-up-mysql-on-aws/\">database</a> by using Amazon's <strong>RDS</strong> service. We've also <a href=\"https://hackersandslackers.com/building-an-api-using-aws/\">briefly covered</a> the general concept behind what <strong>Lambda functions</strong>. In case you've already forgotten, Lambdas are basically just chunks of code in the cloud; think of them as tiny virtual servers, which have already been configured (and locked down) to serve one specific purpose. Because that's literally what it is.</p><p>The data being stored in <strong>RDS </strong>is ultimately what we're targeting, and <strong>Lambdas</strong> serve as the in-between logic to serve up, modify, or add to the proper data. The only piece missing from the picture is <strong>API Gateway</strong>. </p><p>As the name suggests, <strong>API Gateway</strong> is the, uh, <em>gateway</em> that users or systems interact with to obtain what they're seeking. It is (hopefully) the only part of this VPC structure an external user can interact with:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_o-1.jpg\" class=\"kg-image\"><figcaption>Simple API to interact with RDS.</figcaption></figure><p>Serving as a \"gateway\" is obviously what all APIs so, but the term is also true in the sense that <strong>API Gateway</strong> is completely configured via UI, thus engineers of any programming background can safely modify <em>endpoints</em>, <em>methods,</em> <em>CORs</em> configurations, or any of the high-level API structure without being locked into a programming language. <strong>API Gateway</strong> is therefore very much an enterprise-geared product: it lends itself to large teams and scaling. That said, if it were to be compared to building an API via a framework designed to do such things (such as Express or Flask) the experience is undoubtedly more clunky. The trade-off being made for speed is immediate visibility, assurance, and a higher chance for collaboration.</p><h2 id=\"the-challenge-of-building-a-well-designed-api\">The Challenge of Building a \"Well-Designed\" API</h2><p>Good APIs are premeditated. A complex API might accept multiple methods per endpoint, allow advanced filtering of results, or handle advanced Authentication. Neither of us have the time to attempt covering all of those things in detail, but I <em>will</em> leave you with the knowledge that all these features are very much possible.  </p><p>The <strong>API Gateway</strong> interface is where you'd get started. Let's blow through the world's most inappropriately fast explanation of building APIs ever ,and check out the UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_overview3.png\" class=\"kg-image\"><figcaption>It ain't pretty, but it works.</figcaption></figure><ul><li>Your <strong>APIs </strong>are listed on the left. You can create more than one, if you're some sort of sadist.</li><li>The <strong>Resources </strong>pane is the full structure of your API. At the highest level, 'resources' refers to <strong>Endpoints,</strong> which are the URLs your API will ultimately expose.</li><li>Every <strong>Endpoint</strong> can contain whichever <strong>Methods</strong> you choose to associate with them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST method could contain entirely unrelated logic from a PUT method: its your responsibility to make sure your API design makes sense.</li><li>Finally, each <strong>Method </strong>has their expected <strong>Request </strong>and <strong>Response</strong> structures defined individually, which what the horribly designed box diagram is attempting to explain on the right. The box on the left labeled CLIENT refers to the requester, where the box on the right represents the triggered action.</li></ul><p>This UI is your bread and butter. I hope you're strapped in, because walking through this interface is going to be hella boring for all of us.</p><h3 id=\"creating-a-method-request\">Creating a Method Request</h3><p>The first step to creating an endpoint (let's say a GET endpoint) is to set the expectation for what the user will send to us:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodrequest_o.jpg\" class=\"kg-image\"><figcaption>Awe yea, authorization.</figcaption></figure><ol><li><strong>Authorization</strong> allows you to restrict users from using your API unless they follow your IAM policy.</li><li><strong>Request Validator</strong> lets you chose if you'd like this validation to happen via the body, query string parameters, headers, or all of the above.</li><li><strong>API Keys</strong> are useful if you're creating an API to sell commercially or enforce limited access. If your business model revolves around selling an API, you can realistically do this.</li><li><strong>Query String Parameters</strong> are... actually forget it, you know this by now.</li><li>See above.</li><li>If preferred, the <strong>Request Body </strong>can be assigned a <strong>model,</strong> which is essentially a JSON schema. If a request is made to your endpoint which does not match the request body model, it is a malformed request. We'll cover <strong>models</strong> in the advanced course, once somebody actually starts paying me to write this stuff.</li></ol><h3 id=\"method-execution-aka-what-do-we-do-with-this\">Method Execution: AKA \"What do we do with this?\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodexecution_o.jpg\" class=\"kg-image\"><figcaption>Set the game plan.</figcaption></figure><ol><li><strong>Integration Type</strong> specifies which AWS service will be accepting or affected by this request. The vast majority of the time, this will be Lambda. If you're wondering why other AWS Services aren't present, this has been made intentional over time as just about any AWS service you can interact with will still need logic to do anything useful: you can't just shove a JSON object in a database's face and expect to get results. Unless you're using MongoDB or something.</li><li><strong>Lambda Proxies</strong> are generally a bad idea. They auto-format your Lambda's <em>request</em> and <em>response</em> body to follow a very  specific structure, which is presumably intended to help speed up or standardize development. The downside is these structures are bloated and most likely contain useless information. To get an idea of what these structures look like, check them out <a href=\"https://github.com/bbilger/jrestless/tree/master/aws/gateway/jrestless-aws-gateway-handler#response-schema\">here</a>.</li><li>The <strong>Region</strong> your Lambda hosted lives in.</li><li>Name of the <strong>Lamba Function</strong> your request will be directed to.</li><li><strong>Execution role</strong> refers to the IAM role your Lambda policy will be a part of. This is kind of an obnoxious concept, but your function has permissions as though it were a user. This is presumably Amazon's way of thinking ahead to extending human rights to robots.</li><li><strong>Caller Credentials</strong> refers to API keys, assuming you chose to use them. If this is checked, the API will not be usable without an API key, thus making it difficult to test</li><li><strong>Credentials Cache</strong> probably refers to expiring credentials or something, I'm sure you'll figure it out.</li><li><strong>Timeout</strong> can be increased if you're dealing with an API call that takes a lot of time to respond, such as occasions with heavy data sets.</li><li><strong>URL Paths </strong>probably do something, I don't know. Who really cares?</li></ol><h3 id=\"intermission-the-part-where-things-happen\">INTERMISSION: The Part Where Things Happen</h3><p>The next step in the flow would be where the AWS service we selected to handle the request would do its thing. We'll get into that next time.</p><h3 id=\"response-codes-and-headers\">Response Codes and Headers</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/response_o.jpg\" class=\"kg-image\"><figcaption>Keep it 200 baby.</figcaption></figure><ol><li>While AWS provides users with standard <strong>error codes</strong> and generic errors, you can add your own specific error/success messages. Props to whoever puts in the effort.</li><li><strong>Header Mappings</strong> are the headings returned with the response. For example, this is where you might solve cross-domain issues via the <em>Access-Control-Allow-Origin</em> header.</li></ol><p>3. <strong>Mapping Templates</strong> are the <em>Content-Type</em> of the response returned, most commonly <em>application/json.</em></p><h3 id=\"method-response\">Method Response</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-8.54.51-PM_o.png\" class=\"kg-image\"><figcaption>I almost never spend time here</figcaption></figure><p>This step is a continuation of the previous step. I'm not entirely sure what the point in splitting this into two screens is, but I'm guessing its not important.</p><h2 id=\"reap-your-rewards\">Reap Your Rewards</h2><p>At long last, this brings us to the end of our journey. This is presumably where  you've executed a successful AWS test or something. However, there's a final step before you go live; deploying your API:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-9.02.52-PM_o.png\" class=\"kg-image\"><figcaption>Deploy your API to a live \"stage\" and retire.</figcaption></figure><p>Next time we'll cover the logical, less boring part of writing actual code behind these endpoints.</p>","url":"https://hackersandslackers.com/creating-apis-with-api-gateway/","uuid":"ce9c1023-431b-4580-b3ca-1a3e2074f9c5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5af8ae23092feb404eb9981e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372f","title":"Deploy Isolated Applications with Google App Engine","slug":"deploy-app-containters-with-gcp-app-engine","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","excerpt":"Doing everything to avoid server configuration or any mild discomfort.","custom_excerpt":"Doing everything to avoid server configuration or any mild discomfort.","created_at_pretty":"25 October, 2018","published_at_pretty":"25 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-24T20:30:22.000-04:00","published_at":"2018-10-25T07:30:00.000-04:00","updated_at":"2019-01-06T11:26:06.000-05:00","meta_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","meta_description":"Doing everything to avoid server configuration or any mild discomfort.","og_description":"Doing everything to avoid server configuration or any mild discomfort.","og_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","og_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","twitter_description":"Doing everything to avoid server configuration or any mild discomfort.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","twitter_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"We've been on a bit of a tear lately on Google Cloud lately (or at least I\nhave), and I have no desire to stop any time soon. I probably should though...\n our analytics show that half our viewers are just people struggling to us AWS.\nSpeaking of capitalizing on shitty UI, stay tuned in the future where we'll\noffer grossly overpriced unauthorized AWS certification programs.\n\nAWS aside, I'm here to talk about the other  Cloud in town - in particular,\nGoogle's solution to make sure you never configure a webserver again. This is a\ntrend that's been grinding my gears a bit: as much as I appreciate the reduction\nin effort, the costs of choosing proprietary (paid) services to avoid opening a\nLinux shell seems like a dangerous prospect over time as every day developers\nbecome more and more reliant on convenience. Then again, I'm probably just angry\nthat nobody will have to endure the pain of Python development circa 2012. \n\nRandom Old-Man Tangent About Configuring Webservers\nRemember when we all ran Apache servers, and the world decided that mod_python \nshould stop existing for no reason? The replacement was, of course, mod_wsgi:  \nan entirely undocumented way of running Python on an Apache server created by a\nsingle guy from Google (who has apparently opted to spend the entirety of his\nlife attempting to explain mod_wsgi on StackOverflow\n[https://stackoverflow.com/users/128141/graham-dumpleton]). \n\nBesides mod_wsgi, the Nginx alternatives (Gunicorn  and uWSGI) are almost\nequally insufferable to implement. Much of this can be attributed to tutorials\n(such as those posted by DigitalOcean) which dominate SEO, merely because those\ntutorials include glaring inexcusable typos  in their configuration files. This\nresults in an infinite Google search feedback loop, where you find what seems to\nbe a perfectly fine tutorial... plus 10 pages of frustrated developers\nbacklinking to said tutorial, trying to figure out where the hell the missing\ncolon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm\npretty sure at this point nobody cares to put up with monetized troubleshooting\nbullshit schemes (calling it now: the slightly-false-tutorial is an elaborate\nSEO scam).  So yeah, app containers it is then.\n\nThe Benefits of App Engine\nBesides not needing to know anything about Linux, hosting on App Engine provides\na few other benefits. Considering all microservices are obfuscated in the cloud,\nwe can easily hook into other services such as setting up CRON jobs, Tasks, and\nDNS, for instance. GCP's catalogue of offerings is destined to grow, whether\nthose offerings are ferociously released from Google's ambitious backlog, or the\nresult of a partnership utilizing Google Cloud for architecture, such as MongoDB\ncloud and others. Prepare to witness fierce and unapologetic growth from GCP by\nevery metric, year-over-year. \n\nApp Engine  is also intimately close with your source code. Despite the\ndynamically typed nature of Python and Javascript, App Engine will catch fatal\nerrors when attempting to deploy your app which would not happen otherwise.\nAdding this type of interpreter adds a convenient level of 'easy mode,' where\npotentially fatal production errors are caught before deployment is even\npermitted. I even tried deploying some personal projects to App Engine which had\nbeen running live elsewhere, and App Engine was able to catch errors existing in\nmy code which had been shamelessly running in production. Oops.\n\nEven while the app is live, all errors are conveniently detected and reported\nfront and center in the app engine dashboard:\n\n\"No module named App\" seems like a pretty bad error.So yes, there are plenty of\nbenefits and reasons to use App Engine over a VPS: removal of webserver\nconfiguration, build errors caught at runtime, and easy command-line deployments\nname a few of such benefits. The question of whether or not these perks are\nworth the price tag and vendor-lock are a personal decision.\n\nCreating your First App... Engine\nGoogle provides the luxury of creating apps in a variety of languages, including\nhot new -comer to the game, PHP. Lucky us!\n\nHah, .NET is still a thing too.Google will forcefully insist you complete their\nown step-by-step tutorial, which essentially teaches you how to use git clone \nand explains the contents of their YAML file. You can follow this if you want. \n\nMore interestingly is what you'll find when you open the GCP browser shell.\nWhile working through this tutorial, it's impossible to ignore that Google Cloud\nis essentially just a giant VPS across all your projects:\n\nAll of these directories were created in different projects.Just when we were\ndone ranting, it turns out every service we pay for is just a thinly veiled\nobfuscation of something we could probably do on a 10 dollar Droplet. Fuck,\nlet's just move on.\n\nSimple Project Configuration\nPerhaps majority of what one needs to learn to deploy apps is contained within a\nsingle YAML file. Add a YAML file in your directory:\n\nruntime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n\n\nSet your Static directory if you're working in Python. Use Python 3.7.\n\nGoogle also invented its own version of .gitignore  for App Engine called \n.gcloudignore, so be aware of that.\n\nHaving worked with Flask in the past, you should presumably be familiar with a\nstartup script such as the following:\n\nfrom framewrk import create_app\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0')\n\n\nThat's pretty much it man. Just remember that Google prefers requirements.txt \nover other forms of package management (Google will actually bar Pipfile from\nbeing committed, interestingly enough). \n\nIf you're working locally, gcloud app deploy  is all you need to push to\nproduction (doesn't require a git commit, interestingly enough. gcloud app\nbrowse  will open you newly deployed app, and gcloud app logs tail -s default \nwill display your logs when something goes horribly wrong.\n\nAnd there you have it: the practical and completely cynical guide to embracing\nmodern architecture. Join us next time when we pay 50 dollars to deploy a\nsingle-click app simply because we're too lazy or unmotivated to set anything up\nourselves.","html":"<p>We've been on a bit of a tear lately on Google Cloud lately (or at least I have), and I have no desire to stop any time soon. I probably should though...  our analytics show that half our viewers are just people struggling to us AWS. Speaking of capitalizing on shitty UI, stay tuned in the future where we'll offer grossly overpriced unauthorized AWS certification programs.</p><p>AWS aside, I'm here to talk about the <em>other</em> Cloud in town - in particular, Google's solution to make sure you never configure a webserver again. This is a trend that's been grinding my gears a bit: as much as I appreciate the reduction in effort, the costs of choosing proprietary (paid) services to avoid opening a Linux shell seems like a dangerous prospect over time as every day developers become more and more reliant on convenience. Then again, I'm probably just angry that nobody will have to endure the pain of Python development circa 2012. </p><h3 id=\"random-old-man-tangent-about-configuring-webservers\">Random Old-Man Tangent About Configuring Webservers</h3><p>Remember when we all ran Apache servers, and the world decided that <strong>mod_python</strong> should stop existing for no reason? The replacement was, of course, <strong>mod_wsgi</strong>:<strong> </strong>an entirely undocumented way of running Python on an Apache server created by a single guy from Google (who has apparently opted to spend the entirety of his life attempting to <a href=\"https://stackoverflow.com/users/128141/graham-dumpleton\">explain <strong>mod_wsgi</strong> on StackOverflow</a>). </p><p>Besides <strong>mod_wsgi</strong>, the Nginx alternatives (<strong>Gunicorn</strong> and <strong>uWSGI</strong>) are almost equally insufferable to implement. Much of this can be attributed to tutorials (such as those posted by DigitalOcean) which dominate SEO, merely because those tutorials include <em>glaring inexcusable typos</em> in their configuration files. This results in an infinite Google search feedback loop, where you find what seems to be a perfectly fine tutorial... plus 10 pages of frustrated developers backlinking to said tutorial, trying to figure out where the hell the missing colon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm pretty sure at this point nobody cares to put up with monetized troubleshooting bullshit schemes (calling it now: the slightly-false-tutorial is an elaborate SEO scam).  So yeah, app containers it is then.</p><h2 id=\"the-benefits-of-app-engine\">The Benefits of App Engine</h2><p>Besides not needing to know anything about Linux, hosting on App Engine provides a few other benefits. Considering all microservices are obfuscated in the cloud, we can easily hook into other services such as setting up CRON jobs, Tasks, and DNS, for instance. GCP's catalogue of offerings is destined to grow, whether those offerings are ferociously released from Google's ambitious backlog, or the result of a partnership utilizing Google Cloud for architecture, such as MongoDB cloud and others. Prepare to witness fierce and unapologetic growth from GCP by every metric, year-over-year. </p><p><strong>App Engine</strong> is also intimately close with your source code. Despite the dynamically typed nature of Python and Javascript, App Engine will catch fatal errors when attempting to deploy your app which would not happen otherwise. Adding this type of interpreter adds a convenient level of 'easy mode,' where potentially fatal production errors are caught before deployment is even permitted. I even tried deploying some personal projects to App Engine which had been running live elsewhere, and App Engine was able to catch errors existing in my code which had been shamelessly running in production. Oops.</p><p>Even while the app is live, all errors are conveniently detected and reported front and center in the app engine dashboard:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.12.19-PM.png\" class=\"kg-image\"><figcaption>\"No module named App\" seems like a pretty bad error.</figcaption></figure><p>So yes, there are plenty of benefits and reasons to use App Engine over a VPS: removal of webserver configuration, build errors caught at runtime, and easy command-line deployments name a few of such benefits. The question of whether or not these perks are worth the price tag and vendor-lock are a personal decision.</p><h2 id=\"creating-your-first-app-engine\">Creating your First App... Engine</h2><p>Google provides the luxury of creating apps in a variety of languages, including hot new -comer to the game, PHP. Lucky us!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.17.59-PM.png\" class=\"kg-image\"><figcaption>Hah, .NET is still a thing too.</figcaption></figure><p>Google will forcefully insist you complete their own step-by-step tutorial, which essentially teaches you how to use <strong>git clone</strong> and explains the contents of their YAML file. You can follow this if you want. </p><p>More interestingly is what you'll find when you open the GCP browser shell. While working through this tutorial, it's impossible to ignore that Google Cloud is essentially just a giant VPS <em>across all your projects:</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.26.44-PM.png\" class=\"kg-image\"><figcaption>All of these directories were created in different projects.</figcaption></figure><p>Just when we were done ranting, it turns out every service we pay for is just a thinly veiled obfuscation of something we could probably do on a 10 dollar Droplet. Fuck, let's just move on.</p><h3 id=\"simple-project-configuration\">Simple Project Configuration</h3><p>Perhaps majority of what one needs to learn to deploy apps is contained within a single YAML file. Add a YAML file in your directory:</p><pre><code class=\"language-yaml\">runtime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n</code></pre>\n<p>Set your Static directory if you're working in Python. Use Python 3.7.</p><p>Google also invented its own version of <code>.gitignore</code> for App Engine called <code>.gcloudignore</code>, so be aware of that.</p><p>Having worked with Flask in the past, you should presumably be familiar with a startup script such as the following:</p><pre><code class=\"language-python\">from framewrk import create_app\n\napp = create_app()\n\nif __name__ == &quot;__main__&quot;:\n    app.run(host='0.0.0.0')\n</code></pre>\n<p>That's pretty much it man. Just remember that Google prefers <strong>requirements.txt </strong>over other forms of package management (Google will actually bar Pipfile from being committed, interestingly enough). </p><p>If you're working locally, <code>gcloud app deploy</code> is all you need to push to production (doesn't require a git commit, interestingly enough. <code>gcloud app browse</code> will open you newly deployed app, and <code>gcloud app logs tail -s default</code> will display your logs when something goes horribly wrong.</p><p>And there you have it: the practical and completely cynical guide to embracing modern architecture. Join us next time when we pay 50 dollars to deploy a single-click app simply because we're too lazy or unmotivated to set anything up ourselves.</p>","url":"https://hackersandslackers.com/deploy-app-containters-with-gcp-app-engine/","uuid":"70f7a025-6774-4555-9eff-e63eb40c4fdf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd10e9e4ba34679679904f2"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372e","title":"MySQL, Google Cloud, and a REST API that Generates Itself","slug":"mysql-google-cloud-and-a-rest-api-that-autogenerates","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","custom_excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","created_at_pretty":"23 October, 2018","published_at_pretty":"23 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-23T14:57:12.000-04:00","published_at":"2018-10-23T18:47:28.000-04:00","updated_at":"2019-02-02T05:26:16.000-05:00","meta_title":"MySQL, Google Cloud, and a REST API | Hackers and Slackers","meta_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","og_title":"MySQL, Google Cloud, and a REST API that Generates Itself","twitter_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","twitter_title":"MySQL, Google Cloud, and a REST API that Generates Itself","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn’t too long ago that I haphazardly forced us down a journey of exploring\nGoogle Cloud’s cloud SQL service. The focus of this exploration was Google’s\naccompanying REST API for all of its cloud SQL instances. That API turned out to\nbe a relatively disappointing administrative API which did little to extend the\nfeatures you’d expect from the CLI or console.\n\nYou see, I’ve had a dream stuck in my head for a while now. Like most of my\nutopian dreams, this dream is related to data, or more specifically simplifying\nthe manner in which we interact with it. For industry synonymous with AI and\nautomation, many of our very own tools (including ETL tools) involve way too\nmuch manual effort in my opinion. That’s right: I’m talking about the aspiration\nto Slack while we Hack.\n\nThe pitch is this: why do we keep setting up databases, endpoints, and the logic\nto connect them when, 90% of the time, we’re building the same thing over and\nover? Let me guess: there’s a GET endpoint to get records from table X, or a\nPOST endpoint to create users. I know you’ve built this because we all have, but\nwhy do we keep building the same things over and over in isolation? It looks\nlike we might not have to anymore, but first let’s create our database.\n\nCreating a MySQL Instance in GCP \nFull disclosure here: the magical REST API thing is actually independent from\nGoogle Cloud; the service we’ll be using can integrate with any flavor of MySQL\nyou prefer, so go ahead and grab that RDS instance you live so much if you\nreally have to.\n\nFor the rest of us, hit up your GCP console and head into making a new SQL\ninstance. MySQL and Postgres are our only choices here; stick with MySQL.\n\nThere isn’t much to spinning up your instance. Just be sure to create a user and\ndatabase to work from.\n\nOh yeah, and remember to name your instance.Your SQL Firewall and Permissions\nYour instance is set to “public” by default. Oddly, “public” in this case means\n“accessible to everybody on your IP whitelist, which is empty by default,” so\nreally kind of the opposite of public really.\n\nIn fact, if you hypothetically did want to open your instance publicly, Google\nCloud will not allow it. This is good on them, and is actually fairly impressive\nthe depths they go to avoid the IP 0.0.0.0  from ever appearing anywhere in the\ninstance. Go ahead, open the shell and try to add bind address=0.0.0.0 \nyourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s\nversion of MySQL is actually a MariaDB instance)?\n\nThe point is, whitelist your IP address. Simply \"Edit\" your instance and add\nyour address to the authorized networks.\n\nAuthorize that bad boy.The Magic API \nNow, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so\nthis next part is going to feel a bit a bit weird. I’m not sure why, as the\nservice is apparently free, thus I’m clearly not getting paid for any of this.\n\nAnyway, the service is called Apisentris [https://apisentris.com/], and the idea\nis that it will build whatever time-consuming monstrosity of a REST API you were\nplanning to build to access your data for you. Via their own words:\n\nSee, I told you.What does this actually mean? It means if you create a table\ncalled articles  in your database, you will immediately have an endpoint to\nfetch said articles, and it would look like \nhttps://apisentris.com/api/v1/articles. Your client ID and credentials would\nobviously need to be provided to indicate that you're, well, you.\n\nGrabbing entire tables at once would be silly, which is why they also\nautogenerate filters based on the contents of your table:\n\nEndpoints accept query parameters to essentially create a query.Oh yeah, and you\ncan also handle user management via this API as well, if you're building an\nactual app:\n\nPretty easy to hook up into a form or whatever.I'll assume you're sold on the\nidea by now. If a free service that handles the hard parts of backend logic for\nfree isn't your cup of tea, clearly you aren't Slacker material.\n\nSetting it all up\nAs we did before with our own IP, we'll need to whitelist Apisentris' IP the\nsame way in GCP console. Their IP is 104.199.181.125.\n\nCreate a table in your database with some data just to test things out. When\nyou're logged in, you'll be able to see all the endpoints available to you and\nthe associated attributes they have:\n\nNot bad.Any way you slice it, the concept of a self-generating API is very cool\nand yet somehow still not the norm. I'm actually shocked that there are so few\npeople in the Data industry who know \"there must be a better way,\" but then\nagain, data science and software engineering are two very different things. For\nmy fellow Data Engineers out there, take this as a gift and a curse: you have\nthe gift of knowing better from your software background, but are cursed with\nwatching the world not quite realize how pointless half the things they do truly\nare.\n\nOh well. We'll be the ones building the robots anyway.","html":"<p>It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively disappointing administrative API which did little to extend the features you’d expect from the CLI or console.</p><p>You see, I’ve had a dream stuck in my head for a while now. Like most of my utopian dreams, this dream is related to data, or more specifically simplifying the manner in which we interact with it. For industry synonymous with AI and automation, many of our very own tools (including ETL tools) involve way too much manual effort in my opinion. That’s right: I’m talking about the aspiration to Slack while we Hack.</p><p>The pitch is this: why do we keep setting up databases, endpoints, and the logic to connect them when, 90% of the time, we’re building the same thing over and over? Let me guess: there’s a GET endpoint to get records from table X, or a POST endpoint to create users. I know you’ve built this because we all have, but why do we keep building the same things over and over in isolation? It looks like we might not have to anymore, but first let’s create our database.</p><h2 id=\"creating-a-mysql-instance-in-gcp\">Creating a MySQL Instance in GCP </h2><p>Full disclosure here: the magical REST API thing is actually independent from Google Cloud; the service we’ll be using can integrate with any flavor of MySQL you prefer, so go ahead and grab that RDS instance you live so much if you really have to.</p><p>For the rest of us, hit up your GCP console and head into making a new SQL instance. MySQL and Postgres are our only choices here; stick with MySQL.</p><p>There isn’t much to spinning up your instance. Just be sure to create a user and database to work from.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.15.18-PM.png\" class=\"kg-image\"><figcaption>Oh yeah, and remember to name your instance.</figcaption></figure><h3 id=\"your-sql-firewall-and-permissions\">Your SQL Firewall and Permissions</h3><p>Your instance is set to “public” by default. Oddly, “public” in this case means “accessible to everybody on your IP whitelist, which is empty by default,” so really kind of the opposite of public really.</p><p>In fact, if you hypothetically did want to open your instance publicly, Google Cloud will not allow it. This is good on them, and is actually fairly impressive the depths they go to avoid the IP <strong>0.0.0.0</strong> from ever appearing anywhere in the instance. Go ahead, open the shell and try to add <code>bind address=0.0.0.0</code> yourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s version of MySQL is actually a MariaDB instance)?</p><p>The point is, whitelist your IP address. Simply \"Edit\" your instance and add your address to the authorized networks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.12.23-PM.png\" class=\"kg-image\"><figcaption>Authorize that bad boy.</figcaption></figure><h2 id=\"the-magic-api\">The Magic API </h2><p>Now, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so this next part is going to feel a bit a bit weird. I’m not sure why, as the service is apparently free, thus I’m clearly not getting paid for any of this.</p><p>Anyway, the service is called <strong><a href=\"https://apisentris.com/\">Apisentris</a>, </strong>and the idea is that it will build whatever time-consuming monstrosity of a REST API you were planning to build to access your data for you. Via their own words:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.19.16-PM.png\" class=\"kg-image\"><figcaption>See, I told you.</figcaption></figure><p>What does this actually mean? It means if you create a table called <em>articles</em> in your database, you will immediately have an endpoint to fetch said articles, and it would look like <strong>https://apisentris.com/api/v1/articles. </strong>Your client ID and credentials would obviously need to be provided to indicate that you're, well, you.</p><p>Grabbing entire tables at once would be silly, which is why they also autogenerate filters based on the contents of your table:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.25.13-PM.png\" class=\"kg-image\"><figcaption>Endpoints accept query parameters to essentially create a query.</figcaption></figure><p>Oh yeah, and you can also handle user management via this API as well, if you're building an actual app:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.27.43-PM.png\" class=\"kg-image\"><figcaption>Pretty easy to hook up into a form or whatever.</figcaption></figure><p>I'll assume you're sold on the idea by now. If a free service that handles the hard parts of backend logic for free isn't your cup of tea, clearly you aren't Slacker material.</p><h2 id=\"setting-it-all-up\">Setting it all up</h2><p>As we did before with our own IP, we'll need to whitelist Apisentris' IP the same way in GCP console. Their IP is <code>104.199.181.125</code>.</p><p>Create a table in your database with some data just to test things out. When you're logged in, you'll be able to see all the endpoints available to you and the associated attributes they have:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/schema.gif\" class=\"kg-image\"><figcaption>Not bad.</figcaption></figure><p>Any way you slice it, the concept of a self-generating API is very cool and yet somehow still not the norm. I'm actually shocked that there are so few people in the Data industry who know \"there must be a better way,\" but then again, data science and software engineering are two very different things. For my fellow Data Engineers out there, take this as a gift and a curse: you have the gift of knowing better from your software background, but are cursed with watching the world not quite realize how pointless half the things they do truly are.</p><p>Oh well. We'll be the ones building the robots anyway.</p>","url":"https://hackersandslackers.com/mysql-google-cloud-and-a-rest-api-that-autogenerates/","uuid":"c45478bb-54da-4563-89bd-ddd356a234d4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bcf6f08d7ab443ba8b7a5ab"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372d","title":"Working With Google Cloud Functions","slug":"creating-a-python-google-cloud-function","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","excerpt":"GCP scores a victory by trivializing serverless functions.","custom_excerpt":"GCP scores a victory by trivializing serverless functions.","created_at_pretty":"18 October, 2018","published_at_pretty":"19 October, 2018","updated_at_pretty":"14 April, 2019","created_at":"2018-10-18T19:44:02.000-04:00","published_at":"2018-10-18T22:33:07.000-04:00","updated_at":"2019-04-14T07:24:50.000-04:00","meta_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","meta_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. Set up a CRON job to run your function with Cloud Scheduler.","og_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. ","og_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","og_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","twitter_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. ","twitter_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","twitter_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"The more I explore Google Cloud's endless catalog of cloud services, the more I\nlike Google Cloud. This is why before moving forward, I'd like to be transparent\nthat this blog has become little more than thinly veiled Google propaganda,\nwhere I will henceforth bombard you with persuasive and subtle messaging to sell\nyour soul to Google. Let's be honest; they've probably simulated it anyway.\n\nIt should be safe to assume that you're familiar with AWS Lambda Functions\n[https://hackersandslackers.com/creating-endpoints-with-lambda/]  by now, which\nhave served as the backbone of what we refer to as \"serverless.\" These cloud\ncode snippets have restructured entire technology departments, and are partially\nto blame for why almost nobody knows enough basic Linux to configure a web\nserver or build anything without a vendor. Google Cloud Functions don't yet\nserve all the use cases that Lambda functions cover, but for the cases they do\ncover, they seem to be taking the lead.\n\nLambdas vs Cloud Functions\nFirst off, let's talk about a big one: price. AWS charges based on Lambda usage,\nwhereas Google Cloud Functions are free. The only exception to this is when you\nbreak 2 million invocations/month, at which point you'll be hemorrhaging as\nghastly 40 cents per additional million. That's ridiculous. I think we've just\ndiscovered Google Cloud's lead generation strategy.\n\nWhat about in terms of workflow? AWS holds an architecture philosophy of\nchaining services together, into what inevitably becomes a web of self-contained\nbillable items on your invoice. An excellent illustration of this is a  post on \ncommon AWS patterns\n[https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/]  which\nprovides a decent visual of this complexity, while also revealing how much\npeople love this kind of stuff, as though SaaS is the new Legos. To interact\nwith a Lambda function in AWS via HTTP requests, you need to set up an API\nGateway in front, which is arguably a feat more convoluted and complicated than\ncoding. Pair this with an inevitable user permission struggle to get the right\nLambda roles set up, and you quickly have yourself a nightmare- especially  if\nyou're trying to get a single function live. Eventually, you’ll get to write\nsome code or upload a horrendous zip file like some neanderthal (friendly\nreminder: I am entirely biased).\n\nCloud Functions do have their drawbacks in this comparison. Firstly, we cannot\nbuild APIs with Cloud Functions- in fact, without Firebase, we can't even use\nvanity URLs. \n\nAnother huge drawback is Cloud Functions cannot communicate with Google's\nrelational database offering, Cloud SQL. This is big, and what's worse, it feels\nlike an oversight. There are no technical constraints behind this, other than\nGoogle hasn't created an interface to whitelist anything other than IP addresses\nfor Cloud SQL instances.\n\nLastly, we cannot create a fully-fledged API available to be sold or\ndistributed. The is currently no Google Cloud API Gateway equivalent.\n\nDeploying a Cloud Function\nBy the end of this tutorial we'll have utilized the following Google Cloud\nservices/tools:\n\n * A new Google Cloud Function\n * A Google  Source Repository to sync to our Github repo and auto-deploy\n   changes.\n * A CRON job to run our function on a schedule, via Google Cloud Scheduler.\n * The gcloud  CLI to enable us to work locally.\n\nYou'll notice we lack any mentions of API endpoints, methods, stages, or\nanything related to handling web requests. It should not be understated that \nCloud Functions are preconfigured with an endpoint, and all nonsense regarding\nwhether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely.\nThese things are instead handled in the  logic of the function itself, and\nbecause Google Cloud functions running Python are preconfigured with Flask, all\nof that stuff is really trivially easy.  That's right, we've got Flask, Python, \n and GCP  all in a single post. Typing these words feels like eating cake while\nDwyane The Rock Johnson reads me bedtime stories and caresses me as I fall\nasleep. It's great.\n\nIn the Cloud console, go ahead and create a new function. Our function will take\nthe form of an HTTP endpoint:\n\nSingle-page setup. Easy. * Memory Allocated lets us allocate more than the default 256MB to our\n   function. Remember that Cloud functions are free: choose accordingly.\n * Trigger  specifies what will have access to this function. By selecting HTTP,\n   we will immediately receive a URL.\n * Source code  gives us a few options to deploy our code, with cloud source\n   repository  being by far the easiest solution (more on that in a bit).\n * Runtime  allows you to select NodeJS by accident.\n * Function to Execute  needs the name of our entry point function, which is to\n   be found in main.py  or main.js  depending on which language you’ve selected.\n\nIf you're familiar with Lambda functions, the choices of an inline code editor\nor a zip file upload should come as no surprise. Since you're already familiar,\nI don't need to tell you why these methods suck  for any sane workflow. Luckily,\nwe have a better option: syncing our Github repo to a Google Source Repository.\n\nGoogle Source Repositories\nGoogle Source Repositories are repos that function just like Github or Bitbucket\nrepos. They're especially useful for syncing code changes from a Github repo,\nand setting up automatic deployments on commit.\n\nThe Google Source Repositories HomeSetting up this sync is super easy. Create a\nnew repository, specify that we're connecting an external Github repo, and we'll\nbe able to select any repo in our account via the GUI:\n\nSyncing a Github RepoNow when we go back to our editing our function, setting\nthe Source code  field to the name of this new repository will automatically\ndeploy the function whenever a change is committed. With this method, we have\neffectively zero changes to our normal workflow.\n\nCommit to Google Source Repositories Directly\nIf you don't want to sync a Github repo, no problem. We can create a repo\nlocally using the gcloud CLI:\n\n$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n\n\nNow make that puppy go live with gcloud functions deploy totally-dope-function,\nwhere totally-dope-function  is the name of your function, as it should be.\n\nWith our function set up and method in place for deploying code, we can now see\nhow our Cloud Function is doing.\n\nViewing Error Logs\nBecause we have a real endpoint to work with, we don't need to waste any time\ncreating dumb unit tests where we send fake JSON to our function (real talk\nthough, we should always write unit tests).\n\nThe Cloud Function error log screen does a decent job of providing us with a GUI\nto see how our deployed function is running, and where things have gone wrong:\n\nOrange Exclamation Marks Denote ErrorsFiring Our Function on a Schedule\nLet's say our function is a job we're looking to run daily, or perhaps hourly.\nGoogle Cloud Scheduler is a super easy way to trigger functions via CRON.\n\nHow is this free again?The easiest way to handle this is by creating our\nfunction as an HTTP endpoint back when we started. A Cloud Scheduler  job can\nhit this endpoint at any time interval we want - just make sure you wrote your\nendpoint to handle GET requests.\n\nCloud Functions in Short\nGCP seems to have been taking notes on the sidelines on how to improve this\nprocess by removing red-tape around service setup or policy configuration. AWS\nand GCP are tackling opposites approaches; AWS allows you to build a Robust API\ncomplete with staging and testing with the intent that some of these APIs can\neven be sold as standalone products to consumers. GCP takes the opposite\napproach: cloud functions are services intended for developers to develop. That\nshould probably cover the vast majority of use cases anyway.","html":"<p>The more I explore Google Cloud's endless catalog of cloud services, the more I like Google Cloud. This is why before moving forward, I'd like to be transparent that this blog has become little more than thinly veiled Google propaganda, where I will henceforth bombard you with persuasive and subtle messaging to sell your soul to Google. Let's be honest; they've probably simulated it anyway.</p><p>It should be safe to assume that you're familiar with AWS <a href=\"https://hackersandslackers.com/creating-endpoints-with-lambda/\">Lambda Functions</a> by now, which have served as the backbone of what we refer to as \"serverless.\" These cloud code snippets have restructured entire technology departments, and are partially to blame for why almost nobody knows enough basic Linux to configure a web server or build anything without a vendor. Google Cloud Functions don't yet serve all the use cases that Lambda functions cover, but for the cases they do cover, they seem to be taking the lead.</p><h2 id=\"lambdas-vs-cloud-functions\">Lambdas vs Cloud Functions</h2><p>First off, let's talk about a big one: price. AWS charges based on Lambda usage, whereas Google Cloud Functions are <strong>free</strong>. The only exception to this is when you break 2 million invocations/month, at which point you'll be hemorrhaging as ghastly <strong>40 cents per additional million</strong>. That's ridiculous. I think we've just discovered Google Cloud's lead generation strategy.</p><p>What about in terms of workflow? AWS holds an architecture philosophy of chaining services together, into what inevitably becomes a web of self-contained billable items on your invoice. An excellent illustration of this is a  post on <a href=\"https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/\">common AWS patterns</a> which provides a decent visual of this complexity, while also revealing how much people love this kind of stuff, as though SaaS is the new Legos. To interact with a Lambda function in AWS via HTTP requests, you need to set up an API Gateway in front, which is arguably a feat more convoluted and complicated than coding. Pair this with an inevitable user permission struggle to get the right Lambda roles set up, and you quickly have yourself a nightmare- <em>especially</em> if you're trying to get a single function live. Eventually, you’ll get to write some code or upload a horrendous zip file like some neanderthal (friendly reminder: I am entirely biased).</p><p>Cloud Functions do have their drawbacks in this comparison. Firstly, we cannot build APIs with Cloud Functions- in fact, without Firebase, we can't even use vanity URLs. </p><p>Another huge drawback is Cloud Functions cannot communicate with Google's relational database offering, Cloud SQL. This is big, and what's worse, it feels like an oversight. There are no technical constraints behind this, other than Google hasn't created an interface to whitelist anything other than IP addresses for Cloud SQL instances.</p><p>Lastly, we cannot create a fully-fledged API available to be sold or distributed. The is currently no Google Cloud API Gateway equivalent.</p><h2 id=\"deploying-a-cloud-function\">Deploying a Cloud Function</h2><p>By the end of this tutorial we'll have utilized the following Google Cloud services/tools:</p><ul><li>A new <strong>Google Cloud Function</strong></li><li>A <strong>Google</strong> <strong>Source Repository </strong>to sync to our Github repo and auto-deploy changes.</li><li>A CRON job to run our function on a schedule, via <strong>Google Cloud Scheduler</strong>.</li><li>The <strong>gcloud</strong> CLI to enable us to work locally.</li></ul><p>You'll notice we lack any mentions of API endpoints, methods, stages, or anything related to handling web requests. It should not be understated that <em>Cloud Functions are preconfigured with an endpoint</em>, and all nonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely. These things are instead handled in the  logic of the function itself, and because Google Cloud functions running Python are preconfigured with <strong>Flask, </strong>all of that stuff is <em>really trivially easy.</em> That's right, we've got <em>Flask</em>, <em>Python</em>,<em> </em>and <em>GCP</em> all in a single post. Typing these words feels like eating cake while Dwyane The Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's great.</p><p>In the Cloud console, go ahead and create a new function. Our function will take the form of an HTTP endpoint:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/func.gif\" class=\"kg-image\"><figcaption>Single-page setup. Easy.</figcaption></figure><!--kg-card-end: image--><ul><li><strong>Memory Allocated </strong>lets us allocate more than the default 256MB to our function. Remember that Cloud functions are free: choose accordingly.</li><li><strong>Trigger</strong> specifies what will have access to this function. By selecting HTTP, we will immediately receive a URL.</li><li><strong>Source code</strong> gives us a few options to deploy our code, with <em>cloud source repository</em> being by far the easiest solution (more on that in a bit).</li><li><strong>Runtime</strong> allows you to select NodeJS by accident.</li><li><strong>Function to Execute</strong> needs the name of our entry point function, which is to be found in <code>main.py</code> or <code>main.js</code> depending on which language you’ve selected.</li></ul><p>If you're familiar with Lambda functions, the choices of an inline code editor or a zip file upload should come as no surprise. Since you're already familiar, I don't need to tell you why these methods <em>suck</em> for any sane workflow. Luckily, we have a better option: syncing our Github repo to a Google Source Repository.</p><h2 id=\"google-source-repositories\">Google Source Repositories</h2><p>Google Source Repositories are repos that function just like Github or Bitbucket repos. They're especially useful for syncing code changes from a Github repo, and setting up automatic deployments on commit.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-source-repo.png\" class=\"kg-image\"><figcaption>The Google Source Repositories Home</figcaption></figure><!--kg-card-end: image--><p>Setting up this sync is super easy. Create a new repository, specify that we're connecting an external Github repo, and we'll be able to select any repo in our account via the GUI:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-source-github-sync.gif\" class=\"kg-image\"><figcaption>Syncing a Github Repo</figcaption></figure><!--kg-card-end: image--><p>Now when we go back to our editing our function, setting the <strong>Source code</strong> field to the name of this new repository will automatically deploy the function whenever a change is committed. With this method, we have effectively zero changes to our normal workflow.</p><h3 id=\"commit-to-google-source-repositories-directly\">Commit to Google Source Repositories Directly</h3><p>If you don't want to sync a Github repo, no problem. We can create a repo locally using the <strong>gcloud CLI</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n</code></pre>\n<!--kg-card-end: markdown--><p>Now make that puppy go live with <code>gcloud functions deploy totally-dope-function</code>, where <em><strong>totally-dope-function</strong> </em>is the name of your function, as it should be.</p><p>With our function set up and method in place for deploying code, we can now see how our Cloud Function is doing.</p><h2 id=\"viewing-error-logs\">Viewing Error Logs</h2><p>Because we have a real endpoint to work with, we don't need to waste any time creating dumb unit tests where we send fake JSON to our function (real talk though, we should always write unit tests).</p><p>The Cloud Function error log screen does a decent job of providing us with a GUI to see how our deployed function is running, and where things have gone wrong:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/logs.gif\" class=\"kg-image\"><figcaption>Orange Exclamation Marks Denote Errors</figcaption></figure><!--kg-card-end: image--><h2 id=\"firing-our-function-on-a-schedule\">Firing Our Function on a Schedule</h2><p>Let's say our function is a job we're looking to run daily, or perhaps hourly. Google Cloud Scheduler is a super easy way to trigger functions via CRON.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-cloud-scheduler.png\" class=\"kg-image\"><figcaption>How is this free again?</figcaption></figure><!--kg-card-end: image--><p>The easiest way to handle this is by creating our function as an HTTP endpoint back when we started. A <strong>Cloud Scheduler</strong> job can hit this endpoint at any time interval we want - just make sure you wrote your endpoint to handle GET requests.</p><h2 id=\"cloud-functions-in-short\">Cloud Functions in Short</h2><p>GCP seems to have been taking notes on the sidelines on how to improve this process by removing red-tape around service setup or policy configuration. AWS and GCP are tackling opposites approaches; AWS allows you to build a Robust API complete with staging and testing with the intent that some of these APIs can even be sold as standalone products to consumers. GCP takes the opposite approach: cloud functions are services intended for developers to develop. That should probably cover the vast majority of use cases anyway.</p>","url":"https://hackersandslackers.com/creating-a-python-google-cloud-function/","uuid":"ec428cb9-976e-4578-a3de-9120a0dd7352","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bc91ac23d1eab214413b12b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ee","title":"Structuring Your Flask Application","slug":"structuring-your-flask-app","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/flaskblueprints2@2x.jpg","excerpt":"Leverage Blueprints, Flask-Assets, and the Application Factory.","custom_excerpt":"Leverage Blueprints, Flask-Assets, and the Application Factory.","created_at_pretty":"02 September, 2018","published_at_pretty":"15 October, 2018","updated_at_pretty":"23 February, 2019","created_at":"2018-09-02T03:02:29.000-04:00","published_at":"2018-10-15T08:00:00.000-04:00","updated_at":"2019-02-23T11:21:49.000-05:00","meta_title":"Structuring your Flask Application | Hackers and Slackers","meta_description":"Follow best practices when building your Flask apps. Leverage Blueprints, Flask-Assets, and the Application Factory.","og_description":"Follow best practices when building your Flask apps. Leverage Blueprints, Flask-Assets, and the Application Factory.","og_image":"https://hackersandslackers.com/content/images/2018/09/flaskblueprints2@2x.jpg","og_title":"Structuring your Flask App Like an Adult | Hackers and Slackers","twitter_description":"Follow best practices when building your Flask apps. Leverage Blueprints, Flask-Assets, and the Application Factory.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/flaskblueprints2@2x.jpg","twitter_title":"Structuring your Flask App Like an Adult | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Python’s fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Python’s fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"When we first started developing in Flask, most of us took the 5 lines of code\nin the quick-start guide and ran with it. Compared to every other web framework,\ngetting a \"Hello world\" to flash on screen without being hassled with database\nconfigurations, template preferences, or reverse proxy setups felt a lot like\nrobbing a bank.\n\nAt some point or another, we inevitably pause the party and take a look around.\nAll of our views are smashed into a single file named something meaningless like\n app.py. All logic lives in the root directory. We're in our 30s and the app\nwe've just created looks as terrible as our bathrooms. It's time to get our shit\ntogether.\n\nThe Flask Application Factory\nThe overwhelming preference to start a Flask application is to use a structure\ndubbed the Application Factory\n[http://flask.pocoo.org/docs/1.0/patterns/appfactories/]. The gist is to keep\nthe initialization preferences of our application in a single __init__.py  file,\nsometimes borrowing help from peer files such as db.py  or models.py. Either\nway, the gist is to keep global logic separated from the other parts.\n\nA simple app using the application factory method might look something like\nthis:\n\n[app]\n├── myapp/\n│   ├── __init__.py\n│   ├── db.py\n│   ├── forms.py\n│   ├── models.py\n│   ├── views.py\n│   ├── static/\n│   └── templates/\n├── config.py\n├── requirements.txt\n├── setup.py\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── app.json\n└── wsgi.py\n\n\nThe main takeaway here being the presence of the myapp  directory which now\nhouses our app logic, and the presence of our good friend __init__.py.\n\nAn example of what might live in __init.py__  could be something like this:\n\nimport os\nimport sys\nfrom flask import Flask, g\nfrom config import BaseConfig\nfrom flask_login import LoginManager\nfrom flask_pymongo import PyMongo\n\n\ndef create_app():\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_object('config.BaseConfig')\n    login = LoginManager()\n\n    with app.app_context():\n        from . import views\n        from . import auth\n        login.init_app(app)\n        mongo = PyMongo(app, app.config['MONGO_URI'])\n        app.register_blueprint(views.main)\n        app.register_blueprint(admin.admin)\n\n        return app\n\n\nHere we initialize our app and the dependencies we'd like to initialize within a\nsingle function. Most familiar might be the first two lines of our function: The\nfirst creating our app object, and the second loading a config from a class in a\nconfig file.\n\nAnother common practice is to keep libraries which need to run init_app  in this\nfile as well. This could be something like the LoginManager  seen in the example\nabove, or a global database configuration. Lastly, this is also where we would\nregister any Blueprints  as well.\n\nUsing Blueprints in Flask\nWhile the Application Factory is an excellent first step to building cohesive\napps, we haven't solved the problem of organizing our app into separation of\nconcerns. Blueprints  are a way for us to separate our app into parts which\nshare very little with one another. Prime examples would include apps with an \nadmin  panel with an accompanying client-facing  side, or apps where the \"logged\nin\" state is vastly different from the app's \"logged out\"  state. In these\ncases, it seems silly to mix both logic and static assets into a single lump,\nwhich is where Blueprints come in.\n\nNOTE:  If you're a Django person and this is all starting to sound familiar,\nthat's because we can equate Flask's Blueprints  to Django's concept of apps. \nThere are differences and added flexibility, but the concept remains the same.\n\nRegistering a part of your app as a Blueprint begins with the following two\nlines:\n\nfrom flask import Blueprint\n\nauth = Blueprint('auth', __name__)\n\n\nWhen we registered the admin  Blueprint previously in __init__.py, the line \napp.register_blueprint(admin.admin)  is essentially saying \"look for a Blueprint \n named admin  in a module (either file or folder structure) called admin.\"  It's\nimportant not to overlook the concept that Blueprints can either be single files\nor entirely standalone file structures with their own templates and static\nfiles.  For instance, a Flask app with completely decoupled Blueprints might be\nstructured as follows:\n\n[app]\n├── myapp/\n│   ├── __init__.py\n│   ├── admin/\n│   │    ├── __init__.py\n│   │    ├── views.py\n│   │    ├── forms.py\n│   │    ├── static/\n│   │\t └── templates/\n│   ├── frontend/\n│   │    ├── __init__.py\n│   │    ├── views.py\n│   │    ├── forms.py\n│   │    ├── static/\n│   │\t └── templates/  \n│   ├── db.py\n│   ├── forms.py\n│   ├── models.py\n│   └── views.py\n├── config.py\n├── requirements.txt\n├── setup.py\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── app.json\n└── wsgi.py\n        \n\n\nIn the case above, each Blueprint  stands as though it were its own Python\nmodule. Each blueprint contains its own logic, templates, and static files -\npresumably encapsulated in a way which makes sense.\n\nUsing Flask-Assets with Blueprints\nWe've already forced a lot of information down your throat, but there's one last\nthing worth mentioning in this overview of working with Blueprints, which is\nworking with assets. We've previously looked at the Flask-Static-Compress \nlibrary for static asset management, but Blueprints  lend themselves better to \nFlask-Assets  way of thinking.\n\nFlask-Assets  is a library which creates a bundle (aka compressed) of assets\nupfront. Thus, the start of a Blueprint definition might now look something like\nthis:\n\nfrom flask import Blueprint\nfrom flask_assets import Environment, Bundle, build\nimport sass\n\nauth = Blueprint('admin', __name__)\n\nassets = Environment(admin)\nscss = Bundle('scss/main.scss', 'scss/forms.scss', filters='libsass', output='build/css/style.css')\nassets.register('scss_all', scss)\nscss.build()\n\n\nEnvironment  states the context of our asset bundle, which is admin, the current\nBlueprint. \n\nBundle  takes any number of files to compressed together as arguments. Then we\nmust pass the type of \"filter\" the assets are (typically a precompiler) and of\ncourse an output destination for the Bundle.\n\n.register()  registers the bundle we just created, not unlike the way we\nregistered our Blueprint.\n\n.build()  must be called explicitly in order to build the bundle at runtime.\nConversely, we could intentionally exclude .build()  if we expect our assets are\nnot to change.\n\nAnd Now You Know Everything\n...or not, really. The most we should take from this post is:\n\n * There's a better way to structure our apps.\n * There are many potential decisions we can make about the structure of our\n   app.\n * There's way more stuff we need to Google or look up on StackOverflow.\n\nTruthfully, there are plenty of resources within Flask's documentation\n[http://flask.pocoo.org/docs/1.0/tutorial/views/#]  or around the internet that\ncovers the topic of Flask app organization and its granular topics more than\nthis single post could ever hope to. Nonetheless, here's to hoping you're\nfeeling a sense of direction in these crazy, adult lives of ours.","html":"<p>When we first started developing in Flask, most of us took the 5 lines of code in the quick-start guide and ran with it. Compared to every other web framework, getting a \"Hello world\" to flash on screen without being hassled with database configurations, template preferences, or reverse proxy setups felt a lot like robbing a bank.</p><p>At some point or another, we inevitably pause the party and take a look around. All of our views are smashed into a single file named something meaningless like <code>app.py</code>. All logic lives in the root directory. We're in our 30s and the app we've just created looks as terrible as our bathrooms. It's time to get our shit together.</p><h2 id=\"the-flask-application-factory\">The Flask Application Factory</h2><p>The overwhelming preference to start a Flask application is to use a structure dubbed the <a href=\"http://flask.pocoo.org/docs/1.0/patterns/appfactories/\">Application Factory</a>. The gist is to keep the initialization preferences of our application in a single <code>__init__.py</code> file, sometimes borrowing help from peer files such as <code>db.py</code> or <code>models.py</code>. Either way, the gist is to keep global logic separated from the other parts.</p><p>A simple app using the application factory method might look something like this:</p><pre><code class=\"language-shell\">[app]\n├── myapp/\n│   ├── __init__.py\n│   ├── db.py\n│   ├── forms.py\n│   ├── models.py\n│   ├── views.py\n│   ├── static/\n│   └── templates/\n├── config.py\n├── requirements.txt\n├── setup.py\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── app.json\n└── wsgi.py\n</code></pre>\n<p>The main takeaway here being the presence of the <strong>myapp</strong> directory which now houses our app logic, and the presence of our good friend <code>__init__.py</code>.</p><p>An example of what might live in <code>__init.py__</code> could be something like this:</p><pre><code class=\"language-python\">import os\nimport sys\nfrom flask import Flask, g\nfrom config import BaseConfig\nfrom flask_login import LoginManager\nfrom flask_pymongo import PyMongo\n\n\ndef create_app():\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_object('config.BaseConfig')\n    login = LoginManager()\n\n    with app.app_context():\n        from . import views\n        from . import auth\n        login.init_app(app)\n        mongo = PyMongo(app, app.config['MONGO_URI'])\n        app.register_blueprint(views.main)\n        app.register_blueprint(admin.admin)\n\n        return app\n</code></pre>\n<p>Here we initialize our app and the dependencies we'd like to initialize within a single function. Most familiar might be the first two lines of our function: The first creating our app object, and the second loading a config from a class in a config file.</p><p>Another common practice is to keep libraries which need to run <code>init_app</code> in this file as well. This could be something like the <code>LoginManager</code> seen in the example above, or a global database configuration. Lastly, this is also where we would register any <strong>Blueprints</strong> as well.</p><h2 id=\"using-blueprints-in-flask\">Using Blueprints in Flask</h2><p>While the Application Factory is an excellent first step to building cohesive apps, we haven't solved the problem of organizing our app into <em>separation of concerns</em>. <strong>Blueprints</strong> are a way for us to separate our app into parts which share very little with one another. Prime examples would include apps with an <em>admin</em> panel with an accompanying <em>client-facing</em> side, or apps where the \"<em>logged in\" </em>state is vastly different from the app's \"<em>logged out\"</em> state. In these cases, it seems silly to mix both logic and static assets into a single lump, which is where Blueprints come in.</p><p><strong>NOTE:</strong> If you're a Django person and this is all starting to sound familiar, that's because we can equate Flask's <strong>Blueprints</strong> to Django's concept of <strong>apps.</strong> There are differences and added flexibility, but the concept remains the same.</p><p>Registering a part of your app as a Blueprint begins with the following two lines:</p><pre><code class=\"language-python\">from flask import Blueprint\n\nauth = Blueprint('auth', __name__)\n</code></pre>\n<p>When we registered the <code>admin</code> Blueprint previously in <code>__init__.py</code>, the line <code>app.register_blueprint(admin.admin)</code> is essentially saying \"look for a <strong>Blueprint</strong> named <em>admin</em> in a module (either file or folder structure) called <em>admin.\"</em> It's important not to overlook the concept that Blueprints can either be single files or <em>entirely standalone file structures with their own templates and static files.</em> For instance, a Flask app with completely decoupled Blueprints might be structured as follows:</p><pre><code class=\"language-shell\">[app]\n├── myapp/\n│   ├── __init__.py\n│   ├── admin/\n│   │    ├── __init__.py\n│   │    ├── views.py\n│   │    ├── forms.py\n│   │    ├── static/\n│   │\t └── templates/\n│   ├── frontend/\n│   │    ├── __init__.py\n│   │    ├── views.py\n│   │    ├── forms.py\n│   │    ├── static/\n│   │\t └── templates/  \n│   ├── db.py\n│   ├── forms.py\n│   ├── models.py\n│   └── views.py\n├── config.py\n├── requirements.txt\n├── setup.py\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── app.json\n└── wsgi.py\n        \n</code></pre>\n<p>In the case above, each <strong>Blueprint</strong> stands as though it were its own Python module. Each blueprint contains its own logic, templates, and static files - presumably encapsulated in a way which makes sense.</p><h2 id=\"using-flask-assets-with-blueprints\">Using Flask-Assets with Blueprints</h2><p>We've already forced a lot of information down your throat, but there's one last thing worth mentioning in this overview of working with <strong>Blueprints, </strong>which is working with assets. We've previously looked at the <code>Flask-Static-Compress</code> library for static asset management, but <strong>Blueprints</strong> lend themselves better to <code>Flask-Assets</code> way of thinking.</p><p><code>Flask-Assets</code> is a library which creates a bundle (aka compressed) of assets upfront. Thus, the start of a Blueprint definition might now look something like this:</p><pre><code class=\"language-shell\">from flask import Blueprint\nfrom flask_assets import Environment, Bundle, build\nimport sass\n\nauth = Blueprint('admin', __name__)\n\nassets = Environment(admin)\nscss = Bundle('scss/main.scss', 'scss/forms.scss', filters='libsass', output='build/css/style.css')\nassets.register('scss_all', scss)\nscss.build()\n</code></pre>\n<p><code>Environment</code> states the context of our asset bundle, which is admin, the current Blueprint. </p><p><code>Bundle</code> takes any number of files to compressed together as arguments. Then we must pass the type of \"filter\" the assets are (typically a precompiler) and of course an output destination for the Bundle.</p><p><code>.register()</code> registers the bundle we just created, not unlike the way we registered our Blueprint.</p><p><code>.build()</code> must be called explicitly in order to build the bundle at runtime. Conversely, we could intentionally exclude <code>.build()</code> if we expect our assets are not to change.</p><h2 id=\"and-now-you-know-everything\">And Now You Know Everything</h2><p>...or not, really. The most we should take from this post is:</p><ul><li>There's a better way to structure our apps.</li><li>There are many potential decisions we can make about the structure of our app.</li><li>There's way more stuff we need to Google or look up on StackOverflow.</li></ul><p>Truthfully, there are plenty of resources within <a href=\"http://flask.pocoo.org/docs/1.0/tutorial/views/#\">Flask's documentation</a> or around the internet that covers the topic of Flask app organization and its granular topics more than this single post could ever hope to. Nonetheless, here's to hoping you're feeling a sense of direction in these crazy, adult lives of ours.</p>","url":"https://hackersandslackers.com/structuring-your-flask-app/","uuid":"4345eb76-e1ce-471a-94b9-f06a43c3ad27","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b8b8b05852e5c07171fcab7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867371b","title":"Extract Nested Data From Complex JSON","slug":"extract-data-from-complex-json-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","excerpt":"Never manually walk through complex JSON objects again by using this function.","custom_excerpt":"Never manually walk through complex JSON objects again by using this function.","created_at_pretty":"10 October, 2018","published_at_pretty":"10 October, 2018","updated_at_pretty":"22 January, 2019","created_at":"2018-10-10T00:15:29.000-04:00","published_at":"2018-10-10T08:00:00.000-04:00","updated_at":"2019-01-22T15:20:23.000-05:00","meta_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","meta_description":"Never manually walk through complex JSON objects again by using this function","og_description":"Never manually walk through complex JSON objects again by using this function","og_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","og_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","twitter_description":"Never manually walk through complex JSON objects again by using this function","twitter_image":"https://hackersandslackers.com/content/images/2018/10/107@2x.jpg","twitter_title":"Extract Nested Data From Complex JSON Trees | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"We're all data people here, so you already know the scenario: it happens perhaps\nonce a day, perhaps 5, or even more. There's an API you're working with, and\nit's great. It contains all the information you're looking for, but there's just\none problem: the complexity of nested JSON objects is endless, and suddenly the\njob you love needs to be put on hold to painstakingly retrieve the data you\nactually want, and it's 5 levels deep in a nested JSON hell. Nobody feels like\nmuch of a \"scientist\" or an \"engineer\" when half their day becomes dealing with\nkey value errors.\n\nLuckily, we code in Python!  (okay fine, language doesn't make much of a\ndifference here. It felt like a rallying call at the time).\n\nUsing Google Maps API as an Example\nTo visualize the problem, let's take an example somebody might actually want to\nuse.  I think the  Google Maps API is a good candidate to fit the bill here.\n\nWhile Google Maps is actually a collection of APIs, the Google Maps Distance\nMatrix [https://developers.google.com/maps/documentation/distance-matrix/start].\nThe idea is that with a single API call, a user can calculate the distance and\ntime traveled between an origin and an infinite number of destinations. It's a\ngreat full-featured API, but as you might imagine the resulting JSON for\ncalculating commute time between where you stand and every location in the\nconceivable universe  makes an awfully complex JSON structure.\n\nGetting a Taste of JSON Hell\nReal quick, here's an example of the types of parameters this request accepts:\n\nimport requests\nimport API_KEY\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Philadelphia,PA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n\n\nOne origin, one destination. The JSON response for a request this\nstraightforward is quite simple:\n\n{\n    \"destination_addresses\": [\n        \"Philadelphia, PA, USA\"\n    ],\n    \"origin_addresses\": [\n        \"New York, NY, USA\"\n    ],\n    \"rows\": [\n        {\n            \"elements\": [\n                {\n                    \"distance\": {\n                        \"text\": \"94.6 mi\",\n                        \"value\": 152193\n                    },\n                    \"duration\": {\n                        \"text\": \"1 hour 44 mins\",\n                        \"value\": 6227\n                    },\n                    \"status\": \"OK\"\n                }\n            ]\n        }\n    ],\n    \"status\": \"OK\"\n}\n\n\nFor each destination, we're getting two data points: the commute distance, and \nestimated duration. If we hypothetically wanted to extract those values, typing \nresponse['rows'][0]['elements']['distance']['test']  isn't too  crazy. I mean,\nit's somewhat awful and brings on casual thoughts of suicide, but nothing out of\nthe ordinary\n\nNow let's make things interesting by adding a few more stops on our trip:\n\nimport requests \nimport API_KEY\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa County,CA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n\n\nOh fuuucckkkk:\n\n{\n  \"destination_addresses\": [\n    \"Washington, DC, USA\",\n    \"Philadelphia, PA, USA\",\n    \"Santa Barbara, CA, USA\",\n    \"Miami, FL, USA\",\n    \"Austin, TX, USA\",\n    \"Napa County, CA, USA\"\n  ],\n  \"origin_addresses\": [\n    \"New York, NY, USA\"\n  ],\n  \"rows\": [\n    {\n      \"elements\": [\n        {\n          \"distance\": {\n            \"text\": \"227 mi\",\n            \"value\": 365468\n          },\n          \"duration\": {\n            \"text\": \"3 hours 54 mins\",\n            \"value\": 14064\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"94.6 mi\",\n            \"value\": 152193\n          },\n          \"duration\": {\n            \"text\": \"1 hour 44 mins\",\n            \"value\": 6227\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"2,878 mi\",\n            \"value\": 4632197\n          },\n          \"duration\": {\n            \"text\": \"1 day 18 hours\",\n            \"value\": 151772\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"1,286 mi\",\n            \"value\": 2069031\n          },\n          \"duration\": {\n            \"text\": \"18 hours 43 mins\",\n            \"value\": 67405\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"1,742 mi\",\n            \"value\": 2802972\n          },\n          \"duration\": {\n            \"text\": \"1 day 2 hours\",\n            \"value\": 93070\n          },\n          \"status\": \"OK\"\n        },\n        {\n          \"distance\": {\n            \"text\": \"2,871 mi\",\n            \"value\": 4620514\n          },\n          \"duration\": {\n            \"text\": \"1 day 18 hours\",\n            \"value\": 152913\n          },\n          \"status\": \"OK\"\n        }\n      ]\n    }\n  ],\n  \"status\": \"OK\"\n}\n\n\nA lot is happening here. There are objects. There are lists. There are lists of\nobjects which are part of an object. The last thing I'd want to deal with is\ntrying to parse this data only to accidentally get a useless key:value pair like\n \"status\": \"OK\".\n\nCode Snippet To The Rescue\nLet's say we only want the human-readable data from this JSON, which is labeled \n\"text\"  for both distance and duration. We've created a function below dubbed \nextract_values()  to help us resolve this very issue. The idea is that \nextract_values()  is flexible and agnostic, therefore can be imported as a\nmodule into any project you might need.\n\n# recursivejson.py\n\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n\nWe need to pass this function two values:\n\n * A JSON object, such as r.json()  from an API request.\n * The name of the key  we're looking to extract values from.\n\nnames = extract_values('myjson.json', 'name')\nprint(names)\n\n\nRegardless of where the key \"text\"  lives in the JSON, this function returns\nevery value for the instance of \"key.\" Here's our function in action:\n\nimport requests\nimport API_KEY\nfrom recursivejson import extract_values\n\n\ndef google_api_matrix():\n    \"\"\"Example Google Distance Matrix function.\"\"\"\n    endpoint = \"https://maps.googleapis.com/maps/api/distancematrix/json\"\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': \"New York City,NY\",\n       'destinations': \"Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa Valley,CA\",\n       'transit_mode': 'car',\n    }\n\n   r = requests.get(endpoint, params=params)\n   travel_values = extract_values(r.json(), 'text')\n   return travel_values\n\n\nRunning this function will result in the following output:\n\n['227 mi', '3 hours 54 mins', '94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n\n\nOh fiddle me timbers! Because the Google API alternates between distance and \ntrip duration, every other value alternates between distance and time (can we\npause to appreciate this horrible design? There are infinitely better ways to\nstructure this response). Never fear, some simple Python can help us split this\nlist into two lists:\n\nmy_values = extract_values(r.json(), 'text')\n\ndurations = my_values[1::2]\ndistances = my_values[2::1]\n\nprint('DURATIONS = ', durations)\nprint('DISTANCES = ', distances)\n\n\nThis will take our one list and split it in to two  lists, alternating between\neven and odd:\n\nDURATIONS = ['3 hours 54 mins', '1 hour 44 mins', '1 day 18 hours', '18 hours 43 mins', '1 day 2 hours', '1 day 18 hours']\nDISTANCES = ['94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n\n\nGetting Creative With Lists\nA common theme I run in to while extracting lists of values from JSON objects\nlike these is that the lists of values I extract are very much related.  In the\nabove example, for every duration  we have an accompanying distance, which is a\none-to-one basis. Imagine if we wanted to associate these values somehow?\n\nTo use a better example, I recently I used this exact_values()  function to\nextract lists of column names and their data types from a database schema. As\nseparate lists, the data looked something like this:\n\ncolumn_names = ['index', 'first_name', 'last_name', 'join_date']\ncolumn_datatypes = ['integer', 'string', 'string', 'date']\n\n\nClearly these two lists are directly related; the latter is describing the\nformer. How can this be useful? By using Python's zip  method!\n\nschema_dict = dict(zip(column_names, column_datatypes))\nprint(schema_dict)\n\n\nI like to think they call it zip  because it's like zipping up a zipper, where\neach side of the zipper is a list. This output a dictionary where list 1 serves\nas the keys, and list 2 serves as values:\n\n{\n'index': 'integer', \n'first_name': 'string', \n'last_name':'string',\n'join_date': 'date'\n}\n\n\nAnd there you have it folks: a free code snippet to copy and secretly pretend\nyou wrote forever. I've thrown the function up on Github Gists\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b], if such\na thing pleases you.\n\nIn the meantime, zip it up and zip it out. Zippity-do-da, buh bye.","html":"<p>We're all data people here, so you already know the scenario: it happens perhaps once a day, perhaps 5, or even more. There's an API you're working with, and it's great. It contains all the information you're looking for, but there's just one problem: the complexity of nested JSON objects is endless, and suddenly the job you love needs to be put on hold to painstakingly retrieve the data you actually want, and it's 5 levels deep in a nested JSON hell. Nobody feels like much of a \"scientist\" or an \"engineer\" when half their day becomes dealing with key value errors.</p><p>Luckily, we code in <strong><em>Python!</em></strong> (okay fine, language doesn't make much of a difference here. It felt like a rallying call at the time).</p><h2 id=\"using-google-maps-api-as-an-example\">Using Google Maps API as an Example</h2><p>To visualize the problem, let's take an example somebody might actually want to use.  I think the<strong> Google Maps API </strong>is a good candidate to fit the bill here.</p><p>While Google Maps is actually a collection of APIs, the <a href=\"https://developers.google.com/maps/documentation/distance-matrix/start\">Google Maps Distance Matrix</a>. The idea is that with a single API call, a user can calculate the distance and time traveled between an origin and an infinite number of destinations. It's a great full-featured API, but as you might imagine the resulting JSON for calculating commute time between where you stand and <em>every location in the conceivable universe</em> makes an awfully complex JSON structure.</p><h3 id=\"getting-a-taste-of-json-hell\">Getting a Taste of JSON Hell</h3><p>Real quick, here's an example of the types of parameters this request accepts:</p><pre><code class=\"language-python\">import requests\nimport API_KEY\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Philadelphia,PA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n</code></pre>\n<p>One origin, one destination. The JSON response for a request this straightforward is quite simple:</p><pre><code class=\"language-json\">{\n    &quot;destination_addresses&quot;: [\n        &quot;Philadelphia, PA, USA&quot;\n    ],\n    &quot;origin_addresses&quot;: [\n        &quot;New York, NY, USA&quot;\n    ],\n    &quot;rows&quot;: [\n        {\n            &quot;elements&quot;: [\n                {\n                    &quot;distance&quot;: {\n                        &quot;text&quot;: &quot;94.6 mi&quot;,\n                        &quot;value&quot;: 152193\n                    },\n                    &quot;duration&quot;: {\n                        &quot;text&quot;: &quot;1 hour 44 mins&quot;,\n                        &quot;value&quot;: 6227\n                    },\n                    &quot;status&quot;: &quot;OK&quot;\n                }\n            ]\n        }\n    ],\n    &quot;status&quot;: &quot;OK&quot;\n}\n</code></pre>\n<p>For each destination, we're getting two data points: the <em>commute distance</em>, and <em>estimated duration</em>. If we hypothetically wanted to extract those values, typing <code>response['rows'][0]['elements']['distance']['test']</code> isn't <em>too</em> crazy. I mean, it's somewhat awful and brings on casual thoughts of suicide, but nothing out of the ordinary</p><p>Now let's make things interesting by adding a few more stops on our trip:</p><pre><code class=\"language-python\">import requests \nimport API_KEY\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': 'New York City, NY',\n       'destinations': 'Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa County,CA',\n       'transit_mode': 'car'\n    }\n    r = requests.get(endpoint, params=params)\n    return r.json\n</code></pre>\n<p>Oh fuuucckkkk:</p><pre><code class=\"language-json\">{\n  &quot;destination_addresses&quot;: [\n    &quot;Washington, DC, USA&quot;,\n    &quot;Philadelphia, PA, USA&quot;,\n    &quot;Santa Barbara, CA, USA&quot;,\n    &quot;Miami, FL, USA&quot;,\n    &quot;Austin, TX, USA&quot;,\n    &quot;Napa County, CA, USA&quot;\n  ],\n  &quot;origin_addresses&quot;: [\n    &quot;New York, NY, USA&quot;\n  ],\n  &quot;rows&quot;: [\n    {\n      &quot;elements&quot;: [\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;227 mi&quot;,\n            &quot;value&quot;: 365468\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;3 hours 54 mins&quot;,\n            &quot;value&quot;: 14064\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;94.6 mi&quot;,\n            &quot;value&quot;: 152193\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 hour 44 mins&quot;,\n            &quot;value&quot;: 6227\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;2,878 mi&quot;,\n            &quot;value&quot;: 4632197\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 18 hours&quot;,\n            &quot;value&quot;: 151772\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;1,286 mi&quot;,\n            &quot;value&quot;: 2069031\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;18 hours 43 mins&quot;,\n            &quot;value&quot;: 67405\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;1,742 mi&quot;,\n            &quot;value&quot;: 2802972\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 2 hours&quot;,\n            &quot;value&quot;: 93070\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        },\n        {\n          &quot;distance&quot;: {\n            &quot;text&quot;: &quot;2,871 mi&quot;,\n            &quot;value&quot;: 4620514\n          },\n          &quot;duration&quot;: {\n            &quot;text&quot;: &quot;1 day 18 hours&quot;,\n            &quot;value&quot;: 152913\n          },\n          &quot;status&quot;: &quot;OK&quot;\n        }\n      ]\n    }\n  ],\n  &quot;status&quot;: &quot;OK&quot;\n}\n</code></pre>\n<p>A lot is happening here. There are objects. There are lists. There are lists of objects which are part of an object. The last thing I'd want to deal with is trying to parse this data only to accidentally get a useless key:value pair like <strong>\"status\": \"OK\".</strong></p><h2 id=\"code-snippet-to-the-rescue\">Code Snippet To The Rescue</h2><p>Let's say we only want the human-readable data from this JSON, which is labeled <em>\"text\"</em> for both distance and duration. We've created a function below dubbed <code>extract_values()</code> to help us resolve this very issue. The idea is that <code>extract_values()</code> is flexible and agnostic, therefore can be imported as a module into any project you might need.</p><pre><code class=\"language-python\"># recursivejson.py\n\ndef extract_values(obj, key):\n    &quot;&quot;&quot;Pull all values of specified key from nested JSON.&quot;&quot;&quot;\n    arr = []\n\n    def extract(obj, arr, key):\n        &quot;&quot;&quot;Recursively search for values of key in JSON tree.&quot;&quot;&quot;\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n</code></pre>\n<p>We need to pass this function two values:</p><ul><li>A JSON object, such as <code>r.json()</code> from an API request.</li><li>The name of the <strong>key</strong> we're looking to extract values from.</li></ul><pre><code class=\"language-python\">names = extract_values('myjson.json', 'name')\nprint(names)\n</code></pre>\n<p>Regardless of where the key <strong>\"text\"</strong> lives in the JSON, this function returns every value for the instance of <strong>\"key.\" </strong>Here's our function in action:</p><pre><code class=\"language-python\">import requests\nimport API_KEY\nfrom recursivejson import extract_values\n\n\ndef google_api_matrix():\n    &quot;&quot;&quot;Example Google Distance Matrix function.&quot;&quot;&quot;\n    endpoint = &quot;https://maps.googleapis.com/maps/api/distancematrix/json&quot;\n    params = {\n       'units': 'imperial',\n       'key': API_KEY,\n       'origins': &quot;New York City,NY&quot;,\n       'destinations': &quot;Washington,DC|Philadelphia,PA|Santa Barbara,CA|Miami,FL|Austin,TX|Napa Valley,CA&quot;,\n       'transit_mode': 'car',\n    }\n\n   r = requests.get(endpoint, params=params)\n   travel_values = extract_values(r.json(), 'text')\n   return travel_values\n</code></pre>\n<p>Running this function will result in the following output:</p><pre><code class=\"language-python\">['227 mi', '3 hours 54 mins', '94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n</code></pre>\n<p>Oh <em>fiddle me timbers</em>! Because the Google API alternates between <strong>distance </strong>and <strong>trip duration</strong>, every other value alternates between distance and time (can we pause to appreciate this horrible design? There are infinitely better ways to structure this response). Never fear, some simple Python can help us split this list into two lists:</p><pre><code class=\"language-python\">my_values = extract_values(r.json(), 'text')\n\ndurations = my_values[1::2]\ndistances = my_values[2::1]\n\nprint('DURATIONS = ', durations)\nprint('DISTANCES = ', distances)\n</code></pre>\n<p>This will take our one list and split it in to <em>two</em> lists, alternating between even and odd:</p><pre><code class=\"language-python\">DURATIONS = ['3 hours 54 mins', '1 hour 44 mins', '1 day 18 hours', '18 hours 43 mins', '1 day 2 hours', '1 day 18 hours']\nDISTANCES = ['94.6 mi', '1 hour 44 mins', '2,878 mi', '1 day 18 hours', '1,286 mi', '18 hours 43 mins', '1,742 mi', '1 day 2 hours', '2,871 mi', '1 day 18 hours']\n</code></pre>\n<h2 id=\"getting-creative-with-lists\">Getting Creative With Lists</h2><p>A common theme I run in to while extracting lists of values from JSON objects like these is that the lists of values I extract are very much related.  In the above example, for every <em>duration</em> we have an accompanying <em>distance, </em>which is a one-to-one basis. Imagine if we wanted to associate these values somehow?</p><p>To use a better example, I recently I used this <code>exact_values()</code> function to extract lists of column names and their data types from a database schema. As separate lists, the data looked something like this:</p><pre><code class=\"language-python\">column_names = ['index', 'first_name', 'last_name', 'join_date']\ncolumn_datatypes = ['integer', 'string', 'string', 'date']\n</code></pre>\n<p>Clearly these two lists are directly related; the latter is describing the former. How can this be useful? By using Python's <code>zip</code> method!</p><pre><code class=\"language-python\">schema_dict = dict(zip(column_names, column_datatypes))\nprint(schema_dict)\n</code></pre>\n<p>I like to think they call it <em>zip</em> because it's like zipping up a zipper, where each side of the zipper is a list. This output a dictionary where list 1 serves as the keys, and list 2 serves as values:</p><pre><code class=\"language-python\">{\n'index': 'integer', \n'first_name': 'string', \n'last_name':'string',\n'join_date': 'date'\n}\n</code></pre>\n<p>And there you have it folks: a free code snippet to copy and secretly pretend you wrote forever. I've thrown the function up on <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">Github Gists</a>, if such a thing pleases you.</p><p>In the meantime, zip it up and zip it out. Zippity-do-da, buh bye.</p>","url":"https://hackersandslackers.com/extract-data-from-complex-json-python/","uuid":"9a494df4-9e13-45ed-8648-efdda21c55a4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bbd7ce1b936605163ece407"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867371a","title":"Compile Frontend JavaScript with Babel and Gulp","slug":"babel-ecma-script","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","excerpt":"Using ECMAScript functions on the frontend the quick and dirty way.","custom_excerpt":"Using ECMAScript functions on the frontend the quick and dirty way.","created_at_pretty":"04 October, 2018","published_at_pretty":"06 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-03T23:05:08.000-04:00","published_at":"2018-10-06T06:20:00.000-04:00","updated_at":"2019-02-02T04:55:50.000-05:00","meta_title":"Frontend JavaScript with Babel | Hackers and Slackers","meta_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","og_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","og_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","og_title":"Compile Frontend JavaScript with Babel and Gulp","twitter_description":"Babel looks at new ECMA syntax (like require,  const or promise) and compiles code which is logic-equivalent to these features using vanilla Javascript.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/babel-1@2x.jpg","twitter_title":"Compile Frontend JavaScript with Babel and Gulp","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},"tags":[{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"NOTE: This post has not aged particularly well. This is made evident by the\npresence of the word \"Gulp\" present throughout the post.\n\n\n--------------------------------------------------------------------------------\n\nAs you may have already deduced from the occasional glaring holes in my\nprogramming knowledge, I haven’t spent as much of my life programming than,\nwell, anybody who haphazardly threw away an 8-year product management career.\nApparently, a lot can happen in 8 years... especially when it comes to\nJavaScript.\nPutting aside that whole server-side  NodeJS thing or whatever it’s called,\nthere’s years worth of mindfucks for those of us who fairly recently returned to\ndev. In our absence, the world has been Grunting, Gulping, Yarning and even\nNPMing. Let’s assume we’ve gotten past all of that... phew. Oh wait, I forgot to\nmention the monstrosity of “hey by way there’s this mess of a hardly-explained\n(protocol? organization? Fancy Github Repo?) called Babel that we all depend on\nto do meaningful shit in the browser these days.”\n\nIf you’re here because you’re trying to figure what the hell all these ‘const’\nand ‘import’ keywords are doing in modern-day browser JavaScript, consider this\nhalf focus-group and half getting-started tutorial. For the rest of you\nexperienced JS devs out there, perhaps you should stay. No, by all means, I’d\nlike you to sit here and think about the kind of ecosystem you’ve decided to\npatch together. Do it; otherwise, I might just give Kik a call to see what other\ntiny NPM modules they’d like to arbitrarily demand the removal of... remember\nthat? Exactly. Now think about what you’ve done.\n\nECMAScript and the Backwards-Compatible Web\nAny Actionscript veterans in the house? No? Just me? Great. Anyway, There’s\nsomething that ActionScript, JavaScript, and a bunch of Whatever-scripts have in\ncommon: they’re all implementations of ECMAScript: the ancestral equivalent of\ncomparing Latin to Spanish or French, but just one added detail: ECMAScript is\nstill a living language, and constantly improving. If some nation of cool cats\nstill spoke Latin as an active language and were constantly coming up with sick\nnew slang terms, those speaking French or Spanish would be powerless to come up\nwith phrases nearly as cool as “lit” or \"fam\"  (or whatever those Latin\nequivalents would be).\n\nThe problem is web browsers are inherently backward-compatible monstrosities.\nMost revenue-generating companies look at their >5% user base using Internet\nExplorer three and a half, and think “oh my god, that’s five percent of our\npotential revenue” (it isn’t, but whatever). That’s where we stand.\n\nNew features? For the INTERNET?\nI remember one particular weekend I rented a hotel room for the sole purpose of\nwrapping up an app based on MongoDB Stitch at the time. I was pretty confident I\nhad it all figured out, but found, something about Mongo’s quick start guides\nfelt off (I mean, more off than the rest of their nearly unreadable docs). Why\ndid Mongo insist on inline JavaScript? Haven’t we evolved past the days where\nPHP, HTML, CSS, and JS all lived on the same page?\n\nAs everybody-who-wasn’t-me at time already knows, JavaScript has been reaping\nthe benefits of new evolutions in ECMAScript over the years, but there’s a\ncatch: only the most modern browsers know what the hell to do with these new\nfeatures (duh), and they only do so with the dreaded  blocks thrown on HTML\npages. Attempting to use modern ECMA features in a linked JS file which hasn’t\nbeen pre-compiled is like dividing by zero. Seriously, you might die.\n\nBabel to the Rescue\nMore research only lead to more questions. Who is this Browserify wizard\n[http://browserify.org/]  and why is he dead? What is Webpack doing to fund all\ntheir highly produced branding, and why would anybody purchase a Webpack T-shirt\n[https://webpack.threadless.com/]? Finally, the biggest question of all: who the\nhell is this Babel guy who gives off as an electric explosion in your face on\nhard drugs?\n\nI can't even the difference between a compiler and a Marvel franchise anymore.I\nthink it was at this point in my journey of catching up on the last decade where\nI went truly mad and began pacing around my room, verbally assaulting inanimate\nobjects, and eventually even found myself washed up in a Wendys down the street,\nshivering and crying to myself. \n\nNow that I've been released from the psyche ward and are free to wander the\nstreets again, I've managed to piece together a bare-minimum understanding of\nwhat the hell is going on.\n\nWhat the Hell is Going On\nBecause Javascript as a language is destined to be backwards compatible to a\nyear when OJ Simpson was on trial and Windows 95 was considered cutting-edge\ntechnology, Babel  aims to \"improve\" browser Javascript. Because we can't\nactually improve or change the underlying technology, Babel looks at new-fangled\nECMA syntax (containing words like require,const  or promise) and compiles logic\nequivalent to these features with vanilla Javascript. If that sounds similar to\nhow Gulp  takes CSS preprocessors (like LESS  or SASS) and turns those files\ninto interpretable browser code, well, that's exactly what happens. \n\nGulp  is not be the only way to utilize new ECMAScript features. Babel syntax\ncan be compiled a number of ways, such as Babel's CLI\n[https://babeljs.io/docs/en/babel-cli], or from Server-side JS, or Webpack  if\nyou're Mister-Fancy-Pants. In fact, the trend across the board is that compiling\nyour site with modules via Webpack is winning dramatically. For all we know,\nthis could be the last Babel Gulp tutorial ever written.\n\nRight now we're focusing on the ability to use ECMAScript features on the\nfrontend; a common use-case for things like... well... theming the presentation\nlayer for a blog. If we can pull it off with Gulp, you probably won't struggle\nmuch with future methods.\n\nBabel NPM Packages\nAlright, so Babel isn't just a single NPM package. It's a lot. Let's see what's\nthere in a basic package.json:\n\n{\n  \"name\": \"example\",\n  \"scripts\": {\n    \"build\": \"babel src -d lib\"\n  },\n  \"presets\": [\n    \"env\"\n  ],\n  \"engines\": {\n    \"node\": \"8.12.0\"\n  },\n  \"dependencies\": {\n    \"@babel/polyfill\": \"^7.0.0\",\n    \"@babel/runtime\": \"^7.1.2\",\n    \"@babel/runtime-corejs2\": \"^7.1.2\",\n    \"babel-runtime\": \"^6.26.0\",\n    \"gulp-resolve-dependencies\": \"^2.2.0\",\n    \"gulp-sourcemaps\": \"^2.6.4\",\n    \"gulp-uglify-es\": \"^1.0.4\",\n  },\n  \"devDependencies\": {\n    \"@babel/cli\": \"^7.1.2\",\n    \"@babel/core\": \"^7.1.2\",\n    \"@babel/plugin-syntax-dynamic-import\": \"^7.0.0\",\n    \"@babel/plugin-transform-runtime\": \"^7.1.0\",\n    \"@babel/preset-env\": \"^7.1.0\",\n  }\n}\n\nLet's not go through all these in detail- in fact, let's take a look at our \ngulpfile.js:\n\n'use strict';\n\nvar gulp = require('gulp'),\n  concat = require('gulp-concat'),\n  autoprefixer = require('gulp-autoprefixer'),\n  sourcemaps = require('gulp-sourcemaps'),\n  babel = require('gulp-babel'),\n  resolveDependencies = require('gulp-resolve-dependencies'),\n\n\nvar paths = {\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  }\n};\n\nfunction scripts() {\n  return gulp.src(paths.scripts.src)\n    .pipe(babel({\n          presets: ['@babel/env'],\n          plugins: ['@babel/transform-runtime', '@babel/plugin-syntax-dynamic-import']\n    }))\n    .on('error', console.error.bind(console))\n    .pipe(resolveDependencies({\n            pattern: /\\* @requires [\\s-]*(.*\\.js)/g\n        }))\n    .pipe(concat('main.min.js'))\n    .pipe(gulp.dest(paths.scripts.dest));\n}\n\nNote that I've gone ahead and removed anything from the file that has nothing to\ndo with Babel - everything seen here is strictly relevant to building a\n'scripts' task. The gist is this:\n\nbabel  accepts two arrays:\n\n * Presets  refers to the version of ECMAScript we're targeting, and what we\n   have implies the latest.\n * Plugins  allow us to pass any plugins Babel supports; in this case, we're\n   rendering stuff at runtime. Let's not get into it too far.\n\nThe rest of the function is simple: we catch errors if any arise, we resolve\nrequired dependencies, and then we finally bunch everything into a single file\nand shove it in our destination folder.\n\nJust to be Clear\nWhen it comes to the best  way of achieving ECMAScript 2015 or 2016 etc in the\nbrowser,  I'm no expert. As previously mentioned, the understanding I've passed\non to you is a sort of bare minimum to start doing great things. \n\nAs far as what others are doing, I'd be willing to bet that the vast majority of\ndevs utilizing newer ECMAScript functions are running frameworks like React,\nwhile the rest have reached a consensus that Webpack is king and Gulp lame.\nThese things may be true.\n\nNo matter, this new found power of ours opens a lot of doors. Just wait until\nyou see what we do with MongoDB Stitch.","html":"<p><strong>NOTE</strong>: This post has not aged particularly well. This is made evident by the presence of the word \"Gulp\" present throughout the post.</p><hr><p>As you may have already deduced from the occasional glaring holes in my programming knowledge, I haven’t spent as much of my life programming than, well, anybody who haphazardly threw away an 8-year product management career. Apparently, a lot can happen in 8 years... especially when it comes to JavaScript.<br>Putting aside that whole server-side  NodeJS thing or whatever it’s called, there’s years worth of mindfucks for those of us who fairly recently returned to dev. In our absence, the world has been Grunting, Gulping, Yarning and even NPMing. Let’s assume we’ve gotten past all of that... phew. Oh wait, I forgot to mention the monstrosity of “hey by way there’s this mess of a hardly-explained (protocol? organization? Fancy Github Repo?) called Babel that we all depend on to do meaningful shit in the browser these days.”</p><p>If you’re here because you’re trying to figure what the hell all these ‘const’ and ‘import’ keywords are doing in modern-day browser JavaScript, consider this half focus-group and half getting-started tutorial. For the rest of you experienced JS devs out there, perhaps you should stay. No, by all means, I’d like you to sit here and think about the kind of ecosystem you’ve decided to patch together. Do it; otherwise, I might just give Kik a call to see what other tiny NPM modules they’d like to arbitrarily demand the removal of... remember that? Exactly. Now think about what you’ve done.</p><h2 id=\"ecmascript-and-the-backwards-compatible-web\">ECMAScript and the Backwards-Compatible Web</h2><p>Any Actionscript veterans in the house? No? Just me? Great. Anyway, There’s something that ActionScript, JavaScript, and a bunch of Whatever-scripts have in common: they’re all implementations of ECMAScript: the ancestral equivalent of comparing Latin to Spanish or French, but just one added detail: ECMAScript is still a living language, and constantly improving. If some nation of cool cats still spoke Latin as an active language and were constantly coming up with sick new slang terms, those speaking French or Spanish would be powerless to come up with phrases nearly as cool as “lit” or \"fam\"  (or whatever those Latin equivalents would be).</p><p>The problem is web browsers are inherently backward-compatible monstrosities. Most revenue-generating companies look at their &gt;5% user base using Internet Explorer three and a half, and think “oh my god, that’s five percent of our potential revenue” (it isn’t, but whatever). That’s where we stand.</p><h2 id=\"new-features-for-the-internet\">New features? For the INTERNET?</h2><p>I remember one particular weekend I rented a hotel room for the sole purpose of wrapping up an app based on MongoDB Stitch at the time. I was pretty confident I had it all figured out, but found, something about Mongo’s quick start guides felt off (I mean, more off than the rest of their nearly unreadable docs). Why did Mongo insist on inline JavaScript? Haven’t we evolved past the days where PHP, HTML, CSS, and JS all lived on the same page?</p><p>As everybody-who-wasn’t-me at time already knows, JavaScript has been reaping the benefits of new evolutions in ECMAScript over the years, but there’s a catch: only the most modern browsers know what the hell to do with these new features (duh), and they only do so with the dreaded  blocks thrown on HTML pages. Attempting to use modern ECMA features in a linked JS file which hasn’t been pre-compiled is like dividing by zero. Seriously, you might die.</p><h2 id=\"babel-to-the-rescue\">Babel to the Rescue</h2><p>More research only lead to more questions. Who is this <a href=\"http://browserify.org/\">Browserify wizard</a> and why is he dead? What is Webpack doing to fund all their highly produced branding, and why would anybody purchase a <a href=\"https://webpack.threadless.com/\">Webpack T-shirt</a>? Finally, the biggest question of all: who the hell is this Babel guy who gives off as an electric explosion in your face on hard drugs?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/babel2-1.jpg\" class=\"kg-image\"><figcaption><em>I can't even the difference between a compiler and a Marvel franchise anymore.&nbsp;</em></figcaption></figure><p>I think it was at this point in my journey of catching up on the last decade where I went truly mad and began pacing around my room, verbally assaulting inanimate objects, and eventually even found myself washed up in a Wendys down the street, shivering and crying to myself. </p><p>Now that I've been released from the psyche ward and are free to wander the streets again, I've managed to piece together a bare-minimum understanding of what the hell is going on.</p><h3 id=\"what-the-hell-is-going-on\">What the Hell is Going On</h3><p>Because Javascript as a language is destined to be backwards compatible to a year when OJ Simpson was on trial and Windows 95 was considered cutting-edge technology, <strong>Babel</strong> aims to \"improve\" browser Javascript. Because we can't actually improve or change the underlying technology, Babel looks at new-fangled ECMA syntax (containing words like <code>require</code>,  <code>const</code> or <code>promise</code>) and compiles logic equivalent to these features with vanilla Javascript. If that sounds similar to how <strong>Gulp</strong> takes CSS preprocessors (like <em>LESS</em> or <em>SASS</em>) and turns those files into interpretable browser code, well, that's exactly what happens. </p><p><strong>Gulp</strong> is not be the only way to utilize new ECMAScript features. Babel syntax can be compiled a number of ways, such as <a href=\"https://babeljs.io/docs/en/babel-cli\">Babel's CLI</a>, or from Server-side JS, or <strong>Webpack</strong> if you're Mister-Fancy-Pants. In fact, the trend across the board is that compiling your site with modules via Webpack is winning dramatically. For all we know, this could be the last Babel Gulp tutorial ever written.</p><p>Right now we're focusing on the ability to use ECMAScript features on the frontend; a common use-case for things like... well... theming the presentation layer for a blog. If we can pull it off with Gulp, you probably won't struggle much with future methods.</p><h2 id=\"babel-npm-packages\">Babel NPM Packages</h2><p>Alright, so Babel isn't just a single NPM package. It's a lot. Let's see what's there in a basic <code>package.json</code>:</p><pre><code>{\n  \"name\": \"example\",\n  \"scripts\": {\n    \"build\": \"babel src -d lib\"\n  },\n  \"presets\": [\n    \"env\"\n  ],\n  \"engines\": {\n    \"node\": \"8.12.0\"\n  },\n  \"dependencies\": {\n    \"@babel/polyfill\": \"^7.0.0\",\n    \"@babel/runtime\": \"^7.1.2\",\n    \"@babel/runtime-corejs2\": \"^7.1.2\",\n    \"babel-runtime\": \"^6.26.0\",\n    \"gulp-resolve-dependencies\": \"^2.2.0\",\n    \"gulp-sourcemaps\": \"^2.6.4\",\n    \"gulp-uglify-es\": \"^1.0.4\",\n  },\n  \"devDependencies\": {\n    \"@babel/cli\": \"^7.1.2\",\n    \"@babel/core\": \"^7.1.2\",\n    \"@babel/plugin-syntax-dynamic-import\": \"^7.0.0\",\n    \"@babel/plugin-transform-runtime\": \"^7.1.0\",\n    \"@babel/preset-env\": \"^7.1.0\",\n  }\n}</code></pre><p>Let's not go through all these in detail- in fact, let's take a look at our <strong>gulpfile.js</strong>:</p><pre><code>'use strict';\n\nvar gulp = require('gulp'),\n  concat = require('gulp-concat'),\n  autoprefixer = require('gulp-autoprefixer'),\n  sourcemaps = require('gulp-sourcemaps'),\n  babel = require('gulp-babel'),\n  resolveDependencies = require('gulp-resolve-dependencies'),\n\n\nvar paths = {\n  scripts: {\n    src: 'src/js/*.js',\n    dest: 'assets/js'\n  }\n};\n\nfunction scripts() {\n  return gulp.src(paths.scripts.src)\n    .pipe(babel({\n          presets: ['@babel/env'],\n          plugins: ['@babel/transform-runtime', '@babel/plugin-syntax-dynamic-import']\n    }))\n    .on('error', console.error.bind(console))\n    .pipe(resolveDependencies({\n            pattern: /\\* @requires [\\s-]*(.*\\.js)/g\n        }))\n    .pipe(concat('main.min.js'))\n    .pipe(gulp.dest(paths.scripts.dest));\n}</code></pre><p>Note that I've gone ahead and removed anything from the file that has nothing to do with Babel - everything seen here is strictly relevant to building a 'scripts' task. The gist is this:</p><p><strong>babel</strong> accepts two arrays:</p><ul><li><em>Presets</em> refers to the version of ECMAScript we're targeting, and what we have implies the latest.</li><li><em>Plugins</em> allow us to pass any plugins Babel supports; in this case, we're rendering stuff at runtime. Let's not get into it too far.</li></ul><p>The rest of the function is simple: we catch errors if any arise, we resolve required dependencies, and then we finally bunch everything into a single file and shove it in our destination folder.</p><h2 id=\"just-to-be-clear\">Just to be Clear</h2><p>When it comes to the <em>best</em> way of achieving ECMAScript 2015 or 2016 etc in the browser,  I'm no expert. As previously mentioned, the understanding I've passed on to you is a sort of bare minimum to start doing great things. </p><p>As far as what others are doing, I'd be willing to bet that the vast majority of devs utilizing newer ECMAScript functions are running frameworks like React, while the rest have reached a consensus that Webpack is king and Gulp lame. These things may be true.</p><p>No matter, this new found power of ours opens a lot of doors. Just wait until you see what we do with <strong>MongoDB Stitch</strong>.</p><p></p><p></p>","url":"https://hackersandslackers.com/babel-ecma-script/","uuid":"04fe6342-9462-4d3c-9e6d-bc000e17824a","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bb583642361b479aa119366"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673718","title":"Reading and Writing to CSVs in Python","slug":"reading-and-writing-to-csvs-in-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","excerpt":"Playing with tabular data the native Python way.","custom_excerpt":"Playing with tabular data the native Python way.","created_at_pretty":"27 September, 2018","published_at_pretty":"27 September, 2018","updated_at_pretty":"05 November, 2018","created_at":"2018-09-27T13:22:47.000-04:00","published_at":"2018-09-27T18:35:00.000-04:00","updated_at":"2018-11-05T07:55:10.000-05:00","meta_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","meta_description":"Using native Python libraries to interact with tabular data. Pandas not included.\n\n\n\n\n\n\n\n\nar \n\n","og_description":"Using native Python libraries to interact with tabular data. Pandas not included.","og_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","og_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","twitter_description":"Using native Python libraries to interact with tabular data. Pandas not included.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/csvpython2@2x.jpg","twitter_title":"Reading and Writing to CSVs in Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"Tables. Cells. Two-dimensional data. We here at Hackers & Slackers know how to\ntalk dirty, but there's one word we'll be missing from our vocabulary today:\nPandas.Before the remaining audience closes their browser windows in fury, hear\nme out. We love Pandas; so much so that tend to recklessly gunsling this 30mb\nlibrary to perform simple tasks. This isn't always a wise choice. I get it:\nyou're here for data, not software engineering best practices. We all are, but\nin a landscape where engineers and scientists already produce polarizing code\nquality, we're all just a single bloated lambda function away from looking like\n idiots and taking a hit to our credibility. This is a silly predicament when\nthere are plenty of built-in Python libraries at our disposable which work\nperfectly fine. Python’s built in CSV library can cover quite a bit of data\nmanipulation use cases to achieve the same results of large scientific libraries\njust as easily.\n\nBasic CSV Interaction\nRegardless of whether you're reading or writing to CSVs, there are a couple\nlines of code which will stay mostly the same between the two. \n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n     reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\nBefore accomplishing anything, we've stated some critical things in these two\nlines of code:\n\n * All interactions with our CSV will only be valid as long as they live within\n   the with.open  block (comparable to managing database connections).\n * We'll be interacting with a file in our directory called hackers.csv, for\n   which we only need read (or r) permissions\n * We create a reader  object, which is again comparable to managing database \n   cursors  if you're familiar.\n * We have the ability to set the delimiter of our CSV (a curious feature,\n   considering the meaning of C  in the acronym CSV.\n\nIterating Rows\nAn obvious use case you probably have in mind would be to loop through each row\nto see what sort of values we're dealing with. Your first inclination might be\nto do something like this:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\tfor row in reader.readlines():\n\t\tprint('row = ', row)\n\nThat's fine and all, but row  in this case returns a simple list - this is\nobviously problem if you want to access the values of certain columns by column \nname,  as opposed to numeric index (I bet you do). Well, we've got you covered:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in reader.readlines():\n\t\tprint(row['column_name_1'], row['column_name_2'])\n\nChanging reader  to DictReader  outputs a dictionary  per CSV row, as opposed to\na simple list. Are things starting to feel a little Panda-like yet?\n\nBonus: Printing all Keys and Their Values\nLet's get a little weird just for fun. Since our rows are dict objects now, we\ncan print our entire CSV as a series of dicts like so:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in loc_reader:\n            for (k, v) in row.items():\n\t\t\t\tprint(k, ':', v)\n\nSkipping Headers\nAs we read information from CSVs to be repurposed for, say, API calls, we \nprobably  don't want to iterate over the first row of our CSV: this will output\nour key values alone, which would be useless in this context. Consider this:\n\n# read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r') as myCsvFile:\n\tnext(myCsvFile)\n\tfor row in myCsvFile.readlines():\n\t\tprint(row)\n\nWhoa! A different approach.... but somehow just as simple? In this case, we\nleave out reader  altogether (which still works!) but more importantly, we\nintroduce next(). next(myCsvFile)  immediately skips to the next line in a CSV,\nso in our case, we simply skip line one before going into our For loop. Amazing.\n\nWriting to CSVs\nWriting to CSVs isn't much different than reading from them. In fact, almost all\nthe same principles apply, where instances of the word \"read\" are more or less\nreplaced with\" write. Huh. \n\n# write_csv.py\nimport csv\n\nwith open('hackers.csv', 'w') as myCsvFile:\n    columns = ['column_name_1', 'column_name_2']\n    writer = csv.DictWriter(myCsvFile, fieldnames=columns)\n\n    writer.writeheader()\n    writer.writerow({'column_name_1': 'Mark', 'column_name_2': 'Twain'})\n    writer.writerow({'column_name_1': 'Foo', 'column_name_2: 'Bar'})\n\nWe're writing a brand new CSV here: 'hackers.csv' doesn't technically exist yet,\nbut that doesn't stop Python from not giving a shit. Python knows what you mean.\nPython has your back.\n\nHere, we set our headers as a fixed list set by the column  variable. This is a\nstatic way of creating headers, but the same can be done dynamically by passing\nthe keys  of a dict, or whatever it is you like to do. \n\nwriter.writeheader()  knows what we're saying thanks to the aforementioned \nfieldnames  we passed to our writer earlier. Good for you, writer.\n\nBut how do we write rows, you might ask? Why, with writer.writerow(), of course!\nBecause we use DictWriter  similarly to how we used DictReader  earlier, we can\nmap values to our CSV with simple column references. Easy.","html":"<p>Tables. Cells. Two-dimensional data. We here at Hackers &amp; Slackers know how to talk dirty, but there's one word we'll be missing from our vocabulary today: Pandas.Before the remaining audience closes their browser windows in fury, hear me out. We love Pandas; so much so that tend to recklessly gunsling this 30mb library to perform simple tasks. This isn't always a wise choice. I get it: you're here for data, not software engineering best practices. We all are, but in a landscape where engineers and scientists already produce polarizing code quality, we're all just a single bloated lambda function away from looking like  idiots and taking a hit to our credibility. This is a silly predicament when there are plenty of built-in Python libraries at our disposable which work perfectly fine. Python’s built in CSV library can cover quite a bit of data manipulation use cases to achieve the same results of large scientific libraries just as easily.</p><h2 id=\"basic-csv-interaction\">Basic CSV Interaction</h2><p>Regardless of whether you're reading or writing to CSVs, there are a couple lines of code which will stay mostly the same between the two. </p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n     reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')</code></pre><p>Before accomplishing anything, we've stated some critical things in these two lines of code:</p><ul><li>All interactions with our CSV will only be valid as long as they live within the <code>with.open</code> block (comparable to managing database connections).</li><li>We'll be interacting with a file in our directory called <code>hackers.csv</code>, for which we only need read (or <code>r</code>) permissions</li><li>We create a <code>reader</code> object, which is again comparable to managing database <code>cursors</code> if you're familiar.</li><li>We have the ability to set the delimiter of our CSV (a curious feature, considering the meaning of <strong>C</strong> in the acronym <strong>CSV.</strong></li></ul><h3 id=\"iterating-rows\">Iterating Rows</h3><p>An obvious use case you probably have in mind would be to loop through each row to see what sort of values we're dealing with. Your first inclination might be to do something like this:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.reader(myCsvFile, delimiter=',', quotechar='|')\n\tfor row in reader.readlines():\n\t\tprint('row = ', row)</code></pre><p>That's fine and all, but <code>row</code> in this case returns a simple list - this is obviously problem if you want to access the values of certain columns by column <em>name,</em> as opposed to <em>numeric index </em>(I bet you do). Well, we've got you covered:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in reader.readlines():\n\t\tprint(row['column_name_1'], row['column_name_2'])</code></pre><p>Changing <code>reader</code> to <code>DictReader</code> outputs a <em>dictionary</em> per CSV row, as opposed to a simple list. Are things starting to feel a little Panda-like yet?</p><h4 id=\"bonus-printing-all-keys-and-their-values\">Bonus: Printing all Keys and Their Values</h4><p>Let's get a little weird just for fun. Since our rows are dict objects now, we can print our entire CSV as a series of dicts like so:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r', newline='') as myCsvFile:\n    reader = csv.DictReader(myCsvFile)\n\tfor row in loc_reader:\n            for (k, v) in row.items():\n\t\t\t\tprint(k, ':', v)</code></pre><h3 id=\"skipping-headers\">Skipping Headers</h3><p>As we read information from CSVs to be repurposed for, say, API calls, we <em>probably</em> don't want to iterate over the first row of our CSV: this will output our key values alone, which would be useless in this context. Consider this:</p><pre><code># read_csv.py\nimport csv\n\nwith open('hackers.csv', 'r') as myCsvFile:\n\tnext(myCsvFile)\n\tfor row in myCsvFile.readlines():\n\t\tprint(row)</code></pre><p>Whoa! A different approach.... but somehow just as simple? In this case, we leave out <code>reader</code> altogether (which still works!) but more importantly, we introduce <code>next()</code>. <code>next(myCsvFile)</code> immediately skips to the next line in a CSV, so in our case, we simply skip line one before going into our For loop. Amazing.</p><h2 id=\"writing-to-csvs\">Writing to CSVs</h2><p>Writing to CSVs isn't much different than reading from them. In fact, almost all the same principles apply, where instances of the word \"read\" are more or less replaced with\" write. Huh. </p><pre><code># write_csv.py\nimport csv\n\nwith open('hackers.csv', 'w') as myCsvFile:\n    columns = ['column_name_1', 'column_name_2']\n    writer = csv.DictWriter(myCsvFile, fieldnames=columns)\n\n    writer.writeheader()\n    writer.writerow({'column_name_1': 'Mark', 'column_name_2': 'Twain'})\n    writer.writerow({'column_name_1': 'Foo', 'column_name_2: 'Bar'})</code></pre><p>We're writing a brand new CSV here: 'hackers.csv' doesn't technically exist yet, but that doesn't stop Python from not giving a shit. Python knows what you mean. Python has your back.</p><p>Here, we set our headers as a fixed list set by the <code>column</code> variable. This is a static way of creating headers, but the same can be done dynamically by passing the <code>keys</code> of a dict, or whatever it is you like to do. </p><p><code>writer.writeheader()</code> knows what we're saying thanks to the aforementioned <code>fieldnames</code> we passed to our writer earlier. Good for you, writer.</p><p>But how do we write rows, you might ask? Why, with <code>writer.writerow()</code>, of course! Because we use <code>DictWriter</code> similarly to how we used <code>DictReader</code> earlier, we can map values to our CSV with simple column references. Easy.</p>","url":"https://hackersandslackers.com/reading-and-writing-to-csvs-in-python/","uuid":"eb4f019f-e135-49d2-a568-f03f1e622d62","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bad11e75ee4c83af27dda9e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673701","title":"Flask Routing & Sessions: A Subtle Symphony","slug":"the-art-of-building-flask-routes","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/flaskroutes@2x.jpg","excerpt":"With great flexibility comes great responsibility .","custom_excerpt":"With great flexibility comes great responsibility .","created_at_pretty":"17 September, 2018","published_at_pretty":"19 September, 2018","updated_at_pretty":"17 November, 2018","created_at":"2018-09-17T05:05:03.000-04:00","published_at":"2018-09-19T08:58:00.000-04:00","updated_at":"2018-11-16T20:57:42.000-05:00","meta_title":"Flask Routing & Sessions: A Subtle Symphony | Hackers and Slackers","meta_description":"There's nothing wrong with being a worker drone repeating worthless projects and contributing nothing to humanity. I'd personally prefer using Flask.","og_description":"There's nothing wrong with being a worker drone repeating worthless projects and contributing nothing to humanity. I'd personally prefer using Flask.","og_image":"https://hackersandslackers.com/content/images/2018/09/flaskroutes@2x.jpg","og_title":"Flask Routing & Sessions: A Subtle Symphony | Hackers and Slackers","twitter_description":"There's nothing wrong with being a worker drone repeating worthless projects and contributing nothing to humanity. I'd personally prefer using Flask.","twitter_image":"https://hackersandslackers.com/content/images/2018/09/flaskroutes@2x.jpg","twitter_title":"Flask Routing & Sessions: A Subtle Symphony | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Python’s fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Python’s fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"}],"plaintext":"It isn't often you find somebody sad or miserable enough to detail the inner\nworkings of web framework features, such as sessions or routing. This is\nunderstandably so; we use frameworks because presumably hate dealing with these\nthings from scratch. This is especially so when it comes to Flask, which only\nreleased version 1.0 a few months ago, introducing breaking changes rendering\nprevious documentation more-or-less worthless. \n\nGoogling some of Flask's critical features mostly returns one-liners from the\napp's authors (half of which are useless, as they are for older versions of\nFlask). Stack Overflow threads mostly sit in silence, and even Kite\n[https://kite.com/], AKA \"The smart copilot for programmers\" returns blank pages\nof documentation, akin to the blank stare of a clueless Golden Retriever.\n\nIn retrospect, it was probably a poor choice for me to pick up 4 separate\nFlask-based projects during this time.\n\nWe're in a historic place in time where a team of developers put together\nsomething beautiful, yet somehow feels undersold. It seems as though the niche\nmarket of \"those who can't do, teach\" remains untapped for Flask, as the usual\nsuspects have yet to \"do\". This leaves newcomers like myself to hack away for\ntheir own survival in the meantime. I've only just turned that mental corner\nwhere Flask's quirks are as comforting as home-cooked meal, as opposed to\nfrustrating single-word methods containing 6 words of documentation on average.\n\nThe good news is I am still technically alive, after spending weeks building\nFlask applications mostly through trial and error. The bad news is that I've\nbecome Mr. Robot  in the process. That said, if there will ever be an ideal\nmoment in my life to write about Flask, now is the time. As reality slowly slips\naway in 1s and 0s, I may as well pass along  what I've learned.\n\nBroad Strokes\nIt only takes a couple minutes into explaining what Flask is when you realize\nthat Flask, at its core, is overwhelmingly just the “V” in “MVC”. Jinja handles\nthe templates, and something like SQLAlchemy will likely handle your models.\nFlask has an abundance of third-party libraries to handle business logic, but it\nis the core Flask package that we all agreed to gather around. This speaks\nvolumes about the quality of Flask’s simple yet powerful request handling.\n\nI'll  break down as many of Flask's out-of-the-box features, focusing on what\nmatters most (in my opinion). Take a look at some of the Flask libraries we'll\nbe playing around with:\n\n# app.py\nfrom flask import Flask, render_template, request, redirect, Session, g\nimport os\n\nConfiguring Our App\nAs always, we create our app with app = Flask(name). Equally uninteresting is\nour configuration setup, which we'll import via a class in config.py:\n\n# app.py\nfrom flask import Flask, render_template, request, redirect, Session, g\nimport config\nimport os\n\n# Our app\napp = Flask(__name__)\n\n# Load our config variables\napp.config.from_object('config.ProductionConfig')\n\nA number of things in our config are absolutely essential for sessions to work.\nBelow is an example config file:\n\n# config.py\nimport os\n\nclass ProductionConfig():\n    \"\"\"Set app config vars.\"\"\"\n    SECRET_KEY = os.urandom(24)\n    SESSION_TYPE = null\n    SESSION_COOKIE_NAME = 'session name'\n    SESSION_PERMANENT = True\n    PERMANENT_SESSION_LIFETIME = timedelta(days=31) (2678400 seconds)\n\nSECRET_KEY  is critical: this variable needs to exist in out config for sessions\nto function properly. The best way to handle is is by generating a key as seen\nabove.\n\nSESSION_TYPE  allows us to specify where our session data should be stored. This\nis set null by default, but Flask supports a number of options:\n\n * RedisSessionInterface:  Uses the Redis key-value store as a session backend. \n * MemcachedSessionInterface:  Uses the Memcached as a session backend. \n * FileSystemSessionInterface:  werkzeug.contrib.cache.FileSystemCache  as a\n   session backend.\n * MongoDBSessionInterface:  Uses MongoDB as a backend \n   [http://api.mongodb.org/python/current/index.html]via pymongo\n * SqlAlchemySessionInterface:  Uses SQLAlchemy, or rather Flask-SQLAlchemy\n   [https://pythonhosted.org/Flask-SQLAlchemy/]\n\nThere are plenty more variables you can set if you want to take a look here\n[http://flask.pocoo.org/docs/1.0/config/].\n\nSessions and Contexts\nUnlike cookie-based sessions, Flask sessions are handled and stored on the\nserver-side. A session object is simply a dict which is accessible throughout\nthe application a global level, referring to a single 'logged-in user'. As long\nas the session is active, any context of our app will be able to retrieve,\nmodify, or delete the values held in this session object,\n\n# Save a value to the user's session.\nsession['username'] = 'MyUsername' \n\n# IMPORTANT: \"True\" forces our changes to be recognized.\nsession.modified = True: \n\n# Retrieve session values at any time, anywhere \nsession.get('username') = True\n\nSeeing as how sessions are accessible globally, it is also important to note\nthat sessions can last a very long time; pretty much self explanatory given the \nSESSION_PERMANENT = True  configuration option.  It's a good idea to set a\nsession timeout period, or better yet, close them by the user's own request.\nClearing a session is as simple as resetting the session dictionary values back\nto None  by using the pop  method: session.pop('value', None).\n\nThe Application Context\nBesides undying global sessions,  Flask also provides us with a feature with an\nobject more suitable for storing and passing temporary values between app\ncontexts. This object known simply as g. While  technically an abbreviation for\n\"global\",g  is really just a convenient place to store temporarily store values\nwhich you can always depend on to be by your side.\n\n# app.py\nfrom Flask import g\n\nIt's important to note that values assigned to g  only exist within the context\nthey were created by default. For example, if we store information to the object\ndue to some user interaction on the dashboard, these values are lost once the\nuser moves to another part of our app. That said, values assigned to g  can\ntechnically be passed between contexts if we return g.value. This distinction\nbetween always-alive sessions  and every dying g  should be indicative of what\nreach respective object does.  Spoiler alert: sensitive (or contextually\nuseless) data should be stored temporarily with g, where values which will\ncontinuously be useful in determining the functionality of our should should\nreside in session.\n\nInterestingly enough, Flask has a decorator  specifically for terminating values\nsaved to g  in the case we'd want to ensure the swift and total annihilation of\nsuch data. For instance, if we were to assign a database connection to g  using\ng.db = connect_to_database(), we'd want to make sure that connection is closed\nas fast as possible before we forget:\n\n# app.py\ndef db_stuff():\n    g.db = database_connection()\n    g.db.somequeryorwhatever\n    return g.db\n\n@app.teardown_appcontext\ndef teardown_db():\n    db = g.pop('db', None)\n\nRoutes & Decorators\nWe're surely familiar with the concepts behind routing users to deserved views\nby now. Before we look at the juicy stuff, consider this boring route for a\nboring product, where the homepage is a dashboard:\n\n# app.py\n@app.route('/', methods=['GET', 'POST', 'OPTIONS'])\ndef dashboard():\n    \"\"\"Boring route.\"\"\"\n    return render_template('/dashboard.html')\n\nOh snap, our landing page is a /dashboard?  How will we know which user's\ndashboard to display when they visit the dashboard, or any other page for that\nmatter? If only there were a way to intercept every request a user makes to our\napp?\n\nFlask comes with a bunch of insanely useful decorators. Python decorators are\nfunctions which either 'wrap'  other functions in additional logic, or in our\ncase, intercept functions to do with them what we what. Flask has a vast\nplethora of logic decorators, ranging from detecting first-time visitors,\nhandling exceptions, executing logic before/after page loads, etc. Even the\nroute we set above is a decorator!\n\n@flask.before_request\nAdding before_request  to our app allows us to run logic prior to the\naforementioned request. With this power, we can do things like treat users\ndifferently (such as recognized or anonymous users), or just execute some sort\nof unique logic upon page load. \n\nIn this simple case, we check to see if a visitor has an active session every\ntime they hit a route. This way, if a user's session expired between before\nhitting a route in our app, we can prompt them to log in... or whatever. \n\nbefore_request doesn't accept any value parameters - the handler is mostly\nintended to perform tasks such as making a database query necessary for our app\nto run, or make sure users are still logged in.\n\n# app.py\n@app.before_request\ndef before_request():\n    \"\"\"Handle multiple users.\"\"\"\n    if 'username' in session:\n        return render_template('/dashboard.html')\n    else:\n        return render_template('/login.html')\n\n@flask.url_value_preprocessor\nUnlike before_request, url_value_preprocessor  does  accept incoming data. This\nallows us to handle data being posted to any part of our app before we even\nbother serving up views. Not only does this provide a convenient separation of\nconcerns, but also helps us avoid callback hell, which yes, can happen in Python\ntoo.\n\nLet's say we're accepting a POST request, where we create a view for our user's\npersonal details. When the user passes us their email address, we decide to\nretrieve the user's records by hitting an API, and passing the results to the\nview.\n\nWithout modularizing our code, we'd have to handle things like waiting on API\ncalls in the same functions as  our routes. Not only is this shitty repetitive\ncode, but running numerous API calls and rendering a view all at once is going\nto eventually break. Go ahead and ask the NodeJS guys. They'll know.\n\n# app.py\n@app.url_value_preprocessor\ndef url_value_preprocessor(endpoint, values):\n    \"\"\"Handle data sent to any route.\"\"\"\n       if request.args:\n           email = request.args.get('email')\n           r = requests.post(endpoint, headers=headers, data=email)\n           session['usermetadata'] = r\n           session.modified = True\n           return session\n\nYou're Only Getting Started\nWe've only covered small percentage of convenient tools Flask offers us. Go\nahead and see how many decorators [http://flask.pocoo.org/docs/1.0/api/]   you\ncan fuck with. Yeah dude, shit is legit - and we haven't even talked about the\nFlask-Login package yet.\n\nThe beauty of lightweight frameworks is that they focus on the problems that\ndrive us to web frameworks in the first place. Flask is clearly designed to\nhandle serving views, standing up APIs, and handling user management\neffectively. Contrast this with frameworks like Django, which forces rigid app\nsetup in what can commonly be an  hour-long setup or greater. I'll truthfully\nalways have a place in my heart for Django as the fathers of Python MVC: I would\ncan say with confidence that without the creation of Django (as well as the\nofficial $10 dollar intro book from Barnes and Noble) I never would have\ntransitioned from an obnoxious product manager  personality to the kind of guy\nwho owns multiple Python t-shirts. Hmm. Now that I think about it, maybe I\nshould've stayed an office tool as opposed to solving all these complex\nproblems. oh well.\n\nFlask is indicative of a new direction of framework design - or rather lack\nthereof. Programmers who know what they're doing  can express themselves outside\nof traditional boundaries set by other frameworks, surely designed to keep\nidiots from ruining everything. There's nothing wrong with being a worker drone\nrepeating the same worthless projects,  using same libraries, and essentially\ncontributing nothing to humanity. I'd personally prefer to take the freedom and\nspeed of Flask any day.","html":"<p>It isn't often you find somebody sad or miserable enough to detail the inner workings of web framework features, such as sessions or routing. This is understandably so; we use frameworks because presumably hate dealing with these things from scratch. This is especially so when it comes to Flask, which only released version 1.0 a few months ago, introducing breaking changes rendering previous documentation more-or-less worthless. </p><p>Googling some of Flask's critical features mostly returns one-liners from the app's authors (half of which are useless, as they are for older versions of Flask). Stack Overflow threads mostly sit in silence, and even <a href=\"https://kite.com/\">Kite</a>, AKA <em>\"The smart copilot for programmers\" </em>returns blank pages of documentation, akin to the blank stare of a clueless Golden Retriever.</p><p><strong><em>In retrospect, it was probably a poor choice for me to pick up 4 separate Flask-based projects during this time.</em></strong></p><p>We're in a historic place in time where a team of developers put together something beautiful, yet somehow feels undersold. It seems as though the niche market of \"those who can't do, teach\" remains untapped for Flask, as the usual suspects have yet to \"do\". This leaves newcomers like myself to hack away for their own survival in the meantime. I've only just turned that mental corner where Flask's quirks are as comforting as home-cooked meal, as opposed to frustrating single-word methods containing 6 words of documentation on average.</p><p>The good news is I am still technically alive, after spending weeks building Flask applications mostly through trial and error. The bad news is that I've become <em>Mr. Robot</em> in the process. That said, if there will ever be an ideal moment in my life to write about Flask, now is the time. As reality slowly slips away in 1s and 0s, I may as well pass along  what I've learned.</p><h2 id=\"broad-strokes\">Broad Strokes</h2><p>It only takes a couple minutes into explaining what Flask is when you realize that Flask, at its core, is overwhelmingly just the “V” in “MVC”. Jinja handles the templates, and something like SQLAlchemy will likely handle your models. Flask has an abundance of third-party libraries to handle business logic, but it is the core Flask package that we all agreed to gather around. This speaks volumes about the quality of Flask’s simple yet powerful request handling.</p><p>I'll  break down as many of Flask's out-of-the-box features, focusing on what matters most (in my opinion). Take a look at some of the Flask libraries we'll be playing around with:</p><pre><code># app.py\nfrom flask import Flask, render_template, request, redirect, Session, g\nimport os</code></pre><h2 id=\"configuring-our-app\">Configuring Our App</h2><p>As always, we create our app with <code>app = Flask(name)</code><em>. </em>Equally uninteresting is our configuration setup, which we'll import via a class in <code>config.py</code>:</p><pre><code># app.py\nfrom flask import Flask, render_template, request, redirect, Session, g\nimport config\nimport os\n\n# Our app\napp = Flask(__name__)\n\n# Load our config variables\napp.config.from_object('config.ProductionConfig')</code></pre><p>A number of things in our config are absolutely essential for sessions to work. Below is an example config file:</p><pre><code># config.py\nimport os\n\nclass ProductionConfig():\n    \"\"\"Set app config vars.\"\"\"\n    SECRET_KEY = os.urandom(24)\n    SESSION_TYPE = null\n    SESSION_COOKIE_NAME = 'session name'\n    SESSION_PERMANENT = True\n    PERMANENT_SESSION_LIFETIME = timedelta(days=31) (2678400 seconds)</code></pre><p><strong>SECRET_KEY</strong> is critical: this variable needs to exist in out config for sessions to function properly. The best way to handle is is by generating a key as seen above.</p><p><strong>SESSION_TYPE</strong> allows us to specify where our session data should be stored. This is set null by default, but Flask supports a number of options:</p><ul><li><a href=\"https://pythonhosted.org/Flask-Session/#flask.ext.session.RedisSessionInterface\">RedisSessionInterface</a>:<strong> </strong>Uses the Redis key-value store as a session backend. </li><li><a href=\"https://pythonhosted.org/Flask-Session/#flask.ext.session.MemcachedSessionInterface\">MemcachedSessionInterface</a>:<strong> </strong>Uses the Memcached as a session backend. </li><li><a href=\"https://pythonhosted.org/Flask-Session/#flask.ext.session.FileSystemSessionInterface\">FileSystemSessionInterface</a>:<strong> </strong><code>werkzeug.contrib.cache.FileSystemCache</code> as a session backend.</li><li><a href=\"https://pythonhosted.org/Flask-Session/#flask.ext.session.MongoDBSessionInterface\">MongoDBSessionInterface</a>:<strong> </strong>Uses MongoDB as a backend<a href=\"http://api.mongodb.org/python/current/index.html\"> </a>via <code>pymongo</code></li><li><a href=\"https://pythonhosted.org/Flask-Session/#flask.ext.session.SqlAlchemySessionInterface\">SqlAlchemySessionInterface</a>:<strong> </strong>Uses SQLAlchemy, or rather <a href=\"https://pythonhosted.org/Flask-SQLAlchemy/\">Flask-SQLAlchemy</a></li></ul><p>There are plenty more variables you can set if you want to take a look <a href=\"http://flask.pocoo.org/docs/1.0/config/\">here</a>.</p><h2 id=\"sessions-and-contexts\">Sessions and Contexts</h2><p>Unlike cookie-based sessions, Flask sessions are handled and stored on the server-side. A session object is simply a dict which is accessible throughout the application a global level, referring to a single 'logged-in user'. As long as the session is active, any context of our app will be able to retrieve, modify, or delete the values held in this session object,</p><pre><code># Save a value to the user's session.\nsession['username'] = 'MyUsername' \n\n# IMPORTANT: \"True\" forces our changes to be recognized.\nsession.modified = True: \n\n# Retrieve session values at any time, anywhere \nsession.get('username') = True</code></pre><p>Seeing as how sessions are accessible globally, it is also important to note that sessions can last a very long time; pretty much self explanatory given the <code>SESSION_PERMANENT = True</code> configuration option.  It's a good idea to set a session timeout period, or better yet, close them by the user's own request. Clearing a session is as simple as resetting the session dictionary values back to <em>None</em> by using the <strong>pop</strong> method: <code>session.pop('value', None)</code>.</p><h3 id=\"the-application-context\">The Application Context</h3><p>Besides undying global sessions,  Flask also provides us with a feature with an object more suitable for storing and passing temporary values between app contexts. This object known simply as <code>g</code>. While<strong> </strong>technically an abbreviation for \"global\",  <code>g</code> is really just a convenient place to store temporarily store values which you can always depend on to be by your side.</p><pre><code># app.py\nfrom Flask import g</code></pre><p>It's important to note that values assigned to <code>g</code> <em>only exist within the context they were created </em>by default. For example, if we store information to the object due to some user interaction on the dashboard, these values are lost once the user moves to another part of our app. That said, values assigned to <code>g</code> can technically be passed between contexts if we <code>return g.value</code>. This distinction between always-alive <em>sessions</em> and every dying <em>g</em> should be indicative of what reach respective object does.  Spoiler alert: sensitive (or contextually useless) data should be stored temporarily with <code>g</code>, where values which will continuously be useful in determining the functionality of our should should reside in <code>session</code><em>.</em></p><p>Interestingly enough, Flask has a <em>decorator</em> specifically for terminating values saved to <code>g</code> in the case we'd want to ensure the swift and total annihilation of such data. For instance, if we were to assign a database connection to <code>g</code> using  <code>g.db = connect_to_database()</code>, we'd want to make sure that connection is closed as fast as possible before we forget:</p><pre><code># app.py\ndef db_stuff():\n    g.db = database_connection()\n    g.db.somequeryorwhatever\n    return g.db\n\n@app.teardown_appcontext\ndef teardown_db():\n    db = g.pop('db', None)</code></pre><h2 id=\"routes-decorators\">Routes &amp; Decorators</h2><p>We're surely familiar with the concepts behind routing users to deserved views by now. Before we look at the juicy stuff, consider this boring route for a boring product, where the homepage is a dashboard:</p><pre><code># app.py\n@app.route('/', methods=['GET', 'POST', 'OPTIONS'])\ndef dashboard():\n    \"\"\"Boring route.\"\"\"\n    return render_template('/dashboard.html')</code></pre><p>Oh snap, our landing page is a /<em>dashboard?</em> How will we know which user's dashboard to display when they visit the dashboard, or any other page for that matter? If only there were a way to intercept every request a user makes to our app?</p><p>Flask comes with a bunch of insanely useful <strong><em>decorators</em></strong>. Python decorators are functions which either 'wrap'  other functions in additional logic, or in our case, intercept functions to do with them what we what. Flask has a vast plethora of logic decorators, ranging from detecting first-time visitors, handling exceptions, executing logic before/after page loads, etc. Even the route we set above is a decorator!</p><h3 id=\"-flask-before_request\">@flask.before_request</h3><p>Adding <strong>before_request</strong> to our app allows us to run logic prior to the aforementioned request. With this power, we can do things like treat users differently (such as recognized or anonymous users), or just execute some sort of unique logic upon page load. </p><p>In this simple case, we check to see if a visitor has an active session every time they hit a route. This way, if a user's session expired between before hitting a route in our app, we can prompt them to log in... or whatever. </p><p><strong>before_request </strong>doesn't accept any value parameters - the handler is mostly intended to perform tasks such as making a database query necessary for our app to run, or make sure users are still logged in.</p><pre><code># app.py\n@app.before_request\ndef before_request():\n    \"\"\"Handle multiple users.\"\"\"\n    if 'username' in session:\n        return render_template('/dashboard.html')\n    else:\n        return render_template('/login.html')</code></pre><h3 id=\"-flask-url_value_preprocessor\">@flask.url_value_preprocessor</h3><p>Unlike <em>before_request</em><strong>, url_value_preprocessor</strong> <em>does</em> accept incoming data. This allows us to handle data being posted to any part of our app before we even bother serving up views. Not only does this provide a convenient separation of concerns, but also helps us avoid <em>callback hell, </em>which yes, can happen in Python too.</p><p>Let's say we're accepting a POST request, where we create a view for our user's personal details. When the user passes us their email address, we decide to retrieve the user's records by hitting an API, and passing the results to the view.</p><p>Without modularizing our code, we'd have to handle things like waiting on API calls in the same functions as  our routes. Not only is this shitty repetitive code, but running numerous API calls and rendering a view all at once is going to eventually break. Go ahead and ask the NodeJS guys. They'll know.</p><pre><code># app.py\n@app.url_value_preprocessor\ndef url_value_preprocessor(endpoint, values):\n    \"\"\"Handle data sent to any route.\"\"\"\n       if request.args:\n           email = request.args.get('email')\n           r = requests.post(endpoint, headers=headers, data=email)\n           session['usermetadata'] = r\n           session.modified = True\n           return session</code></pre><h2 id=\"you-re-only-getting-started\">You're Only Getting Started</h2><p>We've only covered small percentage of convenient tools Flask offers us. Go ahead and see <a href=\"http://flask.pocoo.org/docs/1.0/api/\">how many decorators</a>  you can fuck with. Yeah dude, shit is legit - and we haven't even talked about the Flask-Login package yet.</p><p>The beauty of lightweight frameworks is that they focus on the problems that drive us to web frameworks in the first place. Flask is clearly designed to handle serving views, standing up APIs, and handling user management effectively. Contrast this with frameworks like <strong>Django</strong>, which forces rigid app setup in what can commonly be an  hour-long setup or greater. I'll truthfully always have a place in my heart for Django as the fathers of Python MVC: I would can say with confidence that without the creation of Django (as well as the official $10 dollar intro book from Barnes and Noble) I never would have transitioned from an obnoxious product manager  personality to the kind of guy who owns multiple Python t-shirts. Hmm. Now that I think about it, maybe I should've stayed an office tool as opposed to solving all these complex problems. oh well.</p><p>Flask is indicative of a new direction of framework design - or rather lack thereof. Programmers who <em>know what they're doing</em> can express themselves outside of traditional boundaries set by other frameworks, surely designed to keep idiots from ruining everything. There's nothing wrong with being a worker drone repeating the same worthless projects,  using same libraries, and essentially contributing nothing to humanity. I'd personally prefer to take the freedom and speed of Flask any day.</p><p></p>","url":"https://hackersandslackers.com/the-art-of-building-flask-routes/","uuid":"67a6407a-5804-4bd0-812d-219561e2488a","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b9f6e3ff79bcf0717187d8b"}}]}},"pageContext":{"slug":"todd","limit":12,"skip":36,"numberOfPages":8,"humanPageNumber":4,"prevPageNumber":3,"nextPageNumber":5,"previousPagePath":"/author/todd/page/3/","nextPagePath":"/author/todd/page/5/"}}