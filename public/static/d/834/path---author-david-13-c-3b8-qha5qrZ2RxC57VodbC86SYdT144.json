{"data":{"ghostAuthor":{"slug":"david","name":"David Aquino","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","cover_image":"http://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/robot_o.jpg","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","location":"NYC","website":null,"twitter":"@_k3n0","facebook":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673719","title":"Creating an AMI with HashiCorp Packer","slug":"hashicorp-packer","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","excerpt":"HashiCorp's version control for infrastructure .","custom_excerpt":"HashiCorp's version control for infrastructure .","created_at_pretty":"02 October, 2018","published_at_pretty":"03 October, 2018","updated_at_pretty":"30 December, 2018","created_at":"2018-10-02T16:05:02.000-04:00","published_at":"2018-10-03T07:00:00.000-04:00","updated_at":"2018-12-30T07:00:11.000-05:00","meta_title":"Creating an AMI with HashiCorp Packer | Hackers and Slackers","meta_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","og_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","og_title":"Creating an AMI with HashiCorp Packer","twitter_description":"Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/packer4@2x.jpg","twitter_title":"Creating an AMI with HashiCorp Packer","authors":[{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null}],"primary_author":{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Hashicorp","slug":"hashicorp","description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","feature_image":null,"meta_description":"Automate serverless architecture for enterprise AWS Cloud instances with Terraform, or leverage products such as Vault and Packer to improve your ecosystem.","meta_title":"Hashicorp Suite | Hackers and Slackers","visibility":"public"}],"plaintext":"Why use Packer [https://www.packer.io/]? Infrastructure as code has become part\nof the buzzword bingo surrounding operational teams and their desired optimal\nworkflows.\n\nOne could theoretically just start with a base AMI and manually update it and\nthen re-save it as a new AMI, but this process is not repeatable.  We can check\nin our desired infrastructure states as code to version control.  This is good\npractice for change control management.  We can readily see what worked before\nand what was changed in the latest update.  If something catastrophic happens or\nwe encounter unforeseen issues, we can always rollback to a previous state.\n\nI'm the first new guy on our ops team in a few years.  We work with a base image\nto create our EC2 instances and that image does not have my ssh keys.  In our\ncurrent workflow, when I spin up a new instance using our latest base AMI, I\ncan't ssh to the box because my key isn't on there.  Amazon has also released\nAmazon Linux 2, so we had a card to update the base AMI in the backlog.  I\npicked up this task and found the HashiCorp tool to be very powerful and useful.\n\n{\n  \"description\": \"Builds a Base Image for EC2 AWS provisioner\",\n  \"variables\":{\n    \"hostname\": \"cne-aws-trusty64\",\n    \"config_dir\": \".\"\n  },\n\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-04681a1dbd79675a5\",\n      \"instance_type\": \"m5.xlarge\",\n      \"ssh_username\": \"ec2-user\",\n      \"ami_name\": \"snapdragon-v3.6.9\",\n      \"subnet_id\": \"subnet-0000000000\",\n      \"tags\": {\n        \"OS_Version\": \"Amazon Linux 2\",\n        \"Release\": \"2017-12\",\n        \"Builder\": \"packer\"\n      },\n      \"ssh_timeout\": \"60m\"\n    }\n  ],\n\n  \"provisioners\": [\n    {\n      \"type\": \"shell\",\n        \"scripts\": [\n          \"scripts/setup-example.sh\"\n        ]\n    } \n  ]\n}\n\n\nIn our setup script, we install dependencies and a configuration management tool\nadds users, and updates permissions as needed for all of our applications.  It's\nbasically the equivalent of whatever you would do manually to achieve a desired\nstate.","html":"<p><strong>Why use <a href=\"https://www.packer.io/\">Packer</a>? </strong>Infrastructure as code has become part of the buzzword bingo surrounding operational teams and their desired optimal workflows.</p><p>One could theoretically just start with a base AMI and manually update it and then re-save it as a new AMI, but this process is not repeatable.  We can check in our desired infrastructure states as code to version control.  This is good practice for change control management.  We can readily see what worked before and what was changed in the latest update.  If something catastrophic happens or we encounter unforeseen issues, we can always rollback to a previous state.</p><p>I'm the first new guy on our ops team in a few years.  We work with a base image to create our EC2 instances and that image does not have my ssh keys.  In our current workflow, when I spin up a new instance using our latest base AMI, I can't ssh to the box because my key isn't on there.  Amazon has also released Amazon Linux 2, so we had a card to update the base AMI in the backlog.  I picked up this task and found the HashiCorp tool to be very powerful and useful.</p><pre><code>{\n  &quot;description&quot;: &quot;Builds a Base Image for EC2 AWS provisioner&quot;,\n  &quot;variables&quot;:{\n    &quot;hostname&quot;: &quot;cne-aws-trusty64&quot;,\n    &quot;config_dir&quot;: &quot;.&quot;\n  },\n\n  &quot;builders&quot;: [\n    {\n      &quot;type&quot;: &quot;amazon-ebs&quot;,\n      &quot;region&quot;: &quot;us-east-1&quot;,\n      &quot;source_ami&quot;: &quot;ami-04681a1dbd79675a5&quot;,\n      &quot;instance_type&quot;: &quot;m5.xlarge&quot;,\n      &quot;ssh_username&quot;: &quot;ec2-user&quot;,\n      &quot;ami_name&quot;: &quot;snapdragon-v3.6.9&quot;,\n      &quot;subnet_id&quot;: &quot;subnet-0000000000&quot;,\n      &quot;tags&quot;: {\n        &quot;OS_Version&quot;: &quot;Amazon Linux 2&quot;,\n        &quot;Release&quot;: &quot;2017-12&quot;,\n        &quot;Builder&quot;: &quot;packer&quot;\n      },\n      &quot;ssh_timeout&quot;: &quot;60m&quot;\n    }\n  ],\n\n  &quot;provisioners&quot;: [\n    {\n      &quot;type&quot;: &quot;shell&quot;,\n        &quot;scripts&quot;: [\n          &quot;scripts/setup-example.sh&quot;\n        ]\n    } \n  ]\n}\n</code></pre>\n<p>In our setup script, we install dependencies and a configuration management tool adds users, and updates permissions as needed for all of our applications.  It's basically the equivalent of whatever you would do manually to achieve a desired state.</p>","url":"https://hackersandslackers.com/hashicorp-packer/","uuid":"0c50713b-1c8c-4071-b65d-e57fd87f3536","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bb3cf6e7ae39d0d60547523"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ef","title":"My First Experience with Docker","slug":"my-first-dockerfile","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/09/docker2@2x.jpg","excerpt":"Reboot EC2 instances with Docker.","custom_excerpt":"Reboot EC2 instances with Docker.","created_at_pretty":"05 September, 2018","published_at_pretty":"05 September, 2018","updated_at_pretty":"30 December, 2018","created_at":"2018-09-04T21:25:10.000-04:00","published_at":"2018-09-05T13:47:18.000-04:00","updated_at":"2018-12-30T07:01:05.000-05:00","meta_title":"Reboot EC2 instances with Docker | Hackers And Slackers","meta_description":"Reboot EC2 instances with Docker","og_description":"My First Experience with Docker","og_image":"https://hackersandslackers.com/content/images/2018/09/docker2@2x.jpg","og_title":"My First Experience with Docker","twitter_description":"Reboot EC2 instances with Docker","twitter_image":"https://hackersandslackers.com/content/images/2018/09/docker2@2x.jpg","twitter_title":"My First Experience with Docker","authors":[{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null}],"primary_author":{"name":"David Aquino","slug":"david","bio":"Spent years in the military to become a killing machine using only 2 CDJs. Automated all of life's inconveniences, including investments in the financial markets.","profile_image":"https://hackersandslackers.com/content/images/2019/03/keno2.jpg","twitter":"@_k3n0","facebook":null,"website":null},"primary_tag":{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},"tags":[{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"We have a .NET application that has been running for years, but once a week, the\napplication fails to recover and needs the server needs to be rebooted.  To\npreemptively reboot the EC2 instance nightly we decided to use Docker and ECS\n scheduled tasks.\n\nHere is what the finished Dockerfile looks like:\n\nFROM amazonlinux:latest\n\nRUN yum -y update\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python get-pip.py\nRUN pip install boto\nCOPY ./win_reboot.py /root/\nRUN chmod +x /root/win_reboot.py\n\nCMD [\"/root/win_reboot.py\"]\n\nECS has a role with the correct access to run reboot. \n\nHere is what the python script looks like:\n\n#!/usr/bin/env python\n\nimport boto.ec2\nimport os\n\nconn = boto.ec2.connect_to_region(\"us-east-1\")\ninstance_id_list = []\nfor instance in os.environ['WINDOWS_EC2'].split(\"|\"): \nfor r in conn.get_all_instances(filters={\"tag:Name\" : instance}):\n\t[instance_id_list.append(i.id) for i in r.instances]    \nconn.reboot_instances(instance_ids=instance_id_list, dry_run=False)\n\nWe need to import boto and os here. Boto is to do the AWS magic of rebooting the\nservers and os to use an environment variable.  Here we use a pipe delimiter to\ntarget multiple EC2 instances. This will help out while testing the container on\nyour local machine because you can pass in environment variables on the command\nline using the -e flag.  There is probably a more efficient and elegant way to\ngo about this, but this works for us. \n\nKeep these files in the same directory and run: \n\ndocker build -t test-name:latest .\n\nIf you it builds successfully, you can try to run it:\n\ndocker run -it test-name:latest /bin/bash\n\nThis will run your container interactively and drop you into a bash shell.\n Alternatively, try running with environment variables passed in.\n\ndocker run test-name:latest -e WINDOWS_EC2='EC2-instance-tagName' -e AWS_DEFAULT_REGION='aws region'-e AWS_ACCESS_KEY_ID='ID GOES HERE' -e AWS_SECRET_ACCESS_KEY='KEY GOES HERE'\n\nIf all of this is working as expected, you can go to ECS in AWS and create your\ntask definition.  It will provide you commands to push your image to ECR. \n\nMaybe I'll add some screenshots here.\n\nAfter that you might find you have some extra docker images and containers to\nclean up locally. The following commands should help.  Consult the docker\ndocumentation for more information. \nhttps://docs.docker.com/engine/reference/commandline/rmi/\n\nfor i in ```docker images | grep '<none>'| awk '{ print $3 }'```; do docker rmi -f $i; done\nfor i in `docker container ls --all | awk '{ print $1 }'`; do docker container rm $i; done","html":"<p>We have a .NET application that has been running for years, but once a week, the application fails to recover and needs the server needs to be rebooted.  To preemptively reboot the EC2 instance nightly we decided to use Docker and ECS  scheduled tasks.</p><p>Here is what the finished Dockerfile looks like:</p><pre><code>FROM amazonlinux:latest\n\nRUN yum -y update\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python get-pip.py\nRUN pip install boto\nCOPY ./win_reboot.py /root/\nRUN chmod +x /root/win_reboot.py\n\nCMD [\"/root/win_reboot.py\"]</code></pre><p>ECS has a role with the correct access to run reboot. </p><p>Here is what the python script looks like:</p><pre><code>#!/usr/bin/env python\n\nimport boto.ec2\nimport os\n\nconn = boto.ec2.connect_to_region(\"us-east-1\")\ninstance_id_list = []\nfor instance in os.environ['WINDOWS_EC2'].split(\"|\"): \nfor r in conn.get_all_instances(filters={\"tag:Name\" : instance}):\n\t[instance_id_list.append(i.id) for i in r.instances]    \nconn.reboot_instances(instance_ids=instance_id_list, dry_run=False)</code></pre><p>We need to import boto and os here. Boto is to do the AWS magic of rebooting the servers and os to use an environment variable.  Here we use a pipe delimiter to target multiple EC2 instances. This will help out while testing the container on your local machine because you can pass in environment variables on the command line using the -e flag.  There is probably a more efficient and elegant way to go about this, but this works for us. </p><p>Keep these files in the same directory and run: </p><pre><code>docker build -t test-name:latest .</code></pre><p>If you it builds successfully, you can try to run it:</p><pre><code>docker run -it test-name:latest /bin/bash</code></pre><p>This will run your container interactively and drop you into a bash shell.  Alternatively, try running with environment variables passed in.</p><pre><code>docker run test-name:latest -e WINDOWS_EC2='EC2-instance-tagName' -e AWS_DEFAULT_REGION='aws region'-e AWS_ACCESS_KEY_ID='ID GOES HERE' -e AWS_SECRET_ACCESS_KEY='KEY GOES HERE'</code></pre><p>If all of this is working as expected, you can go to ECS in AWS and create your task definition.  It will provide you commands to push your image to ECR. </p><p>Maybe I'll add some screenshots here.</p><p>After that you might find you have some extra docker images and containers to clean up locally. The following commands should help.  Consult the docker documentation for more information. <a href=\"https://docs.docker.com/engine/reference/commandline/rmi/\">https://docs.docker.com/engine/reference/commandline/rmi/</a></p><pre><code>for i in ```docker images | grep '&lt;none&gt;'| awk '{ print $3 }'```; do docker rmi -f $i; done\nfor i in `docker container ls --all | awk '{ print $1 }'`; do docker container rm $i; done</code></pre>","url":"https://hackersandslackers.com/my-first-dockerfile/","uuid":"ddb6164b-8bb7-4a8f-b301-ac20261658eb","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b8f30761fc1fc7d92b5c4b4"}}]}},"pageContext":{"slug":"david","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}