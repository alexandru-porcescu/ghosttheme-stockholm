{"data":{"ghostTag":{"slug":"architecture","name":"Architecture","visibility":"public","feature_image":null,"description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c79b0070fa2b110f256e320","title":"Running Jupyter Notebooks on a Ubuntu Server","slug":"running-jupyter-notebooks-on-a-ubuntu-server","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","custom_excerpt":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","created_at_pretty":"01 March, 2019","published_at_pretty":"02 March, 2019","updated_at_pretty":"14 April, 2019","created_at":"2019-03-01T17:19:51.000-05:00","published_at":"2019-03-01T21:15:40.000-05:00","updated_at":"2019-04-14T12:27:20.000-04:00","meta_title":"Running Jupyter Notebooks on a Ubuntu Server | Hackers and Slackers","meta_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","og_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","og_title":"Running Jupyter Notebooks on a Ubuntu Server","twitter_description":"Configuring a VPS from scratch to host Jupyter notebooks with Anaconda.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/jupyter.jpg","twitter_title":"Running Jupyter Notebooks on a Ubuntu Server","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"}],"plaintext":"It dawned on me the other day that for a publication which regularly uses and\ntalks about Jupyter notebooks [https://jupyter.org/], we’ve never actually taken\nthe time to explain what they are or how to start using them. No matter where\nyou may have been in your career, first exposure to Jupyter and the IPython\n[https://ipython.org/]  shell is often a confusingly magical experience. Writing\nprograms line-by-line and receiving feedback in real-time feels more like\npainting oil on canvas and programming. I suppose we can finally chalk up a win\nfor dynamically typed languages.\n\nThere are a couple of barriers for practical devs to overcome before using\nJupyter, the most obvious being hardware costs. If you’re utilizing a full \nAnaconda  installation, chances are you’re not the type of person to mess\naround. Real machine learning algorithms take real resources, and real resources\ntake real money. A few vendors have popped up here are offering managed\ncloud-hosted notebooks for this reason. For those of us who bothered to do the\nmath, it turns out most of these services are more expensive than spinning up a\ndedicated VPS.\n\nData scientists with impressive machines have no problem running notebooks\nlocally for most use cases. While that’s fine and good for scientists, this\nsetup is problematic for those of us with commitments to Python outside of\nnotebooks. Upon installation, Anaconda barges into your system’s ~/.bash_profile\n, shouts “I am the captain now,”  and crowns itself as your system’s default\nPython path. Conda and Pip have some trouble getting along, so for those of us\nwho build Python applications and use notebooks, it's best to keep these things\nisolated.\n\nSetting Up a VPS\nWe're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted\nfor DigitalOcean  in my case, both for simplicity and the fact that I'm\nincredibly broke. Depending on how broke you may or may not be, this is where\nyou'll have to make a judgment call for your system resources:\n\nMy kind sir, I would like to order the most exquisite almost-cheapest Droplet on\nthe menuSSH into that bad boy. You know what to do next:\n\n$ sudo apt update\n$ sudo apt upgrade -y\n\n\nWith that out of the way, next we'll grab the latest version of Python:\n\n$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n\n\nFinally, we'll open port 8888 for good measure, since this is the port Jupyter\nruns on:\n\n$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw allow 22\n$ sudo ufw status\n\n\nTo                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n\n\nCreate a New User\nAs always, we should create a Linux user besides root to do just about anything:\n\n$ adduser myuser\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n\n\nThen, add them to the sudoers  group:\n\n$ usermod -aG sudo myuser\n\n\nLog in as the user:\n\n$ su - myuser\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\n\nInstall The Latest Anaconda Distribution\nAnaconda comes with all the fantastic Data Science Python packages we'll need\nfor our notebook. To find the latest distribution, check here: \nhttps://www.anaconda.com/download/. We'll install this to a /tmp  folder:\n\ncd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n\n\nOnce downloaded, begin the installation:\n\n$ sh Anaconda3-2018.12-Linux-x86_64.sh\n\n\nComplete the resulting prompts:\n\nWelcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n>>>\n\n\nGet ready for the wall of text....\n\n===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n\n\nThis kicks off a rather lengthy install process. Afterward, you'll be prompted\nto add Conda to your startup script. Say yes:\n\ninstallation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n\n\nThe final part of the installation will ask if you'd like to install VS Code.\nDecline this offer because Microsoft sucks.\n\nFinally, reload your /.bashrc file to get apply Conda's changes:\n\n$ source ~/.bashrc\n\n\nSetting Up Conda Environments\nConda installations can be isolated to separate environments similarly to how we\nwould  with Virtualenv. Unlike Virtualenv, however, Conda environments can be\nactivated from anywhere (not just in the directory containing the environment).\nCreate and activate a Conda env:\n\n$ conda create --name myenv python=3\n$ conda activate myenv\n\n\nCongrats, you're now in an active Conda environment!\n\nStarting Up Jupyter\nMake sure you're in a directory you'd like to be running Jupyter in. Entering \njupyter notebook  in this directory should result in the following:\n\n(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n\n\nThis next part is tricky. To run our notebook, we need to reconnect to our VPS\nvia an SSH tunnel. Close the terminal and reconnect to your server with the\nfollowing format:\n\nssh -L 8888:localhost:8888 myuser@your_server_ip\n\n\nIndeed, localhost  is intended to stay the same, but your_server_ip  is to be\nreplaced with the address of your server.\n\nWith that done, let's try this one more time. Remember to reactivate your Conda\nenvironment first!\n\n$ jupyter notebook\n\n\nThis time around, the links which appear in the terminal should work!\n\nWE DID ITBONUS ROUND: Theme Your Notebooks\nIf ugly interfaces bother you as much as they bother me, I highly recommend\ntaking a look at the jupyter-themes package on Github\n[https://github.com/dunovank/jupyter-themes]. This package allows you to\ncustomize the look and feel of your notebook, either as simple as activating a\nstyle, or as complex as setting your margin width. I highly recommend checking\nout the available themes to spice up your notebook!","html":"<p>It dawned on me the other day that for a publication which regularly uses and talks about <a href=\"https://jupyter.org/\">Jupyter notebooks</a>, we’ve never actually taken the time to explain what they are or how to start using them. No matter where you may have been in your career, first exposure to Jupyter and the <a href=\"https://ipython.org/\">IPython</a> shell is often a confusingly magical experience. Writing programs line-by-line and receiving feedback in real-time feels more like painting oil on canvas and programming. I suppose we can finally chalk up a win for dynamically typed languages.</p><p>There are a couple of barriers for practical devs to overcome before using Jupyter, the most obvious being hardware costs. If you’re utilizing a full <strong>Anaconda</strong> installation, chances are you’re not the type of person to mess around. Real machine learning algorithms take real resources, and real resources take real money. A few vendors have popped up here are offering managed cloud-hosted notebooks for this reason. For those of us who bothered to do the math, it turns out most of these services are more expensive than spinning up a dedicated VPS.</p><p>Data scientists with impressive machines have no problem running notebooks locally for most use cases. While that’s fine and good for scientists, this setup is problematic for those of us with commitments to Python outside of notebooks. Upon installation, Anaconda barges into your system’s <code>~/.bash_profile</code>, shouts <strong><em>“I am the captain now,”</em></strong> and crowns itself as your system’s default Python path. Conda and Pip have some trouble getting along, so for those of us who build Python applications and use notebooks, it's best to keep these things isolated.</p><h2 id=\"setting-up-a-vps\">Setting Up a VPS</h2><p>We're going to spin up a barebones Ubuntu 18.04 instance from scratch. I opted for <strong>DigitalOcean</strong> in my case, both for simplicity and the fact that I'm incredibly broke. Depending on how broke you may or may not be, this is where you'll have to make a judgment call for your system resources:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/digitaloceanvps.png\" class=\"kg-image\"><figcaption>My kind sir, I would like to order the most exquisite almost-cheapest Droplet on the menu</figcaption></figure><!--kg-card-end: image--><p>SSH into that bad boy. You know what to do next:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt update\n$ sudo apt upgrade -y\n</code></pre>\n<!--kg-card-end: markdown--><p>With that out of the way, next we'll grab the latest version of Python:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt install python3-pip python3-dev\n$ sudo -H pip3 install --upgrade pip\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we'll open port 8888 for good measure, since this is the port Jupyter runs on:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo ufw enable\n$ sudo ufw allow 8888\n$ sudo ufw allow 22\n$ sudo ufw status\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">To                         Action      From\n--                         ------      ----\nOpenSSH                    ALLOW       Anywhere\n8888                       ALLOW       Anywhere\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"create-a-new-user\">Create a New User</h3><p>As always, we should create a Linux user besides root to do just about anything:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ adduser myuser\n\nAdding user `myuser' ...\nAdding new group `myuser' (1001) ...\nAdding new user `myuser' (1001) with group `myuser' ...\nCreating home directory `/home/myuser' ...\nCopying files from `/etc/skel' ...\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nChanging the user information for myuser\nEnter the new value, or press ENTER for the default\n        Full Name []: My User\n        Room Number []: 420\n        Work Phone []: 555-1738\n        Home Phone []: derrrr\n        Other []: i like turtles\nIs the information correct? [Y/n] y\n</code></pre>\n<!--kg-card-end: markdown--><p>Then, add them to the <strong>sudoers</strong> group:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ usermod -aG sudo myuser\n</code></pre>\n<!--kg-card-end: markdown--><p>Log in as the user:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ su - myuser\nTo run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.\nSee &quot;man sudo_root&quot; for details.\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"install-the-latest-anaconda-distribution\">Install The Latest Anaconda Distribution</h3><p>Anaconda comes with all the fantastic Data Science Python packages we'll need for our notebook. To find the latest distribution, check here: <a href=\"https://www.anaconda.com/download/\">https://www.anaconda.com/download/</a>. We'll install this to a <code>/tmp</code> folder:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">cd /tmp\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Once downloaded, begin the installation:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sh Anaconda3-2018.12-Linux-x86_64.sh\n</code></pre>\n<!--kg-card-end: markdown--><p>Complete the resulting prompts:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">Welcome to Anaconda3 2018.12\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n&gt;&gt;&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Get ready for the wall of text....</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">===================================\n\nCopyright 2015, Anaconda, Inc.\n\nAll rights reserved under the 3-clause BSD License:\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n.......\n\n\nDo you accept the license terms? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>This kicks off a rather lengthy install process. Afterward, you'll be prompted to add Conda to your startup script. Say <strong>yes</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">installation finished.\nDo you wish the installer to prepend the Anaconda3 install location\nto PATH in your /home/myuser/.bashrc ? [yes|no]\n</code></pre>\n<!--kg-card-end: markdown--><p>The final part of the installation will ask if you'd like to install VS Code. Decline this offer because Microsoft sucks.</p><p>Finally, reload your <strong>/.bashrc </strong>file to get apply Conda's changes:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ source ~/.bashrc\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"setting-up-conda-environments\">Setting Up Conda Environments</h3><p>Conda installations can be isolated to separate environments similarly to how we would  with Virtualenv. Unlike Virtualenv, however, Conda environments can be activated from anywhere (not just in the directory containing the environment). Create and activate a Conda env:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ conda create --name myenv python=3\n$ conda activate myenv\n</code></pre>\n<!--kg-card-end: markdown--><p>Congrats, you're now in an active Conda environment!</p><h3 id=\"starting-up-jupyter\">Starting Up Jupyter</h3><p>Make sure you're in a directory you'd like to be running Jupyter in. Entering <code>jupyter notebook</code> in this directory should result in the following:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">(jupyter_env) myuser@jupyter:~$ jupyter notebook\n[I 21:23:21.198 NotebookApp] Writing notebook server cookie secret to /run/user/1001/jupyter/notebook_cookie_secret\n[I 21:23:21.361 NotebookApp] Serving notebooks from local directory: /home/myuser/jupyter\n[I 21:23:21.361 NotebookApp] The Jupyter Notebook is running at:\n[I 21:23:21.361 NotebookApp] http://localhost:8888/?token=1fefa6ab49a498a3f37c959404f7baf16b9a2eda3eaa6d72\n[I 21:23:21.361 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 21:23:21.361 NotebookApp] No web browser found: could not locate runnable browser.\n[C 21:23:21.361 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=1u2grit856t5yig5f37tf5iu5y4gfi73tfty5hf\n</code></pre>\n<!--kg-card-end: markdown--><p>This next part is tricky. To run our notebook, we need to reconnect to our VPS via an SSH tunnel. Close the terminal and reconnect to your server with the following format:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">ssh -L 8888:localhost:8888 myuser@your_server_ip\n</code></pre>\n<!--kg-card-end: markdown--><p>Indeed, <code>localhost</code> is intended to stay the same, but <code>your_server_ip</code> is to be replaced with the address of your server.</p><p>With that done, let's try this one more time. Remember to reactivate your Conda environment first!</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ jupyter notebook\n</code></pre>\n<!--kg-card-end: markdown--><p>This time around, the links which appear in the terminal should work!</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/03/Screen-Shot-2019-03-01-at-7.01.42-PM.png\" class=\"kg-image\"><figcaption>WE DID IT</figcaption></figure><!--kg-card-end: image--><h2 id=\"bonus-round-theme-your-notebooks\">BONUS ROUND: Theme Your Notebooks</h2><p>If ugly interfaces bother you as much as they bother me, I highly recommend taking a look at the <a href=\"https://github.com/dunovank/jupyter-themes\">jupyter-themes package on Github</a>. This package allows you to customize the look and feel of your notebook, either as simple as activating a style, or as complex as setting your margin width. I highly recommend checking out the available themes to spice up your notebook!</p><!--kg-card-begin: gallery--><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/gruvbox-dark-python.png\" width=\"1013\" height=\"903\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/grade3_table.png\" width=\"1293\" height=\"809\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/jtplotDark_reach.png\" width=\"8400\" height=\"3600\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/oceans16_code_headers.png\" width=\"1293\" height=\"808\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/onedork_code_headers.png\" width=\"1293\" height=\"808\"></div></div><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/solarized-dark_iruby.png\" width=\"951\" height=\"498\"></div><div class=\"kg-gallery-image\"><img src=\"https://hackersandslackers.com/content/images/2019/03/chesterish_code_headers.png\" width=\"1293\" height=\"808\"></div></div></div></figure><!--kg-card-end: gallery-->","url":"https://hackersandslackers.com/running-jupyter-notebooks-on-a-ubuntu-server/","uuid":"0cfc9046-2e28-46a2-9f95-8851a9aea770","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c79b0070fa2b110f256e320"}},{"node":{"id":"Ghost__Post__5c65c207042dc633cf14a610","title":"S3 File Management With The Boto3 Python SDK","slug":"manage-s3-assests-with-boto3-python-sdk","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","custom_excerpt":"Modify and manipulate thousands of files in your S3 (or DigitalOcean) Bucket.","created_at_pretty":"14 February, 2019","published_at_pretty":"18 February, 2019","updated_at_pretty":"28 February, 2019","created_at":"2019-02-14T14:31:19.000-05:00","published_at":"2019-02-18T08:00:00.000-05:00","updated_at":"2019-02-27T23:07:27.000-05:00","meta_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","meta_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","og_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","og_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","twitter_description":"Modify and manipulate thousands of files in your S3 (or Digital Ocean) Bucket with the Boto3 Python SDK.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/botopython-1-4.jpg","twitter_title":"S3 File Management With The Boto3 Python SDK | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"}],"plaintext":"It's incredible the things human beings can adapt to in life-or-death\ncircumstances, isn't it? In this particular case it wasn't my personal life in\ndanger, but rather the life of this very blog. I will allow for a brief pause\nwhile the audience shares gasps of disbelief. We must stay strong and collect\nourselves from such distress.\n\nLike most things I despise, the source of this unnecessary headache was a SaaS\nproduct. I won't name any names here, but it was Cloudinary. Yep, totally them.\nWe'd been using their (supposedly) free service for hosting our blog's images\nfor about a month now. This may be a lazy solution to a true CDN, sure, but\nthere's only so much we can do when well over half of Ghost's 'officially\nrecommended' storage adapters are depreciated or broken. That's a whole other\nthing.\n\nI'll spare the details, but at some point we reached one of the 5 or 6 rate\nlimits on our account which had conveniently gone unmentioned (official\nviolations include storage, bandwidth, lack of galactic credits, and a refusal\nto give up Park Place from the previously famous McDonalds Monopoly game-\nseriously though, why not ask for Broadway)? The terms were simple: pay 100\ndollars of protection money to the sharks a matter of days. Or, ya know, don't.\n\nWeapons Of Mass Content Delivery\nHostage situations aside, the challenge was on: how could move thousands of\nimages to a new CDN within hours of losing all  of our data, or without\nexperiencing significant downtime? Some further complications:\n\n * There’s no real “export” button on Cloudinary. Yes, I know,  they’ve just\n   recently released some rest API that may or may not generate a zip file of a\n   percentage of your files at a time. Great. \n * We’re left with 4-5 duplicates of every image. Every time a transform is\n   applied to an image, it leaves behind unused duplicates.\n * We need to revert to the traditional YYYY/MM folder structure, which was\n   destroyed.\n\nThis is gonna be good. You'd be surprised what can be Macgyvered out of a single\nPython Library and a few SQL queries. Let's focus on Boto3  for now.\n\nBoto3: It's Not Just for AWS Anymore\nDigitalOcean  offers a dead-simple CDN service which just so happens to be fully\ncompatible with Boto3. Let's not linger on that fact too long before we consider\nthe possibility that DO is just another AWS reseller. Moving on.\n\nInitial Configuration\nSetting up Boto3 is simple just as long as you can manage to find your API key\nand secret:\n\nimport json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\nFrom here forward, whenever we need to reference our 'bucket', we do so via \nclient.\n\nFast Cut Back To Our Dramatic Storyline\nIn our little scenario, I took a first stab at populating our bucket as a rough \npass. I created our desired folder structure and tossed everything we owned\nhastily into said folders, mostly by rough guesses and by gauging the publish\ndate of posts. So we've got our desired folder structure, but the content is a \nmess.\n\nCDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n\n\nSo we're dealing with a three-tiered folder hierarchy here. You're probably\nthinking \"oh great, this is where we recap some basics about recursion for the\n1ooth time...\" but you're wrong!  Boto3 deals with the pains of recursion for us\nif we so please. If we were to run client.list_objects_v2()  on the root of our\nbucket, Boto3 would return the file path of every single file in that bucket\nregardless of where it lives.\n\nLetting an untested script run wild and make transformations to your production\ndata sounds like fun and games, but I'm not willing to risk losing the hundreds \nof god damned Lynx pictures I draw every night for a mild sense of amusement.\nInstead, we're going to have Boto3 loop through each folder one at a time so\nwhen our script does  break, it'll happen in a predictable way that we can just\npick back up. I guess that means.... we're pretty much opting into recursion.\nFine, you were right.\n\nThe Art of Retrieving Objects\nRunning client.list_objects_v2()  sure sounded straightforward when I omitted\nall the details, but this method can achieve some quite powerful things for its\nsize. list_objects_v2 is essentially our bread and butter behind this script.\n\"But why list_objects_v2 instead of list_objects,\"  you may ask? I don't know,\nbecause AWS is a bloated shit show? Does Amazon even know? Why don't we ask\ntheir documentation?\n\nWell that explains... Nothing.Well, I'm sure list_objects had a vulnerability or something. Surely it's been\nsunsetted by now. Anything else just wouldn't make any sense.\n\n...Oh. It's right there. Next to version 2.That's the last time I'll mention\nthat AWS sucks in this post... I promise.\n\nGetting All Folders in a Subdirectory\nTo humor you, let's see what getting all objects in a bucket would look like:\n\ndef get_everything_ever():\n    \"\"\"Retrieve all folders underneath the specified directory.\"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n\n\nWe've passed pretty much nothing meaningful to list_objects_v2(), so it will\ncome back to us with every file, folder, woman and child it can find in your\npoor bucket with great vengeance and furious anger:\n\noh god oh god oh godHere, I'll even be fair and only return the file names/paths\ninstead of each object:\n\nAh yes, totally reasonable for thousands of files.Instead, we'll solve this like\nGentlemen. Oh, but first, let's clean those god-awful strings being returned as\nkeys. That simply won't do, so build yourself a function. We'll need it.\n\nfrom urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\nThat's better.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''\n\nCheck out list_objects_v2()  this time. We restrict listing objects to the\ndirectory we want: posts/. By further specifying Delimiter='/', we're asking for\nfolders to be returned only. This gives us a nice list of folders to walk\nthrough, one by one.\n\nShit's About to go Down\nWe're about to get complex here and we haven't even created an entry point yet.\nHere's the deal below:\n\n * get_folders()  gets us all folders within the base directory we're interested\n   in.\n * For each folder, we loop through the contents of each folder via the \n   get_objects_in_folder()  function.\n * Because Boto3 can be janky, we need to format the string coming back to us as\n   \"keys\", also know as the \"absolute paths to each object\". We use the unquote \n   feature in sanitize_object_key()  quite often to fix this and return workable\n   file paths.\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''\n\nRECAP\nAll of this until now has been neatly assembled groundwork. Now that we have the\npower to quickly and predictably loop through every file we want, we can finally\nstart to fuck some shit up.\n\nOur Script's Core Logic\nNot every transformation I chose to apply to my images will be relevant to\neverybody; instead, let's take a look at our completed script, and I'll let you\ndecide which snippets you'd like to drop in for yourself!\n\nHere's our core script that successfully touches every desired object in our\nbucket, without applying any logic just yet:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n\n\nThere we have it: the heart of our script. Now let's look at a brief catalog of\nwhat we could potentially do here.\n\nChoose Your Own Adventure\nPurge Files We Know Are Trash\nThis is an easy one. Surely your buckets get bloated with unused garbage over\ntime... in my example, I somehow managed to upload a bunch of duplicate images\nfrom my Dropbox, all with the suffix  (Todds-MacBook-Pro.local's conflicted copy\nYYYY-MM-DD). Things like that can be purged easily:\n\ndef purge_unwanted_objects(item):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=item)\n        return True\n    return False\n\n\nDownload CDN Locally\nIf we want to apply certain image transformations, it could be a good idea to\nback up everything in our CDN locally. This will save all objects in our CDN to\na relative path which matches the folder hierarchy of our CDN; the only catch is\nwe need to make sure those folders exist prior to running the script:\n\n...\nimport botocore\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\nCreate Retina Images\nWith the Retina.js  plugin, serving any image of filename x.jpg  will also look\nfor a corresponding file name x@2x.jpg  to serve on Retina devices. Because our\nimages are exported as high-res, all we need to do is write a function to copy\neach image and modify the file name:\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\nCreate Standard Resolution Images\nBecause we started with high-res images and copied them, we can now scale down\nour original images to be normal size. resize_width()  is a method of the \nresizeimage  library which scales the width of an image while keeping the\nheight-to-width aspect ratio in-tact. There's a lot happening below, such as\nusing io  to 'open' our file without actually downloading it, etc:\n\n...\nimport PIL\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\nUpload Local Images\nAfter modifying our images locally, we'll need to upload the new images to our\nCDN:\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\nPut It All Together\nThat should be enough to get your imagination running wild. What does all of\nthis look like together?:\n\nimport os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    \"\"\"Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using \"Prefix\" attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    \"\"\"\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    \"\"\"List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to \"folderpath\" parameter.\n    4. Return list of objects in folder.\n    \"\"\"\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    \"\"\"Replace character encodings with actual characters.\"\"\"\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    \"\"\"Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    \"\"\"\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    \"\"\"Rename our file to specify that it is a Retina image.\n\n    1. Insert \"@2x\" at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    \"\"\"\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(\"created: \", newname)\n\n\ndef create_standard_res_image(obj):\n    \"\"\"Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    \"\"\"\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    \"\"\"Upload standard def images created locally.\"\"\"\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    \"\"\"Delete item from bucket if name meets criteria.\"\"\"\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=\"hackers\", Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) < 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    \"\"\"Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    \"\"\"\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n\n\nWell that's a doozy.\n\nIf you feel like getting creative, there's even more you can do to optimize the\nassets in your bucket or CDN. For example: grabbing each image and rewriting the\nfile in WebP format. I'll let you figure that one out on your own.\n\nAs always, the source for this can be found on Github\n[https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36].","html":"<p>It's incredible the things human beings can adapt to in life-or-death circumstances, isn't it? In this particular case it wasn't my personal life in danger, but rather the life of this very blog. I will allow for a brief pause while the audience shares gasps of disbelief. We must stay strong and collect ourselves from such distress.</p><p>Like most things I despise, the source of this unnecessary headache was a SaaS product. I won't name any names here, but it was Cloudinary. Yep, totally them. We'd been using their (supposedly) free service for hosting our blog's images for about a month now. This may be a lazy solution to a true CDN, sure, but there's only so much we can do when well over half of Ghost's 'officially recommended' storage adapters are depreciated or broken. That's a whole other thing.</p><p>I'll spare the details, but at some point we reached one of the 5 or 6 rate limits on our account which had conveniently gone unmentioned (official violations include storage, bandwidth, lack of galactic credits, and a refusal to give up Park Place from the previously famous McDonalds Monopoly game- seriously though, why not ask for Broadway)? The terms were simple: pay 100 dollars of protection money to the sharks a matter of days. Or, ya know, don't.</p><h2 id=\"weapons-of-mass-content-delivery\">Weapons Of Mass Content Delivery</h2><p>Hostage situations aside, the challenge was on: how could move thousands of images to a new CDN within hours of losing <em>all</em> of our data, or without experiencing significant downtime? Some further complications:</p><ul><li>There’s no real “export” button on Cloudinary. <em>Yes, I know,</em> they’ve just recently released some rest API that may or may not generate a zip file of a percentage of your files at a time. Great. </li><li>We’re left with 4-5 duplicates of every image. Every time a transform is applied to an image, it leaves behind unused duplicates.</li><li>We need to revert to the traditional YYYY/MM folder structure, which was destroyed.</li></ul><p>This is gonna be good. You'd be surprised what can be Macgyvered out of a single Python Library and a few SQL queries. Let's focus on <strong>Boto3</strong> for now.</p><h2 id=\"boto3-it-s-not-just-for-aws-anymore\">Boto3: It's Not Just for AWS Anymore</h2><p><strong>DigitalOcean</strong> offers a dead-simple CDN service which just so happens to be fully compatible with Boto3. Let's not linger on that fact too long before we consider the possibility that DO is just another AWS reseller. Moving on.</p><h3 id=\"initial-configuration\">Initial Configuration</h3><p>Setting up Boto3 is simple just as long as you can manage to find your API key and secret:</p><pre><code class=\"language-python\">import json\nimport boto3\nfrom botocore.client import Config\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n</code></pre>\n<p>From here forward, whenever we need to reference our 'bucket', we do so via <code>client</code>.</p><h3 id=\"fast-cut-back-to-our-dramatic-storyline\">Fast Cut Back To Our Dramatic Storyline</h3><p>In our little scenario, I took a first stab at populating our bucket as a <em><strong>rough </strong></em>pass. I created our desired folder structure and tossed everything we owned hastily into said folders, mostly by rough guesses and by gauging the publish date of posts. So we've got our desired folder structure, but the content is a <strong>mess</strong>.</p><pre><code class=\"language-shell\">CDN\n├── Posts\n│   ├── /2017\n│   │   ├── 11\n│   ├── /2018\n│   │   ├── 03\n│   │   ├── 04\n│   │   ├── 05\n│   │   ├── 06\n│   │   ├── 07\n│   │   ├── 08\n│   │   ├── 09\n│   │   ├── 10\n│   │   ├── 11\n│   │   └── 12\n│   ├── /2019\n│   │   ├── 01\n│   │   └── 02\n│   └── /lynx\n├── /bunch\n├── /of\n├── /other\n└── /shit\n</code></pre>\n<p>So we're dealing with a three-tiered folder hierarchy here. You're probably thinking \"oh great, this is where we recap some basics about recursion for the 1ooth time...\" but you're <strong>wrong!</strong> Boto3 deals with the pains of recursion for us if we so please. If we were to run <code>client.list_objects_v2()</code> on the root of our bucket, Boto3 would return the file path of every single file in that bucket regardless of where it lives.</p><p>Letting an untested script run wild and make transformations to your production data sounds like fun and games, but I'm not willing to risk losing the <em>hundreds</em> of god damned Lynx pictures I draw every night for a mild sense of amusement. Instead, we're going to have Boto3 loop through each folder one at a time so when our script <em>does</em> break, it'll happen in a predictable way that we can just pick back up. I guess that means.... we're pretty much opting into recursion. Fine, you were right.</p><h2 id=\"the-art-of-retrieving-objects\">The Art of Retrieving Objects</h2><p>Running <code>client.list_objects_v2()</code> sure sounded straightforward when I omitted all the details, but this method can achieve some quite powerful things for its size. <strong>list_objects_v2 </strong>is essentially our bread and butter behind this script. \"But why <strong>list_objects_v2 </strong>instead of <strong>list_objects,\"</strong> you may ask? I don't know, because AWS is a bloated shit show? Does Amazon even know? Why don't we ask their documentation?</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.png\" class=\"kg-image\"><figcaption>Well that explains... Nothing.</figcaption></figure><p>Well, I'm sure <strong>list_objects </strong>had a vulnerability or something. Surely it's been sunsetted by now. Anything else just wouldn't make any sense.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/list_objects_2.gif\" class=\"kg-image\"><figcaption>...Oh. It's right there. Next to version 2.</figcaption></figure><p>That's the last time I'll mention that AWS sucks in this post... I promise.</p><h3 id=\"getting-all-folders-in-a-subdirectory\">Getting All Folders in a Subdirectory</h3><p>To humor you, let's see what getting all objects in a bucket would look like:</p><pre><code class=\"language-python\">def get_everything_ever():\n    &quot;&quot;&quot;Retrieve all folders underneath the specified directory.&quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n      )\n</code></pre>\n<p>We've passed pretty much nothing meaningful to <code>list_objects_v2()</code>, so it will come back to us with every file, folder, woman and child it can find in your poor bucket with great vengeance and furious anger:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/allthethings.gif\" class=\"kg-image\"><figcaption>oh god oh god oh god</figcaption></figure><p>Here, I'll even be fair and only return the file names/paths instead of each object:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/keys.gif\" class=\"kg-image\"><figcaption>Ah yes, totally reasonable for thousands of files.</figcaption></figure><p>Instead, we'll solve this like Gentlemen. Oh, but first, let's clean those god-awful strings being returned as keys. That simply won't do, so build yourself a function. We'll need it.</p><pre><code class=\"language-python\">from urllib.parse import unquote # omg new import\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n</code></pre>\n<p>That's better.</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n '''</code></pre>\n<p>Check out <code>list_objects_v2()</code> this time. We restrict listing objects to the directory we want: <code>posts/</code>. By further specifying <code>Delimiter='/'</code>, we're asking for folders to be returned only. This gives us a nice list of folders to walk through, one by one.</p><h2 id=\"shit-s-about-to-go-down\">Shit's About to go Down</h2><p>We're about to get complex here and we haven't even created an entry point yet. Here's the deal below:</p><ul><li><code>get_folders()</code> gets us all folders within the base directory we're interested in.</li><li>For each folder, we loop through the contents of each folder via the <code>get_objects_in_folder()</code> function.</li><li>Because Boto3 can be janky, we need to format the string coming back to us as \"keys\", also know as the \"absolute paths to each object\". We use the <code>unquote</code> feature in <code>sanitize_object_key()</code> quite often to fix this and return workable file paths.</li></ul><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n '''</code></pre>\n<h3 id=\"recap\">RECAP</h3><p>All of this until now has been neatly assembled groundwork. Now that we have the power to quickly and predictably loop through every file we want, we can finally start to fuck some shit up.</p><h2 id=\"our-script-s-core-logic\">Our Script's Core Logic</h2><p>Not every transformation I chose to apply to my images will be relevant to everybody; instead, let's take a look at our completed script, and I'll let you decide which snippets you'd like to drop in for yourself!</p><p>Here's our core script that successfully touches every desired object in our bucket, without applying any logic just yet:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nfrom botocore\nfrom urllib.parse import unquote\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Logic TBD.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    # OUR APP LOGIC WILL GO HERE\n\n\noptimize_cdn_objects()\n</code></pre>\n<p>There we have it: the heart of our script. Now let's look at a brief catalog of what we could potentially do here.</p><h2 id=\"choose-your-own-adventure\">Choose Your Own Adventure</h2><h3 id=\"purge-files-we-know-are-trash\">Purge Files We Know Are Trash</h3><p>This is an easy one. Surely your buckets get bloated with unused garbage over time... in my example, I somehow managed to upload a bunch of duplicate images from my Dropbox, all with the suffix<strong> (Todds-MacBook-Pro.local's conflicted copy YYYY-MM-DD)</strong>. Things like that can be purged easily:</p><pre><code class=\"language-python\">def purge_unwanted_objects(item):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx']\n    if any(x in item for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=item)\n        return True\n    return False\n</code></pre>\n<h3 id=\"download-cdn-locally\">Download CDN Locally</h3><p>If we want to apply certain image transformations, it could be a good idea to back up everything in our CDN locally. This will save all objects in our CDN to a relative path which matches the folder hierarchy of our CDN; the only catch is we need to make sure those folders exist prior to running the script:</p><pre><code class=\"language-python\">...\nimport botocore\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n</code></pre>\n<h3 id=\"create-retina-images\">Create Retina Images</h3><p>With the <strong>Retina.js</strong> plugin, serving any image of filename <code>x.jpg</code> will also look for a corresponding file name <code>x@2x.jpg</code> to serve on Retina devices. Because our images are exported as high-res, all we need to do is write a function to copy each image and modify the file name:</p><pre><code class=\"language-python\">def create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n</code></pre>\n<h3 id=\"create-standard-resolution-images\">Create Standard Resolution Images</h3><p>Because we started with high-res images and copied them, we can now scale down our original images to be normal size. <code>resize_width()</code> is a method of the <code>resizeimage</code> library which scales the width of an image while keeping the height-to-width aspect ratio in-tact. There's a lot happening below, such as using <code>io</code> to 'open' our file without actually downloading it, etc:</p><pre><code class=\"language-python\">...\nimport PIL\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n</code></pre>\n<h3 id=\"upload-local-images\">Upload Local Images</h3><p>After modifying our images locally, we'll need to upload the new images to our CDN:</p><pre><code class=\"language-python\">def upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n</code></pre>\n<h2 id=\"put-it-all-together\">Put It All Together</h2><p>That should be enough to get your imagination running wild. What does all of this look like together?:</p><pre><code class=\"language-python\">import os\nimport json\nimport boto3\nfrom botocore.client import Config\nimport botocore\nfrom urllib.parse import unquote\nimport PIL\n\n\n# Initialize a session using DigitalOcean Spaces.\nsession = boto3.session.Session()\nclient = session.client('s3',\n                        region_name='nyc3',\n                        endpoint_url='https://nyc3.digitaloceanspaces.com',\n                        aws_access_key_id=os.environ.get('KEY'),\n                        aws_secret_access_key=os.environ.get('SECRET'))\n\n\ndef get_folders():\n    &quot;&quot;&quot;Retrieve all folders within a specified directory.\n\n    1. Set bucket name.\n    2. Set delimiter (a character that our target files have in common).\n    3. Set folder path to objects using &quot;Prefix&quot; attribute.\n    4. Create list of all recursively discovered folder names.\n    5. Return list of folders.\n    &quot;&quot;&quot;\n    get_folder_objects = client.list_objects_v2(\n        Bucket='hackers',\n        Delimiter='',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix='posts/',\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    folders = [item['Key'] for item in get_folder_objects['Contents']]\n    return folders\n\n\ndef get_objects_in_folder(folderpath):\n    &quot;&quot;&quot;List all objects in the provided directory.\n\n    1. Set bucket name.\n    2. Leave delimiter blank to fetch all files.\n    3. Set folder path to &quot;folderpath&quot; parameter.\n    4. Return list of objects in folder.\n    &quot;&quot;&quot;\n    objects = client.list_objects_v2(\n        Bucket='hackers',\n        EncodingType='url',\n        MaxKeys=1000,\n        Prefix=folderpath,\n        ContinuationToken='',\n        FetchOwner=False,\n        StartAfter=''\n        )\n    return objects\n\n\ndef sanitize_object_key(obj):\n    &quot;&quot;&quot;Replace character encodings with actual characters.&quot;&quot;&quot;\n    new_key = unquote(unquote(obj))\n    return new_key\n\n\ndef save_images_locally(obj):\n    &quot;&quot;&quot;Download target object.\n\n    1. Try downloading the target object.\n    2. If image doesn't exist, throw error.\n    &quot;&quot;&quot;\n    try:\n        client.download_file(Key=obj, Filename=obj, Bucket='hackers')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == &quot;404&quot;:\n            print(&quot;The object does not exist.&quot;)\n        else:\n            raise\n\n\ndef create_retina_image(item):\n    &quot;&quot;&quot;Rename our file to specify that it is a Retina image.\n\n    1. Insert &quot;@2x&quot; at end of filename.\n    2. Copy original image with new filename.\n    3. Keep both files as per retina.js.\n    &quot;&quot;&quot;\n    indx = item.index('.')\n    newname = item[:indx] + '@2x' + item[indx:]\n    newname = sanitize_object_key(newname)\n    client.copy_object(Bucket='hackers',\n                       CopySource='hackers/' + item,\n                       Key=newname)\n    print(&quot;created: &quot;, newname)\n\n\ndef create_standard_res_image(obj):\n    &quot;&quot;&quot;Resizes large images to an appropriate size.\n\n    1. Set maximum bounds for standard-def image.\n    2. Get the image file type.\n    3. Open the local image.\n    4. Resize image.\n    5. Save the file locally.\n    &quot;&quot;&quot;\n    size = 780, 2000\n    indx = obj.index('/')\n    filename = obj[indx:]\n    filetype = filename.split('.')[-1].upper()\n    filetype = filetype.replace('JPG', 'JPEG')\n    outfile = obj.replace('@2x', '')\n    print('created ', outfile, ' locally with filetype ', filetype)\n    # Use PIL to resize image\n    img = PIL.Image.open(obj)\n    img.thumbnail(size, PIL.Image.ANTIALIAS)\n    img.save(outfile, filetype, optimize=True, quality=100)\n\n\ndef upload_local_images(obj):\n    &quot;&quot;&quot;Upload standard def images created locally.&quot;&quot;&quot;\n    if '@2x' in obj:\n        outfile = obj.replace('@2x', '')\n        client.upload_file(Filename=outfile,\n                           Bucket='hackers',\n                           Key=outfile)\n        print('uploaded: ', outfile)\n\n\ndef purge_unwanted_objects(obj):\n    &quot;&quot;&quot;Delete item from bucket if name meets criteria.&quot;&quot;&quot;\n    banned = ['Todds-iMac', 'conflicted', 'Lynx', 'psd', 'lynx']\n    if any(x in obj for x in banned):\n        client.delete_object(Bucket=&quot;hackers&quot;, Key=obj)\n        return True\n    # Determine if image is Lynx post\n    filename = obj.split('/')[-1]\n    if len(filename) &lt; 7:\n        sample = filename[:2]\n        if int(sample):\n            print(int(sample))\n    return False\n\n\ndef optimize_cdn_objects():\n    &quot;&quot;&quot;Perform tasks on objects in our CDN.\n\n    1. Loop through folders in subdirectory.\n    2. In each folder, loop through all objects.\n    3. Sanitize object key name.\n    3. Remove 'garbage' files by recognizing what substrings they have.\n    4. If file not deleted, check to see if file is an image (search for '.')\n    5. Rename image to be retina compatible.\n    6. Save image locally.\n    7. Create standard resolution version of image locally.\n    8. Upload standard resolution images to CDN.\n    &quot;&quot;&quot;\n    for folder in get_folders():\n        folderpath = sanitize_object_key(folder)\n        objects = get_objects_in_folder(folderpath)\n        for obj in objects['Contents']:\n            item = sanitize_object_key(obj['Key'])\n            purged = purge_unwanted_objects(item)\n            if not purged:\n                if '.' in item:\n                    create_standard_res_image(item)\n                    save_images_locally(item)\n                    create_retina_image(item)\n                    upload_local_images(item)\n\n\noptimize_cdn_objects()\n\n</code></pre>\n<p>Well that's a doozy.</p><p>If you feel like getting creative, there's even more you can do to optimize the assets in your bucket or CDN. For example: grabbing each image and rewriting the file in WebP format. I'll let you figure that one out on your own.</p><p>As always, the source for this can be found on <a href=\"https://gist.github.com/toddbirchard/cf88c9347397f265ebe0d8c7d2150b36\">Github</a>.</p>","url":"https://hackersandslackers.com/manage-s3-assests-with-boto3-python-sdk/","uuid":"56141448-0264-4d77-8fc8-a24f3d271493","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c65c207042dc633cf14a610"}},{"node":{"id":"Ghost__Post__5c570ae30b20340296f57709","title":"Easily Build GraphQL APIs with Prisma","slug":"easily-build-graphql-apis-with-prisma","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","custom_excerpt":"Jump on the GraphQL Bandwagon with a little help from Prisma.","created_at_pretty":"03 February, 2019","published_at_pretty":"03 February, 2019","updated_at_pretty":"29 March, 2019","created_at":"2019-02-03T10:38:11.000-05:00","published_at":"2019-02-03T16:33:15.000-05:00","updated_at":"2019-03-29T14:47:01.000-04:00","meta_title":"Build GraphQL APIs with Prisma | Hackers and Slackers","meta_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","og_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","og_title":"Build GraphQL APIs with Prisma","twitter_description":"Embrace GraphQL by leveraging Prisma: a free service which generates a GraphQL API atop any database.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/prisma2-1.jpg","twitter_title":"Build GraphQL APIs with Prisma","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},"tags":[{"name":"GraphQL","slug":"graphql","description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","feature_image":null,"meta_description":"Ditch REST endpoints and build APIs that make sense with your workflow. Get started with Prisma or Apollo toolkits, and join the GraphQL bandwagon.","meta_title":"Build a GraphQL API | Hackers and Slackers","visibility":"public"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"NodeJS","slug":"nodejs","description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","feature_image":null,"meta_description":"All things related to backend JavaScript. Learn frameworks or take our word for selecting the right NPM packages.","meta_title":"NodeJS | Hackers and Slackers","visibility":"public"},{"name":"#GraphQL Hype","slug":"graphql-hype","description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","feature_image":"https://hackersandslackers.com/content/images/2019/03/graphqlseries.jpg","meta_description":"Learn GraphQL syntax and see the reasons why the future of APIs is here to stay. We walk through everything from server/client setup to intricate tricks.","meta_title":"GraphQL Hype","visibility":"internal"}],"plaintext":"The technology sector is reeling after an official statement was released by the\nUN's International Council of Coolness last week. The statement clearly states\nwhat status-quo developers have feared for months: if you haven't shifted from\nREST to GraphQL by now, you are officially recognized by the international\ncommunity to hold \"uncool\" status. A humanitarian crisis is already unfolding as\nrefugees of coolness are threatening to overtake borders, sparking fears of an\ninflux of Thinkpad Laptops, IntelliJ, and other Class A  uncool narcotics.\n\nHold up: is GraphQL That Dramatic of an Improvement over REST?\nIn all honesty, I've found that the only way to properly answer this question is\nto first utter \"kinda,\" then mull back and forth for a little while, and then\nfinishing with a weak statement like \"so pretty much, yeah.\"\n\nLet’s put it this way. When you’re first familiarizing yourself with a set of\ndata, what do you do? Do you read extensive documentation about the SQL table\nyou’re about to check out? Do you read the entire spec for your version\nPostgreSQL to see if it contains the functionality that might be missing for\nsome reason? I’m going to guess you do neither of these- chances are you just\nlook at the data. \n\nUsing any REST API is inherently a context-switch. No matter how many APIs\nyou’ve worked with in the past, you’ll never be able to know a new API’s\nendpoints, quirks, or the awful manner in which the creator has abandoned any\ndistinction between GET, POST, or PUT methods altogether. GraphQL is not\nnecessarily more technologically impressive than REST, but it does  provide us a\nsyntax and workflow comparable to working directly with databases with which\nwe're already familiar.\n\nRemember when us young guys justified replacing older devs when we came out of\nthe gate with NodeJS, arguing that context-switching changes everything? GraphQL\nis just that: a \"better\" technology with less mental context-switching, which\nconveniently serves a double-purpose for enterprises looking to fire anybody\nthey perceive to be dead weight over the age of 30. Good luck finding a better\nsynopsis than that.\n\nWhat’s this Prisma Nonsense? \nPrisma [https://www.prisma.io/]  is a free (FREE!) service that provides with\nthe tools to create an API client, as well as an Admin panel to manage it.\nWithout any prior knowledge of GraphQL needed, Prisma provides us with:\n\n * A CLI which’s stand up a web server which will serve as our API: either cloud\n   or self-hosted.\n * Automatic integration with your database of choice (including cloud DBs, such\n   as RDS).\n * A clever obfuscation of data models via a simple config file. No classes, no\n   code, no bullshit.\n * A \"playground\" interface which allows us to mess around in GraphQL syntax\n   against our models without breaking everything.\n * A web GUI which displays the relationships between all of these things and\n   their usage.\n\nIn short, Prisma does our jobs for us. Now that tasks associated with building\nAPIs, creating ORMs, and managing databases have all been trivialized, we can\nfinally cut some more of that dead weight we mentioned earlier- specifically\nBob, the asshole coming up on his 35th birthday sitting on his high-horse just\nbecause he has an obligation to feed 3 beautiful children. Sorry Bob, it just\nwasn't working out.\n\nPrisma does  provide the option to set up a test environment on their cloud, but\nlet's do something useful with our lives for once and build something\nproduction-ready. In this case, that means standing up a 5-dollar Digital Ocean\nDroplet.\n\nCreate a Prisma Account\nGet over to the sexually appealing Prisma Cloud landing page\n[https://www.prisma.io/cloud]  and make yourself an account. When prompted, make\nsure you select Deploy a new Prisma Service.\n\nExample services are for sissys.You should then be prompted with the following\nscreen. It will instruct you to install an NPM package, but there are a few\nthings we need to do first.\n\nWhen she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my\nlove.\"Installing Prisma Dependencies on a Fresh VPS\nSSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this\ntutorial. If you happen to be using Ubuntu as well, feel free to copy + paste\nall the stuff I'm sure you've done a million times already. First, we need to\ninstall Node:\n\n$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n\n\nBefore you do anything crazy like copy & paste those two lines from Prisma,\nyou're going to need to set up Docker a few steps later, so you might as well do\nthat now: \n\n1. Install Docker Dependencies\n$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n\n2. Add Docker Key\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n\n\n3. Get Docker Repository\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\n\n4. Finally Install Docker\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGood job, you're doing great.\n\nInstall & Activate The Prisma CLI\nCool, now we can carry on with Prisma's demands. Install the Prisma CLI\nglobally, and then use said CLI to log in to Prisma.\n\n$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n\n\nWait a couple of seconds after entering the login prompt, and you'll notice your\nbrowser window will have changed to indicate that you're now logged in. \n\nThe next step will create the local files which serve as the heart and soul of\nour API. Make sure you init Prisma  in whichever directory you like to keep\nthings in:\n\n$ cd /my/desired/directory/\n$ prisma init my-prisma\n\n\nInitiating the project will kickstart a quick and painless interrogation\nprocess. Keep in mind that it's recommended to use Prisma with a fresh database\ninstance; in my case, I spun up a cloud PostgreSQL instance.\n\n? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n\n\nCompleting this will result in the following structure:\n\nmy-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n\n\nWe're almost there cowboy and/or cowgirl. \n\nSet Phasers to \"Deploy\"\nPrisma is going to stand itself up on port 4466, which is closed by default on\nmost servers. Make sure you have this port open:\n\n$ ufw allow 4466\n\n\nFinally, we need to set a secret  in order to connect to Prisma cloud. Open the \ndocker-compose.yml  file and uncomment the managementApiSecret  line. Replace\nthe value with some sort of deep dark personal secret of yours.\n\n$ vim docker-compose.yml\n\n\nversion: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - \"4466:4466\"\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n\n\nFor some reason, Prisma does not automatically specify your SSL preferences in\ndocker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your\ndatabase requires SSL, be sure to add ssl: true  to the end of your\ndocker-compose config. Otherwise, your deployment will fail.As per the comment\nin our yml  file, we need to export the secret we specify with \nmanagementApiSecret: my-secret  as an environment variable. Back in your Prisma\ndirectory, export your secret as such:\n\n$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n\n\nThis secret is used to generate a token to secure our endpoint. Skipping this\nstep would result in exposing your database to the world with full read/write\naccess to anybody.\n\nIt's Game Time\nIt's time to deploy, baby! Do it, push the button! DO IT NOW!\n\n$ docker-compose up -d\n$ prisma deploy\n\n\nDeploying for the first time does a few things. It'll stand up a 'playground'\ninterface on your local server (localhost:4466), as well as automatically get\nyou set up with Prisma Cloud, which is essentially an admin interface for your\ndeployment hosted on Prisma's site.\n\nCheck Out Your Workspace \nVisit [Your Server's IP]:4466 to see what you've done:\n\nA playground for children of all agesCheck it out! Along with documentation of\nthe generic data models Prisma shipped with, you can test queries or mutations\non the left side of the UI, and receive responses on the right. Sure beats\nPostman imho.\n\nDon't Look Down: You're in the Cloud\nYou can now add your server to Prisma Cloud to get the benefits of their admin\npanel. From here, you can modify information directly, review usage metrics, and\nmanage multiple instances:\n\nBreaking News: Prisma is Too Cool For School.Working With Prisma And GraphQL\nNow that we've spun up this shiny new toy, let's be sure we know how to drive\nit.\n\nOn your VPS, take a look at the datamodels.prisma  file:\n\n$ vim datamodels.prisma\n\n\nYou should see a data model called User (everybody has this model). To add or\nmodify data models, all we need to do is change the fields as we see fit, set\ntheir data type, and specify whether or not we'd like the field to be unique.\nBelow I've added a couple of new 'fields.'\n\ntype User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n\n\nDeploying Prisma again with these changes will modify our database's table\nstructure to match the new model:\n\n$ prisma deploy\n\n\nThere you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to\ncompletely falsify the existence of a GraphQL certification and throw that on\nthere, too. If you're the kind of person who enjoys reading technical posts like\nthis in your free time, chances are you're already qualified for the job. Unless\nyou're Bob.","html":"<p>The technology sector is reeling after an official statement was released by the UN's International Council of Coolness last week. The statement clearly states what status-quo developers have feared for months: if you haven't shifted from REST to GraphQL by now, you are officially recognized by the international community to hold \"uncool\" status. A humanitarian crisis is already unfolding as refugees of coolness are threatening to overtake borders, sparking fears of an influx of Thinkpad Laptops, IntelliJ, and other <em>Class A</em> uncool narcotics.</p><h3 id=\"hold-up-is-graphql-that-dramatic-of-an-improvement-over-rest\">Hold up: is GraphQL That Dramatic of an Improvement over REST?</h3><p>In all honesty, I've found that the only way to properly answer this question is to first utter \"kinda,\" then mull back and forth for a little while, and then finishing with a weak statement like \"so pretty much, yeah.\"</p><p>Let’s put it this way. When you’re first familiarizing yourself with a set of data, what do you do? Do you read extensive documentation about the SQL table you’re about to check out? Do you read the entire spec for your version PostgreSQL to see if it contains the functionality that might be missing for some reason? I’m going to guess you do neither of these- chances are you <em>just look at the data. </em></p><p>Using any REST API is inherently a context-switch. No matter how many APIs you’ve worked with in the past, you’ll never be able to know a new API’s endpoints, quirks, or the awful manner in which the creator has abandoned any distinction between GET, POST, or PUT methods altogether. GraphQL is not necessarily more technologically impressive than REST, but it <em>does</em> provide us a syntax and workflow comparable to working directly with databases with which we're already familiar.</p><p>Remember when us young guys justified replacing older devs when we came out of the gate with NodeJS, arguing that context-switching <em>changes everything</em>? GraphQL is just that: a \"better\" technology with less mental context-switching, which conveniently serves a double-purpose for enterprises looking to fire anybody they perceive to be dead weight over the age of 30. Good luck finding a better synopsis than that.</p><h2 id=\"what-s-this-prisma-nonsense\">What’s this Prisma Nonsense? </h2><p><a href=\"https://www.prisma.io/\">Prisma</a> is a free (FREE!) service that provides with the tools to create an API client, as well as an Admin panel to manage it. Without any prior knowledge of GraphQL needed, Prisma provides us with:</p><ul><li>A CLI which’s stand up a web server which will serve as our API: either cloud or self-hosted.</li><li>Automatic integration with your database of choice (including cloud DBs, such as RDS).</li><li>A clever obfuscation of data models via a simple config file. No classes, no code, no bullshit.</li><li>A \"playground\" interface which allows us to mess around in GraphQL syntax against our models without breaking everything.</li><li>A web GUI which displays the relationships between all of these things and their usage.</li></ul><p>In short, Prisma does our jobs for us. Now that tasks associated with building APIs, creating ORMs, and managing databases have all been trivialized, we can finally cut some more of that dead weight we mentioned earlier- specifically Bob, the asshole coming up on his 35th birthday sitting on his high-horse just because he has an obligation to feed 3 beautiful children. Sorry Bob, it just wasn't working out.</p><p>Prisma <em>does</em> provide the option to set up a test environment on their cloud, but let's do something useful with our lives for once and build something production-ready. In this case, that means standing up a 5-dollar Digital Ocean Droplet.</p><h3 id=\"create-a-prisma-account\">Create a Prisma Account</h3><p>Get over to the sexually appealing <a href=\"https://www.prisma.io/cloud\">Prisma Cloud landing page</a> and make yourself an account. When prompted, make sure you select <strong>Deploy a new Prisma Service</strong>.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step1.png\" class=\"kg-image\"><figcaption>Example services are for sissys.</figcaption></figure><!--kg-card-end: image--><p>You should then be prompted with the following screen. It will instruct you to install an NPM package, but there are a few things we need to do first.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prisma-step2.png\" class=\"kg-image\"><figcaption>When she says \"waiting for login,\" she means \"I'd wait a lifetime for you, my love.\"</figcaption></figure><!--kg-card-end: image--><h2 id=\"installing-prisma-dependencies-on-a-fresh-vps\">Installing Prisma Dependencies on a Fresh VPS</h2><p>SSH into whichever VPS you've chosen. I'll be using a Ubuntu instance for this tutorial. If you happen to be using Ubuntu as well, feel free to copy + paste all the stuff I'm sure you've done a million times already. First, we need to install Node:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ apt update\n$ apt upgrade -y\n$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n$ sudo apt-get install -y nodejs\n$ sudo apt-get install gcc g++ make\n$ sudo npm install -g npm@latest\n</code></pre>\n<!--kg-card-end: markdown--><p>Before you do anything crazy like copy &amp; paste those two lines from Prisma, you're going to need to set up Docker a few steps later, so you might as well do that now: </p><h3 id=\"1-install-docker-dependencies\">1. Install Docker Dependencies</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"2-add-docker-key\">2. Add Docker Key</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n$ sudo apt-key fingerprint 0EBFCD88\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"3-get-docker-repository\">3. Get Docker Repository</h3><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ sudo add-apt-repository \\\n   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable&quot;\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"4-finally-install-docker\">4. Finally Install Docker</h3><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre>\n<!--kg-card-end: markdown--><p>Good job, you're doing great.</p><h2 id=\"install-activate-the-prisma-cli\">Install &amp; Activate The Prisma CLI</h2><p>Cool, now we can carry on with Prisma's demands. Install the Prisma CLI globally, and then use said CLI to log in to Prisma.  </p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ npm install -g prisma\n$ prisma login -k eyJhbGciGYU78tfuyLALALTHISKEYISFAKELOL69KFGs\n</code></pre>\n<!--kg-card-end: markdown--><p>Wait a couple of seconds after entering the login prompt, and you'll notice your browser window will have changed to indicate that you're now logged in. </p><p>The next step will create the local files which serve as the heart and soul of our API. Make sure you init <code>Prisma</code> in whichever directory you like to keep things in:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">$ cd /my/desired/directory/\n$ prisma init my-prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>Initiating the project will kickstart a quick and painless interrogation process. Keep in mind that it's recommended to use Prisma with a fresh database instance; in my case, I spun up a cloud PostgreSQL instance.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">? Set up a new Prisma server or deploy to an existing server? Use existing database\n? What kind of database do you want to deploy to?:\n? Does your database contain existing data?:\n? Enter database host:\n? Enter database port:\n? Enter database user:\n? Enter database password: \n? Enter database name (the database includes the schema):\n? Use SSL?:\n</code></pre>\n<!--kg-card-end: markdown--><p>Completing this will result in the following structure:</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">my-prisma\n├── datamodel.prisma\n├── docker-compose.yml\n├── generated\n│   └── prisma-client\n│       ├── index.ts\n│       └── prisma-schema.ts\n└── prisma.yml\n</code></pre>\n<!--kg-card-end: markdown--><p>We're almost there cowboy and/or cowgirl. </p><h2 id=\"set-phasers-to-deploy\">Set Phasers to \"Deploy\"</h2><p>Prisma is going to stand itself up on port <strong>4466</strong>, which is closed by default on most servers. Make sure you have this port open:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ ufw allow 4466\n</code></pre>\n<!--kg-card-end: markdown--><p>Finally, we need to set a <em>secret</em> in order to connect to Prisma cloud. Open the <code>docker-compose.yml</code> file and uncomment the <code>managementApiSecret</code> line. Replace the value with some sort of deep dark personal secret of yours.</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim docker-compose.yml\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">version: '3'\nservices:\n  prisma:\n    image: prismagraphql/prisma:1.25\n    restart: always\n    ports:\n    - &quot;4466:4466&quot;\n    environment:\n      PRISMA_CONFIG: |\n        port: 4466\n        # uncomment the next line and provide the env var \n        managementApiSecret: my-secret\n        databases:\n          default:\n            connector: postgres\n            host: 123.45.678.90\n            database: databasename\n            user: username\n            password: password\n            rawAccess: true\n            port: '5432'\n            migrations: true\n\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><div class=\"proptip\">\nFor some reason, Prisma does not automatically specify your SSL preferences in docker-compose, even if you explicity answer \"yes\" to the SSL prompt. If your database requires SSL, be sure to add <code>ssl: true</code> to the end of your docker-compose config. Otherwise, your deployment will fail. \n</div><!--kg-card-end: html--><p>As per the comment in our <code>yml</code> file, we need to export the secret we specify with <code>managementApiSecret: my-secret</code> as an environment variable. Back in your Prisma directory, export your secret as such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ export PRISMA_MANAGEMENT_API_SECRET=my-secret\n</code></pre>\n<!--kg-card-end: markdown--><p>This secret is used to generate a token to secure our endpoint. Skipping this step would result in exposing your database to the world with full read/write access to anybody.</p><h2 id=\"it-s-game-time\">It's Game Time</h2><p>It's time to deploy, baby! Do it, push the button! DO IT NOW!</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ docker-compose up -d\n$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying for the first time does a few things. It'll stand up a 'playground' interface on your local server (localhost:4466), as well as automatically get you set up with Prisma Cloud, which is essentially an admin interface for your deployment hosted on Prisma's site.</p><h3 id=\"check-out-your-workspace\">Check Out Your Workspace </h3><p>Visit [Your Server's IP]:4466 to see what you've done:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/posts/2019/02/playground.png\" class=\"kg-image\"><figcaption>A playground for children of all ages</figcaption></figure><!--kg-card-end: image--><p>Check it out! Along with documentation of the generic data models Prisma shipped with, you can test queries or mutations on the left side of the UI, and receive responses on the right. Sure beats Postman imho.</p><h3 id=\"don-t-look-down-you-re-in-the-cloud\">Don't Look Down: You're in the Cloud</h3><p>You can now add your server to Prisma Cloud to get the benefits of their admin panel. From here, you can modify information directly, review usage metrics, and manage multiple instances:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/prismacloud.gif\" class=\"kg-image\"><figcaption>Breaking News: Prisma is Too Cool For School.</figcaption></figure><!--kg-card-end: image--><h2 id=\"working-with-prisma-and-graphql\">Working With Prisma And GraphQL</h2><p>Now that we've spun up this shiny new toy, let's be sure we know how to drive it.</p><p>On your VPS, take a look at the <code>datamodels.prisma</code> file:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ vim datamodels.prisma\n</code></pre>\n<!--kg-card-end: markdown--><p>You should see a data model called User (everybody has this model). To add or modify data models, all we need to do is change the fields as we see fit, set their data type, and specify whether or not we'd like the field to be unique. Below I've added a couple of new 'fields.'</p><!--kg-card-begin: markdown--><pre><code class=\"language-yaml\">type User {\n  id: ID! @unique\n  name: String!\n  email: String! @unique\n  gravatar: String!\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Deploying Prisma again with these changes will modify our database's table structure to match the new model:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ prisma deploy\n</code></pre>\n<!--kg-card-end: markdown--><p>There you have it: one more buzzword to put your resum\u001d\u001de. In fact, feel free to completely falsify the existence of a GraphQL certification and throw that on there, too. If you're the kind of person who enjoys reading technical posts like this in your free time, chances are you're already qualified for the job. Unless you're Bob.</p>","url":"https://hackersandslackers.com/easily-build-graphql-apis-with-prisma/","uuid":"86286c72-478c-4108-8bef-89ca01caf043","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c570ae30b20340296f57709"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673758","title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","slug":"complex-features-in-mongodb-cloud","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","excerpt":"Using functions, webhooks, and values to utilize external APIs.","custom_excerpt":"Using functions, webhooks, and values to utilize external APIs.","created_at_pretty":"12 December, 2018","published_at_pretty":"14 December, 2018","updated_at_pretty":"01 January, 2019","created_at":"2018-12-12T18:26:09.000-05:00","published_at":"2018-12-14T08:00:00.000-05:00","updated_at":"2019-01-01T09:36:10.000-05:00","meta_title":"Building Complex Features in MongoDB Cloud | Hackers and Slackers","meta_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","og_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","og_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","og_title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","twitter_description":"Using functions, webhooks, and values to utilize external APIs. Today’s challenge: auto-tagging images using AI.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/mongowebhooks@2x.jpg","twitter_title":"Complex Features in MongoDB Cloud: Add Image Tags with AI","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Friends, family, and most importantly, strangers: I approach you today with a\ntale of renewed inspiration. After loudly broadcasting my own confusion and\nmediocre ability to actually implement an effective cloud via MongoDB Stitch, my\nineptitude has been answered with an early Christmas gift. \n\nMy incessant complaining gained some acknowledgement from a couple of folks over\nat MongoDB. Perhaps the timing is simply by chance, but since then I've begun\nnoticing something some subtleties in the Stitch documentation; namely that if\nyou look hard enough, some of it begins to make sense. Either way, I'm chalking\nthis one up as a Christmas Miracle.\n\nLet's Automate Stuff: More Webhooks, Less Labor\nTo demonstrate what building an end-to-end sexy feature looks like in MongoDB\nStitch, I'm going to borrow some help from some old friends: the team behind \nClarifai. \n\nClarifai is one of the early players in the field of what I'm sure we'll\ncreatively refer to as AI as a service. More specifically, they provide an API\nfor image recognition which returns impressive metadata simply by passing an\nimage URL. Best part is, unless you're abusing the shit out of 5000 requests per\nmonth, the API is essentially free:\n\nPredict\n Search\n Custom Model Training\n Add or Edit Input Images\n 5,000  free operations\n 5,000  free operations\n 5,000  free operations\n 5,000  free operations\n Pre-Built Models: \n$1.20 / 1,000  operations\n\nCustom Models:\n$3.20 / 1,000  operations\n$1.20 / 1,000  operations\n $1.20 / 1,000  operations\n $1.20 / 1,000  operations\n If we were to try to fit any more instances of the words \"AI\" and \"Cloud\" into\nthis post, things could quickly derail into a shitty  IBM Watson commercial.\n\n(PS: Blockchain.)\n\nStoring Our Clarifai API Key\nIf you're following along, hit up Clarifai [https://clarifai.com/]  to grab your\nAPI key, no strings attached.\n\nAnd no, nobody is paying me to me to write about their SaaS products.Copy and\npaste your brand new key and head over to the MongoDB Stitch Console\n[https://stitch.mongodb.com]. In our Stitch project, we're going to store our\nkey as a value (you might recall this as being a convenient way to store\nsecrets).\n\nCopy and paste your key as a string in a new value. The only catch is we'll be\nformatting our key as Key #####################, simply because this is the\nformat the API expects to receive when we pass our key as a header to the\nClarifai API.\n\nWarning: Mild Architecting Ahead\nBefore going too far into code, let's recap how this functionality will probably\nwork.\n\nIn our actual application, we'll be identifying images needing alt  tags (either\nvia frontend or backend logic). At that point, we should find the src  attribute\nof said <img>  tags and pass it to a Stitch function; preferably one that makes\na post request to Clarifai. \n\nThis is in fact too simple to be true, as there is one gotcha: Stitch functions \ncannot make http requests on their own. They can,  however, invoke Stitch \nWebhooks. These webhooks share nearly identical syntax and structure to \nfunctions, with a few exceptions:\n\n * Webhooks have endpoints (duh).\n * They have explicit inbound/outbound rules restricting what can invoke them.\n * There are options to set authorization via key or otherwise.\n\nWith all that in mind, our end-to-end flow will end up looking something like\nthis:\n\n 1. Our application identifies an image needing tags an invokes a serverless \n    function.\n 2. The function  constructs the body of the request we'll be making to Clarifai \n     with containing the URL of the image.\n 3. As crazy as it sounds, we then POST to a Stitch endpoint, which in turns\n    makes the actual  POST request to Clarifai. The request is made with the\n    body passed from our function, as well as the API key we stored earlier.\n 4. We'll receive a response of tags which we can do something with on the\n    application-side.\n\nWriting our Function\nWe'll start by writing a simple function as our go-between for our app and our\nservice:\n\nexports = function(img){\n   const http = context.services.get(\"GetClarifaiTags\");\n   var data = {\n        \"inputs\": [\n          {\n            \"data\": {\n              \"image\": {\n                \"url\": img\n              }\n            }\n          }\n        ]\n      };\n      \n    var header_data = {\"Content-Type\": [ \"application/json\" ]};\n   \n    return http.post({\n        url: \"https://webhooks.mongodb-stitch.com/api/client/v2.0/app/hackers-uangn/service/GetClarifaiTags/incoming_webhook/GetTagsForNewImage\",\n        headers: header_data,\n        body: JSON.stringify(data)\n      })\n      .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n\n\nThe first thing we do is reference our webhook (which we haven't created yet)\nwith this line:\n\nconst http = context.services.get(\"GetClarifaiTags\");\n\n\nIn the context of a function, context.services.get()  allows us to reference and\ninteract with other services we've created in Stitch. It's important to note\nthat we pass the user-created name of service we want to interact with. This is\none of the reasons why Stitch's documentation is so confusing - they\nconsistently use \"http\"  as an example service name. This seems to imply that\nwe'd want to import a type  of service as opposed to an instance  of a service,\nwhich is wrong.  \n\ndata  is the body of our request, which abides by Clarifai's documentation on\nhow to user their predict API. We need to pass this as a string to our webhook,\nthus we use JSON.stringify(data).\n\nIt's also important to note the structure of Mongo's headers when making\nrequests; notice that the value of each key pair is a list, as exemplified by \n\"Content-Type\": [ \"application/json\" ].\n\nAs you might imagine, these things in combination can cause a whole lot of\nconfusion. Hopefully you know a good blog to point these things out to you\nbeforehand.\n\nCreate a Webhook via 'HTTP Services'\nMove into the \"Services\" tab to create our webhook. Select HTTP  from the list\nof options:\n\nKind of a weird mix of services imho.Set your webhook to be a POST request.\nAuthentication shouldn't be a problem for us since we're only exposing this hook\nto our function, plus there are other ways to handle this.\n\nTIP: Don't post screenshots of sensitive endpoint URLs on the internet.The\nsyntax and methods available for writing a webhook are almost exactly the same\nas when writing regular functions. The one thing to note would be the presence\nof payload  being passed into the function; this object contains both the\nparameters and the body of requests being received by this endpoint. \npayload.body  gives us the body, whereas payload.query.arg  will give us the\nparameters.\n\nexports = function(payload){\n  const http = context.services.get(\"GetClarifaiTags\");\n  const token = context.values.get(\"clarifai_key\");\n  \n  var data = {};\n  if (payload.body) {\n    data = payload.body;\n  }\n  var header_data = {\n    \"Authorization\": [token], \n    \"Content-Type\": [\"application/json\"]\n  };\n\n    return http.post({\n        url: \"https://api.clarifai.com/v2/models/aaa03c23b3724a16a56b629203edc62c/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs\",\n        body: data,\n        headers: header_data\n      })\n      .then(response => {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n\n\nJust as we can access services within functions, we can similarly access values\nvia context.values.get(\"myValue\").\n\nNow that we have both the body and our API key ready, we can actually go ahead\nand construct a valid request to Clarifai. The syntax should be\nself-explanatory, but here's the Stitch http service documentation\n[https://docs.mongodb.com/stitch/services/http-actions/http.post/]  just in\ncase.\n\nWhy Did we Have to Make a Weird Webhook which is both Receiving and Posting\nInformation?\nThis is an excellent question and served to be a huge source of confusion for\nwhat must have been months. Go back to the \"Services\" tab, and pay close\nattention: for each service we create, a set of Rules  are automatically created\nand attached to our service. HTTP services have all functionality disabled room\ndefault, with little to no mention of the existence of rules in the first place. \n This is important for two reasons:\n\n 1. It's a silly UI hiccup that can waste the majority of your waking life.\n 2. This means that only  services can do things like post to external APIs.\n    This is why we didn't simply keep our logic in one function.\n\nOur Workflow in Practice\nAssuming you've added some logic to your app to pick out image URLs needing\ntags, our chain of events should be complete and return results to our\napplication. The POST request we make will return a response to the POST request\nof our function, and our function will return the results to our application.\nWe've successfully created a complex, albeit confusing, cloud architecture or\nexternal services.\n\nThis is where your imagination should hopefully kick in. You'll notice I have a\nfew services such as the endpoints which receive updates every time a JIRA issue\nis created or updated. This is what powers our public-facing kanban board.\n[https://hackersandslackers.com/projects/]","html":"<p>Friends, family, and most importantly, strangers: I approach you today with a tale of renewed inspiration. After loudly broadcasting my own confusion and mediocre ability to actually implement an effective cloud via MongoDB Stitch, my ineptitude has been answered with an early Christmas gift. </p><p>My incessant complaining gained some acknowledgement from a couple of folks over at MongoDB. Perhaps the timing is simply by chance, but since then I've begun noticing something some subtleties in the Stitch documentation; namely that if you look hard enough, some of it begins to make sense. Either way, I'm chalking this one up as a Christmas Miracle.</p><h2 id=\"let-s-automate-stuff-more-webhooks-less-labor\">Let's Automate Stuff: More Webhooks, Less Labor</h2><p>To demonstrate what building an end-to-end sexy feature looks like in MongoDB Stitch, I'm going to borrow some help from some old friends: the team behind <strong>Clarifai</strong>. </p><p>Clarifai is one of the early players in the field of what I'm sure we'll creatively refer to as <em>AI as a service. </em>More specifically, they provide an API for image recognition which returns impressive metadata simply by passing an image URL. Best part is, unless you're abusing the shit out of 5000 requests per month, the API is essentially free:</p><style>\n    table td {\n        text-align:left;\n        font-size: .95em;\n    }\n</style>\n\n<div class=\"tableContainer\">\n  <table>\n    <thead>\n      <th>Predict</th>\n      <th>Search</th>\n      <th>Custom Model Training</th>\n      <th>Add or Edit Input Images</th>\n    </thead>\n    <tbody>\n      <tr>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n        <td><strong>5,000</strong> free operations</td>\n      </tr>\n      <tr>\n        <td>\n          <strong>Pre-Built Models: </strong><br>\n          <small><strong>$1.20 / 1,000</strong> operations</small><br><br>\n          <strong>Custom Models:</strong><br>\n          <small><strong>$3.20 / 1,000</strong> operations</small><br>\n        </td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n        <td><strong>$1.20 / 1,000</strong> operations</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<p>If we were to try to fit any more instances of the words \"AI\" and \"Cloud\" into this post, things could quickly derail into a shitty  IBM Watson commercial.</p><p><em>(PS: Blockchain.)</em></p><h2 id=\"storing-our-clarifai-api-key\">Storing Our Clarifai API Key</h2><p>If you're following along, hit up <a href=\"https://clarifai.com/\">Clarifai</a> to grab your API key, no strings attached.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/clarifaiapi_o.png\" class=\"kg-image\"><figcaption>And no, nobody is paying me to me to write about their SaaS products.</figcaption></figure><p>Copy and paste your brand new key and head over to the <a href=\"https://stitch.mongodb.com\">MongoDB Stitch Console</a>. In our Stitch project, we're going to store our key as a <strong>value </strong>(you might recall this as being a convenient way to store secrets).</p><p>Copy and paste your key as a string in a new value. The only catch is we'll be formatting our key as <code>Key #####################</code>, simply because this is the format the API expects to receive when we pass our key as a header to the Clarifai API.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mongovalues_o-1.png\" class=\"kg-image\"></figure><h2 id=\"warning-mild-architecting-ahead\">Warning: Mild Architecting Ahead</h2><p>Before going too far into code, let's recap how this functionality will probably work.</p><p>In our actual application, we'll be identifying images needing <code>alt</code> tags (either via frontend or backend logic). At that point, we should find the <code>src</code> attribute of said <code>&lt;img&gt;</code> tags and pass it to a Stitch function; preferably one that makes a post request to <strong>Clarifai</strong>. </p><p>This is in fact too simple to be true, as there is one gotcha: Stitch <strong>functions</strong> cannot make http requests on their own. They <em>can,</em> however, invoke Stitch <strong>Webhooks. </strong>These webhooks share nearly identical syntax and structure to <strong>functions</strong>, with a few exceptions:</p><ul><li>Webhooks have endpoints (duh).</li><li>They have explicit inbound/outbound rules restricting what can invoke them.</li><li>There are options to set authorization via key or otherwise.</li></ul><p>With all that in mind, our end-to-end flow will end up looking something like this:</p><ol><li>Our application identifies an image needing tags an invokes a serverless <strong>function.</strong></li><li>The <strong>function</strong> constructs the body of the request we'll be making to <strong>Clarifai</strong> with containing the URL of the image.</li><li>As crazy as it sounds, we then POST to a Stitch endpoint, which in turns makes the <em>actual</em> POST request to Clarifai. The request is made with the body passed from our function, as well as the API key we stored earlier.</li><li>We'll receive a response of tags which we can do something with on the application-side.</li></ol><h2 id=\"writing-our-function\">Writing our Function</h2><p>We'll start by writing a simple function as our go-between for our app and our service:</p><pre><code class=\"language-javascript\">exports = function(img){\n   const http = context.services.get(&quot;GetClarifaiTags&quot;);\n   var data = {\n        &quot;inputs&quot;: [\n          {\n            &quot;data&quot;: {\n              &quot;image&quot;: {\n                &quot;url&quot;: img\n              }\n            }\n          }\n        ]\n      };\n      \n    var header_data = {&quot;Content-Type&quot;: [ &quot;application/json&quot; ]};\n   \n    return http.post({\n        url: &quot;https://webhooks.mongodb-stitch.com/api/client/v2.0/app/hackers-uangn/service/GetClarifaiTags/incoming_webhook/GetTagsForNewImage&quot;,\n        headers: header_data,\n        body: JSON.stringify(data)\n      })\n      .then(response =&gt; {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n</code></pre>\n<p>The first thing we do is reference our webhook (which we haven't created yet) with this line:</p><pre><code class=\"language-javascript\">const http = context.services.get(&quot;GetClarifaiTags&quot;);\n</code></pre>\n<p>In the context of a function, <code>context.services.get()</code> allows us to reference and interact with other services we've created in Stitch. It's important to note that we pass <strong>the user-created name of service </strong>we want to interact with. This is one of the reasons why Stitch's documentation is so confusing - they consistently use <em>\"http\"</em> as an example service name. This seems to imply that we'd want to import a <em>type</em> of service as opposed to an <em>instance</em> of a service, which is <strong>wrong.</strong> </p><p><code>data</code> is the body of our request, which abides by <a href=\"https://clarifai.com/developer/guide/predict#predict\">Clarifai's documentation on how to user their <em>predict</em> API</a>. We need to pass this as a string to our webhook, thus we use <code>JSON.stringify(data)</code>.</p><p>It's also important to note the structure of Mongo's headers when making requests; notice that the value of each key pair is a <strong>list, </strong>as exemplified by <code>\"Content-Type\": [ \"application/json\" ]</code>.</p><p>As you might imagine, these things in combination can cause a whole lot of confusion. Hopefully you know a good blog to point these things out to you beforehand.</p><h2 id=\"create-a-webhook-via-http-services-\">Create a Webhook via 'HTTP Services'</h2><p>Move into the \"Services\" tab to create our webhook. Select <strong>HTTP</strong> from the list of options:</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mongoservices.png\" class=\"kg-image\"><figcaption>Kind of a weird mix of services imho.</figcaption></figure><p>Set your webhook to be a POST request. Authentication shouldn't be a problem for us since we're only exposing this hook to our function, plus there are other ways to handle this.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/Screen-Shot-2018-12-13-at-4.23.27-PM_o.png\" class=\"kg-image\"><figcaption>TIP: Don't post screenshots of sensitive endpoint URLs on the internet.</figcaption></figure><p>The syntax and methods available for writing a webhook are almost exactly the same as when writing regular functions. The one thing to note would be the presence of <strong>payload</strong> being passed into the function; this object contains <em><strong>both the parameters and the body </strong></em>of requests being received by this endpoint. <code>payload.body</code> gives us the body, whereas <code>payload.query.arg</code> will give us the parameters.</p><pre><code class=\"language-javascript\">exports = function(payload){\n  const http = context.services.get(&quot;GetClarifaiTags&quot;);\n  const token = context.values.get(&quot;clarifai_key&quot;);\n  \n  var data = {};\n  if (payload.body) {\n    data = payload.body;\n  }\n  var header_data = {\n    &quot;Authorization&quot;: [token], \n    &quot;Content-Type&quot;: [&quot;application/json&quot;]\n  };\n\n    return http.post({\n        url: &quot;https://api.clarifai.com/v2/models/aaa03c23b3724a16a56b629203edc62c/versions/aa7f35c01e0642fda5cf400f543e7c40/outputs&quot;,\n        body: data,\n        headers: header_data\n      })\n      .then(response =&gt; {\n      // The response body is encoded as raw BSON.Binary. Parse it to JSON.\n      const ejson_body = EJSON.parse(response.body.text());\n      return ejson_body;\n    });\n};\n</code></pre>\n<p>Just as we can access services within functions, we can similarly access values via <code>context.values.get(\"myValue\")</code>.</p><p>Now that we have both the body and our API key ready, we can actually go ahead and construct a valid request to Clarifai. The syntax should be self-explanatory, but here's the <a href=\"https://docs.mongodb.com/stitch/services/http-actions/http.post/\">Stitch http service documentation</a> just in case.</p><h3 id=\"why-did-we-have-to-make-a-weird-webhook-which-is-both-receiving-and-posting-information\">Why Did we Have to Make a Weird Webhook which is both Receiving and Posting Information?</h3><p>This is an excellent question and served to be a huge source of confusion for what must have been months. Go back to the \"Services\" tab, and pay close attention: for each service we create, a set of <strong>Rules</strong> are automatically created and attached to our service. <strong>HTTP services have all functionality disabled room default, with little to no mention of the existence of rules in the first place.</strong> This is important for two reasons:</p><ol><li>It's a silly UI hiccup that can waste the majority of your waking life.</li><li>This means that <em>only</em> services can do things like post to external APIs. This is why we didn't simply keep our logic in one function.</li></ol><h2 id=\"our-workflow-in-practice\">Our Workflow in Practice</h2><p>Assuming you've added some logic to your app to pick out image URLs needing tags, our chain of events should be complete and return results to our application. The POST request we make will return a response to the POST request of our function, and our function will return the results to our application. We've successfully created a complex, albeit confusing, cloud architecture or external services.</p><p>This is where your imagination should hopefully kick in. You'll notice I have a few services such as the endpoints which receive updates every time a <strong>JIRA </strong>issue is created or updated. This is what powers our <a href=\"https://hackersandslackers.com/projects/\">public-facing kanban board.</a></p>","url":"https://hackersandslackers.com/complex-features-in-mongodb-cloud/","uuid":"91acc3b3-88c2-4313-aedd-adf1eac1dc36","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5c1199114b9896120b3c1b34"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867373d","title":"MongoDB Cloud: \"Backend as a Service\" with Atlas & Stitch","slug":"mongodb-cloud-backend-as-a-service-with-atlas-and-stitch","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","custom_excerpt":"MongoDB's silent transformation from an open-source database to enterprise cloud provider.","created_at_pretty":"13 November, 2018","published_at_pretty":"15 November, 2018","updated_at_pretty":"15 February, 2019","created_at":"2018-11-13T16:05:20.000-05:00","published_at":"2018-11-15T08:00:00.000-05:00","updated_at":"2019-02-15T12:49:05.000-05:00","meta_title":"MongoDB Cloud: \"Backend as a Service\" | Hackers and Slackers","meta_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","og_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","og_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","twitter_description":"Breaking down MongoDB Atlas and MongoDB Stitch to fully assess the capabilities of an unlikely cloud platform.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/mongodbcloud@2x.jpg","twitter_title":"MongoDB Cloud: \"Backend as a Service\" with Atlas And Stitch","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"JavaScript","slug":"javascript","description":"JavaScript covering both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","feature_image":null,"meta_description":"JavaScript topics, both Frontend and NodeJS. Build bundles with Webpack or Parcel, create task runners, or endure our criticism of the JavaScript ecosystem.","meta_title":"Javascript Tutorials | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#MongoDB Cloud","slug":"mongodb-cloud","description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mongodbcloudseries.jpg","meta_description":"All you need to know about MongoDB’s official cloud offering. Dive into MongoDB Atlas, or the architecture & microservices provided by MongoDB Stitch.","meta_title":"MongoDB Cloud","visibility":"internal"}],"plaintext":"Unless you've been living under a rock (or only visit this site via work-related\nGoogle Searches, like most people) you've probably heard me drone on here and\nthere about MongoDB Atlas  and MongoDB Stitch. I even went so far as to hack\ntogether an awful workflow that somehow utilized Tableau as an ETL tool to feed\nJIRA information into Mongo. I'd like to formally apologize for that entire\nseries: I can't imagine there's a single soul on this planet interested in\nlearning about all of those things simultaneously. Such hobbies reserved for\nmasochists with blogging addictions. I apologize. Let's start over.\n\nFirst off, this is not a tutorial on how to use MongoDB: the database. I have\nzero interest cluttering the internet by reiterating what a MEAN stack is for\nthe ten thousandth time, nor will I bore you with core NoSQL concepts you\nalready understand. I'm here to talk about the giant on the horizon we didn't\nsee coming, where MongoDB the database decided to become MongoDB Inc\n[https://en.wikipedia.org/wiki/MongoDB_Inc.]:  the enterprise cloud provider.\nThe same MongoDB that recently purchased mLab\n[https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab]\n, the other  cloud-hosted solution for Mongo databases. MongoDB the company is\nbold enough to place its bets on building a cloud far  simpler and restricted\nthan either AWS or GCloud. The core of that bet implies that most of us aren't\nexactly building unicorn products as much as we're reinventing the wheel: and\nthey're probably right.\n\nWelcome to our series on MongoDB cloud, where we break down every service\nMongoDB has to offer; one by one.\n\nWhat is MongoDB Cloud, and Does it Exist?\nWhat I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual\nname of the suite MongoDB offers) is actually two products:\n\n * MongoDB Atlas: A cloud-hosted MongoDB cluster with a beefy set of features.\n   Real-time dashboards, high-availability, security features,  an awesome\n   desktop client, and a CLI to top it all off.\n * MongoDB Stitch: A group of services designed to interact with Atlas in every\n   conceivable way, including creating endpoints, triggers, user authentication\n   flows, serverless functions, and a UI to handle all of this.\n\nI'm spying on you and every query you make.Atlas as a Standalone Database\nThere are plenty of people who simply want an instance of MongoDB hosted in the\ncloud as-is: just ask the guys at mLab. This was in fact how I got pulled into\nMongo's cloud myself.\n\nMongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo,\nwhich Mongo itself is confident in by offering a free tier\n[https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/]  of Atlas to\nprospective buyers. If you're a company or enterprise, the phrases High\nAvailability, Horizontal Scalability, relatively Higher Performance  will\nprobably be enough for you. But for us hobbyists, why pay for a Mongo cloud\ninstance?\n\nMongo themselves gives this comparison:\n\nOverview\n MongoDB Atlas\n Compose\n ObjectRocket\n Free Tier\n Yes\nStorage: 512 MB\nRAM: Variable\n No\n30-day free trial\n No\n30-day free trial\n Live migration\n Yes\nNo\nNo\nChoice of cloud providers\n AWS, Azure & GCP\n AWS, Softlayer & GCP\nAvailable in 2 regions for each provider\n Rackspace\n Choice of instance configuration\n Yes\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n No\nConfiguration based on required storage capacity only. No way to independently\nselect underlying hardware configurations\n Availability of latest MongoDB version\n Yes\nNew versions of the database are available on MongoDB Atlas as soon as they are\nreleased\n No\nNew versions typically available 1-2 quarters following database release\nNo\nNew versions typically available 1-2 quarters following database release\nReplica Set Configuration\n Up to 7 replicas\nAll replicas configured as data-bearing nodes\n 3 data-bearing nodes\nOne of the data-bearing nodes is hidden and used for backups only\n 3 data-bearing nodes\nAutomatic Sharding Support\n Yes\nNo\nYes\nData explorer\n Yes\nYes\nNo\nSQL-based BI Connectivity\n Yes\nNo\nNo\nPause and resume clusters\n Yes\nNo\nNo\nDatabase supported in on-premise deployments\n Yes\nMongoDB Enterprise Advanced [/products/mongodb-enterprise-advanced]\n No\nNo\nGlobal writes Low-latency writes from anywhere in the world Yes\n No\n No\n Cross-region replication Distribute data around the world for multi-region\nfault tolerance and local reads Yes\n No\n No\n Monitoring of database health with automated alerting\n Yes\nMongoDB Atlas UI & support for APM platforms (New Relic)\n Yes\nNew Relic\n Yes\nNew Relic\n Continuous backup\n Yes\nBackups maintained\nseconds behind production cluster\n No\nBackups taken with mongodump against hidden replica set member\n No\nBackups taken with mongodump\n Queryable backups\n Yes\nNo\nNo\nAutomated & consistent snapshots of sharded clusters\n Yes\nNot Applicable\nNo support for auto-sharding\n No\nRequires manually coordinating the recovery of mongodumps across shards\n Access control & IP whitelisting\n Yes\nYes\nYes\nAWS VPC Peering\n Yes\nBeta Release\nYes\nAdditional Charge\n Encryption of data in-flight\n Yes\nTLS/SSL as standard\n Yes\nYes\nEncryption of data at-rest\n Yes\nAvailable for AWS deployments; always on with Azure and GCP\n No\nYes\nAvailable only with specific pricing plans and data centers\n LDAP Integration\n Yes\n No\nNo\n Database-level auditing\nTrack DDL, DML, DCL operations\n Yes\n No\nNo\n Bring your own KMS\n Yes\n No\nNo\n Realistically there are probably only a number of items that stand out on the\ncomparison list when we go strictly database-to-database. Freedom over instance\nconfiguration sounds great, but in practice is more similar to putting a cap on\nhow much MongoDB decides to charge you that month (by the way, it's usually a\nlot; keep this mind). Having the Latest Version  seems great, but this can just\nas easily mean breaking production unannounced as much as it means new features.\n\nMongoDB clearly wins over the enterprise space with Continuous & queryable\nbackups, integration with LDAP, and automatic sharding support. Truthfully if\nthis were merely a database-level feature and cost comparison, the decision to\ngo with  MongoDB Atlas  would come down to how much you like their pretty\ndesktop interface:\n\nA perfectly legitimate reason to pay up, imho.So let's say MongoDB Atlas is\nmarginally better than a competitor in the confined realm of \"being a database.\"\nAre Stitch microservices enough to justify keeping your instance with the\nMongoDB team?\n\nService-by-Service Breakdown of Stitch\nStitch is kind of like if AWS exited in an alternative universe, where JSON and\nJavaScript were earth's only technologies. Thinking back to how we create APIs\nin AWS, the status quo almost always involves spinning up a Dynamo  (NoSQL)\ndatabase to put behind Lambda functions, accessible by API Gateway endpoints.\nStitch's core use case revolves around this use-case of end-user-accessing-data,\nwith a number of services dedicated specifically to supporting or improving this\nflow. The closest comparison to Stitch would be GCloud's Firebase. \n\nSo what makes Stitch so special?\n\nService 1: Querying Atlas Securely via Frontend Code\nSomething that cannot be understated is the ability to query Atlas via frontend\nJavascript. We're not passing API keys, Secrets, or any sort of nonsense;\nbecause you're configured things correctly, whitelisted domains can run queries\nof any complexity without ever interacting with an app's backend.  This is not a\ncrazy use case: consider this blog for example, or more so lately, mobile\napplications:\n\n<script src=\"https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js\"></script>\n<script>\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('<DATABASE>');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => \n    db.collection('<COLLECTION>').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() => \n    db.collection('<COLLECTION>').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs => {\n      console.log(\"Found docs\", docs)\n      console.log(\"[MongoDB Stitch] Connected to Stitch\")\n  }).catch(err => {\n    console.error(err)\n  });\n</script>\n\n\nThis isn't to say we're allowing any user to query any data all willy-nilly just\nbecause they're on our whitelisted IP: all data stored in Atlas is restricted to\nspecified Users  by defining User Roles. Joe Schmoe can't just inject a query\ninto any presumed database and wreak havoc, because Joe Schmoe can only access\ndata we've permitted his user account to view or write to. What is this \"user\naccount\" you ask? This brings us to the next big feature...\n\nService 2: End-User Account Creation & Management\nStitch will handle user account creation for you without the boilerplate.\nCreating an app with user accounts is a huge pain in the ass. Cheeky phrases\nlike 'Do the OAuth Dance'  can't ever hope to minimize the agonizing repetitive\npain of creating user accounts or managing relationships between users and data\n(can user X  see a comment from user Y?). Stitch allows most of the intolerably\nbenign logic behind these features to be handled via a UI.\n\nIt would be a far cry to say these processes have been \"trivialized\", but the\ntime saved is perhaps just enough to keep a coding hobbyist interested in their\nside projects as opposed to giving up and playing Rocket League.\n\nAs far as the permissions to read comments go... well, here's a self-explanatory\nscreenshot of how Stitch handles read/write document permission in its simplest\nform:\n\nOwners of comments can write their comments. Everybody else reads. Seems simple.\nService 3: Serverless Functions\nStitch functions are akin to AWS Lambda functions, but much easier to configure\nfor cross-service integration (and also limited to JavaScript ECMA 2015 or\nsomething). Functions benefit from the previous two features, in that they too\ncan be triggered from a whitelisted app's frontend, and are governed by a simple\n\"rules\" system, eliminating the need for security group configurations etc.\n\nThis is what calling a function from an app's frontend looks like:\n\n<script>\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user => {\n     client.callFunction(\"numCards\", [\"In Progress\"]).then(results => {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n</script>\n\n\nFunctions can run any query against Atlas, retrieve values  (such as environment\nvariables), and even call other functions. Functions can also be fired by\ndatabase triggers,  where a change to a collection will prompt an action such as\nan alert.\n\nService 4: HTTP Webhooks\nWebhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to\none another in that they are one-off URLs to perform single tasks. We could\nnever build a well-designed API using Stitch Webhooks, as we could with API\nGateway; this simply isn't the niche MongoDB is trying to hit (the opposite, in\nfact). \n\nConfiguration for a single Webhook.This form with a mere 6 fields clearly\nillustrates what Stitch intends to do: trivializing the creation of\ntraditionally non-trivial features.\n\nService 5: Storing 'Values' in Stitch\nA \"value\" is equivalent to an environment variable. These can be used to store\nAPI keys, secrets, or whatever. Of course, values are retrieved via functions.\n\nShhh, it's a secret ;)Service 6+: A Bunch of Mostly Bloated Extras\nFinally, Stitch has thrown in a few third-party integrations for good measure.\nSome integrations like S3 Integration could definitely come in handy, but it's\nworth asking why Mongo constantly over advertises their integrations with Github \n and Twilio. We've already established that we can create endpoints which accept\ninformation, and we can make functions which GET  information... so isn't\nanything with an API pretty easy to 'integrate' with?\n\nThis isn't to say the extra services aren't useful, they just seem a bit... odd.\nIt feels a lot like bloating the catalog, but the catalog isn't nearly bloated\nenough where it feels normal (like Heroku add-ons, for example). The choice to\nlaunch Stitch with a handful of barely-useful integrations only comes off as\nmore and more aimless as time passes; as months turn to years and no additions\nor updates are made to service offerings, it's worth questioning what the vision\nhad been for the product in the first place. In my experience, feature sets like\nthese happen when Product Managers are more powerful than they are useful.\n\nThe Breathtaking Climax: Is Stitch Worth It?\nI've been utilizing Stitch to fill in the blanks in development for months now,\nperhaps nearly a year. Each time I find myself working with Stitch or looking at\nthe bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive\ntoy with an infuriating lack of accurate documentation.\n\n  Stitch is very much a copy-and-paste-cookie-cutter-code  type of product,\nwhich begs the question of why their tutorials are recklessly outdated;\nsometimes to the point where MongoDB's own tutorial source code doesn't work. \nThere are so many use cases and potential benefits to Stitch, so why is the \nGithub repo [https://github.com/mongodb/stitch-examples]  containing example\ncode snippets so unmaintained, and painfully irrelevant? Lastly, why am I\nselling this product harder than their own internal team?\n\nStitch is a good product with a lot of unfortunate oversight. That said, Google\nFirebase still doesn't even have an \"import data\" feature, so I suppose it's\ntime to dig deep into this vendor lock and write a 5-post series about it before\nSilicon Valley's best and brightest get their shit together enough to actually\ncreate something useful and intuitive for other human beings to use. In the\nmeantime, feel free to steal source from tutorials I'll be posting, because\nthey'll be sure to, you know, actually work.","html":"<p>Unless you've been living under a rock (or only visit this site via work-related Google Searches, like most people) you've probably heard me drone on here and there about <strong>MongoDB Atlas</strong> and <strong>MongoDB Stitch</strong>. I even went so far as to hack together an awful workflow that somehow utilized Tableau as an ETL tool to feed JIRA information into Mongo. I'd like to formally apologize for that entire series: I can't imagine there's a single soul on this planet interested in learning about all of those things simultaneously. Such hobbies reserved for masochists with blogging addictions. I apologize. Let's start over.</p><p>First off, this is not a tutorial on how to use <em>MongoDB: the database</em>. I have zero interest cluttering the internet by reiterating what a MEAN stack is for the ten thousandth time, nor will I bore you with core NoSQL concepts you already understand. I'm here to talk about the giant on the horizon we didn't see coming, where MongoDB the database decided to become <a href=\"https://en.wikipedia.org/wiki/MongoDB_Inc.\"><strong>MongoDB Inc</strong></a><strong>:</strong> the enterprise cloud provider. The same MongoDB that recently purchased <a href=\"https://www.mongodb.com/press/mongodb-strengthens-global-cloud-database-with-acquisition-of-mlab\">mLab</a>, the <em>other</em> cloud-hosted solution for Mongo databases. MongoDB the company is bold enough to place its bets on building a cloud <em>far</em> simpler and restricted than either AWS or GCloud. The core of that bet implies that most of us aren't exactly building unicorn products as much as we're reinventing the wheel: and they're probably right.</p><p>Welcome to our series on MongoDB cloud, where we break down every service MongoDB has to offer; one by one.</p><h2 id=\"what-is-mongodb-cloud-and-does-it-exist\">What is MongoDB Cloud, and Does it Exist?</h2><p>What I refer to as \"MongoDB Cloud\" (which, for some reason, isn't the actual name of the suite MongoDB offers) is actually two products:</p><ul><li><strong>MongoDB Atlas</strong>: A cloud-hosted MongoDB cluster with a beefy set of features. Real-time dashboards, high-availability, security features,  an awesome desktop client, and a CLI to top it all off.</li><li><strong>MongoDB Stitch: </strong>A group of services designed to interact with Atlas in every conceivable way, including creating endpoints, triggers, user authentication flows, serverless functions, and a UI to handle all of this.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/metrics.gif\" class=\"kg-image\"><figcaption>I'm spying on you and every query you make.</figcaption></figure><h3 id=\"atlas-as-a-standalone-database\">Atlas as a Standalone Database</h3><p>There are plenty of people who simply want an instance of MongoDB hosted in the cloud as-is: just ask the guys at mLab. This was in fact how I got pulled into Mongo's cloud myself.</p><p>MongoDB Atlas has plenty of advantages over a self-hosted instance of Mongo, which Mongo itself is confident in by offering a <a href=\"https://docs.mongodb.com/manual/tutorial/atlas-free-tier-setup/\">free tier</a> of Atlas to prospective buyers. If you're a company or enterprise, the phrases <strong>High Availability</strong>, <strong>Horizontal Scalability, </strong>relatively <strong>Higher Performance</strong> will probably be enough for you. But for us hobbyists, why pay for a Mongo cloud instance?</p><p>Mongo themselves gives this comparison:</p><div class=\"tableContainer\">\n<table class=\"table left\">\n  <thead>\n    <tr>\n      <th>\n        <strong>Overview</strong>\n      </th>\n      <th>\n        <strong>MongoDB Atlas</strong>\n      </th>\n      <th>\n        <strong>Compose</strong>\n      </th>\n      <th>\n        <strong>ObjectRocket</strong>\n      </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\n        Free Tier\n      </td>\n      <td>\n        Yes<br><small>Storage: 512 MB<br>RAM: Variable</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n      <td>\n        No<br><small>30-day free trial</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Live migration\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of cloud providers\n      </td>\n      <td>\n        AWS, Azure &amp; GCP\n      </td>\n      <td>\n        AWS, Softlayer &amp; GCP<br><small>Available in 2 regions for each provider</small>\n      </td>\n      <td>\n        Rackspace\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Choice of instance configuration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n      <td>\n        No<br><small>Configuration based on required storage capacity only. No way to independently select underlying hardware configurations</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Availability of latest MongoDB version\n      </td>\n      <td>\n        Yes<br><small>New versions of the database are available on MongoDB Atlas as soon as they are released</small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n      <td>\n        No<br><small>New versions typically available 1-2 quarters following database release<br></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Replica Set Configuration\n      </td>\n      <td>\n        Up to 7 replicas<br><small>All replicas configured as data-bearing nodes</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small>One of the data-bearing nodes is hidden and used for backups only</small>\n      </td>\n      <td>\n        3 data-bearing nodes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automatic Sharding Support\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Data explorer\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        SQL-based BI Connectivity\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Pause and resume clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database supported in on-premise deployments\n      </td>\n      <td>\n        Yes<br><small><a href=\"/products/mongodb-enterprise-advanced\">MongoDB Enterprise Advanced</a></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Global writes <small>Low-latency writes from anywhere in the world </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Cross-region replication <small>Distribute data around the world for multi-region fault tolerance and local reads </small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No\n      </td>\n      <td>\n        No\n      </td>\n    </tr><tr>\n      <td>\n        Monitoring of database health with automated alerting\n      </td>\n      <td>\n        Yes<br><small>MongoDB Atlas UI &amp; support for APM platforms (New Relic)</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n      <td>\n        Yes<br><small>New Relic</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Continuous backup\n      </td>\n      <td>\n        Yes<br><small>Backups maintained<br>seconds behind production cluster</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump against hidden replica set member</small>\n      </td>\n      <td>\n        No<br><small>Backups taken with mongodump</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Queryable backups\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Automated &amp; consistent snapshots of sharded clusters\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Not Applicable<br><small>No support for auto-sharding</small>\n      </td>\n      <td>\n        No<br><small>Requires manually coordinating the recovery of mongodumps across shards</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Access control &amp; IP whitelisting\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        AWS VPC Peering\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Beta Release<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Additional Charge</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data in-flight\n      </td>\n      <td>\n        Yes<br><small>TLS/SSL as standard</small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n      <td>\n        Yes<br><small></small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Encryption of data at-rest\n      </td>\n      <td>\n        Yes<br><small>Available for AWS deployments; always on with Azure and GCP</small>\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        Yes<br><small>Available only with specific pricing plans and data centers</small>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        LDAP Integration\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Database-level auditing<br><small>Track DDL, DML, DCL operations</small>\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n    <tr>\n      <td>\n        Bring your own KMS\n      </td>\n      <td>\n        Yes\n      </td>\n      <td>\n        No<br><small></small>\n      </td>\n      <td>\n        No\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Realistically there are probably only a number of items that stand out on the comparison list when we go strictly database-to-database. Freedom over <strong>instance configuration </strong>sounds great, but in practice is more similar to putting a cap on how much MongoDB decides to charge you that month (by the way, it's usually a lot; keep this mind). Having the <strong>Latest Version</strong> seems great, but this can just as easily mean breaking production unannounced as much as it means new features.</p><p>MongoDB clearly wins over the enterprise space with <strong>Continuous &amp; queryable backups</strong>, integration with <strong>LDAP, </strong>and <strong>automatic sharding support. </strong>Truthfully if this were merely a database-level feature and cost comparison, the decision to go with<strong> MongoDB Atlas</strong> would come down to how much you like their pretty desktop interface:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/compass.gif\" class=\"kg-image\"><figcaption>A perfectly legitimate reason to pay up, imho.</figcaption></figure><p>So let's say MongoDB Atlas is marginally better than a competitor in the confined realm of \"being a database.\" Are Stitch microservices enough to justify keeping your instance with the MongoDB team?</p><h2 id=\"service-by-service-breakdown-of-stitch\">Service-by-Service Breakdown of Stitch</h2><p>Stitch is kind of like if AWS exited in an alternative universe, where JSON and JavaScript were earth's only technologies. Thinking back to how we create APIs in AWS, the status quo almost always involves spinning up a <strong>Dynamo</strong> (NoSQL) database to put behind <strong>Lambda </strong>functions, accessible by <strong>API Gateway </strong>endpoints. Stitch's core use case revolves around this use-case of <em>end-user-accessing-data</em>, with a number of services dedicated specifically to supporting or improving this flow. The closest comparison to Stitch would be GCloud's <strong>Firebase</strong>. </p><p>So what makes Stitch so special?</p><h3 id=\"service-1-querying-atlas-securely-via-frontend-code\">Service 1: Querying Atlas Securely via Frontend Code</h3><p>Something that cannot be understated is the ability to query Atlas via frontend Javascript. We're not passing API keys, Secrets, or any sort of nonsense; because you're configured things correctly, whitelisted domains can run queries of any complexity <em>without ever interacting with an app's backend.</em> This is not a crazy use case: consider this blog for example, or more so lately, mobile applications:</p><pre><code class=\"language-javascript\">&lt;script src=&quot;https://s3.amazonaws.com/stitch-sdks/js/bundles/4.0.8/stitch.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  const client = stitch.Stitch.initializeDefaultAppClient('myapp');\n\n  const db = client.getServiceClient(stitch.RemoteMongoClient.factory, 'mongodb-atlas').db('&lt;DATABASE&gt;');\n\n  client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; \n    db.collection('&lt;COLLECTION&gt;').updateOne({owner_id: client.auth.user.id}, {$set:{number:42}}, {upsert:true})\n  ).then(() =&gt; \n    db.collection('&lt;COLLECTION&gt;').find({owner_id: client.auth.user.id}, { limit: 100}).asArray()\n  ).then(docs =&gt; {\n      console.log(&quot;Found docs&quot;, docs)\n      console.log(&quot;[MongoDB Stitch] Connected to Stitch&quot;)\n  }).catch(err =&gt; {\n    console.error(err)\n  });\n&lt;/script&gt;\n</code></pre>\n<p>This isn't to say we're allowing any user to query any data all willy-nilly just because they're on our whitelisted IP: all data stored in Atlas is restricted to specified <strong>Users</strong> by defining <strong>User Roles. </strong>Joe Schmoe can't just inject a query into any presumed database and wreak havoc, because Joe Schmoe can only access data we've permitted his user account to view or write to. What is this \"user account\" you ask? This brings us to the next big feature...</p><h3 id=\"service-2-end-user-account-creation-management\">Service 2: End-User Account Creation &amp; Management</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-5.59.03-PM.png\" class=\"kg-image\"><figcaption>Stitch will handle user account creation for you without the boilerplate.</figcaption></figure><p>Creating an app with user accounts is a huge pain in the ass. Cheeky phrases like '<strong>Do the OAuth Dance'</strong> can't ever hope to minimize the agonizing repetitive pain of creating user accounts or managing relationships between users and data (can <em>user X</em> see a comment from <em>user Y</em>?). Stitch allows most of the intolerably benign logic behind these features to be handled via a UI.</p><p>It would be a far cry to say these processes have been \"trivialized\", but the time saved is perhaps just enough to keep a coding hobbyist interested in their side projects as opposed to giving up and playing Rocket League.</p><p>As far as the permissions to read comments go... well, here's a self-explanatory screenshot of how Stitch handles read/write document permission in its simplest form:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-13-at-6.09.25-PM.png\" class=\"kg-image\"><figcaption>Owners of comments can write their comments. Everybody else reads. Seems simple.</figcaption></figure><h3 id=\"service-3-serverless-functions\">Service 3: Serverless Functions</h3><p>Stitch functions are akin to AWS Lambda functions, but much easier to configure for cross-service integration (and also limited to JavaScript ECMA 2015 or something). Functions benefit from the previous two features, in that they too can be triggered from a whitelisted app's frontend, and are governed by a simple \"rules\" system, eliminating the need for security group configurations etc.</p><p>This is what calling a function from an app's frontend looks like:</p><pre><code class=\"language-javascript\">&lt;script&gt;\n    client.auth.loginWithCredential(new stitch.AnonymousCredential()).then(user =&gt; {\n     client.callFunction(&quot;numCards&quot;, [&quot;In Progress&quot;]).then(results =&gt; {\n       $('#progress .count').text(results + ' issues');\n     })\n    });\n&lt;/script&gt;\n</code></pre>\n<p>Functions can run any query against Atlas, retrieve <em>values</em> (such as environment variables), and even call other functions. Functions can also be fired by database <strong>triggers,</strong> where a change to a collection will prompt an action such as an alert.</p><h3 id=\"service-4-http-webhooks\">Service 4: HTTP Webhooks</h3><p>Webhooks are a fast way to toss up endpoints. Stitch endpoints are agnostic to one another in that they are one-off URLs to perform single tasks. We could never build a well-designed API using Stitch Webhooks, as we could with <strong>API Gateway</strong>; this simply isn't the niche MongoDB is trying to hit (the opposite, in fact). </p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-4.33.27-PM.png\" class=\"kg-image\"><figcaption>Configuration for a single Webhook.</figcaption></figure><p>This form with a mere 6 fields clearly illustrates what Stitch intends to do: trivializing the creation of traditionally non-trivial features.</p><h3 id=\"service-5-storing-values-in-stitch\">Service 5: Storing 'Values' in Stitch</h3><p>A \"value\" is equivalent to an environment variable. These can be used to store API keys, secrets, or whatever. Of course, values are retrieved via functions.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-11-14-at-7.45.28-PM.png\" class=\"kg-image\"><figcaption>Shhh, it's a secret ;)</figcaption></figure><h3 id=\"service-6-a-bunch-of-mostly-bloated-extras\">Service 6+: A Bunch of Mostly Bloated Extras</h3><p>Finally, Stitch has thrown in a few third-party integrations for good measure. Some integrations like <strong>S3 Integration </strong>could definitely come in handy, but it's worth asking why Mongo constantly over advertises their integrations with <strong>Github</strong> and <strong>Twilio</strong>. We've already established that we can create endpoints which accept information, and we can make functions which <em>GET</em> information... so isn't anything with an API pretty easy to 'integrate' with?</p><p>This isn't to say the extra services aren't useful, they just seem a bit... odd. It feels a lot like bloating the catalog, but the catalog isn't nearly bloated enough where it feels normal (like Heroku add-ons, for example). The choice to launch Stitch with a handful of barely-useful integrations only comes off as more and more aimless as time passes; as months turn to years and no additions or updates are made to service offerings, it's worth questioning what the vision had been for the product in the first place. In my experience, feature sets like these happen when Product Managers are more powerful than they are useful.</p><h2 id=\"the-breathtaking-climax-is-stitch-worth-it\">The Breathtaking Climax: Is Stitch Worth It?</h2><p>I've been utilizing Stitch to fill in the blanks in development for months now, perhaps nearly a year. Each time I find myself working with Stitch or looking at the bill, I can't decide if it's been a Godsend for its nich\u001dé, or an expensive toy with an infuriating lack of accurate documentation.</p><p> Stitch is very much a <em>copy-and-paste-cookie-cutter-code</em> type of product, which begs the question of why their tutorials are recklessly outdated; sometimes to the point where MongoDB's own tutorial source code <em>doesn't work. </em>There are so many use cases and potential benefits to Stitch, so why is the <a href=\"https://github.com/mongodb/stitch-examples\">Github repo</a> containing example code snippets so unmaintained, and painfully irrelevant? Lastly, why am I selling this product harder than their own internal team?</p><p>Stitch is a good product with a lot of unfortunate oversight. That said, Google Firebase still doesn't even have an \"import data\" feature, so I suppose it's time to dig deep into this vendor lock and write a 5-post series about it before Silicon Valley's best and brightest get their shit together enough to actually create something useful and intuitive for other human beings to use. In the meantime, feel free to steal source from tutorials I'll be posting, because they'll be sure to, you know, actually work.</p>","url":"https://hackersandslackers.com/mongodb-cloud-backend-as-a-service-with-atlas-and-stitch/","uuid":"5555fa6e-07f0-4f9a-8069-e1e68868e608","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5beb3c900dbec217f3ce801b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673733","title":"Python-Lambda: The Essential Library for AWS Cloud Functions","slug":"improve-your-aws-lambda-workflow-with-python-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","custom_excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","created_at_pretty":"07 November, 2018","published_at_pretty":"08 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-07T16:01:48.000-05:00","published_at":"2018-11-07T19:13:20.000-05:00","updated_at":"2019-01-05T13:22:03.000-05:00","meta_title":"Simplify Lambda Deployment with python-lambda | Hackers and Slackers","meta_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","og_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","twitter_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","twitter_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our series about building AWS APIs\n[https://hackersandslackers.com/tag/aws-api/], we've covered a lot of ground\naround learning the AWS ecosystem. Now that we're all feeling a bit more\ncomfortable, it may be time to let everybody in on the world's worst-kept\nsecret: Almost nobody builds architecture by interacting with the AWS UI\ndirectly. There are plenty examples of how this is done, with the main example\nbeing HashiCorp:  an entire business model based around the premise that AWS has\na shitty UI, to the point where it's easier to write code to make things which\nwill host your code. What a world.\n\nIn the case of creating Python Lambda functions, the \"official\" (aka: manual)\nworkflow of deploying your code to AWS is something horrible like this:\n\n * You start a project locally and begin development.\n * You opt to use virtualenv, because you're well aware that you're going to\n   need the source for any packages you use available.\n * When you're ready to 'deploy' to AWS, you copy all your dependencies from \n   /site-packages  and move them into your root directory, temporarily creating\n   an abomination of a project structure.\n * With your project fully bloated and confused, you cherry pick the files\n   needed to zip into an archive.\n * Finally, you upload your code via zip either to Lambda directory or to S3,\n   only to run your code, realize its broken, and need to start all over.\n\nThere Must be a Better Way\nIndeed there is, and surprisingly enough the solution is 100% Python (sorry\nHashiCorp, we'll talk another time). This \"better way\" is my personal method of\nleveraging the following:\n\n * The official AWS CLI\n   [https://docs.aws.amazon.com/cli/latest/userguide/installing.html].\n * Pipenv [https://pipenv.readthedocs.io/en/latest/]  as an environment manager.\n * Python's python-lambda [https://github.com/nficano/python-lambda]  package:\n   the magic behind it all.\n\nObligatory \"Installing the CLI\" Recap\nFirst off, make sure you're using a compatible version of Python on your system,\nas AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by\nthe way, Python 2.7 doesn't count as compatible - let it die before your career\ndoes).\n\n$ pip3 install awscli --upgrade --user\n\n\nIf you're working off an EC2 instance, it has come to my attention pip3 does not\ncome preinstalled. Remember to run: * $ apt update\n * $ apt upgrade\n * $ apt install python3-pip\n\nYou may be prompted to run apt install awscli  as well.Awesome, now that we have\nthe CLI installed on the real  version of Python, we need to store your\ncredentials. Your Access Key ID and Secret Access Key can be found in your IAM\npolicy manager.\n\n$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:\n\nOn both Linux and OSX, this should generate files found under cd ~/.aws  which\nwill be referenced by default whenever you use an AWS service moving forward.\n\nSet Up Your Environment\nAs mentioned, we'll use pipenv  for easy environment management. We'll create an\nenvironment using Lambda's preferred Python version:\n\n$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n\n\nSomething you should be aware of at the time of writing: Pip's latest version,\n18.1, is actually a breaking change  for Pipenv. Thus, the first thing we should\ndo is force usage of pip 18.0 (is there even a fix for this yet?). This is\nsolved by typing pip3 install pip==18.0  with the Pipenv shell activated. Now\nlet's get to the easy part.\n\npython-lambda: The Savior of AWS\nSo far we've made our lives easier in two ways: we're keeping our AWS\ncredentials safe and far away from ourselves, and we have what is by far the\nsuperior Python package management solution. But this is all foreplay leading up\nto python-lambda:\n\n$ pip3 install python-lambda\n\n\nThis library alone is about to do you the following favors:\n\n * Initiate your Lambda project structure for you.\n * Isolate Lambda configuration to a  config.yaml  file, covering everything\n   from the name of your entry point, handler function, and even\n   program-specific variables.\n * Allow you to run tests locally, where a test.json  file simulates a request\n   being made to your function locally.\n * Build a production-ready zip file with all dependencies completely separated \n   from your beautiful file structure.\n * The ability to deploy directly  to S3 or Lambda with said zip file from\n   command-line.\n\nCheck out the commands for yourself:\n\nCommands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n\n\nInitiate your project\nRunning lambda init  will generate the following file structure:\n\n.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n\n\nChecking out the entry point: service.py\npython-lambda starts you off with a basic handler as an example of a working\nproject. Feel free to rename service.py  and its handler function to whatever\nyou please, as we can configure that in a bit.\n\n# -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n\n\nEasy configuration via configure.yaml\nThe base config generated by lambda init  looks like this:\n\nregion: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n\n\nLook familiar? These are all the properties you would normally have to set up\nvia the UI. As an added bonus, you can store values (such as S3 bucket names for\nboto3) in this file as well. That's dope.\n\nSetting up event.json\nThe default event.json  is about as simplistic as you can get, and naturally not\nvery helpful at first (it isn't meant to be). These are the contents:\n\n{\n  \"pi\": 3.14,\n  \"e\": 2.718\n}\n\n\nWe can replace this a real test JSON which we can grab from Lambda itself.\nHere's an example of a Cloudwatch event we can use instead:\n\n{\n  \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n  \"detail-type\": \"Scheduled Event\",\n  \"source\": \"aws.events\",\n  \"account\": \"{{account-id}}\",\n  \"time\": \"1970-01-01T00:00:00Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n  ],\n  \"pi\": 3.14,\n  \"e\": 2.718\n  \"detail\": {}\n}\n\n\nRemember that event.json  is what is being passed to our handler as the event \nparameter. Thus, now we can run our Lambda function locally  to see if it works:\n\n$ lambda invoke\n5.8580000000000005\n\n\nPretty cool if you ask me.\n\nDeploy it, Ship it, Roll Credits\nAfter you express your coding genius, remember to output pip freeze >\nrequirements.txt. python-lambda  will use this as a reference for which packages\nneed to be included. This is neat because we can use Pipenv and the benefits of\nthe workflow it provides while still easily outputting what we need to deploy. \n\nBecause we already specified which Lambda we're going to deploy to in \nconfig.yaml, we can deploy to that Lambda immediately. lambda deploy  will use\nthe zip upload method, whereas lambda deploy-s3  will store your source on S3.\n\nIf you'd like to deploy the function yourself, run with lambda build  which will\nzip your source code plus dependencies  neatly into a /dist  directory. Suddenly\nwe never have to compromise our project structure, and now we can easily source\ncontrol our Lambdas by .gitignoring our build folders while hanging on to our\nPipfiles.\n\nHere's to hoping you never need to deploy Lambdas using any other method ever\nagain. Cheers.","html":"<p>In our series about building <a href=\"https://hackersandslackers.com/tag/aws-api/\">AWS APIs</a>, we've covered a lot of ground around learning the AWS ecosystem. Now that we're all feeling a bit more comfortable, it may be time to let everybody in on the world's worst-kept secret: Almost nobody builds architecture by interacting with the AWS UI directly. There are plenty examples of how this is done, with the main example being <strong>HashiCorp:</strong> an entire business model based around the premise that AWS has a shitty UI, to the point where it's easier to write code to make things which will host your code. What a world.</p><p>In the case of creating Python Lambda functions, the \"official\" (aka: manual) workflow of deploying your code to AWS is something horrible like this:</p><ul><li>You start a project locally and begin development.</li><li>You opt to use <strong>virtualenv, </strong>because you're well aware that you're going to need the source for any packages you use available.</li><li>When you're ready to 'deploy' to AWS, you <em>copy all your dependencies from </em><code>/site-packages</code> <em>and move them into your root directory</em>, temporarily creating an abomination of a project structure.</li><li>With your project fully bloated and confused, you cherry pick the files needed to zip into an archive.</li><li>Finally, you upload your code via zip either to Lambda directory or to S3, only to run your code, realize its broken, and need to start all over.</li></ul><h2 id=\"there-must-be-a-better-way\">There Must be a Better Way</h2><p>Indeed there is, and surprisingly enough the solution is 100% Python (sorry HashiCorp, we'll talk another time). This \"better way\" is my personal method of leveraging the following:</p><ul><li>The official <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\">AWS CLI</a>.</li><li><a href=\"https://pipenv.readthedocs.io/en/latest/\">Pipenv</a> as an environment manager.</li><li>Python's <strong><a href=\"https://github.com/nficano/python-lambda\">python-lambda</a></strong> package: the magic behind it all.</li></ul><h3 id=\"obligatory-installing-the-cli-recap\">Obligatory \"Installing the CLI\" Recap</h3><p>First off, make sure you're using a compatible version of Python on your system, as AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by the way, <em>Python 2.7 </em>doesn't count as compatible - let it die before your career does).</p><pre><code class=\"language-python\">$ pip3 install awscli --upgrade --user\n</code></pre>\n<div class=\"protip\">\n    If you're working off an EC2 instance, it has come to my attention pip3 does not come preinstalled. Remember to run:\n<ul>\n    <li><code>$ apt update</code></li>\n    <li><code>$ apt upgrade</code></li>\n    <li><code>$ apt install python3-pip</code></li>\n</ul>\n    \n    You may be prompted to run <code>apt install awscli</code> as well.\n</div><p>Awesome, now that we have the CLI installed on the <em>real</em> version of Python, we need to store your credentials. Your Access Key ID and Secret Access Key can be found in your IAM policy manager.</p><pre><code>$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:</code></pre><p>On both Linux and OSX, this should generate files found under <code>cd ~/.aws</code> which will be referenced by default whenever you use an AWS service moving forward.</p><h2 id=\"set-up-your-environment\">Set Up Your Environment</h2><p>As mentioned, we'll use <code>pipenv</code> for easy environment management. We'll create an environment using Lambda's preferred Python version:</p><pre><code class=\"language-python\">$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n</code></pre>\n<p>Something you should be aware of at the time of writing: Pip's latest version, 18.1, is actually a <em>breaking change</em> for Pipenv. Thus, the first thing we should do is force usage of pip 18.0 (is there even a fix for this yet?). This is solved by typing <code>pip3 install pip==18.0</code> with the Pipenv shell activated. Now let's get to the easy part.</p><h2 id=\"python-lambda-the-savior-of-aws\">python-lambda: The Savior of AWS</h2><p>So far we've made our lives easier in two ways: we're keeping our AWS credentials safe and far away from ourselves, and we have what is by far the superior Python package management solution. But this is all foreplay leading up to <code>python-lambda</code>:</p><pre><code class=\"language-bash\">$ pip3 install python-lambda\n</code></pre>\n<p>This library alone is about to do you the following favors:</p><ul><li>Initiate your Lambda project structure for you.</li><li>Isolate Lambda configuration to a<em> config.yaml</em> file, covering everything from the name of your entry point, handler function, and even program-specific variables.</li><li>Allow you to run tests locally, where a <em>test.json</em> file simulates a request being made to your function locally.</li><li>Build a production-ready zip file with all dependencies <em>completely separated </em>from your beautiful file structure.</li><li>The ability to deploy <em>directly</em> to S3 or Lambda with said zip file from command-line.</li></ul><p>Check out the commands for yourself:</p><pre><code class=\"language-bash\">Commands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n</code></pre>\n<h3 id=\"initiate-your-project\">Initiate your project</h3><p>Running <code>lambda init</code> will generate the following file structure:</p><pre><code class=\"language-bash\">.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n</code></pre>\n<h3 id=\"checking-out-the-entry-point-service-py\">Checking out the entry point: service.py</h3><p>python-lambda starts you off with a basic handler as an example of a working project. Feel free to rename <code>service.py</code> and its handler function to whatever you please, as we can configure that in a bit.</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n</code></pre>\n<h3 id=\"easy-configuration-via-configure-yaml\">Easy configuration via configure.yaml</h3><p>The base config generated by <code>lambda init</code> looks like this:</p><pre><code class=\"language-yaml\">region: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n</code></pre>\n<p>Look familiar? These are all the properties you would normally have to set up via the UI. As an added bonus, you can store values (such as S3 bucket names for boto3) in this file as well. That's dope.</p><h3 id=\"setting-up-event-json\">Setting up event.json</h3><p>The default <code>event.json</code> is about as simplistic as you can get, and naturally not very helpful at first (it isn't meant to be). These are the contents:</p><pre><code class=\"language-json\">{\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n}\n</code></pre>\n<p>We can replace this a real test JSON which we can grab from Lambda itself. Here's an example of a Cloudwatch event we can use instead:</p><pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;cdc73f9d-aea9-11e3-9d5a-835b769c0d9c&quot;,\n  &quot;detail-type&quot;: &quot;Scheduled Event&quot;,\n  &quot;source&quot;: &quot;aws.events&quot;,\n  &quot;account&quot;: &quot;{{account-id}}&quot;,\n  &quot;time&quot;: &quot;1970-01-01T00:00:00Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [\n    &quot;arn:aws:events:us-east-1:123456789012:rule/ExampleRule&quot;\n  ],\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n  &quot;detail&quot;: {}\n}\n</code></pre>\n<p>Remember that <code>event.json</code> is what is being passed to our handler as the <code>event</code> parameter. Thus, now we can run our Lambda function <em>locally</em> to see if it works:</p><pre><code class=\"language-bash\">$ lambda invoke\n5.8580000000000005\n</code></pre>\n<p>Pretty cool if you ask me.</p><h2 id=\"deploy-it-ship-it-roll-credits\">Deploy it, Ship it, Roll Credits</h2><p>After you express your coding genius, remember to output <code>pip freeze &gt; requirements.txt</code>. <strong>python-lambda</strong> will use this as a reference for which packages need to be included. This is neat because we can use Pipenv and the benefits of the workflow it provides while still easily outputting what we need to deploy. </p><p>Because we already specified which Lambda we're going to deploy to in <code>config.yaml</code>, we can deploy to that Lambda immediately. <code>lambda deploy</code> will use the zip upload method, whereas <code>lambda deploy-s3</code> will store your source on S3.</p><p>If you'd like to deploy the function yourself, run with <code>lambda build</code> which will zip your source code <em>plus dependencies</em> neatly into a /<em>dist</em> directory. Suddenly we never have to compromise our project structure, and now we can easily source control our Lambdas by <em>.gitignoring </em>our build folders while hanging on to our Pipfiles.</p><p>Here's to hoping you never need to deploy Lambdas using any other method ever again. Cheers.</p>","url":"https://hackersandslackers.com/improve-your-aws-lambda-workflow-with-python-lambda/","uuid":"08ad7706-8dd7-4475-875e-880c017de8d5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be352bc2aa81b1606ab77a7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673651","title":"Building an API with Amazon's API Gateway","slug":"creating-apis-with-api-gateway","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","custom_excerpt":"Building APIs: The final frontier of cool-stuff-to-do-in-AWS.","created_at_pretty":"13 May, 2018","published_at_pretty":"29 October, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-05-13T17:29:07.000-04:00","published_at":"2018-10-29T19:41:00.000-04:00","updated_at":"2019-01-05T13:28:10.000-05:00","meta_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","meta_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","og_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","og_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","twitter_description":"Use Amazon's API Gateway to design powerful APIs to interact with other AWS services.","twitter_image":"https://hackersandslackers.com/content/images/2018/05/apigateway2@2x.jpg","twitter_title":"Building an API with Amazon's API Gateway | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our last adventure, we ventured off to create our very own cloud database\n[https://hackersandslackers.com/setting-up-mysql-on-aws/]  by using Amazon's RDS \n service. We've also briefly covered\n[https://hackersandslackers.com/building-an-api-using-aws/]  the general concept\nbehind what Lambda functions. In case you've already forgotten, Lambdas are\nbasically just chunks of code in the cloud; think of them as tiny virtual\nservers, which have already been configured (and locked down) to serve one\nspecific purpose. Because that's literally what it is.\n\nThe data being stored in RDS is ultimately what we're targeting, and Lambdas \nserve as the in-between logic to serve up, modify, or add to the proper data.\nThe only piece missing from the picture is API Gateway. \n\nAs the name suggests, API Gateway  is the, uh, gateway  that users or systems\ninteract with to obtain what they're seeking. It is (hopefully) the only part of\nthis VPC structure an external user can interact with:\n\nSimple API to interact with RDS.Serving as a \"gateway\" is obviously what all\nAPIs so, but the term is also true in the sense that API Gateway  is completely\nconfigured via UI, thus engineers of any programming background can safely\nmodify endpoints, methods,  CORs  configurations, or any of the high-level API\nstructure without being locked into a programming language. API Gateway  is\ntherefore very much an enterprise-geared product: it lends itself to large teams\nand scaling. That said, if it were to be compared to building an API via a\nframework designed to do such things (such as Express or Flask) the experience\nis undoubtedly more clunky. The trade-off being made for speed is immediate\nvisibility, assurance, and a higher chance for collaboration.\n\nThe Challenge of Building a \"Well-Designed\" API\nGood APIs are premeditated. A complex API might accept multiple methods per\nendpoint, allow advanced filtering of results, or handle advanced\nAuthentication. Neither of us have the time to attempt covering all of those\nthings in detail, but I will  leave you with the knowledge that all these\nfeatures are very much possible.\n\nThe API Gateway  interface is where you'd get started. Let's blow through the\nworld's most inappropriately fast explanation of building APIs ever ,and check\nout the UI:\n\nIt ain't pretty, but it works. * Your APIs are listed on the left. You can create more than one, if you're\n   some sort of sadist.\n * The Resources pane is the full structure of your API. At the highest level,\n   'resources' refers to Endpoints,  which are the URLs your API will ultimately\n   expose.\n * Every Endpoint  can contain whichever Methods  you choose to associate with\n   them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST\n   method could contain entirely unrelated logic from a PUT method: its your\n   responsibility to make sure your API design makes sense.\n * Finally, each Method has their expected Request and Response  structures\n   defined individually, which what the horribly designed box diagram is\n   attempting to explain on the right. The box on the left labeled CLIENT refers\n   to the requester, where the box on the right represents the triggered action.\n\nThis UI is your bread and butter. I hope you're strapped in, because walking\nthrough this interface is going to be hella boring for all of us.\n\nCreating a Method Request\nThe first step to creating an endpoint (let's say a GET endpoint) is to set the\nexpectation for what the user will send to us:\n\nAwe yea, authorization. 1. Authorization  allows you to restrict users from using your API unless they\n    follow your IAM policy.\n 2. Request Validator  lets you chose if you'd like this validation to happen\n    via the body, query string parameters, headers, or all of the above.\n 3. API Keys  are useful if you're creating an API to sell commercially or\n    enforce limited access. If your business model revolves around selling an\n    API, you can realistically do this.\n 4. Query String Parameters  are... actually forget it, you know this by now.\n 5. See above.\n 6. If preferred, the Request Body can be assigned a model,  which is\n    essentially a JSON schema. If a request is made to your endpoint which does\n    not match the request body model, it is a malformed request. We'll cover \n    models  in the advanced course, once somebody actually starts paying me to\n    write this stuff.\n\nMethod Execution: AKA \"What do we do with this?\"\nSet the game plan. 1. Integration Type  specifies which AWS service will be accepting or affected\n    by this request. The vast majority of the time, this will be Lambda. If\n    you're wondering why other AWS Services aren't present, this has been made\n    intentional over time as just about any AWS service you can interact with\n    will still need logic to do anything useful: you can't just shove a JSON\n    object in a database's face and expect to get results. Unless you're using\n    MongoDB or something.\n 2. Lambda Proxies  are generally a bad idea. They auto-format your Lambda's \n    request  and response  body to follow a very  specific structure, which is\n    presumably intended to help speed up or standardize development. The\n    downside is these structures are bloated and most likely contain useless\n    information. To get an idea of what these structures look like, check them\n    out here.\n 3. The Region  your Lambda hosted lives in.\n 4. Name of the Lamba Function  your request will be directed to.\n 5. Execution role  refers to the IAM role your Lambda policy will be a part of.\n    This is kind of an obnoxious concept, but your function has permissions as\n    though it were a user. This is presumably Amazon's way of thinking ahead to\n    extending human rights to robots.\n 6. Caller Credentials  refers to API keys, assuming you chose to use them. If\n    this is checked, the API will not be usable without an API key, thus making\n    it difficult to test\n 7. Credentials Cache  probably refers to expiring credentials or something, I'm\n    sure you'll figure it out.\n 8. Timeout  can be increased if you're dealing with an API call that takes a\n    lot of time to respond, such as occasions with heavy data sets.\n 9. URL Paths probably do something, I don't know. Who really cares?\n\nINTERMISSION: The Part Where Things Happen\nThe next step in the flow would be where the AWS service we selected to handle\nthe request would do its thing. We'll get into that next time.\n\nResponse Codes and Headers\nKeep it 200 baby. 1. While AWS provides users with standard error codes  and generic errors, you\n    can add your own specific error/success messages. Props to whoever puts in\n    the effort.\n 2. Header Mappings  are the headings returned with the response. For example,\n    this is where you might solve cross-domain issues via the \n    Access-Control-Allow-Origin  header.\n\n3. Mapping Templates  are the Content-Type  of the response returned, most\ncommonly application/json.\n\nMethod Response\nI almost never spend time hereThis step is a continuation of the previous step.\nI'm not entirely sure what the point in splitting this into two screens is, but\nI'm guessing its not important.\n\nReap Your Rewards\nAt long last, this brings us to the end of our journey. This is presumably where\n you've executed a successful AWS test or something. However, there's a final\nstep before you go live; deploying your API:\n\nDeploy your API to a live \"stage\" and retire.Next time we'll cover the logical,\nless boring part of writing actual code behind these endpoints.","html":"<p>In our last adventure, we ventured off to create our very own cloud <a href=\"https://hackersandslackers.com/setting-up-mysql-on-aws/\">database</a> by using Amazon's <strong>RDS</strong> service. We've also <a href=\"https://hackersandslackers.com/building-an-api-using-aws/\">briefly covered</a> the general concept behind what <strong>Lambda functions</strong>. In case you've already forgotten, Lambdas are basically just chunks of code in the cloud; think of them as tiny virtual servers, which have already been configured (and locked down) to serve one specific purpose. Because that's literally what it is.</p><p>The data being stored in <strong>RDS </strong>is ultimately what we're targeting, and <strong>Lambdas</strong> serve as the in-between logic to serve up, modify, or add to the proper data. The only piece missing from the picture is <strong>API Gateway</strong>. </p><p>As the name suggests, <strong>API Gateway</strong> is the, uh, <em>gateway</em> that users or systems interact with to obtain what they're seeking. It is (hopefully) the only part of this VPC structure an external user can interact with:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_o-1.jpg\" class=\"kg-image\"><figcaption>Simple API to interact with RDS.</figcaption></figure><p>Serving as a \"gateway\" is obviously what all APIs so, but the term is also true in the sense that <strong>API Gateway</strong> is completely configured via UI, thus engineers of any programming background can safely modify <em>endpoints</em>, <em>methods,</em> <em>CORs</em> configurations, or any of the high-level API structure without being locked into a programming language. <strong>API Gateway</strong> is therefore very much an enterprise-geared product: it lends itself to large teams and scaling. That said, if it were to be compared to building an API via a framework designed to do such things (such as Express or Flask) the experience is undoubtedly more clunky. The trade-off being made for speed is immediate visibility, assurance, and a higher chance for collaboration.</p><h2 id=\"the-challenge-of-building-a-well-designed-api\">The Challenge of Building a \"Well-Designed\" API</h2><p>Good APIs are premeditated. A complex API might accept multiple methods per endpoint, allow advanced filtering of results, or handle advanced Authentication. Neither of us have the time to attempt covering all of those things in detail, but I <em>will</em> leave you with the knowledge that all these features are very much possible.  </p><p>The <strong>API Gateway</strong> interface is where you'd get started. Let's blow through the world's most inappropriately fast explanation of building APIs ever ,and check out the UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/apigateway_overview3.png\" class=\"kg-image\"><figcaption>It ain't pretty, but it works.</figcaption></figure><ul><li>Your <strong>APIs </strong>are listed on the left. You can create more than one, if you're some sort of sadist.</li><li>The <strong>Resources </strong>pane is the full structure of your API. At the highest level, 'resources' refers to <strong>Endpoints,</strong> which are the URLs your API will ultimately expose.</li><li>Every <strong>Endpoint</strong> can contain whichever <strong>Methods</strong> you choose to associate with them (GET, POST, PUT, etc). Even if they belong to the same endpoint, a POST method could contain entirely unrelated logic from a PUT method: its your responsibility to make sure your API design makes sense.</li><li>Finally, each <strong>Method </strong>has their expected <strong>Request </strong>and <strong>Response</strong> structures defined individually, which what the horribly designed box diagram is attempting to explain on the right. The box on the left labeled CLIENT refers to the requester, where the box on the right represents the triggered action.</li></ul><p>This UI is your bread and butter. I hope you're strapped in, because walking through this interface is going to be hella boring for all of us.</p><h3 id=\"creating-a-method-request\">Creating a Method Request</h3><p>The first step to creating an endpoint (let's say a GET endpoint) is to set the expectation for what the user will send to us:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodrequest_o.jpg\" class=\"kg-image\"><figcaption>Awe yea, authorization.</figcaption></figure><ol><li><strong>Authorization</strong> allows you to restrict users from using your API unless they follow your IAM policy.</li><li><strong>Request Validator</strong> lets you chose if you'd like this validation to happen via the body, query string parameters, headers, or all of the above.</li><li><strong>API Keys</strong> are useful if you're creating an API to sell commercially or enforce limited access. If your business model revolves around selling an API, you can realistically do this.</li><li><strong>Query String Parameters</strong> are... actually forget it, you know this by now.</li><li>See above.</li><li>If preferred, the <strong>Request Body </strong>can be assigned a <strong>model,</strong> which is essentially a JSON schema. If a request is made to your endpoint which does not match the request body model, it is a malformed request. We'll cover <strong>models</strong> in the advanced course, once somebody actually starts paying me to write this stuff.</li></ol><h3 id=\"method-execution-aka-what-do-we-do-with-this\">Method Execution: AKA \"What do we do with this?\"</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/methodexecution_o.jpg\" class=\"kg-image\"><figcaption>Set the game plan.</figcaption></figure><ol><li><strong>Integration Type</strong> specifies which AWS service will be accepting or affected by this request. The vast majority of the time, this will be Lambda. If you're wondering why other AWS Services aren't present, this has been made intentional over time as just about any AWS service you can interact with will still need logic to do anything useful: you can't just shove a JSON object in a database's face and expect to get results. Unless you're using MongoDB or something.</li><li><strong>Lambda Proxies</strong> are generally a bad idea. They auto-format your Lambda's <em>request</em> and <em>response</em> body to follow a very  specific structure, which is presumably intended to help speed up or standardize development. The downside is these structures are bloated and most likely contain useless information. To get an idea of what these structures look like, check them out <a href=\"https://github.com/bbilger/jrestless/tree/master/aws/gateway/jrestless-aws-gateway-handler#response-schema\">here</a>.</li><li>The <strong>Region</strong> your Lambda hosted lives in.</li><li>Name of the <strong>Lamba Function</strong> your request will be directed to.</li><li><strong>Execution role</strong> refers to the IAM role your Lambda policy will be a part of. This is kind of an obnoxious concept, but your function has permissions as though it were a user. This is presumably Amazon's way of thinking ahead to extending human rights to robots.</li><li><strong>Caller Credentials</strong> refers to API keys, assuming you chose to use them. If this is checked, the API will not be usable without an API key, thus making it difficult to test</li><li><strong>Credentials Cache</strong> probably refers to expiring credentials or something, I'm sure you'll figure it out.</li><li><strong>Timeout</strong> can be increased if you're dealing with an API call that takes a lot of time to respond, such as occasions with heavy data sets.</li><li><strong>URL Paths </strong>probably do something, I don't know. Who really cares?</li></ol><h3 id=\"intermission-the-part-where-things-happen\">INTERMISSION: The Part Where Things Happen</h3><p>The next step in the flow would be where the AWS service we selected to handle the request would do its thing. We'll get into that next time.</p><h3 id=\"response-codes-and-headers\">Response Codes and Headers</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/response_o.jpg\" class=\"kg-image\"><figcaption>Keep it 200 baby.</figcaption></figure><ol><li>While AWS provides users with standard <strong>error codes</strong> and generic errors, you can add your own specific error/success messages. Props to whoever puts in the effort.</li><li><strong>Header Mappings</strong> are the headings returned with the response. For example, this is where you might solve cross-domain issues via the <em>Access-Control-Allow-Origin</em> header.</li></ol><p>3. <strong>Mapping Templates</strong> are the <em>Content-Type</em> of the response returned, most commonly <em>application/json.</em></p><h3 id=\"method-response\">Method Response</h3><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-8.54.51-PM_o.png\" class=\"kg-image\"><figcaption>I almost never spend time here</figcaption></figure><p>This step is a continuation of the previous step. I'm not entirely sure what the point in splitting this into two screens is, but I'm guessing its not important.</p><h2 id=\"reap-your-rewards\">Reap Your Rewards</h2><p>At long last, this brings us to the end of our journey. This is presumably where  you've executed a successful AWS test or something. However, there's a final step before you go live; deploying your API:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-29-at-9.02.52-PM_o.png\" class=\"kg-image\"><figcaption>Deploy your API to a live \"stage\" and retire.</figcaption></figure><p>Next time we'll cover the logical, less boring part of writing actual code behind these endpoints.</p>","url":"https://hackersandslackers.com/creating-apis-with-api-gateway/","uuid":"ce9c1023-431b-4580-b3ca-1a3e2074f9c5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5af8ae23092feb404eb9981e"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372f","title":"Deploy Isolated Applications with Google App Engine","slug":"deploy-app-containters-with-gcp-app-engine","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","excerpt":"Doing everything to avoid server configuration or any mild discomfort.","custom_excerpt":"Doing everything to avoid server configuration or any mild discomfort.","created_at_pretty":"25 October, 2018","published_at_pretty":"25 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-24T20:30:22.000-04:00","published_at":"2018-10-25T07:30:00.000-04:00","updated_at":"2019-01-06T11:26:06.000-05:00","meta_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","meta_description":"Doing everything to avoid server configuration or any mild discomfort.","og_description":"Doing everything to avoid server configuration or any mild discomfort.","og_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","og_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","twitter_description":"Doing everything to avoid server configuration or any mild discomfort.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/appengine@2x.jpg","twitter_title":"Deploy App Containters with GCP App Engine | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"We've been on a bit of a tear lately on Google Cloud lately (or at least I\nhave), and I have no desire to stop any time soon. I probably should though...\n our analytics show that half our viewers are just people struggling to us AWS.\nSpeaking of capitalizing on shitty UI, stay tuned in the future where we'll\noffer grossly overpriced unauthorized AWS certification programs.\n\nAWS aside, I'm here to talk about the other  Cloud in town - in particular,\nGoogle's solution to make sure you never configure a webserver again. This is a\ntrend that's been grinding my gears a bit: as much as I appreciate the reduction\nin effort, the costs of choosing proprietary (paid) services to avoid opening a\nLinux shell seems like a dangerous prospect over time as every day developers\nbecome more and more reliant on convenience. Then again, I'm probably just angry\nthat nobody will have to endure the pain of Python development circa 2012. \n\nRandom Old-Man Tangent About Configuring Webservers\nRemember when we all ran Apache servers, and the world decided that mod_python \nshould stop existing for no reason? The replacement was, of course, mod_wsgi:  \nan entirely undocumented way of running Python on an Apache server created by a\nsingle guy from Google (who has apparently opted to spend the entirety of his\nlife attempting to explain mod_wsgi on StackOverflow\n[https://stackoverflow.com/users/128141/graham-dumpleton]). \n\nBesides mod_wsgi, the Nginx alternatives (Gunicorn  and uWSGI) are almost\nequally insufferable to implement. Much of this can be attributed to tutorials\n(such as those posted by DigitalOcean) which dominate SEO, merely because those\ntutorials include glaring inexcusable typos  in their configuration files. This\nresults in an infinite Google search feedback loop, where you find what seems to\nbe a perfectly fine tutorial... plus 10 pages of frustrated developers\nbacklinking to said tutorial, trying to figure out where the hell the missing\ncolon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm\npretty sure at this point nobody cares to put up with monetized troubleshooting\nbullshit schemes (calling it now: the slightly-false-tutorial is an elaborate\nSEO scam).  So yeah, app containers it is then.\n\nThe Benefits of App Engine\nBesides not needing to know anything about Linux, hosting on App Engine provides\na few other benefits. Considering all microservices are obfuscated in the cloud,\nwe can easily hook into other services such as setting up CRON jobs, Tasks, and\nDNS, for instance. GCP's catalogue of offerings is destined to grow, whether\nthose offerings are ferociously released from Google's ambitious backlog, or the\nresult of a partnership utilizing Google Cloud for architecture, such as MongoDB\ncloud and others. Prepare to witness fierce and unapologetic growth from GCP by\nevery metric, year-over-year. \n\nApp Engine  is also intimately close with your source code. Despite the\ndynamically typed nature of Python and Javascript, App Engine will catch fatal\nerrors when attempting to deploy your app which would not happen otherwise.\nAdding this type of interpreter adds a convenient level of 'easy mode,' where\npotentially fatal production errors are caught before deployment is even\npermitted. I even tried deploying some personal projects to App Engine which had\nbeen running live elsewhere, and App Engine was able to catch errors existing in\nmy code which had been shamelessly running in production. Oops.\n\nEven while the app is live, all errors are conveniently detected and reported\nfront and center in the app engine dashboard:\n\n\"No module named App\" seems like a pretty bad error.So yes, there are plenty of\nbenefits and reasons to use App Engine over a VPS: removal of webserver\nconfiguration, build errors caught at runtime, and easy command-line deployments\nname a few of such benefits. The question of whether or not these perks are\nworth the price tag and vendor-lock are a personal decision.\n\nCreating your First App... Engine\nGoogle provides the luxury of creating apps in a variety of languages, including\nhot new -comer to the game, PHP. Lucky us!\n\nHah, .NET is still a thing too.Google will forcefully insist you complete their\nown step-by-step tutorial, which essentially teaches you how to use git clone \nand explains the contents of their YAML file. You can follow this if you want. \n\nMore interestingly is what you'll find when you open the GCP browser shell.\nWhile working through this tutorial, it's impossible to ignore that Google Cloud\nis essentially just a giant VPS across all your projects:\n\nAll of these directories were created in different projects.Just when we were\ndone ranting, it turns out every service we pay for is just a thinly veiled\nobfuscation of something we could probably do on a 10 dollar Droplet. Fuck,\nlet's just move on.\n\nSimple Project Configuration\nPerhaps majority of what one needs to learn to deploy apps is contained within a\nsingle YAML file. Add a YAML file in your directory:\n\nruntime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n\n\nSet your Static directory if you're working in Python. Use Python 3.7.\n\nGoogle also invented its own version of .gitignore  for App Engine called \n.gcloudignore, so be aware of that.\n\nHaving worked with Flask in the past, you should presumably be familiar with a\nstartup script such as the following:\n\nfrom framewrk import create_app\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0')\n\n\nThat's pretty much it man. Just remember that Google prefers requirements.txt \nover other forms of package management (Google will actually bar Pipfile from\nbeing committed, interestingly enough). \n\nIf you're working locally, gcloud app deploy  is all you need to push to\nproduction (doesn't require a git commit, interestingly enough. gcloud app\nbrowse  will open you newly deployed app, and gcloud app logs tail -s default \nwill display your logs when something goes horribly wrong.\n\nAnd there you have it: the practical and completely cynical guide to embracing\nmodern architecture. Join us next time when we pay 50 dollars to deploy a\nsingle-click app simply because we're too lazy or unmotivated to set anything up\nourselves.","html":"<p>We've been on a bit of a tear lately on Google Cloud lately (or at least I have), and I have no desire to stop any time soon. I probably should though...  our analytics show that half our viewers are just people struggling to us AWS. Speaking of capitalizing on shitty UI, stay tuned in the future where we'll offer grossly overpriced unauthorized AWS certification programs.</p><p>AWS aside, I'm here to talk about the <em>other</em> Cloud in town - in particular, Google's solution to make sure you never configure a webserver again. This is a trend that's been grinding my gears a bit: as much as I appreciate the reduction in effort, the costs of choosing proprietary (paid) services to avoid opening a Linux shell seems like a dangerous prospect over time as every day developers become more and more reliant on convenience. Then again, I'm probably just angry that nobody will have to endure the pain of Python development circa 2012. </p><h3 id=\"random-old-man-tangent-about-configuring-webservers\">Random Old-Man Tangent About Configuring Webservers</h3><p>Remember when we all ran Apache servers, and the world decided that <strong>mod_python</strong> should stop existing for no reason? The replacement was, of course, <strong>mod_wsgi</strong>:<strong> </strong>an entirely undocumented way of running Python on an Apache server created by a single guy from Google (who has apparently opted to spend the entirety of his life attempting to <a href=\"https://stackoverflow.com/users/128141/graham-dumpleton\">explain <strong>mod_wsgi</strong> on StackOverflow</a>). </p><p>Besides <strong>mod_wsgi</strong>, the Nginx alternatives (<strong>Gunicorn</strong> and <strong>uWSGI</strong>) are almost equally insufferable to implement. Much of this can be attributed to tutorials (such as those posted by DigitalOcean) which dominate SEO, merely because those tutorials include <em>glaring inexcusable typos</em> in their configuration files. This results in an infinite Google search feedback loop, where you find what seems to be a perfectly fine tutorial... plus 10 pages of frustrated developers backlinking to said tutorial, trying to figure out where the hell the missing colon is in their Nginx config. Spoiler alert: that's not the only typo, and I'm pretty sure at this point nobody cares to put up with monetized troubleshooting bullshit schemes (calling it now: the slightly-false-tutorial is an elaborate SEO scam).  So yeah, app containers it is then.</p><h2 id=\"the-benefits-of-app-engine\">The Benefits of App Engine</h2><p>Besides not needing to know anything about Linux, hosting on App Engine provides a few other benefits. Considering all microservices are obfuscated in the cloud, we can easily hook into other services such as setting up CRON jobs, Tasks, and DNS, for instance. GCP's catalogue of offerings is destined to grow, whether those offerings are ferociously released from Google's ambitious backlog, or the result of a partnership utilizing Google Cloud for architecture, such as MongoDB cloud and others. Prepare to witness fierce and unapologetic growth from GCP by every metric, year-over-year. </p><p><strong>App Engine</strong> is also intimately close with your source code. Despite the dynamically typed nature of Python and Javascript, App Engine will catch fatal errors when attempting to deploy your app which would not happen otherwise. Adding this type of interpreter adds a convenient level of 'easy mode,' where potentially fatal production errors are caught before deployment is even permitted. I even tried deploying some personal projects to App Engine which had been running live elsewhere, and App Engine was able to catch errors existing in my code which had been shamelessly running in production. Oops.</p><p>Even while the app is live, all errors are conveniently detected and reported front and center in the app engine dashboard:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.12.19-PM.png\" class=\"kg-image\"><figcaption>\"No module named App\" seems like a pretty bad error.</figcaption></figure><p>So yes, there are plenty of benefits and reasons to use App Engine over a VPS: removal of webserver configuration, build errors caught at runtime, and easy command-line deployments name a few of such benefits. The question of whether or not these perks are worth the price tag and vendor-lock are a personal decision.</p><h2 id=\"creating-your-first-app-engine\">Creating your First App... Engine</h2><p>Google provides the luxury of creating apps in a variety of languages, including hot new -comer to the game, PHP. Lucky us!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.17.59-PM.png\" class=\"kg-image\"><figcaption>Hah, .NET is still a thing too.</figcaption></figure><p>Google will forcefully insist you complete their own step-by-step tutorial, which essentially teaches you how to use <strong>git clone</strong> and explains the contents of their YAML file. You can follow this if you want. </p><p>More interestingly is what you'll find when you open the GCP browser shell. While working through this tutorial, it's impossible to ignore that Google Cloud is essentially just a giant VPS <em>across all your projects:</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-24-at-10.26.44-PM.png\" class=\"kg-image\"><figcaption>All of these directories were created in different projects.</figcaption></figure><p>Just when we were done ranting, it turns out every service we pay for is just a thinly veiled obfuscation of something we could probably do on a 10 dollar Droplet. Fuck, let's just move on.</p><h3 id=\"simple-project-configuration\">Simple Project Configuration</h3><p>Perhaps majority of what one needs to learn to deploy apps is contained within a single YAML file. Add a YAML file in your directory:</p><pre><code class=\"language-yaml\">runtime: python37\napi_version: 1\n\nhandlers:\n  # This configures Google App Engine to serve the files in the app's static\n  # directory.\n- url: static\n  static_dir: static\n\n  # This handler routes all requests not caught above to your main app.\n  # Required when static routes are defined. \n  # Can be omitted when there are no static files defined.\n- url: /.*\n  script: auto\n</code></pre>\n<p>Set your Static directory if you're working in Python. Use Python 3.7.</p><p>Google also invented its own version of <code>.gitignore</code> for App Engine called <code>.gcloudignore</code>, so be aware of that.</p><p>Having worked with Flask in the past, you should presumably be familiar with a startup script such as the following:</p><pre><code class=\"language-python\">from framewrk import create_app\n\napp = create_app()\n\nif __name__ == &quot;__main__&quot;:\n    app.run(host='0.0.0.0')\n</code></pre>\n<p>That's pretty much it man. Just remember that Google prefers <strong>requirements.txt </strong>over other forms of package management (Google will actually bar Pipfile from being committed, interestingly enough). </p><p>If you're working locally, <code>gcloud app deploy</code> is all you need to push to production (doesn't require a git commit, interestingly enough. <code>gcloud app browse</code> will open you newly deployed app, and <code>gcloud app logs tail -s default</code> will display your logs when something goes horribly wrong.</p><p>And there you have it: the practical and completely cynical guide to embracing modern architecture. Join us next time when we pay 50 dollars to deploy a single-click app simply because we're too lazy or unmotivated to set anything up ourselves.</p>","url":"https://hackersandslackers.com/deploy-app-containters-with-gcp-app-engine/","uuid":"70f7a025-6774-4555-9eff-e63eb40c4fdf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd10e9e4ba34679679904f2"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372e","title":"MySQL, Google Cloud, and a REST API that Generates Itself","slug":"mysql-google-cloud-and-a-rest-api-that-autogenerates","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","custom_excerpt":"Deploy a MySQL database that auto-creates endpoints for itself.","created_at_pretty":"23 October, 2018","published_at_pretty":"23 October, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-10-23T14:57:12.000-04:00","published_at":"2018-10-23T18:47:28.000-04:00","updated_at":"2019-02-02T05:26:16.000-05:00","meta_title":"MySQL, Google Cloud, and a REST API | Hackers and Slackers","meta_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","og_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","og_title":"MySQL, Google Cloud, and a REST API that Generates Itself","twitter_description":"Create a MySQL database that makes its own endpoints for accessing and manipulating data.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/mysqlagain2@2x.jpg","twitter_title":"MySQL, Google Cloud, and a REST API that Generates Itself","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Working with MySQL","slug":"working-with-mysql","description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysql1_o-1.jpg","meta_description":"Learn about MySQL database configuration, the query language, and cloud hosted instances.","meta_title":"Working with MySQL","visibility":"internal"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"}],"plaintext":"It wasn’t too long ago that I haphazardly forced us down a journey of exploring\nGoogle Cloud’s cloud SQL service. The focus of this exploration was Google’s\naccompanying REST API for all of its cloud SQL instances. That API turned out to\nbe a relatively disappointing administrative API which did little to extend the\nfeatures you’d expect from the CLI or console.\n\nYou see, I’ve had a dream stuck in my head for a while now. Like most of my\nutopian dreams, this dream is related to data, or more specifically simplifying\nthe manner in which we interact with it. For industry synonymous with AI and\nautomation, many of our very own tools (including ETL tools) involve way too\nmuch manual effort in my opinion. That’s right: I’m talking about the aspiration\nto Slack while we Hack.\n\nThe pitch is this: why do we keep setting up databases, endpoints, and the logic\nto connect them when, 90% of the time, we’re building the same thing over and\nover? Let me guess: there’s a GET endpoint to get records from table X, or a\nPOST endpoint to create users. I know you’ve built this because we all have, but\nwhy do we keep building the same things over and over in isolation? It looks\nlike we might not have to anymore, but first let’s create our database.\n\nCreating a MySQL Instance in GCP \nFull disclosure here: the magical REST API thing is actually independent from\nGoogle Cloud; the service we’ll be using can integrate with any flavor of MySQL\nyou prefer, so go ahead and grab that RDS instance you live so much if you\nreally have to.\n\nFor the rest of us, hit up your GCP console and head into making a new SQL\ninstance. MySQL and Postgres are our only choices here; stick with MySQL.\n\nThere isn’t much to spinning up your instance. Just be sure to create a user and\ndatabase to work from.\n\nOh yeah, and remember to name your instance.Your SQL Firewall and Permissions\nYour instance is set to “public” by default. Oddly, “public” in this case means\n“accessible to everybody on your IP whitelist, which is empty by default,” so\nreally kind of the opposite of public really.\n\nIn fact, if you hypothetically did want to open your instance publicly, Google\nCloud will not allow it. This is good on them, and is actually fairly impressive\nthe depths they go to avoid the IP 0.0.0.0  from ever appearing anywhere in the\ninstance. Go ahead, open the shell and try to add bind address=0.0.0.0 \nyourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s\nversion of MySQL is actually a MariaDB instance)?\n\nThe point is, whitelist your IP address. Simply \"Edit\" your instance and add\nyour address to the authorized networks.\n\nAuthorize that bad boy.The Magic API \nNow, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so\nthis next part is going to feel a bit a bit weird. I’m not sure why, as the\nservice is apparently free, thus I’m clearly not getting paid for any of this.\n\nAnyway, the service is called Apisentris [https://apisentris.com/], and the idea\nis that it will build whatever time-consuming monstrosity of a REST API you were\nplanning to build to access your data for you. Via their own words:\n\nSee, I told you.What does this actually mean? It means if you create a table\ncalled articles  in your database, you will immediately have an endpoint to\nfetch said articles, and it would look like \nhttps://apisentris.com/api/v1/articles. Your client ID and credentials would\nobviously need to be provided to indicate that you're, well, you.\n\nGrabbing entire tables at once would be silly, which is why they also\nautogenerate filters based on the contents of your table:\n\nEndpoints accept query parameters to essentially create a query.Oh yeah, and you\ncan also handle user management via this API as well, if you're building an\nactual app:\n\nPretty easy to hook up into a form or whatever.I'll assume you're sold on the\nidea by now. If a free service that handles the hard parts of backend logic for\nfree isn't your cup of tea, clearly you aren't Slacker material.\n\nSetting it all up\nAs we did before with our own IP, we'll need to whitelist Apisentris' IP the\nsame way in GCP console. Their IP is 104.199.181.125.\n\nCreate a table in your database with some data just to test things out. When\nyou're logged in, you'll be able to see all the endpoints available to you and\nthe associated attributes they have:\n\nNot bad.Any way you slice it, the concept of a self-generating API is very cool\nand yet somehow still not the norm. I'm actually shocked that there are so few\npeople in the Data industry who know \"there must be a better way,\" but then\nagain, data science and software engineering are two very different things. For\nmy fellow Data Engineers out there, take this as a gift and a curse: you have\nthe gift of knowing better from your software background, but are cursed with\nwatching the world not quite realize how pointless half the things they do truly\nare.\n\nOh well. We'll be the ones building the robots anyway.","html":"<p>It wasn’t too long ago that I haphazardly forced us down a journey of exploring Google Cloud’s cloud SQL service. The focus of this exploration was Google’s accompanying REST API for all of its cloud SQL instances. That API turned out to be a relatively disappointing administrative API which did little to extend the features you’d expect from the CLI or console.</p><p>You see, I’ve had a dream stuck in my head for a while now. Like most of my utopian dreams, this dream is related to data, or more specifically simplifying the manner in which we interact with it. For industry synonymous with AI and automation, many of our very own tools (including ETL tools) involve way too much manual effort in my opinion. That’s right: I’m talking about the aspiration to Slack while we Hack.</p><p>The pitch is this: why do we keep setting up databases, endpoints, and the logic to connect them when, 90% of the time, we’re building the same thing over and over? Let me guess: there’s a GET endpoint to get records from table X, or a POST endpoint to create users. I know you’ve built this because we all have, but why do we keep building the same things over and over in isolation? It looks like we might not have to anymore, but first let’s create our database.</p><h2 id=\"creating-a-mysql-instance-in-gcp\">Creating a MySQL Instance in GCP </h2><p>Full disclosure here: the magical REST API thing is actually independent from Google Cloud; the service we’ll be using can integrate with any flavor of MySQL you prefer, so go ahead and grab that RDS instance you live so much if you really have to.</p><p>For the rest of us, hit up your GCP console and head into making a new SQL instance. MySQL and Postgres are our only choices here; stick with MySQL.</p><p>There isn’t much to spinning up your instance. Just be sure to create a user and database to work from.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.15.18-PM.png\" class=\"kg-image\"><figcaption>Oh yeah, and remember to name your instance.</figcaption></figure><h3 id=\"your-sql-firewall-and-permissions\">Your SQL Firewall and Permissions</h3><p>Your instance is set to “public” by default. Oddly, “public” in this case means “accessible to everybody on your IP whitelist, which is empty by default,” so really kind of the opposite of public really.</p><p>In fact, if you hypothetically did want to open your instance publicly, Google Cloud will not allow it. This is good on them, and is actually fairly impressive the depths they go to avoid the IP <strong>0.0.0.0</strong> from ever appearing anywhere in the instance. Go ahead, open the shell and try to add <code>bind address=0.0.0.0</code> yourself, wise guy (another fun fact you’ll learn in the shell: apparently GCP’s version of MySQL is actually a MariaDB instance)?</p><p>The point is, whitelist your IP address. Simply \"Edit\" your instance and add your address to the authorized networks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.12.23-PM.png\" class=\"kg-image\"><figcaption>Authorize that bad boy.</figcaption></figure><h2 id=\"the-magic-api\">The Magic API </h2><p>Now, we’ve never actually endorsed any SaaS products on Hackers and Slackers, so this next part is going to feel a bit a bit weird. I’m not sure why, as the service is apparently free, thus I’m clearly not getting paid for any of this.</p><p>Anyway, the service is called <strong><a href=\"https://apisentris.com/\">Apisentris</a>, </strong>and the idea is that it will build whatever time-consuming monstrosity of a REST API you were planning to build to access your data for you. Via their own words:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.19.16-PM.png\" class=\"kg-image\"><figcaption>See, I told you.</figcaption></figure><p>What does this actually mean? It means if you create a table called <em>articles</em> in your database, you will immediately have an endpoint to fetch said articles, and it would look like <strong>https://apisentris.com/api/v1/articles. </strong>Your client ID and credentials would obviously need to be provided to indicate that you're, well, you.</p><p>Grabbing entire tables at once would be silly, which is why they also autogenerate filters based on the contents of your table:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.25.13-PM.png\" class=\"kg-image\"><figcaption>Endpoints accept query parameters to essentially create a query.</figcaption></figure><p>Oh yeah, and you can also handle user management via this API as well, if you're building an actual app:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-23-at-5.27.43-PM.png\" class=\"kg-image\"><figcaption>Pretty easy to hook up into a form or whatever.</figcaption></figure><p>I'll assume you're sold on the idea by now. If a free service that handles the hard parts of backend logic for free isn't your cup of tea, clearly you aren't Slacker material.</p><h2 id=\"setting-it-all-up\">Setting it all up</h2><p>As we did before with our own IP, we'll need to whitelist Apisentris' IP the same way in GCP console. Their IP is <code>104.199.181.125</code>.</p><p>Create a table in your database with some data just to test things out. When you're logged in, you'll be able to see all the endpoints available to you and the associated attributes they have:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/schema.gif\" class=\"kg-image\"><figcaption>Not bad.</figcaption></figure><p>Any way you slice it, the concept of a self-generating API is very cool and yet somehow still not the norm. I'm actually shocked that there are so few people in the Data industry who know \"there must be a better way,\" but then again, data science and software engineering are two very different things. For my fellow Data Engineers out there, take this as a gift and a curse: you have the gift of knowing better from your software background, but are cursed with watching the world not quite realize how pointless half the things they do truly are.</p><p>Oh well. We'll be the ones building the robots anyway.</p>","url":"https://hackersandslackers.com/mysql-google-cloud-and-a-rest-api-that-autogenerates/","uuid":"c45478bb-54da-4563-89bd-ddd356a234d4","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bcf6f08d7ab443ba8b7a5ab"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372d","title":"Working With Google Cloud Functions","slug":"creating-a-python-google-cloud-function","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","excerpt":"GCP scores a victory by trivializing serverless functions.","custom_excerpt":"GCP scores a victory by trivializing serverless functions.","created_at_pretty":"18 October, 2018","published_at_pretty":"19 October, 2018","updated_at_pretty":"14 April, 2019","created_at":"2018-10-18T19:44:02.000-04:00","published_at":"2018-10-18T22:33:07.000-04:00","updated_at":"2019-04-14T07:24:50.000-04:00","meta_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","meta_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. Set up a CRON job to run your function with Cloud Scheduler.","og_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. ","og_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","og_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","twitter_description":"Create serverless functions using Google Cloud's Cloud Functions and Source Repositories. ","twitter_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","twitter_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"The more I explore Google Cloud's endless catalog of cloud services, the more I\nlike Google Cloud. This is why before moving forward, I'd like to be transparent\nthat this blog has become little more than thinly veiled Google propaganda,\nwhere I will henceforth bombard you with persuasive and subtle messaging to sell\nyour soul to Google. Let's be honest; they've probably simulated it anyway.\n\nIt should be safe to assume that you're familiar with AWS Lambda Functions\n[https://hackersandslackers.com/creating-endpoints-with-lambda/]  by now, which\nhave served as the backbone of what we refer to as \"serverless.\" These cloud\ncode snippets have restructured entire technology departments, and are partially\nto blame for why almost nobody knows enough basic Linux to configure a web\nserver or build anything without a vendor. Google Cloud Functions don't yet\nserve all the use cases that Lambda functions cover, but for the cases they do\ncover, they seem to be taking the lead.\n\nLambdas vs Cloud Functions\nFirst off, let's talk about a big one: price. AWS charges based on Lambda usage,\nwhereas Google Cloud Functions are free. The only exception to this is when you\nbreak 2 million invocations/month, at which point you'll be hemorrhaging as\nghastly 40 cents per additional million. That's ridiculous. I think we've just\ndiscovered Google Cloud's lead generation strategy.\n\nWhat about in terms of workflow? AWS holds an architecture philosophy of\nchaining services together, into what inevitably becomes a web of self-contained\nbillable items on your invoice. An excellent illustration of this is a  post on \ncommon AWS patterns\n[https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/]  which\nprovides a decent visual of this complexity, while also revealing how much\npeople love this kind of stuff, as though SaaS is the new Legos. To interact\nwith a Lambda function in AWS via HTTP requests, you need to set up an API\nGateway in front, which is arguably a feat more convoluted and complicated than\ncoding. Pair this with an inevitable user permission struggle to get the right\nLambda roles set up, and you quickly have yourself a nightmare- especially  if\nyou're trying to get a single function live. Eventually, you’ll get to write\nsome code or upload a horrendous zip file like some neanderthal (friendly\nreminder: I am entirely biased).\n\nCloud Functions do have their drawbacks in this comparison. Firstly, we cannot\nbuild APIs with Cloud Functions- in fact, without Firebase, we can't even use\nvanity URLs. \n\nAnother huge drawback is Cloud Functions cannot communicate with Google's\nrelational database offering, Cloud SQL. This is big, and what's worse, it feels\nlike an oversight. There are no technical constraints behind this, other than\nGoogle hasn't created an interface to whitelist anything other than IP addresses\nfor Cloud SQL instances.\n\nLastly, we cannot create a fully-fledged API available to be sold or\ndistributed. The is currently no Google Cloud API Gateway equivalent.\n\nDeploying a Cloud Function\nBy the end of this tutorial we'll have utilized the following Google Cloud\nservices/tools:\n\n * A new Google Cloud Function\n * A Google  Source Repository to sync to our Github repo and auto-deploy\n   changes.\n * A CRON job to run our function on a schedule, via Google Cloud Scheduler.\n * The gcloud  CLI to enable us to work locally.\n\nYou'll notice we lack any mentions of API endpoints, methods, stages, or\nanything related to handling web requests. It should not be understated that \nCloud Functions are preconfigured with an endpoint, and all nonsense regarding\nwhether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely.\nThese things are instead handled in the  logic of the function itself, and\nbecause Google Cloud functions running Python are preconfigured with Flask, all\nof that stuff is really trivially easy.  That's right, we've got Flask, Python, \n and GCP  all in a single post. Typing these words feels like eating cake while\nDwyane The Rock Johnson reads me bedtime stories and caresses me as I fall\nasleep. It's great.\n\nIn the Cloud console, go ahead and create a new function. Our function will take\nthe form of an HTTP endpoint:\n\nSingle-page setup. Easy. * Memory Allocated lets us allocate more than the default 256MB to our\n   function. Remember that Cloud functions are free: choose accordingly.\n * Trigger  specifies what will have access to this function. By selecting HTTP,\n   we will immediately receive a URL.\n * Source code  gives us a few options to deploy our code, with cloud source\n   repository  being by far the easiest solution (more on that in a bit).\n * Runtime  allows you to select NodeJS by accident.\n * Function to Execute  needs the name of our entry point function, which is to\n   be found in main.py  or main.js  depending on which language you’ve selected.\n\nIf you're familiar with Lambda functions, the choices of an inline code editor\nor a zip file upload should come as no surprise. Since you're already familiar,\nI don't need to tell you why these methods suck  for any sane workflow. Luckily,\nwe have a better option: syncing our Github repo to a Google Source Repository.\n\nGoogle Source Repositories\nGoogle Source Repositories are repos that function just like Github or Bitbucket\nrepos. They're especially useful for syncing code changes from a Github repo,\nand setting up automatic deployments on commit.\n\nThe Google Source Repositories HomeSetting up this sync is super easy. Create a\nnew repository, specify that we're connecting an external Github repo, and we'll\nbe able to select any repo in our account via the GUI:\n\nSyncing a Github RepoNow when we go back to our editing our function, setting\nthe Source code  field to the name of this new repository will automatically\ndeploy the function whenever a change is committed. With this method, we have\neffectively zero changes to our normal workflow.\n\nCommit to Google Source Repositories Directly\nIf you don't want to sync a Github repo, no problem. We can create a repo\nlocally using the gcloud CLI:\n\n$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n\n\nNow make that puppy go live with gcloud functions deploy totally-dope-function,\nwhere totally-dope-function  is the name of your function, as it should be.\n\nWith our function set up and method in place for deploying code, we can now see\nhow our Cloud Function is doing.\n\nViewing Error Logs\nBecause we have a real endpoint to work with, we don't need to waste any time\ncreating dumb unit tests where we send fake JSON to our function (real talk\nthough, we should always write unit tests).\n\nThe Cloud Function error log screen does a decent job of providing us with a GUI\nto see how our deployed function is running, and where things have gone wrong:\n\nOrange Exclamation Marks Denote ErrorsFiring Our Function on a Schedule\nLet's say our function is a job we're looking to run daily, or perhaps hourly.\nGoogle Cloud Scheduler is a super easy way to trigger functions via CRON.\n\nHow is this free again?The easiest way to handle this is by creating our\nfunction as an HTTP endpoint back when we started. A Cloud Scheduler  job can\nhit this endpoint at any time interval we want - just make sure you wrote your\nendpoint to handle GET requests.\n\nCloud Functions in Short\nGCP seems to have been taking notes on the sidelines on how to improve this\nprocess by removing red-tape around service setup or policy configuration. AWS\nand GCP are tackling opposites approaches; AWS allows you to build a Robust API\ncomplete with staging and testing with the intent that some of these APIs can\neven be sold as standalone products to consumers. GCP takes the opposite\napproach: cloud functions are services intended for developers to develop. That\nshould probably cover the vast majority of use cases anyway.","html":"<p>The more I explore Google Cloud's endless catalog of cloud services, the more I like Google Cloud. This is why before moving forward, I'd like to be transparent that this blog has become little more than thinly veiled Google propaganda, where I will henceforth bombard you with persuasive and subtle messaging to sell your soul to Google. Let's be honest; they've probably simulated it anyway.</p><p>It should be safe to assume that you're familiar with AWS <a href=\"https://hackersandslackers.com/creating-endpoints-with-lambda/\">Lambda Functions</a> by now, which have served as the backbone of what we refer to as \"serverless.\" These cloud code snippets have restructured entire technology departments, and are partially to blame for why almost nobody knows enough basic Linux to configure a web server or build anything without a vendor. Google Cloud Functions don't yet serve all the use cases that Lambda functions cover, but for the cases they do cover, they seem to be taking the lead.</p><h2 id=\"lambdas-vs-cloud-functions\">Lambdas vs Cloud Functions</h2><p>First off, let's talk about a big one: price. AWS charges based on Lambda usage, whereas Google Cloud Functions are <strong>free</strong>. The only exception to this is when you break 2 million invocations/month, at which point you'll be hemorrhaging as ghastly <strong>40 cents per additional million</strong>. That's ridiculous. I think we've just discovered Google Cloud's lead generation strategy.</p><p>What about in terms of workflow? AWS holds an architecture philosophy of chaining services together, into what inevitably becomes a web of self-contained billable items on your invoice. An excellent illustration of this is a  post on <a href=\"https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/\">common AWS patterns</a> which provides a decent visual of this complexity, while also revealing how much people love this kind of stuff, as though SaaS is the new Legos. To interact with a Lambda function in AWS via HTTP requests, you need to set up an API Gateway in front, which is arguably a feat more convoluted and complicated than coding. Pair this with an inevitable user permission struggle to get the right Lambda roles set up, and you quickly have yourself a nightmare- <em>especially</em> if you're trying to get a single function live. Eventually, you’ll get to write some code or upload a horrendous zip file like some neanderthal (friendly reminder: I am entirely biased).</p><p>Cloud Functions do have their drawbacks in this comparison. Firstly, we cannot build APIs with Cloud Functions- in fact, without Firebase, we can't even use vanity URLs. </p><p>Another huge drawback is Cloud Functions cannot communicate with Google's relational database offering, Cloud SQL. This is big, and what's worse, it feels like an oversight. There are no technical constraints behind this, other than Google hasn't created an interface to whitelist anything other than IP addresses for Cloud SQL instances.</p><p>Lastly, we cannot create a fully-fledged API available to be sold or distributed. The is currently no Google Cloud API Gateway equivalent.</p><h2 id=\"deploying-a-cloud-function\">Deploying a Cloud Function</h2><p>By the end of this tutorial we'll have utilized the following Google Cloud services/tools:</p><ul><li>A new <strong>Google Cloud Function</strong></li><li>A <strong>Google</strong> <strong>Source Repository </strong>to sync to our Github repo and auto-deploy changes.</li><li>A CRON job to run our function on a schedule, via <strong>Google Cloud Scheduler</strong>.</li><li>The <strong>gcloud</strong> CLI to enable us to work locally.</li></ul><p>You'll notice we lack any mentions of API endpoints, methods, stages, or anything related to handling web requests. It should not be understated that <em>Cloud Functions are preconfigured with an endpoint</em>, and all nonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely. These things are instead handled in the  logic of the function itself, and because Google Cloud functions running Python are preconfigured with <strong>Flask, </strong>all of that stuff is <em>really trivially easy.</em> That's right, we've got <em>Flask</em>, <em>Python</em>,<em> </em>and <em>GCP</em> all in a single post. Typing these words feels like eating cake while Dwyane The Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's great.</p><p>In the Cloud console, go ahead and create a new function. Our function will take the form of an HTTP endpoint:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/func.gif\" class=\"kg-image\"><figcaption>Single-page setup. Easy.</figcaption></figure><!--kg-card-end: image--><ul><li><strong>Memory Allocated </strong>lets us allocate more than the default 256MB to our function. Remember that Cloud functions are free: choose accordingly.</li><li><strong>Trigger</strong> specifies what will have access to this function. By selecting HTTP, we will immediately receive a URL.</li><li><strong>Source code</strong> gives us a few options to deploy our code, with <em>cloud source repository</em> being by far the easiest solution (more on that in a bit).</li><li><strong>Runtime</strong> allows you to select NodeJS by accident.</li><li><strong>Function to Execute</strong> needs the name of our entry point function, which is to be found in <code>main.py</code> or <code>main.js</code> depending on which language you’ve selected.</li></ul><p>If you're familiar with Lambda functions, the choices of an inline code editor or a zip file upload should come as no surprise. Since you're already familiar, I don't need to tell you why these methods <em>suck</em> for any sane workflow. Luckily, we have a better option: syncing our Github repo to a Google Source Repository.</p><h2 id=\"google-source-repositories\">Google Source Repositories</h2><p>Google Source Repositories are repos that function just like Github or Bitbucket repos. They're especially useful for syncing code changes from a Github repo, and setting up automatic deployments on commit.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-source-repo.png\" class=\"kg-image\"><figcaption>The Google Source Repositories Home</figcaption></figure><!--kg-card-end: image--><p>Setting up this sync is super easy. Create a new repository, specify that we're connecting an external Github repo, and we'll be able to select any repo in our account via the GUI:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-source-github-sync.gif\" class=\"kg-image\"><figcaption>Syncing a Github Repo</figcaption></figure><!--kg-card-end: image--><p>Now when we go back to our editing our function, setting the <strong>Source code</strong> field to the name of this new repository will automatically deploy the function whenever a change is committed. With this method, we have effectively zero changes to our normal workflow.</p><h3 id=\"commit-to-google-source-repositories-directly\">Commit to Google Source Repositories Directly</h3><p>If you don't want to sync a Github repo, no problem. We can create a repo locally using the <strong>gcloud CLI</strong>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-bash\">$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n</code></pre>\n<!--kg-card-end: markdown--><p>Now make that puppy go live with <code>gcloud functions deploy totally-dope-function</code>, where <em><strong>totally-dope-function</strong> </em>is the name of your function, as it should be.</p><p>With our function set up and method in place for deploying code, we can now see how our Cloud Function is doing.</p><h2 id=\"viewing-error-logs\">Viewing Error Logs</h2><p>Because we have a real endpoint to work with, we don't need to waste any time creating dumb unit tests where we send fake JSON to our function (real talk though, we should always write unit tests).</p><p>The Cloud Function error log screen does a decent job of providing us with a GUI to see how our deployed function is running, and where things have gone wrong:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/logs.gif\" class=\"kg-image\"><figcaption>Orange Exclamation Marks Denote Errors</figcaption></figure><!--kg-card-end: image--><h2 id=\"firing-our-function-on-a-schedule\">Firing Our Function on a Schedule</h2><p>Let's say our function is a job we're looking to run daily, or perhaps hourly. Google Cloud Scheduler is a super easy way to trigger functions via CRON.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hackersandslackers.com/content/images/2019/04/google-cloud-scheduler.png\" class=\"kg-image\"><figcaption>How is this free again?</figcaption></figure><!--kg-card-end: image--><p>The easiest way to handle this is by creating our function as an HTTP endpoint back when we started. A <strong>Cloud Scheduler</strong> job can hit this endpoint at any time interval we want - just make sure you wrote your endpoint to handle GET requests.</p><h2 id=\"cloud-functions-in-short\">Cloud Functions in Short</h2><p>GCP seems to have been taking notes on the sidelines on how to improve this process by removing red-tape around service setup or policy configuration. AWS and GCP are tackling opposites approaches; AWS allows you to build a Robust API complete with staging and testing with the intent that some of these APIs can even be sold as standalone products to consumers. GCP takes the opposite approach: cloud functions are services intended for developers to develop. That should probably cover the vast majority of use cases anyway.</p>","url":"https://hackersandslackers.com/creating-a-python-google-cloud-function/","uuid":"ec428cb9-976e-4578-a3de-9120a0dd7352","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bc91ac23d1eab214413b12b"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736ca","title":"PostgreSQL Cloud Database on Google Cloud","slug":"cloud-sql-postgres-on-gcp","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","excerpt":"Deep Dive into Cloud SQL and its out-of-the-box API.","custom_excerpt":"Deep Dive into Cloud SQL and its out-of-the-box API.","created_at_pretty":"09 August, 2018","published_at_pretty":"10 August, 2018","updated_at_pretty":"28 February, 2019","created_at":"2018-08-09T12:49:05.000-04:00","published_at":"2018-08-10T05:52:00.000-04:00","updated_at":"2019-02-27T23:37:34.000-05:00","meta_title":"Cloud-Hosted Postgres on Google Cloud | Hackers and Slackers","meta_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.","og_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.\n\n#Postgres #GoogleCloud #Databases","og_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","og_title":"Cloud SQL Postgres on GCP","twitter_description":"A sexy relational database on the hottest cloud provider on the market is hard to resist. Especially when it comes with a REST API.\n\n#Postgres #GoogleCloud #Databases\n\n","twitter_image":"https://hackersandslackers.com/content/images/2019/02/postgres4.jpg","twitter_title":"Cloud SQL Postgres on GCP","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"PostgreSQL","slug":"postgresql","description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","feature_image":null,"meta_description":"Our preferred relational database of choice which deserves more love. Learn the advantages that PostgreSQL provides over closed-source competitors.","meta_title":"Working with PostgreSQL | Hackers and Slackers","visibility":"public"},{"name":"DevOps","slug":"devops","description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","feature_image":null,"meta_description":"Configuring server-side infrastructure, cloud architecture, and sometimes networking. Even automate your DevOps workflow with products from Hashicorp.","meta_title":"DevOps: Networking And Server Configuration | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"}],"plaintext":"Well folks, I have a confession to make. I've been maintaining an affair with\ntwo lovers. That's right; they're none other than PostgreSQL, and Google Cloud.\nWhile such polygamy may be shunned by the masses, I believe that somehow, some\nway, we can just make this ménage à trois work. What entices me about Cloud SQL\nis the existence of the Cloud SQL API\n[https://cloud.google.com/sql/docs/postgres/admin-api/]  , which generates\npredictable REST endpoints for presumably reading and writing to your database.\nPlease allow a moment of silence for the old workflow of API Gateways and Lambda\nfunctions. RIP.\n\nWe’ll get to APIs eventually, but for now we have one glaring obstacle: creating\nour DB, and connecting to it in a way vaguely resembles something secure*.\n\nNote: today may or may not be opposite day.Creating Our Cloud Database\nHit up the Cloud SQL [https://console.cloud.google.com/sql/]  section of your\nconsole to get this party started. Database creation on GCP is surprisingly\neasy.\n\nThat's pretty much it tbh.Databases Gone Wild: Postgres Exposed\nThere are plenty of correct ways to connect to your Postgres database correctly\nand securely. You can set up SSL for an IP\n[https://cloud.google.com/sql/docs/postgres/connect-admin-ip], connect using a\nproxy [https://cloud.google.com/sql/docs/postgres/connect-admin-proxy], or even\nvia internal cloud functions\n[https://cloud.google.com/sql/docs/postgres/connect-cloud-functions]. You may\nwant to consider doing one of those things. I'll be doing this a different way,\nbecause I'd rather get my useless data on a hackable public database than\nrewrite Google tutorials:\n\nDo as I say, not as I do.This is where you can feel free to go ahead and\npopulate data into your DB via whichever GUI you'd prefer. It'll be easier to\nsee which API calls work if there's actual data involved.\n\nPick whichever overpriced client suits you best!Enabling the API\nAs always with GCP, we need to explicitly activate the API for SQL; that way,\nthey can charge us money forever, long after we've forgotten this tutorial. We\ncan do this here\n[https://console.cloud.google.com/flows/enableapi?apiid=sqladmin]. Are you\nstarting to feel excited? I know I am; just think, all those API calls right\naround the corner, coming from a real SQL database. Wow. \n\nIn the overall process, we've made it here: the part where we run into OAuth2:\n\nRefresh tokens? Scopes? Uh oh.I'll admit it took me a good amount of time to\ndecrypt the information which failed to conveyed here. After clicking into every\nrelated link and failing at attempts to hit the API via Postman, the bad vibes\nstarted kicking in. What if this isn't the dream after all? To spare you the\nprocess, let me introduce you to a very useful GCP tool.\n\nGoogle API Explorer\nGoogle's API explorer is a GUI for playing with any API, connected to any of\nyour services. This is a cool way to preview what the exact scope of an API is\nbefore you sign up for it. Better yet, you can use placeholder User_IDs  and \nUser_Secrets  since this is basically just a sandbox.\n\nInteractive API learning tools beat reading documentation any day.After\nselecting an 'endpoint' and specifying some details like your project and\ndatabase instance, you can immediately see (theoretical) results of what the\nlive API can do. This is very useful, but I'm afraid this is where things get\ndark.\n\nHello Darkness My Old Friend\nYou may have noticed a lot of similar words or phrases popping up in these\nendpoints. Words such as \"admin\"  and \"list\", while lacking phrases such as \n\"show me my god damn data\". Google's Cloud SQL API  is NOT, in fact, an API to\ninteract with your data, but rather an admin API which enables you to do things\nprobably better suited for, you know, admin consoles.\n\nAs a big fan of GCP, this is but one of a number of growing pains I've\nexperienced with the platform so far. For instance, this entire blog along with\nits VPC has temporary deleted today, because apparently the phrases \"remove my\nproject from Firebase\"  and \"delete my project along with everything I love\" are\nsentimentally similar enough to leave that language vague and awkward.\n\nWhere Do We Go From Here?\nTo reiterate, the problem we were originally looking to solve was to find a\nservice which could (after what, 30 years?) make relational database reading and\nwriting trivial, especially in the case of apps which are simply themes without\na configurable backend, such as this blog.\n\nMongoDB Atlas  is an organizational mess which can't even describe their own\nproduct. Firebase  has yet to implement an import feature, so unless you feel\nlike writing loops to write to an experimental NoSQL database (I don't), we're\nstill kind of screwed. I know there are guys like Dreamfactory out there, but\nthese services are the sketchy ones who email you every day just for looking at\na trial. Also, anything related to Oracle or running on Oracle products (by\nchoice) sucks. There, I said it. Java developers will probably be too bust with\ngarbage collection and getting sued to argue with me anyway.\n\nAll that said, it feels like the \"Backend as a service\" thing is looming over\nthe horizon. There just doesn't seem to be anybody who's executed this\neffectively yet.\n\nUPDATE:  As it turns out, there is a service out there that accomplishes\neverything we hoped to achieve in Google cloud, and it is called Apisentris\n[https://apisentris.com/]. It's awesome, it's free, and the guy behind it is a\nchill dude.","html":"<p>Well folks, I have a confession to make. I've been maintaining an affair with two lovers. That's right; they're none other than PostgreSQL, and Google Cloud. While such polygamy may be shunned by the masses, I believe that somehow, some way, we can just make this ménage à trois work. What entices me about Cloud SQL is the existence of the <a href=\"https://cloud.google.com/sql/docs/postgres/admin-api/\">Cloud SQL API</a> , which generates predictable REST endpoints for presumably reading and writing to your database. Please allow a moment of silence for the old workflow of API Gateways and Lambda functions. RIP.</p><p>We’ll get to APIs eventually, but for now we have one glaring obstacle: creating our DB, and connecting to it in a way vaguely resembles something secure*.</p><span style=\"color: #669ab5; font-style: italic; font-size: 15px; float: right;\">Note: today may or may not be opposite day.</span><h2 id=\"creating-our-cloud-database\">Creating Our Cloud Database</h2><p>Hit up the <a href=\"https://console.cloud.google.com/sql/\">Cloud SQL</a> section of your console to get this party started. Database creation on GCP is surprisingly easy.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/setuppostgres.png\" class=\"kg-image\"><figcaption>That's pretty much it tbh.</figcaption></figure><h2 id=\"databases-gone-wild-postgres-exposed\">Databases Gone Wild: Postgres Exposed  </h2><p>There are plenty of <em>correct </em>ways to connect to your Postgres database correctly and securely. You can <a href=\"https://cloud.google.com/sql/docs/postgres/connect-admin-ip\">set up SSL for an IP</a>, connect <a href=\"https://cloud.google.com/sql/docs/postgres/connect-admin-proxy\">using a proxy</a>, or even via internal <a href=\"https://cloud.google.com/sql/docs/postgres/connect-cloud-functions\">cloud functions</a>. You may want to consider doing one of those things. I'll be doing this a different way, because I'd rather get my useless data on a hackable public database than rewrite Google tutorials:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/public.png\" class=\"kg-image\"><figcaption>Do as I say, not as I do.</figcaption></figure><p>This is where you can feel free to go ahead and populate data into your DB via whichever GUI you'd prefer. It'll be easier to see which API calls work if there's actual data involved.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-08-10-at-4.15.22-AM.png\" class=\"kg-image\"><figcaption>Pick whichever overpriced client suits you best!</figcaption></figure><h2 id=\"enabling-the-api\">Enabling the API</h2><p>As always with GCP, we need to explicitly activate the API for SQL; that way, they can charge us money forever, long after we've forgotten this tutorial. We can do this <a href=\"https://console.cloud.google.com/flows/enableapi?apiid=sqladmin\">here</a>. Are you starting to feel excited? I know I am; just think, all those API calls right around the corner, coming from a real SQL database. Wow. </p><p>In the overall process, we've made it <em>here</em>: the part where we run into OAuth2:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-08-10-at-4.21.55-AM.png\" class=\"kg-image\"><figcaption>Refresh tokens? Scopes? Uh oh.</figcaption></figure><p>I'll admit it took me a good amount of time to decrypt the information which failed to conveyed here. After clicking into every related link and failing at attempts to hit the API via Postman, the bad vibes started kicking in. What if this isn't the dream after all? To spare you the process, let me introduce you to a very useful GCP tool.</p><h2 id=\"google-api-explorer\">Google API Explorer</h2><p>Google's API explorer is a GUI for playing with any API, connected to any of your services. This is a cool way to preview what the exact scope of an API is before you sign up for it. Better yet, you can use placeholder <em>User_IDs</em> and <em>User_Secrets</em> since this is basically just a sandbox.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sandbox.gif\" class=\"kg-image\"><figcaption>Interactive API learning tools beat reading documentation any day.</figcaption></figure><p>After selecting an 'endpoint' and specifying some details like your project and database instance, you can immediately see (theoretical) results of what the live API can do. This is very useful, but I'm afraid this is where things get dark.</p><h3 id=\"hello-darkness-my-old-friend\">Hello Darkness My Old Friend</h3><p>You may have noticed a lot of similar words or phrases popping up in these endpoints. Words such as <em>\"admin\"</em> and \"<em>list\"</em>, while lacking phrases such as <em>\"show me my god damn data\". </em>Google's <strong>Cloud SQL API</strong> is NOT, in fact, an API to interact with your data, but rather an <em>admin </em>API which enables you to do things probably better suited for, you know, admin consoles.</p><p>As a big fan of GCP, this is but one of a number of growing pains I've experienced with the platform so far. For instance, this entire blog along with its VPC has temporary deleted today, because apparently the phrases <em>\"remove my project from Firebase\"</em> and <em>\"delete my project along with everything I love\" </em>are sentimentally similar enough to leave that language vague and awkward.</p><h2 id=\"where-do-we-go-from-here\">Where Do We Go From Here?</h2><p>To reiterate, the problem we were originally looking to solve was to find a service which could (after what, 30 years?) make relational database reading and writing trivial, especially in the case of apps which are simply themes without a configurable backend, such as this blog.</p><p><em>MongoDB Atlas</em> is an organizational mess which can't even describe their own product. <em>Firebase</em> has yet to implement an import feature, so unless you feel like writing loops to write to an experimental NoSQL database (I don't), we're still kind of screwed. I know there are guys like <em>Dreamfactory </em>out there, but these services are the sketchy ones who email you every day just for looking at a trial. Also, anything related to Oracle or running on Oracle products (by choice) sucks. There, I said it. Java developers will probably be too bust with garbage collection and getting sued to argue with me anyway.</p><p>All that said, it feels like the \"Backend as a service\" thing is looming over the horizon. There just doesn't seem to be anybody who's executed this effectively yet.</p><p><strong>UPDATE:</strong> As it turns out, there is a service out there that accomplishes everything we hoped to achieve in Google cloud, and it is called <a href=\"https://apisentris.com/\">Apisentris</a>. It's awesome, it's free, and the guy behind it is a chill dude.</p>","url":"https://hackersandslackers.com/cloud-sql-postgres-on-gcp/","uuid":"76daacf1-55b4-4ede-b21d-29fd727e1d50","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b6c70819dcd9d3270b58635"}}]}},"pageContext":{"slug":"architecture","limit":12,"skip":0,"numberOfPages":2,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":2,"previousPagePath":null,"nextPagePath":"/tag/architecture/page/2/"}}