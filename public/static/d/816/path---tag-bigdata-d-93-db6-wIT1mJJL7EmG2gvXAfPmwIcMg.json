{"data":{"ghostTag":{"slug":"bigdata","name":"Big Data","visibility":"public","feature_image":null,"description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc."},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c47584f4f3823107c9e8f23","title":"Google BigQuery's Python SDK: Creating Tables Programmatically","slug":"getting-started-google-big-query-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","custom_excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","created_at_pretty":"22 January, 2019","published_at_pretty":"02 February, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-22T12:52:15.000-05:00","published_at":"2019-02-02T09:24:00.000-05:00","updated_at":"2019-03-28T17:06:20.000-04:00","meta_title":"Google BigQuery's Python SDK: Creating Tables | Hackers and Slackers","meta_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","og_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","twitter_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","twitter_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"GCP is on the rise, and it's getting harder and harder to have conversations\naround data without addressing the 500-pound gorilla in the room: Google\nBigQuery. With most enterprises comfortably settled into their Apache-based Big\nData stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers\nAnd Slackers is no such enterprise. Thus, we aren't afraid to ask the Big\nquestion: how much easier would life be with BigQuery?\n\nBig Data, BigQuery\nIn short, BigQuery trivializes the act of querying against multiple,\nunpredictable data sources. To better understand when this is useful, it would\nbetter serve us to identify the types of questions BigQuery can answer. Such as:\n\n * What are our users doing across our multiple systems? How do we leverage log\n   files outputted by multiple systems to find out?\n * How can we consolidate information about employee information, payroll, and\n   benefits, when these all live in isolated systems?\n * What the hell am I supposed to do with all these spreadsheets?\n\nUnlike previous solutions, BigQuery solves these problems in a single product\nand does so with SQL-like query syntax,  a web interface, and 7 native Client\nLibraries.  There are plenty of reasons to love BigQuery, but let's start with\none we've recently already talked about: the auto-generation of table schemas. \n\nMatt has demonstrated how to approach this problem manually with the help of\nPandas\n[https://hackersandslackers.com/downcast-numerical-columns-python-pandas/]. I\nprovided a more gimmicky approach by leveraging the Python table-schema library\n[https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/]. With\nBigQuery, we find yet another alternative which is neither manual or gimmicky:\nperfect for those who are lazy, rich, and demand perfection (AKA: your clients,\nprobably).\n\nFirst, we'll need to get our data into BigQuery\n\nUploading Data into Google Cloud Storage via the Python SDK\nBigQuery requires us to go through Google Cloud Storage as a buffer before\ninputting data into tables. No big deal, we'll write a script!\n\nWe're assuming that you have a basic knowledge of Google Cloud, Google Cloud\nStorage, and how to download a JSON Service Account key\n[https://cloud.google.com/bigquery/docs/reference/libraries]  to store locally\n(hint: click the link).\n\nfrom google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n\n\nThe above is nearly a copy + paste of Google Cloud's sample code for the Google\nCloud Storage Python SDK:\n\n * bucket_uri  is found by inspecting any bucket's information on Google Cloud.\n * bucket_name  is... well, you know.\n * bucket_target  represents the resulting file structure representing the saved\n   CSV when completed.\n * local_dataset  is the path to a CSV we've stored locally: we can assume that\n   we've grabbed some data from somewhere, like an API, and tossed into a local\n   file temporarily.\n\nSuccessfully executing the above results in the following message:\n\nFile data/test.csv uploaded to datasets/data_upload.csv.\n\n\nInserting Data from Cloud Storage to BigQuery\nThat was the easy part. Let's move on to the good stuff:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n\n\nWe've added the function insert_bigquery()  to handle creating a BigQuery table\nout of a CSV.\n\nAfter we set our client, we create a dataset reference. In BigQuery, tables can\nbelong to a 'dataset,' which is a grouping of tables. Compare this concept to\nMongoDB's collections, or PostgreSQL's schemas. Note that this process is made\nmuch easier by the fact that we stored our project key locally: otherwise, we'd\nhave to specify which Google Cloud project we're looking for, etc.\n\nWith the dataset specified, we begin to build our \"job\" object with \nLoadJobConfig. This is like loading a gun before unleashing a shotgun blast into\nthe face of our problems. Alternatively, a more relevant comparison could be\nwith the Python requests  library and the act of prepping an API request before\nexecution.\n\nWe set job_config.autodetect  to be True, obviously. \njob_config.skip_leading_rows  reserves our header row from screwing things up.\n\nload_job  puts our request together, and load_job.result()  executes said job.\nThe .result()  method graciously puts the rest of our script on hold until the\nspecified job is completed. In our case, we want this happen: it simplifies our\nscript so that we don't need to verify this manually before moving on.\n\nLet's see what running that job with our fake data looks like in the BigQuery\nUI:\n\nAll my fake friends are here!Getting Our Flawlessly Inferred Table Schema\nBigQuery surely gets table schemas wrong some of the time. That said, I have yet\nto see it happen. Let's wrap this script up:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    \"\"\"Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n\n\nWith the addition of get_bigquery_schema(), our script is complete!\n\nTableReference()  is similar to the dataset reference we went over earlier, only\nfor tables (duh). This allows us to call upon get_table(), which returns a Table\nclass representing the table we just created. Amongst the methods of that class,\nwe can call .schema(), which gives us precisely what we want: a beautiful\nrepresentation of a Table schema, generated from raw CSV information, where\nthere previously was none.\n\nBehold the fruits of your labor:\n\n[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n\n\nThere you have it; a correctly inferred schema, from data which wasn't entirely\nclean in the first place (our dates are in MM/DD/YY  format as opposed to \nMM/DD/YYYY, but Google still gets it right. How? Because Google).\n\nIt Doesn't End Here\nI hope it goes without saying that abusing Google BigQuery's API to generate\nschemas for you is only a small, obscure use case of what Google BigQuery is\nintended to do, and what it can do for you. That said, I need to stop this\nfanboying post before anybody realizes I'll promote their products for free\nforever (I think I may have passed that point).\n\nIn case you're interested, the source code for this script has been uploaded as\na Gist here\n[https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e]. Have at\nit, and remember to think Big™*.\n\n*Not a real trademark, I'm making things up again.","html":"<p>GCP is on the rise, and it's getting harder and harder to have conversations around data without addressing the 500-pound gorilla in the room: Google BigQuery. With most enterprises comfortably settled into their Apache-based Big Data stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers And Slackers is no such enterprise. Thus, we aren't afraid to ask the Big question: how much easier would life be with BigQuery?</p><h2 id=\"big-data-bigquery\">Big Data, BigQuery</h2><p>In short, BigQuery trivializes the act of querying against multiple, unpredictable data sources. To better understand when this is useful, it would better serve us to identify the types of questions BigQuery can answer. Such as:</p><ul><li>What are our users doing across our multiple systems? How do we leverage log files outputted by multiple systems to find out?</li><li>How can we consolidate information about employee information, payroll, and benefits, when these all live in isolated systems?</li><li>What the hell am I supposed to do with all these spreadsheets?</li></ul><p>Unlike previous solutions, BigQuery solves these problems in a single product and does so with <strong>SQL-like query syntax,</strong> a <strong>web interface</strong>, and <strong>7 native Client Libraries.</strong> There are plenty of reasons to love BigQuery, but let's start with one we've recently already talked about: the <em>auto-generation of table schemas</em>. </p><p>Matt has demonstrated how to approach this problem <a href=\"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/\">manually with the help of Pandas</a>. I provided a more gimmicky approach by leveraging the <a href=\"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/\">Python table-schema library</a>. With BigQuery, we find yet another alternative which is neither manual or gimmicky: perfect for those who are lazy, rich, and demand perfection (AKA: your clients, probably).</p><p>First, we'll need to get our data into BigQuery</p><h2 id=\"uploading-data-into-google-cloud-storage-via-the-python-sdk\">Uploading Data into Google Cloud Storage via the Python SDK</h2><p>BigQuery requires us to go through Google Cloud Storage as a buffer before inputting data into tables. No big deal, we'll write a script!</p><p>We're assuming that you have a basic knowledge of Google Cloud, Google Cloud Storage, and how to download a <a href=\"https://cloud.google.com/bigquery/docs/reference/libraries\">JSON Service Account key</a> to store locally (hint: click the link).</p><pre><code class=\"language-python\">from google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n</code></pre>\n<p>The above is nearly a copy + paste of Google Cloud's sample code for the Google Cloud Storage Python SDK:</p><ul><li><code>bucket_uri</code> is found by inspecting any bucket's information on Google Cloud.</li><li><code>bucket_name</code> is... well, you know.</li><li><code>bucket_target</code><strong> </strong>represents the resulting file structure representing the saved CSV when completed.</li><li><code>local_dataset</code> is the path to a CSV we've stored locally: we can assume that we've grabbed some data from somewhere, like an API, and tossed into a local file temporarily.</li></ul><p>Successfully executing the above results in the following message:</p><pre><code class=\"language-shell\">File data/test.csv uploaded to datasets/data_upload.csv.\n</code></pre>\n<h2 id=\"inserting-data-from-cloud-storage-to-bigquery\">Inserting Data from Cloud Storage to BigQuery</h2><p>That was the easy part. Let's move on to the good stuff:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n</code></pre>\n<p>We've added the function <code>insert_bigquery()</code> to handle creating a BigQuery table out of a CSV.</p><p>After we set our client, we create a <strong>dataset reference</strong>. In BigQuery, tables can belong to a 'dataset,' which is a grouping of tables. Compare this concept to MongoDB's <strong>collections, </strong>or PostgreSQL's <strong>schemas</strong>. Note that this process is made much easier by the fact that we stored our project key locally: otherwise, we'd have to specify which Google Cloud project we're looking for, etc.</p><p>With the dataset specified, we begin to build our \"job\" object with <code>LoadJobConfig</code>. This is like loading a gun before unleashing a shotgun blast into the face of our problems. Alternatively, a more relevant comparison could be with the Python <code>requests</code> library and the act of prepping an API request before execution.</p><p>We set <code>job_config.autodetect</code> to be <code>True</code>, obviously. <code>job_config.skip_leading_rows</code> reserves our header row from screwing things up.</p><p><code>load_job</code> puts our request together, and <code>load_job.result()</code> executes said job. The <code>.result()</code> method graciously puts the rest of our script on hold until the specified job is completed. In our case, we want this happen: it simplifies our script so that we don't need to verify this manually before moving on.</p><p>Let's see what running that job with our fake data looks like in the BigQuery UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-01-at-7.42.52-PM.png\" class=\"kg-image\"><figcaption>All my fake friends are here!</figcaption></figure><h2 id=\"getting-our-flawlessly-inferred-table-schema\">Getting Our Flawlessly Inferred Table Schema</h2><p>BigQuery surely gets table schemas wrong <em>some </em>of the time. That said, I have yet to see it happen. Let's wrap this script up:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    &quot;&quot;&quot;Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n</code></pre>\n<p>With the addition of <code>get_bigquery_schema()</code>, our script is complete!</p><p><code>TableReference()</code> is similar to the dataset reference we went over earlier, only for tables (duh). This allows us to call upon <code>get_table()</code>, which returns a Table class representing the table we just created. Amongst the methods of that class, we can call <code>.schema()</code>, which gives us precisely what we want: a beautiful representation of a Table schema, generated from raw CSV information, where there previously was none.</p><p>Behold the fruits of your labor:</p><pre><code class=\"language-python\">[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n</code></pre>\n<p>There you have it; a correctly inferred schema, from data which wasn't entirely clean in the first place (our dates are in <strong>MM/DD/YY</strong> format as opposed to <strong>MM/DD/YYYY</strong>, but Google still gets it right. How? Because Google).</p><h3 id=\"it-doesn-t-end-here\">It Doesn't End Here</h3><p>I hope it goes without saying that abusing Google BigQuery's API to generate schemas for you is only a small, obscure use case of what Google BigQuery is intended to do, and what it can do for you. That said, I need to stop this fanboying post before anybody realizes I'll promote their products for free forever (I think I may have passed that point).</p><p>In case you're interested, the source code for this script has been uploaded as a Gist <a href=\"https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e\">here</a>. Have at it, and remember to think Big<strong>™*</strong>.</p><span style=\"color: #a6a6a6;font-style: italic; font-size: .8em;\">*Not a real trademark, I'm making things up again.</span>","url":"https://hackersandslackers.com/getting-started-google-big-query-python/","uuid":"30051eb2-7fa7-4a09-91fd-c3f11966b398","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47584f4f3823107c9e8f23"}},{"node":{"id":"Ghost__Post__5c47b2bcf850c0618c1a59a0","title":"From CSVs to Tables: Infer Data Types From Raw Spreadsheets","slug":"infer-datatypes-from-csvs-to-create","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","excerpt":"The quest to never explicitly set a table schema ever again.","custom_excerpt":"The quest to never explicitly set a table schema ever again.","created_at_pretty":"23 January, 2019","published_at_pretty":"23 January, 2019","updated_at_pretty":"19 February, 2019","created_at":"2019-01-22T19:18:04.000-05:00","published_at":"2019-01-23T07:00:00.000-05:00","updated_at":"2019-02-19T04:02:36.000-05:00","meta_title":"Infer SQL Data Types From Raw Spreadsheets | Hackers and Slackers ","meta_description":"We join forces with Pandas, SQLAlchemy, PyTorch, Databricks, and tableschema with one goal in mind: to never explicitly create a table schema ever again.","og_description":"The quest to never explicitly set a table schema ever again.","og_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","og_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","twitter_description":"The quest to never explicitly set a table schema ever again.","twitter_image":"https://hackersandslackers.com/content/images/2019/01/schema@2x.jpg","twitter_title":"From CSVs to Tables: Infer Schema Data Types From Raw Spreadsheets","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"SQL","slug":"sql","description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","feature_image":"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/sql-tag.jpg","meta_description":"Configure relational databases, brush up on your query language syntax, or find third-party services to interact with your data.","meta_title":"Working with SQL | Hackers and Slackers","visibility":"public"},{"name":"Apache","slug":"apache","description":"Apache’s suite of big data products: Hadoop, Spark, Kafka, and so forth.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"}],"plaintext":"Back in August of last year (roughly 8 months ago), I hunched over my desk at 4\nam desperate to fire off a post before boarding a flight the next morning. The\narticle was titled Creating Database Schemas: a Job for Robots, or Perhaps\nPandas. It was my intent at the time to solve a common annoyance: creating\ndatabase tables out of raw data, without the obnoxious process of explicitly\nsetting each column's datatype. I had a few leads that led me to believe I had\nthe answer... boy was I wrong.\n\nThe task seems somewhat reasonable from the surface. Surely we can spot columns\nwhere the data is always in integers, or match the expected format of a date,\nright? If anything, we'll fall back to text  or varchar  and call it a day.\nHell, even MongoDB's Compass does a great job of this by merely uploading a\nCSV... this has got to be some trivial task handled by third-party libraries by\nnow.\n\nFor one reason or another, searching for a solution to this problem almost\nalways comes up empty. Software developers probably have little need for\ndynamically generated tables if their applications run solely on self-defined\nmodels. Full-time Data Scientists have access to plenty of expensive tools which\nseem to claim this functionality, yet it all seems so... inaccessible.\n\nIs This NOT a Job For Pandas?\nFrom my experience, no. Pandas does offer hope but doesn't seem to get the job\ndone quite right. Let's start with a dataset so you can see what I mean. Here's\na bunch of fake identities I'll be using to mimic the outcome I experienced when\nworking with real data:\n\nidinitiatedhiredateemailfirstnamelastnametitledepartmentlocationcountrytype\n1000354352015-12-11T09:16:20.722-08:003/22/67GretchenRMorrow@jourrapide.com\nGretchenMorrowPower plant operatorPhysical ProductBritling CafeteriasUnited\nKingdomEmployee1000564352015-12-15T10:11:24.604-08:006/22/99\nElizabethLSnow@armyspy.comElizabethSnowOxygen therapistPhysical ProductGrade A\nInvestmentUnited States of AmericaEmployee1000379552015-12-16T14:31:32.765-08:00\n5/31/74AlbertMPeterson@einrot.comAlbertPetersonPsychologistPhysical ProductGrass\nRoots Yard ServicesUnited States of AmericaEmployee100035435\n2016-01-20T11:15:47.249-08:009/9/69JohnMLynch@dayrep.comJohnLynchEnvironmental\nhydrologistPhysical ProductWaccamaw's HomeplaceUnited States of AmericaEmployee\n1000576572016-01-21T12:45:38.261-08:004/9/83TheresaJCahoon@teleworm.usTheresa\nCahoonPersonal chefPhysical ProductCala FoodsUnited States of AmericaEmployee\n1000567472016-02-01T11:25:39.317-08:006/26/98KennethHPayne@dayrep.comKenneth\nPayneCentral office operatorFrontlineMagna ConsultingUnited States of America\nEmployee1000354352016-02-01T11:28:11.953-08:004/16/82LeifTSpeights@fleckens.hu\nLeifSpeightsStaff development directorFrontlineRivera Property MaintenanceUnited\nStates of AmericaEmployee1000354352016-02-01T12:21:01.756-08:008/6/80\nJamesSRobinson@teleworm.usJamesRobinsonScheduling clerkFrontlineDiscount\nFurniture ShowcaseUnited States of AmericaEmployee100074688\n2016-02-01T13:29:19.147-08:0012/14/74AnnaDMoberly@jourrapide.comAnnaMoberly\nPlaywrightPhysical ProductThe WizUnited States of AmericaEmployee100665778\n2016-02-04T14:40:05.223-08:009/13/66MarjorieBCrawford@armyspy.comMarjorie\nCrawfordCourt, municipal, and license clerkPhysical ProductThe Serendipity Dip\nUnited KingdomEmployee1008768762016-02-24T12:39:25.872-08:0012/19/67\nLyleCHackett@fleckens.huLyleHackettAirframe mechanicPhysical ProductInfinity\nInvestment PlanUnited States of AmericaEmployee100658565\n2016-02-29T15:52:12.933-08:0011/17/83MaryJDensmore@jourrapide.comMaryDensmore\nEmployer relations representativeFrontlineOne-Up RealtorsUnited States of\nAmericaEmployee1007665472016-03-01T12:32:53.357-08:0010/1/87\nCindyRDiaz@armyspy.comCindyDiazStudent affairs administratorPhysical ProductMr.\nAG'sUnited States of AmericaEmployee1000456772016-03-02T12:07:44.264-08:00\n8/16/65AndreaTLigon@einrot.comAndreaLigonRailroad engineerCentral GrowthRobinson\nFurnitureUnited States of AmericaEmployeeThere are some juicy datatypes in\nthere: integers, timestamps, dates, strings.... and those are only the first\nfour columns! Let's load this thing into a DataFrame and see what information we\ncan get that way:\n\nimport pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n\n\nUsing Pandas' info()  should do the trick! This returns a list of columns and\ntheir data types:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n\n\n...Or not. What is this garbage? Only one of our 11 columns identified a data\ntype, and it was incorrectly listed as a float! Okay, so maybe Pandas doesn't\nhave a secret one-liner for this. So who does?\n\nWhat about PySpark?\nIt's always been a matter of time before we'd turn to Apache's family of aged\ndata science products. Hadoop, Spark, Kafka... all of them have a particular\nmusty stench about them that tastes like \"I feel like I should be writing in\nJava right now.\" Heads up: they do  want you to write in Java. Misery loves\ncompany.\n\nNonetheless, PySpark  does  support reading data as DataFrames in Python, and\nalso comes with the elusive ability to infer schemas. Installing Hadoop and\nSpark locally still kind of sucks for solving this one particular problem. Cue \nDatabricks [https://databricks.com/]: a company that spun off from the Apache\nteam way back in the day, and offers free cloud notebooks integrated with- you\nguessed it: Spark.\n\nWith Databricks, we can upload our CSV and load it into a DataFrame by spinning\nup a free notebook. The source looks something like this:\n\n# File location and type\nfile_location = \"/FileStore/tables/fake.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n\nLet's see out the output looks:\n\ndf:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n\n\nNot bad! We correctly 'upgraded' our ID from float to integer, and we managed to\nget the timestamp correct also. With a bit of messing around, we could probably\nhave even gotten the date correct too, given that we stated the format\nbeforehand.\n\nA look at the Databricks Notebook interface.And Yet, This Still Kind of Sucks\nEven though we can solve our problem in a notebook, we still haven't solved the\nuse case: I want a drop-in solution to create tables out of CSVs... whenever I\nwant! I want to accomplish this while writing any app, at the drop of a hat\nwithout warning. I don't want to install Hadoop and have Java errors coming back\nat me through my terminal. Don't EVER  let me see Java in my terminal. UGH:\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\n\nPython's \"tableschema\" Library\nThankfully, there's at least one other person out there who has shared this\ndesire. That brings us to tableschema\n[https://github.com/frictionlessdata/tableschema-py], a\nnot-quite-perfect-but-perhaps-good-enough library to gunsling data like some\nkind of wild data cowboy. Let's give it a go:\n\nimport csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n\n\nIf our dataset is particularly large, we can use the limit  attribute to limit\nthe sample size to the first X  number of rows. Another nice feature is the \nconfidence  attribute: a 0-1 ratio for allowing casting errors during the\ninference. Here's what comes back:\n\n{\n  \"fields\": [{\n    \"name\": \"id\",\n    \"type\": \"integer\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"initiated\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"hiredate\",\n    \"type\": \"date\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"email\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"firstname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"lastname\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"title\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"department\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"location\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"country\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }, {\n    \"name\": \"type\",\n    \"type\": \"string\",\n    \"format\": \"default\"\n  }],\n  \"missingValues\": [\"\"]\n}\n\n\nHey, that's good enough for me! Now let's automate the shit out this.\n\nCreating a Table in SQLAlchemy With Our New Schema\nI'm about to throw a bunch in your face right here. Here's a monster of a class:\n\nfrom sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    \"\"\"Infer a table schema from a CSV.\"\"\"\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        \"\"\"Pull latest data.\"\"\"\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        \"\"\"Infers schema from CSV.\"\"\"\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        \"\"\"Get names of columns.\"\"\"\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        \"\"\"Convert schema to recognizable by SQLAlchemy.\"\"\"\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          \"\"\"Create new table from CSV and generated schema.\"\"\"\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n\n\nThe first thing worth mentioning is I'm importing a function\n[https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b]  from my\npersonal secret library to extract values from JSON objects. I've spoken about\nit before\n[https://hackersandslackers.com/extract-data-from-complex-json-python/]. \n\nLet's break down this class:\n\n * get_data()  reads our CSV into a Pandas DataFrame.\n * get_schema_from_csv()  kicks off building a Schema that SQLAlchemy can use to\n   build a table.\n * get_column_names()  simply pulls column names as half our schema.\n * get_column_datatypes()  manually replaces the datatype names we received from\n    tableschema  and replaces them with SQLAlchemy datatypes.\n * create_new_table  Uses a beautiful marriage between Pandas and SQLAlchemy to\n   create a table in our database with the correct datatypes mapped.\n\nPromising Potential, Room to Grow\nWhile tableschema  works some of the time, it isn't perfect. The base of what we\naccomplish still stands: we now have a reliable formula for how we would create\nschemas on the fly if we trust our schemas to be accurate.\n\nJust wait until next time when we introduce Google BigQuery  into the mix.","html":"<p>Back in August of last year (roughly 8 months ago), I hunched over my desk at 4 am desperate to fire off a post before boarding a flight the next morning. The article was titled <strong><em>Creating Database Schemas: a Job for Robots, or Perhaps Pandas</em></strong>. It was my intent at the time to solve a common annoyance: creating database tables out of raw data, without the obnoxious process of explicitly setting each column's datatype. I had a few leads that led me to believe I had the answer... boy was I wrong.</p><p>The task seems somewhat reasonable from the surface. Surely we can spot columns where the data is always in integers, or match the expected format of a date, right? If anything, we'll fall back to <strong>text</strong> or <strong>varchar</strong> and call it a day. Hell, even MongoDB's Compass does a great job of this by merely uploading a CSV... this has got to be some trivial task handled by third-party libraries by now.</p><p>For one reason or another, searching for a solution to this problem almost always comes up empty. Software developers probably have little need for dynamically generated tables if their applications run solely on self-defined models. Full-time Data Scientists have access to plenty of expensive tools which seem to claim this functionality, yet it all seems so... inaccessible.</p><h2 id=\"is-this-not-a-job-for-pandas\">Is This NOT a Job For Pandas?</h2><p>From my experience, no. Pandas does offer hope but doesn't seem to get the job done quite right. Let's start with a dataset so you can see what I mean. Here's a bunch of fake identities I'll be using to mimic the outcome I experienced when working with real data:</p>\n<div class=\"row tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">id</th>\n<th title=\"Field #2\">initiated</th>\n<th title=\"Field #3\">hiredate</th>\n<th title=\"Field #4\">email</th>\n<th title=\"Field #5\">firstname</th>\n<th title=\"Field #6\">lastname</th>\n<th title=\"Field #7\">title</th>\n<th title=\"Field #8\">department</th>\n<th title=\"Field #9\">location</th>\n<th title=\"Field #10\">country</th>\n<th title=\"Field #11\">type</th>\n</tr></thead>\n<tbody><tr><td align=\"right\">100035435</td>\n<td>2015-12-11T09:16:20.722-08:00</td>\n<td>3/22/67</td>\n<td>GretchenRMorrow@jourrapide.com</td>\n<td>Gretchen</td>\n<td>Morrow</td>\n<td>Power plant operator</td>\n<td>Physical Product</td>\n<td>Britling Cafeterias</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056435</td>\n<td>2015-12-15T10:11:24.604-08:00</td>\n<td>6/22/99</td>\n<td>ElizabethLSnow@armyspy.com</td>\n<td>Elizabeth</td>\n<td>Snow</td>\n<td>Oxygen therapist</td>\n<td>Physical Product</td>\n<td>Grade A Investment</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100037955</td>\n<td>2015-12-16T14:31:32.765-08:00</td>\n<td>5/31/74</td>\n<td>AlbertMPeterson@einrot.com</td>\n<td>Albert</td>\n<td>Peterson</td>\n<td>Psychologist</td>\n<td>Physical Product</td>\n<td>Grass Roots Yard Services</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-01-20T11:15:47.249-08:00</td>\n<td>9/9/69</td>\n<td>JohnMLynch@dayrep.com</td>\n<td>John</td>\n<td>Lynch</td>\n<td>Environmental hydrologist</td>\n<td>Physical Product</td>\n<td>Waccamaw&#39;s Homeplace</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100057657</td>\n<td>2016-01-21T12:45:38.261-08:00</td>\n<td>4/9/83</td>\n<td>TheresaJCahoon@teleworm.us</td>\n<td>Theresa</td>\n<td>Cahoon</td>\n<td>Personal chef</td>\n<td>Physical Product</td>\n<td>Cala Foods</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100056747</td>\n<td>2016-02-01T11:25:39.317-08:00</td>\n<td>6/26/98</td>\n<td>KennethHPayne@dayrep.com</td>\n<td>Kenneth</td>\n<td>Payne</td>\n<td>Central office operator</td>\n<td>Frontline</td>\n<td>Magna Consulting</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T11:28:11.953-08:00</td>\n<td>4/16/82</td>\n<td>LeifTSpeights@fleckens.hu</td>\n<td>Leif</td>\n<td>Speights</td>\n<td>Staff development director</td>\n<td>Frontline</td>\n<td>Rivera Property Maintenance</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100035435</td>\n<td>2016-02-01T12:21:01.756-08:00</td>\n<td>8/6/80</td>\n<td>JamesSRobinson@teleworm.us</td>\n<td>James</td>\n<td>Robinson</td>\n<td>Scheduling clerk</td>\n<td>Frontline</td>\n<td>Discount Furniture Showcase</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100074688</td>\n<td>2016-02-01T13:29:19.147-08:00</td>\n<td>12/14/74</td>\n<td>AnnaDMoberly@jourrapide.com</td>\n<td>Anna</td>\n<td>Moberly</td>\n<td>Playwright</td>\n<td>Physical Product</td>\n<td>The Wiz</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100665778</td>\n<td>2016-02-04T14:40:05.223-08:00</td>\n<td>9/13/66</td>\n<td>MarjorieBCrawford@armyspy.com</td>\n<td>Marjorie</td>\n<td>Crawford</td>\n<td>Court, municipal, and license clerk</td>\n<td>Physical Product</td>\n<td>The Serendipity Dip</td>\n<td>United Kingdom</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100876876</td>\n<td>2016-02-24T12:39:25.872-08:00</td>\n<td>12/19/67</td>\n<td>LyleCHackett@fleckens.hu</td>\n<td>Lyle</td>\n<td>Hackett</td>\n<td>Airframe mechanic</td>\n<td>Physical Product</td>\n<td>Infinity Investment Plan</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100658565</td>\n<td>2016-02-29T15:52:12.933-08:00</td>\n<td>11/17/83</td>\n<td>MaryJDensmore@jourrapide.com</td>\n<td>Mary</td>\n<td>Densmore</td>\n<td>Employer relations representative</td>\n<td>Frontline</td>\n<td>One-Up Realtors</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100766547</td>\n<td>2016-03-01T12:32:53.357-08:00</td>\n<td>10/1/87</td>\n<td>CindyRDiaz@armyspy.com</td>\n<td>Cindy</td>\n<td>Diaz</td>\n<td>Student affairs administrator</td>\n<td>Physical Product</td>\n<td>Mr. AG&#39;s</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n<tr><td align=\"right\">100045677</td>\n<td>2016-03-02T12:07:44.264-08:00</td>\n<td>8/16/65</td>\n<td>AndreaTLigon@einrot.com</td>\n<td>Andrea</td>\n<td>Ligon</td>\n<td>Railroad engineer</td>\n<td>Central Growth</td>\n<td>Robinson Furniture</td>\n<td>United States of America</td>\n<td>Employee</td>\n</tr>\n</tbody></table>\n</div><p>There are some juicy datatypes in there: <strong>integers</strong>, <strong>timestamps</strong>, <strong>dates</strong>, <strong>strings</strong>.... and those are only the first four columns! Let's load this thing into a DataFrame and see what information we can get that way:</p><pre><code class=\"language-python\">import pandas as pd\n\ncsv = 'data/fake.csv'\n\nworkers_df = pd.read_csv(csv, header=0, encoding='utf-8')\nmeta = workers_df.info(verbose=True)\nprint(meta)\n</code></pre>\n<p>Using Pandas' <code>info()</code> should do the trick! This returns a list of columns and their data types:</p><pre><code class=\"language-bash\">&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 11 columns):\nid            14 non-null float64\ninitiated     14 non-null object\nhiredate      14 non-null object\nemail         14 non-null object\nfirstname     14 non-null object\nlastname      14 non-null object\ntitle         14 non-null object\ndepartment    14 non-null object\nlocation      14 non-null object\ncountry       14 non-null object\ntype          14 non-null object\ndtypes: float64(1), object(10)\nmemory usage: 2.0+ KB\nNone\n</code></pre>\n<p>...Or not. What is this garbage? Only one of our 11 columns identified a data type, and it was incorrectly listed as a <strong>float</strong>! Okay, so maybe Pandas doesn't have a secret one-liner for this. So who does?</p><h2 id=\"what-about-pyspark\">What about PySpark?</h2><p>It's always been a matter of time before we'd turn to Apache's family of aged data science products. Hadoop, Spark, Kafka... all of them have a particular musty stench about them that tastes like \"I feel like I should be writing in Java right now.\" Heads up: they <em>do</em> want you to write in Java. Misery loves company.</p><p>Nonetheless, <strong>PySpark</strong> <em>does</em> support reading data as DataFrames in Python, and also comes with the elusive ability to infer schemas. Installing Hadoop and Spark locally still kind of sucks for solving this one particular problem. Cue <strong><a href=\"https://databricks.com/\">Databricks</a></strong>: a company that spun off from the Apache team way back in the day, and offers free cloud notebooks integrated with- you guessed it: Spark.</p><p>With Databricks, we can upload our CSV and load it into a DataFrame by spinning up a free notebook. The source looks something like this:</p><pre><code class=\"language-python\"># File location and type\nfile_location = &quot;/FileStore/tables/fake.csv&quot;\nfile_type = &quot;csv&quot;\n\n# CSV options\ninfer_schema = &quot;true&quot;\nfirst_row_is_header = &quot;true&quot;\ndelimiter = &quot;,&quot;\n\ndf = spark.read.format(file_type) \\\n  .option(&quot;inferSchema&quot;, infer_schema) \\\n  .option(&quot;header&quot;, first_row_is_header) \\\n  .option(&quot;sep&quot;, delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n</code></pre>\n<p>Let's see out the output looks:</p><pre><code class=\"language-bash\">df:pyspark.sql.dataframe.DataFrame\nid:integer\ninitiated:timestamp\nhiredate:string\nemail:string\nfirstname:string\nlastname:string\ntitle:string\ndepartment:string\nlocation:string\ncountry:string\ntype:string\n</code></pre>\n<p>Not bad! We correctly 'upgraded' our ID from float to integer, and we managed to get the timestamp correct also. With a bit of messing around, we could probably have even gotten the date correct too, given that we stated the format beforehand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-01-22-at-8.41.30-PM.png\" class=\"kg-image\"><figcaption>A look at the Databricks Notebook interface.</figcaption></figure><h3 id=\"and-yet-this-still-kind-of-sucks\">And Yet, This Still Kind of Sucks</h3><p>Even though we can solve our problem in a notebook, we still haven't solved the use case: I want a drop-in solution to create tables out of CSVs... whenever I want! I want to accomplish this while writing any app, at the drop of a hat without warning. I don't want to install Hadoop and have Java errors coming back at me through my terminal. Don't <em>EVER</em> let me see Java in my terminal. UGH:</p><pre><code class=\"language-bash\">py4j.protocol.Py4JJavaError: An error occurred while calling o43.count.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:166)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:148)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:136)\n        at org.apache.xbean.asm6.ClassReader.&lt;init&gt;(ClassReader.java:237)\n        at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n        at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\n        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n        at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n        at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n        at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n        at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n        at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n</code></pre>\n<h2 id=\"python-s-tableschema-library\">Python's \"tableschema\" Library</h2><p>Thankfully, there's at least one other person out there who has shared this desire. That brings us to <a href=\"https://github.com/frictionlessdata/tableschema-py\">tableschema</a>, a not-quite-perfect-but-perhaps-good-enough library to gunsling data like some kind of wild data cowboy. Let's give it a go:</p><pre><code class=\"language-python\">import csv\nfrom tableschema import Table\n\n\ndata = 'data/fake.csv'\nschema = infer(data, limit=500, headers=1, confidence=0.85)\nprint(schema)\n</code></pre>\n<p>If our dataset is particularly large, we can use the <code>limit</code> attribute to limit the sample size to the first <strong>X</strong> number of rows. Another nice feature is the <code>confidence</code> attribute: a 0-1 ratio for allowing casting errors during the inference. Here's what comes back:</p><pre><code class=\"language-json\">{\n  &quot;fields&quot;: [{\n    &quot;name&quot;: &quot;id&quot;,\n    &quot;type&quot;: &quot;integer&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;initiated&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;hiredate&quot;,\n    &quot;type&quot;: &quot;date&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;email&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;firstname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;lastname&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;title&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;department&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;location&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;country&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }, {\n    &quot;name&quot;: &quot;type&quot;,\n    &quot;type&quot;: &quot;string&quot;,\n    &quot;format&quot;: &quot;default&quot;\n  }],\n  &quot;missingValues&quot;: [&quot;&quot;]\n}\n</code></pre>\n<p>Hey, that's good enough for me! Now let's automate the shit out this.</p><h2 id=\"creating-a-table-in-sqlalchemy-with-our-new-schema\">Creating a Table in SQLAlchemy With Our New Schema</h2><p>I'm about to throw a bunch in your face right here. Here's a monster of a class:</p><pre><code class=\"language-python\">from sqlalchemy import create_engine\nimport config\nimport pandas as pd\nimport psycopg2\nfrom tableschema import Table, infer, Schema\nfrom functions.recursivejson import extract_values\nfrom sqlalchemy.types import Integer, Text, Date\n\n\nclass CreateTablesFromCSVs:\n    &quot;&quot;&quot;Infer a table schema from a CSV.&quot;&quot;&quot;\n\n    __uri = config.PG_URI\n    __engine = create_engine(__uri, convert_unicode=True, echo=True)\n    __data = 'data/fake.csv'\n    \n    @classmethod\n    def get_data(cls):\n        &quot;&quot;&quot;Pull latest data.&quot;&quot;&quot;\n        test_df = pd.read_csv(cls.__data, header=0, encoding='utf-8')\n        return test_df\n\n    @classmethod\n    def get_schema_from_csv(cls, csv):\n        &quot;&quot;&quot;Infers schema from CSV.&quot;&quot;&quot;\n        table = Table(csv)\n        table.infer(limit=500, confidence=0.55)\n        schema = table.schema.descriptor\n        names = cls.get_column_names(schema, 'name')\n        datatypes = cls.get_column_datatypes(schema, 'type')\n        schema_dict = dict(zip(names, datatypes))\n        return schema_dict\n\n    @classmethod\n    def get_column_names(cls, schema, key):\n        &quot;&quot;&quot;Get names of columns.&quot;&quot;&quot;\n        names = extract_values(schema, key)\n        return names\n\n    @classmethod\n    def get_column_datatypes(cls, schema, key):\n        &quot;&quot;&quot;Convert schema to recognizable by SQLAlchemy.&quot;&quot;&quot;\n        values = extract_values(schema, key)\n        for i, value in enumerate(values):\n            if value == 'integer':\n                values[i] = Integer\n            elif value == 'string':\n                values[i] = Text\n            elif value == 'date':\n                values[i] = Date\n        return values\n        \n    @classmethod\n    def create_new_table(cls, data, schema):\n          &quot;&quot;&quot;Create new table from CSV and generated schema.&quot;&quot;&quot;\n          workday_table.to_sql('faketable',\n                               con=cls.__engine,\n                               schema='testschema',\n                               if_exists='replace',\n                               chunksize=300,\n                               dtype=schema)\n                                 \ndata = CreateTablesFromCSVs.get_schema_from_csv()\nschema = CreateTablesFromCSVs.get_schema_from_csv(data)\nCreateTablesFromCSVs.create_new_table(data, schema)\n</code></pre>\n<p>The first thing worth mentioning is I'm <a href=\"https://gist.github.com/toddbirchard/b6f86f03f6cf4fc9492ad4349ee7ff8b\">importing a function</a> from my personal secret library to extract values from JSON objects. I've <a href=\"https://hackersandslackers.com/extract-data-from-complex-json-python/\">spoken about it before</a>. </p><p>Let's break down this class:</p><ul><li><code>get_data()</code> reads our CSV into a Pandas DataFrame.</li><li><code>get_schema_from_csv()</code> kicks off building a Schema that SQLAlchemy can use to build a table.</li><li><code>get_column_names()</code> simply pulls column names as half our schema.</li><li><code>get_column_datatypes()</code> manually replaces the datatype names we received from <strong>tableschema</strong> and replaces them with SQLAlchemy datatypes.</li><li><code>create_new_table</code> Uses a beautiful marriage between Pandas and SQLAlchemy to create a table in our database with the correct datatypes mapped.</li></ul><h3 id=\"promising-potential-room-to-grow\">Promising Potential, Room to Grow</h3><p>While <strong>tableschema</strong> works some of the time, it isn't perfect. The base of what we accomplish still stands: we now have a reliable formula for how we would create schemas on the fly if we trust our schemas to be accurate.</p><p>Just wait until next time when we introduce <strong>Google BigQuery</strong> into the mix.</p>","url":"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/","uuid":"addbd45d-f9a5-4beb-8b01-2c835b442750","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47b2bcf850c0618c1a59a0"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a8","title":"Lynx Roundup, July 20th","slug":"lynx-roundup-july-20th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","excerpt":"Kafka!  New brain cell!  Computers made of liquid!","custom_excerpt":"Kafka!  New brain cell!  Computers made of liquid!","created_at_pretty":"13 July, 2018","published_at_pretty":"20 July, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-07-13T04:34:04.000-04:00","published_at":"2018-07-20T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:04.000-04:00","meta_title":"Lynx Roundup, July 20th | Hackers and Slackers","meta_description":"Kafka!  New brain cell!  Computers made of liquid!","og_description":"Kafka!  New brain cell!  Computers made of liquid!","og_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","og_title":"Lynx Roundup, July 20th","twitter_description":"Kafka!  New brain cell!  Computers made of liquid!","twitter_image":"https://hackersandslackers.com/content/images/lynx/73@2x.jpg","twitter_title":"Lynx Roundup, July 20th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Science News","slug":"science-news","description":"Breakthroughs in general science.","feature_image":null,"meta_description":"Breakthroughs in general science.","meta_title":"Science News | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"http://www.admintome.com/blog/kafka-python-and-google-analytics/\n\n\n\nhttps://neurosciencenews.com/learning-neurons-9516/\n\n\n\nhttp://cs231n.github.io/understanding-cnn/\n\n\n\nhttps://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/\n\n\n\nhttps://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing","html":"<p></p><p><a href=\"http://www.admintome.com/blog/kafka-python-and-google-analytics/\">http://www.admintome.com/blog/kafka-python-and-google-analytics/</a></p><p></p><p><a href=\"https://neurosciencenews.com/learning-neurons-9516/\">https://neurosciencenews.com/learning-neurons-9516/</a></p><p></p><p><a href=\"http://cs231n.github.io/understanding-cnn/\">http://cs231n.github.io/understanding-cnn/</a></p><p></p><p><a href=\"https://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/\">https://code.fb.com/data-infrastructure/spiral-self-tuning-services-via-real-time-machine-learning/</a></p><p></p><p><a href=\"https://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing\">https://www.nist.gov/news-events/news/2018/06/nist-researchers-simulate-simple-logic-nanofluidic-computing</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-20th/","uuid":"556f0ff4-3929-47ac-87a7-fe061c056c38","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b4863fcc6a9e951f8a6cc67"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867369b","title":"Data Could Save Humanity if it Weren't for Humanity","slug":"data-could-save-humanity","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","excerpt":"A compelling case for robot overlords.","custom_excerpt":"A compelling case for robot overlords.","created_at_pretty":"03 July, 2018","published_at_pretty":"20 July, 2018","updated_at_pretty":"19 February, 2019","created_at":"2018-07-03T03:41:14.000-04:00","published_at":"2018-07-20T00:14:00.000-04:00","updated_at":"2019-02-19T03:42:08.000-05:00","meta_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity | Hackers and Slackers","meta_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","og_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","og_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","twitter_description":"We all agree that 'data addiction' is reaching a peak, yet clueless about what's next. Specualation of “The Future of AI” is unimaginative at best.","twitter_image":"https://hackersandslackers.com/content/images/2018/07/savehumanity@2x.jpg","twitter_title":"Data Could Solve Humanity's Problems, if it Weren't for Humanity","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Science","slug":"datascience","description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","feature_image":null,"meta_description":"Watch as we attempt to maintain a delicate harmony of math, engineering, and intuition to solve larger-than-life problems.","meta_title":"Data Science | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"}],"plaintext":"A decade has passed since I stumbled into technical product development. Looking\nback, I've spent that time almost exclusively in the niche of data-driven\nproducts and engineering. While it seems obvious now, I realized in the 2000s\nthat you could generally create two types of product: you could either build a\n(likely uninspired) UI for existing data, or you could build products which\nproduced new data or interpreted existing data in a new useful way. Betting on\nthe latter seemed like an obvious choice. The late 2000’s felt like building\napps for the sake of apps most of the time. Even today Product Hint is littered\nwith weather apps and rehashed tools, solving problems so insignificant that\nthey almost seem satirical.\n\nYears passed, thus our data-centric tools evolved to fit cultural mind shifts in\nbusinesses which speculated on how these tools could be used. This began to\nbuild a clear yet slowly-growing narrative about how enterprises consider data\nanalysis in their org structures. Unfortunately, I can't say that much about\nthat shift has been positive. There are a number of major problems I believe we\nneed to address:\n\n *   SaaS is created with the goal of selling the product to enterprises. While\n   humanity's understanding of data science reach unprecedented territory, we\n   choose to perfect the  sales pitch  while neglecting education on these\n   tools.\n * As an atrocity to science, individual actors commonly cherry pick information\n   to confirm conclusions for personal benefit, without checks and balances. \n * Data which contradicts knee jerk assumptions made by executives are sometimes\n   taken as personal threats or attacks.\n * Most importantly, data professionals are horribly siloed. Analysts,\n   scientists, and engineers waste far too much time drawing lines between\n   roles: I find it absolutely absurd  to unanimously agree that tool X is for\n   BI  while tool Y is for data cleanup. Considering we all know  these tools\n   are running stacks on Python, R, SQL, etc, there is no reason to succumb to\n   the limitations of proprietary software (such as Tableau). We've turned a\n   blind eye to the possibility of 'data as a service': a chance to overlap\n   responsibilities by building a better  tool to reduce friction, as opposed to\n   increase it in the interest of selling more software.\n\nWhile we might all agree that collective 'data addiction' is reaching a peak,\nmost of us barely know what we mean by that. We conceptually understand that\ndata is important, but our imaginations on how to utilize this power effectively\nleaves a lot to be desired. IBM Watson probably had profound capabilities, but\nits failure lies with the humans tasked to make this technology relevant and\nuseful for humankind.\n\nThe Analytics Honeymoon\nAs I imagine most in the Product Management professionals do, I originally\nconsidered  data analysis to come in the form of web and app analytics. This was\na one-dimensional era; consumer-facing data served the sole purpose of\noptimizing sales and ad revenue, and there were much fewer choices of\nEnterprise-level tools to fall in love with. While the cheaper tools were just\nfine, corporate America had already fallen in love with a fickle mistress known\nas Omniture. \n\nOmniture was in fact in many ways the superior product on the market. As I'm\nsure Sales reps explained in those years, Omniture allowed for a vast level of\nevent tracking customization which was otherwise rare at the time: with the\nproper logic, effort, and willingness, executives could theoretically identify\ngranular issues in their product's conversation flow: issues which came attached\nwith cold hard facts in the form of numbers.\n\nThus, a game of numbers it was: in order to receive the level of granular detail\nexecutives wanted, there came a nominal fee. Well, many fees in fact: the\nproduct out-priced competitors tenfold upfront for the license itself. Since you\njust agreed to spend that much money on proprietary software, it only makes\nsense at that point that you should then hire a certified affiliated consultant\nto implement the custom reports, and then of course pay the lifelong upkeep that\ncomes with tracking events in ever-changing software. Despite all of these\ncosts, companies consistently moved forward with the choice under the\nrealization that the money saved from this data could far outweigh the cost.\n\nSo what happened when we actually collected all that data?\n\nIf You Could Get That Analytics Report, That Would be Great.\nEnterprises and data analysis were made for each other... but in a way that most\nclosely resembles a cliché romcom starring Julia Roberts. This romance follows\nthe beat of a metronome: while executives begin to grasp the impact of\ndata-based decisions, the commitment to actually acknowledge the abundance of\nthis information has its own lifespan. A/B testing, conversation funnels,\nsegmentation, etc: while the foreplay of implementing these buzzwords rolls on\nfor a number of weeks, it gives powerful figures time to reflect on one thing in\nparticular: they've owed their success to a lack of quantifiable accountability,\nand numbers are right around the corner. While this might not be a conscious\nact, it is an entirely real phenomenon.\n\nI've traditionally been a product manager, yet during that period of my career,\nI've found myself nominated to be Gatekeeper of Digital and Financial Data...\nfor whatever reason (it's worth reiterating that I am not nor ever have been an\nanalyst). The phenomenon followed a pattern. Given their expectations,\nexecutives reach their nerves end when the budget they allocated for\nenterprise-level sotftware is still under configuration, and has produced no\nresults. There's a reason why patience is a virtue isn't a phrase you see in\nmany sales pitches: we want what we're being sold, and we want it now. \n\nThats where I'd typically come in. As a product manager, analytics is a valuable\nweapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed\nlike ammunition for change. When a company's problems are become as large as\nthey are obvious, some numerical correlations are nearly common sense..\n\nCue the dashboards, custom reports, event tracking, you name it. Often times\nexecutives would set aside a weekly cadence to review the expensive conclusions\nour software could finally produce. The weekly email newsletters I would produce\nwould be met with a euphoric chain of satisfied stakeholders, time and time\nagain. Finally it seemed, the conclusions were clear and our problems were\nquantifiable. And yet, nothing seemed to change.\n\nHuman Insecurities Versus World Problems\nAs a data enthusiast, I did what we all would've done: I placed analytics\ntracking on our analytics reports themselves. \"Great stuff, groundbreaking work\nhere\" said one CEO, who I'd seen had not bothered to click the link provided. At\na certain point, I began attaching empty Excel spreadsheets and posting dead\nlinks as the content of our beloved reports. Those too were 'groundbreaking',\napparently.\n\nThis is far from an isolated phenomenon in technology. Meeting after meeting,\nclient after client, I took front-row seats to blatant dismissal of numerical\nevidence in favor of  ego-driven decisions. Test A would prove to yield 30%\nhigher conversion rates than Test B, but Test B would prevail thanks to the\nsubjective emotional opinions of talking heads. In retrospect, I can see now how\na grown adult with a household would find the sudden introduction of facts\nthreatening. We have have imposter syndrome, and the twenty-something year old\nanalyst attempting to improve a company will almost always lose to an adult\nprotecting a family.\n\nConsider a recent example uncovered by mistake. While auditing usage for a\nwidely-know project management tool, something seemed off about our volume of\nusage with a product costing us unspeakable dollars. Our department had mostly\nbeen tasked  to upkeep this 'critical' internal system at all costs. As it turns\nout, over 80% of all activity had been out own internal upkeep. That's millions\nof dollars invested in something never used over the course of several years,\nall for the purpose of upholding a guise of value-add.\n\nBI, Data Science, and the Choice to Make That Distinction\nI've spent the last several months working deep under the hood attempting to\ndismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about\nto rip in to a Tableau tangent here, but I promise there's a point.\n\nI was introduced to Tableau as a tool to fill a niche: quick analysis and\none-off extracts on tight timelines. The type of timelines where digging into\nPandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t\nan option. I was pleased with its ability to serve this purpose- such that it\nsparked a spontaneous 1 thousand purchase for a personal license. Tableau\nDesktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the\nrest of my life.\n\nFrom my naïve perspective it seemed logical that Tableau could help assist in\nthe data cleanup and automation I had been handling in scripts previously. This\ncould not be more incorrect. Even with full access to my own Tableau instance,\nit is clear that Tableau has one motive only: to show you your data, and ensure\nyou don't take it elsewhere. Consider this:\n\nCheck out the worksheets and and dashboards you've published to Server.\nConsidering these are equivalent to simple database views, you'd expect the API\ncalls to be exposed in your dev tools... why not? They aren't.\n\nTableau runs on a Postgres database on your personal server. However, no mention\nof \"postgres\" or anything of the sort is searchable to a useful degree. There is\na highly protected Tableau superadmin account which has controls to all tables\nand views in this server, but most research will point users to unlock the\n\"readonly\" user which is essentially a red herring account, or perhaps useful if\nyou're spying on your employee's actions.\n\nAnd then we have the Tableau server API. Ah, what a gift it would be to query\nthose views we created, running on scheduled extracts, so that we might build\nsomething from this information. As it turns out, Tableau's REST API does little\nmore than reveal meta data about files you already knew about. Just in case you\nwere wondering the date it was created, for some weird useless reason.\n\nI'm not just picking on Tableau here (although I'll continue my series about\nhacking them soon enough). This has exposed a massive dichotomy in the way we\nsee and treat data as a profession, or rather, a series of professions. between\nthose who look at data, and those who manipulate, iterate one, and create things\nwith Data. Nobody has ever expressed this realization to me, and many of you\nlikely still don't see what the big deal is. That, to me, is the big deal.\n\nData should be a passion to those looking to improve humanity, without a doubt.\nIf we know personalities are wining the battles against numbers, and feel numb\nto the fact that our proprietary tools prevent us from using data effectively,\nthere's something to be said about the complacency of humanity as we commit to\nconsumption over production. \n\nCompany attitudes towards data are one thing, but individuals are an entirely\ndifferent story. That's a long-winded post for another time.","html":"<p>A decade has passed since I stumbled into technical product development. Looking back, I've spent that time almost exclusively in the niche of data-driven products and engineering. While it seems obvious now, I realized in the 2000s that you could generally create two types of product: you could either build a (likely uninspired) UI for existing data, or you could build products which produced new data or interpreted existing data in a new useful way. Betting on the latter seemed like an obvious choice. The late 2000’s felt like building apps for the sake of apps most of the time. Even today Product Hint is littered with weather apps and rehashed tools, solving problems so insignificant that they almost seem satirical.</p><p>Years passed, thus our data-centric tools evolved to fit cultural mind shifts in businesses which speculated on how these tools could be used. This began to build a clear yet slowly-growing narrative about how enterprises consider data analysis in their org structures. Unfortunately, I can't say that much about that shift has been positive. There are a number of major problems I believe we need to address:</p><ul><li><em> </em>SaaS is created with the goal of <em>selling </em>the product to enterprises. While humanity's understanding of data science reach unprecedented territory, we choose to perfect the<em> sales pitch</em> while neglecting education on these tools.</li><li>As an atrocity to science, individual actors commonly cherry pick information to confirm conclusions for personal benefit, without checks and balances. </li><li>Data which contradicts knee jerk assumptions made by executives are sometimes taken as personal threats or attacks.</li><li>Most importantly, data professionals are <em><strong>horribly siloed. </strong></em>Analysts, scientists, and engineers waste far too much time drawing lines between roles: I find it <em><strong>absolutely absurd</strong></em> to unanimously agree that <strong>tool X is for BI</strong> while <strong>tool Y is for data cleanup</strong>. Considering we <em>all know</em> these tools are running stacks on Python, R, SQL, etc, there is no reason to succumb to the limitations of proprietary software (such as Tableau). We've turned a blind eye to the possibility of 'data as a service': a chance to overlap responsibilities by building a <em>better</em> tool to reduce friction, as opposed to increase it in the interest of selling more software.</li></ul><p>While we might all agree that collective 'data addiction' is reaching a peak, most of us barely know what we mean by that. We conceptually understand that data is important, but our imaginations on how to utilize this power effectively leaves a lot to be desired. IBM Watson probably had profound capabilities, but its failure lies with the humans tasked to make this technology relevant and useful for humankind.</p><h2 id=\"the-analytics-honeymoon\">The Analytics Honeymoon</h2><p>As I imagine most in the Product Management professionals do, I originally considered  data analysis to come in the form of web and app analytics. This was a one-dimensional era; consumer-facing data served the sole purpose of optimizing sales and ad revenue, and there were much fewer choices of Enterprise-level tools to fall in love with. While the cheaper tools were just fine, corporate America had already fallen in love with a fickle mistress known as Omniture. </p><p>Omniture was in fact in many ways the superior product on the market. As I'm sure Sales reps explained in those years, Omniture allowed for a vast level of event tracking customization which was otherwise rare at the time: with the proper logic, effort, and willingness, executives could theoretically identify granular issues in their product's conversation flow: issues which came attached with cold hard facts in the form of numbers.</p><p>Thus, a game of numbers it was: in order to receive the level of granular detail executives wanted, there came a nominal fee. Well, many fees in fact: the product out-priced competitors tenfold upfront for the license itself. Since you just agreed to spend that much money on proprietary software, it only makes sense at that point that you should then hire a certified affiliated consultant to implement the custom reports, and then of course pay the lifelong upkeep that comes with tracking events in ever-changing software. Despite all of these costs, companies consistently moved forward with the choice under the realization that the money saved from this data could far outweigh the cost.</p><p>So what happened when we actually collected all that data?</p><h2 id=\"if-you-could-get-that-analytics-report-that-would-be-great-\">If You Could Get That Analytics Report, That Would be Great.</h2><p>Enterprises and data analysis were made for each other... but in a way that most closely resembles a cliché romcom starring Julia Roberts. This romance follows the beat of a metronome: while executives begin to grasp the impact of data-based decisions, the commitment to actually acknowledge the abundance of this information has its own lifespan. A/B testing, conversation funnels, segmentation, etc: while the foreplay of implementing these buzzwords rolls on for a number of weeks, it gives powerful figures time to reflect on one thing in particular: they've owed their success to a lack of quantifiable accountability, and numbers are right around the corner. While this might not be a conscious act, it is an entirely real phenomenon.</p><p>I've traditionally been a product manager, yet during that period of my career, I've found myself nominated to be Gatekeeper of Digital and Financial Data... for whatever reason (it's worth reiterating that I am not nor ever have been an analyst). The phenomenon followed a pattern. Given their expectations, executives reach their nerves end when the budget they allocated for enterprise-level sotftware is still under configuration, and has produced no results. There's a reason why patience is a virtue isn't a phrase you see in many sales pitches: we want what we're being sold, and we want it now. </p><p>Thats where I'd typically come in. As a product manager, analytics is a valuable weapon, so unspeakable amounts of unsolicited data thrown into my lap  seemed like ammunition for change. When a company's problems are become as large as they are obvious, some numerical correlations are nearly common sense..</p><p>Cue the dashboards, custom reports, event tracking, you name it. Often times executives would set aside a weekly cadence to review the expensive conclusions our software could finally produce. The weekly email newsletters I would produce would be met with a euphoric chain of satisfied stakeholders, time and time again. Finally it seemed, the conclusions were clear and our problems were quantifiable. And yet, nothing seemed to change.</p><h2 id=\"human-insecurities-versus-world-problems\">Human Insecurities Versus World Problems</h2><p>As a data enthusiast, I did what we all would've done: I placed analytics tracking on our analytics reports themselves. \"Great stuff, groundbreaking work here\" said one CEO, who I'd seen had not bothered to click the link provided. At a certain point, I began attaching empty Excel spreadsheets and posting dead links as the content of our beloved reports. Those too were 'groundbreaking', apparently.</p><p>This is far from an isolated phenomenon in technology. Meeting after meeting, client after client, I took front-row seats to blatant dismissal of numerical evidence in favor of  ego-driven decisions. Test A would prove to yield 30% higher conversion rates than Test B, but Test B would prevail thanks to the subjective emotional opinions of talking heads. In retrospect, I can see now how a grown adult with a household would find the sudden introduction of facts threatening. We have have imposter syndrome, and the twenty-something year old analyst attempting to improve a company will almost always lose to an adult protecting a family.</p><p>Consider a recent example uncovered by mistake. While auditing usage for a widely-know project management tool, something seemed off about our volume of usage with a product costing us unspeakable dollars. Our department had mostly been tasked  to upkeep this 'critical' internal system at all costs. As it turns out, over 80% of all activity had been out own internal upkeep. That's millions of dollars invested in something never used over the course of several years, all for the purpose of upholding a guise of value-add.</p><h2 id=\"bi-data-science-and-the-choice-to-make-that-distinction\">BI, Data Science, and the Choice to Make That Distinction</h2><p>I've spent the last several months working deep under the hood attempting to dismantle our undisputed BI overlords over at Tableau. Fair warning: I'm about to rip in to a Tableau tangent here, but I promise there's a point.</p><p>I was introduced to Tableau as a tool to fill a niche: quick analysis and one-off extracts on tight timelines. The type of timelines where digging into Pandas and potentially entering a .bash_profile hell with Anaconda simply wasn’t an option. I was pleased with its ability to serve this purpose- such that it sparked a spontaneous 1 thousand purchase for a personal license. Tableau Desktop, Tableau Prep, and Tableau server; a decision I’ll likely regret for the rest of my life.</p><p>From my naïve perspective it seemed logical that Tableau could help assist in the data cleanup and automation I had been handling in scripts previously. This could not be more incorrect. Even with full access to my own Tableau instance, it is clear that Tableau has one motive only: to show you your data, and ensure you don't take it elsewhere. Consider this:</p><p>Check out the worksheets and and dashboards you've published to Server. Considering these are equivalent to simple database views, you'd expect the API calls to be exposed in your dev tools... why not? They aren't.</p><p>Tableau runs on a Postgres database on your personal server. However, no mention of \"postgres\" or anything of the sort is searchable to a useful degree. There is a highly protected Tableau superadmin account which has controls to all tables and views in this server, but most research will point users to unlock the \"readonly\" user which is essentially a red herring account, or perhaps useful if you're spying on your employee's actions.</p><p>And then we have the Tableau server API. Ah, what a gift it would be to query those views we created, running on scheduled extracts, so that we might build something from this information. As it turns out, Tableau's REST API does little more than reveal meta data about files you already knew about. Just in case you were wondering the date it was created, for some weird useless reason.</p><p>I'm not just picking on Tableau here (although I'll continue my series about hacking them soon enough). This has exposed a massive dichotomy in the way we see and treat data as a profession, or rather, a series of professions. between those who look at data, and those who manipulate, iterate one, and create things with Data. Nobody has ever expressed this realization to me, and many of you likely still don't see what the big deal is. That, to me, is the big deal.</p><p>Data should be a passion to those looking to improve humanity, without a doubt. If we know personalities are wining the battles against numbers, and feel numb to the fact that our proprietary tools prevent us from using data effectively, there's something to be said about the complacency of humanity as we commit to consumption over production. </p><p>Company attitudes towards data are one thing, but individuals are an entirely different story. That's a long-winded post for another time.</p>","url":"https://hackersandslackers.com/data-could-save-humanity/","uuid":"01f56f59-a9ad-494a-993d-216716f68a7d","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5b3b289ad0ac8a143588f360"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb86736a4","title":"Lynx Roundup, July 16th","slug":"lynx-roundup-july-16th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","custom_excerpt":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","created_at_pretty":"13 July, 2018","published_at_pretty":"16 July, 2018","updated_at_pretty":"28 July, 2018","created_at":"2018-07-13T04:18:17.000-04:00","published_at":"2018-07-16T07:30:00.000-04:00","updated_at":"2018-07-28T16:26:42.000-04:00","meta_title":"Lynx Roundup, July 16th | Hackers and Slackers","meta_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","og_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","og_title":"Lynx Roundup, July 16th","twitter_description":"How likely is likely?  Awesome TensorFlow tutorial, and a guide on maffs for Deep Learning","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx6@2x.jpg","twitter_title":"Lynx Roundup, July 16th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Machine Learning","slug":"machine-learning","description":"The latest developments in machine learning tools and technology available to data scientists.","feature_image":null,"meta_description":"The latest developments in machine learning tools and technology available to data scientists.","meta_title":"Machine Learning | Hackers and Slackers","visibility":"public"},{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"}],"plaintext":"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\n\n\n\nhttp://mattturck.com/bigdata2018\n[http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&utm_medium=email&utm_source=Data_Elixir_189]\n\n\n\nOne of the best-written tutorials I've ever seen:\nhttps://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\n\n\n\nhttps://robertheaton.com/2018/06/25/how-to-read/\n\n\n\nhttp://explained.ai/matrix-calculus/index.html","html":"<p><a href=\"https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is\">https://hbr.org/2018/07/if-you-say-something-is-likely-how-likely-do-people-think-it-is</a></p><p></p><p><a href=\"http://mattturck.com/bigdata2018/?utm_campaign=Data_Elixir&amp;utm_medium=email&amp;utm_source=Data_Elixir_189\">http://mattturck.com/bigdata2018</a></p><p></p><p>One of the best-written tutorials I've ever seen:<br><a href=\"https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\">https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/</a></p><p></p><p><a href=\"https://robertheaton.com/2018/06/25/how-to-read/\">https://robertheaton.com/2018/06/25/how-to-read/</a></p><p></p><p><a href=\"http://explained.ai/matrix-calculus/index.html\">http://explained.ai/matrix-calculus/index.html</a></p>","url":"https://hackersandslackers.com/lynx-roundup-july-16th/","uuid":"f886dfa1-1d10-423e-a926-2f5c8bbe7085","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b486049c6a9e951f8a6cc54"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673679","title":"Lynx Roundup, June 18th","slug":"lynx-roundup-june-18th","featured":false,"feature_image":"https://hackersandslackers.com/content/images/lynx/lynx49@2x.jpg","excerpt":"Daily roundup of Data Science news around the industry, 6/18/2018.","custom_excerpt":"Daily roundup of Data Science news around the industry, 6/18/2018.","created_at_pretty":"15 June, 2018","published_at_pretty":"18 June, 2018","updated_at_pretty":"25 July, 2018","created_at":"2018-06-14T20:34:05.000-04:00","published_at":"2018-06-18T07:00:00.000-04:00","updated_at":"2018-07-24T22:06:03.000-04:00","meta_title":"Lynx Roundup, June 18th | Hackers and Slackers","meta_description":"Daily roundup of Data Science news around the industry, 6/18/2018.","og_description":"Daily roundup of Data Science news around the industry, 6/18/2018.","og_image":"https://hackersandslackers.com/content/images/lynx/lynx49@2x.jpg","og_title":"Lynx Roundup, June 18th","twitter_description":"Daily roundup of Data Science news around the industry, 6/18/2018.","twitter_image":"https://hackersandslackers.com/content/images/lynx/lynx49@2x.jpg","twitter_title":"Lynx Roundup, June 18th","authors":[{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null}],"primary_author":{"name":"Matthew Alhonte","slug":"matt","bio":"Super villain in somebody's action hero movie. Experienced a radioactive freak accident at a young age, which rendered him part-snake and strangely adept at Python.\n\n","profile_image":"https://hackersandslackers.com/content/images/2019/03/matt.jpg","twitter":"@MattAlhonte","facebook":null,"website":null},"primary_tag":{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Roundup","slug":"roundup","description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","feature_image":null,"meta_description":"Subscribe to our daily roundups of top data science news articles, slimmed down to only the good stuff.","meta_title":"Lynx Roundup | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Statistics","slug":"statistics","description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","feature_image":null,"meta_description":"Critical mathematical concepts needed to derive meaning and conclusions from data.","meta_title":"Statistics | Hackers and Slackers","visibility":"public"},{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Science News","slug":"science-news","description":"Breakthroughs in general science.","feature_image":null,"meta_description":"Breakthroughs in general science.","meta_title":"Science News | Hackers and Slackers","visibility":"public"}],"plaintext":"I love it when you call me Big Data\n\nhttp://blog.adnansiddiqi.me/getting-started-with-apache-kafka-in-python/\n\nhttps://towardsdatascience.com/python-sets-and-set-theory-2ace093d1607\n\nMore useful than it sounds.\n\nhttp://blog.lerner.co.il/python-parentheses-primer/\n\nhttps://machinelearningmastery.com/statistical-sampling-and-resampling/\n\nhttp://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268\nRelated: https://arxiv.org/abs/1806.04793","html":"<p>I love it when you call me Big Data</p>\n<p><a href=\"http://blog.adnansiddiqi.me/getting-started-with-apache-kafka-in-python/\">http://blog.adnansiddiqi.me/getting-started-with-apache-kafka-in-python/</a></p>\n<p><a href=\"https://towardsdatascience.com/python-sets-and-set-theory-2ace093d1607\">https://towardsdatascience.com/python-sets-and-set-theory-2ace093d1607</a></p>\n<p>More useful than it sounds.</p>\n<p><a href=\"http://blog.lerner.co.il/python-parentheses-primer/\">http://blog.lerner.co.il/python-parentheses-primer/</a></p>\n<p><a href=\"https://machinelearningmastery.com/statistical-sampling-and-resampling/\">https://machinelearningmastery.com/statistical-sampling-and-resampling/</a></p>\n<p><a href=\"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268\">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268</a><br>\nRelated: <a href=\"https://arxiv.org/abs/1806.04793\">https://arxiv.org/abs/1806.04793</a></p>\n","url":"https://hackersandslackers.com/lynx-roundup-june-18th/","uuid":"f62fde42-7bdd-4c1d-899a-a23c53b49749","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5b23097df37f772d33bc1eba"}}]}},"pageContext":{"slug":"bigdata","limit":12,"skip":0,"numberOfPages":1,"humanPageNumber":1,"prevPageNumber":null,"nextPageNumber":null,"previousPagePath":null,"nextPagePath":null}}