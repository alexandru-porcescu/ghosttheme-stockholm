{"data":{"ghostTag":{"slug":"python","name":"Python","visibility":"public","feature_image":null,"description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold"},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__5c307c9493bed0776a0a3d80","title":"Using Redis to Store Information in Python Applications","slug":"using-redis-with-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-2.jpg","excerpt":"A temporary data store for everything from session variables to chat queues.","custom_excerpt":"A temporary data store for everything from session variables to chat queues.","created_at_pretty":"05 January, 2019","published_at_pretty":"05 January, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-05T04:44:52.000-05:00","published_at":"2019-01-05T08:21:00.000-05:00","updated_at":"2019-03-28T05:41:12.000-04:00","meta_title":"Using Redis to Store Information in Python Apps | Hackers and Slackers","meta_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","og_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","og_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-2.jpg","og_title":"Using Redis to Store Information in Python Applications","twitter_description":"Take the guesswork out of storing values in memory: use Redis in your Python stack to have full control over session variables.","twitter_image":"https://hackersandslackers.com/content/images/2019/03/redis-1-1.jpg","twitter_title":"Using Redis to Store Information in Python Applications","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},"tags":[{"name":"NoSQL","slug":"nosql","description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","feature_image":null,"meta_description":"Schemaless data for gunslingers. Heavily focused on databases such as MongoDB, as well as hosting said databases as cloud instances.","meta_title":"NoSQL | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"We’re hacking into the new year here at Hackers and Slackers, and in the\nprocess, we’ve received plenty of new gifts to play with. Nevermind how Santa\nmanages to fit physically non-existent SaaS products under the Christmas tree.\nWe ask for abstract enterprise software every year, and this time we happened to\nget a little red box.\n\nIf you've never personally used Redis, the name probably sounds familiar as\nyou've been bombarded with obscure technology brand names in places like the\nHeroku marketplace, or your unacceptably nerdy Twitter account (I assure you,\nmine is worse). So what is Redis, you ask? Well, it's a NoSQL datasto- wait,\nwhere are you... NO! Don't leave! It's not like THAT, I swear!\n\nWhat Redis is and When to Use It\nRedis stores information in the familiar key/value pair format, but the term\n‘NoSQL’ is more of a technicality than an indicator of use cases synonymous with\nNoSQL databases of the past. Redis looks the part for the very purpose it\nserves: a box that you fill with crap which may or may not be important down the\nline. It’s the perfect place to put a Starbucks gift card or the clothes you’ve\nalready worn which aren’t quite ready to be washed yet.\n\nAll Users go to Heaven: Cloud Storage for User Sessions\nPerhaps the most common use case is a glorified session cache. Similar to the\nway users might store temporary app information in cookies, Redis holds on to\ninformation which is fleeting. The difference is we now own this information\ninside our very own box, thus the Redis motto: “your box, your rules.”* \n\n* I made this up: it holds zero truth.Because temporary user information is in\nour hands as opposed to a fickle browser, we can decide just how  temporary our\n“cache” is, having it persist across sessions or even devices. While local\nmemory storage may as well be a place for throwaway information, and databases\nfor persistent or eternal information, Redis is somewhere in between. As users\ninteract and the information they create within our app evolves, we may choose\nat any point to promote information stored in Redis to a database, or perhaps\nhave it stick around a little while longer. They’ll be thrilled to see their\nshopping cart still filled with the stupid things they almost bought while they\nwere drunk yesterday.\n\nWhen Variables are on a Bagel, You can Have Variables Any Time \nIn other words, Redis is great for solving the need of globally accessible\nvariables throughout an entire application, on a per-user basis. Users who\naccidentally quit your app, move to a new context, or merely use your app for\nlonger than your QA team are easier to manage when their temporary information\nis in a safe and global environment. Compare this to saving a user’s Pac-Man\nscore to a global variable:  the moment an app like Pac-Man crashes or restarts,\nthat session is gone forever. Thus dies another three-letter app obscenity\nbelonging to a leaderboard.\n\nSpeaking of Leaderboards...\nRedis is great at counting in increments. This is probably made evident by the\nfact that it is a computer, and these are the things computers do. Something\nelse that’s made great by counting: queues! Cues of tasks, notifications, chats,\ndisappearing naked photos, etc: all of these things are ideally suited for our\nred box.\n\nGetting a Red Box of Your Own\nPlaying around with a cloud-hosted Redis box will cost you maybe 5 bucks\n(monthly if you forget to cancel). Redis is open source so there are plenty of\nvendors to choose from with little differentiation between them. I’ll consider\nrecommending whichever vendor offers to bribe me the most, but in the meantime\nI’ll leave the window shopping to you.\n\nSetting up Redis should feel like setting up a cloud SQL database, except\nsmaller and cuter. You’ll be able to pick adorable features for your box of\npossibly-but-not-surely-worthless stuff, such as hosted region, etc. Once you're\nset up you should have a host URL for reaching your instance:\n\nredis-1738.c62.us-east-1-4.ec2.cloud.fakeredisprovider.com:42069\n\nNow we’re cooking with gas.\n\nUsing the redis-py Python Library\nThe main Python Redis library is typed as redis, as in pip install Redis. The\neasiest way to connect in any case is via a URI connection string, like such:\n\nr = redis.Redis( url='rediss://:password@hostname:port/0')\n\n\nNote the unique structure of the URI above:\n\n * rediss://: precedes all Redis URIs; NOTE THE TRAILING COLON.\n * password  comes next, with the interesting choice to bypass usernames.\n * hostname  is the instance's URL... almost always a thinly veiled repurposed\n   EC2 instance. That's right, we're being sold simple open source software\n   hosted on AWS. Don't think about it.\n * port is your preferred port of call after pillaging British trade ships. Just\n   making sure you're still here.\n * /database brings up the rear, which is the name of your database.\n\nAs with regular databases, other connection methods exist such as via SSL\ncertificates, etc.\n\nStoring and Getting Values\nThis is your bread and butter for interacting with Redis:\n\n * .set():  Set a key/value pair by either overwriting or creating a new value\n * .get():  Retrieve a value by naming the associated key\n * hmget():  Accepts a variable number of keys, and will return values for each\n   if they exist\n * hmset():  Set multiple values to a single key.\n * hgetall():  Get all values for a key where a key has been assigned multiple\n   values.\n\nIt’s important to note that Redis by default returns bytes as opposed to\nstrings. As a result, it is important to remember the encoding/decoding of\nvalues in order to retrieve them properly. For example:\n\n# Setting a Value\nr.set('uri', str(app.config['SQLALCHEMY_DATABASE_URI']).encode('utf-8'))\n\n# Getting a Value\nr.get('uri').decode('utf-8')\n\n\nIf you happen to be remotely sane, you probably don't want to deal with encoding\nand decoding values over and again. Luckily we can ensure that responses are\nalways decoded for us by setting the decode_responses  parameter to True  when\nsetting up our Redis instance:\n\nredis.StrictRedis(host=\"localhost\", port=6379, charset=\"utf-8\", decode_responses=True)\n\n\nThe redis-py documentation [https://redis-py.readthedocs.io/en/latest/] \nactually goes wayyy deeper than the 5 methods listed above. If you ever somehow\nmanage to cover all of it, I have many questions about the type of person you\nare.\n\nMore Redis Libraries for Python\nIf the above encoding/decoding seems annoying, you aren’t the first. That’s why\nlibraries like Redisworks [https://github.com/seperman/redisworks]  exist.\nRedisworks allows for the seamless exchange of Python data types to and from\nRedis. Want to shove a Python dict down your box’s throat? Go ahead! You won’t\neven have to think about it very hard. There are plenty of similar libraries all\naimed to make sad lives easier.\n\nWant more? How about Asyncio’s very own asynchronous Redis library\n[https://asyncio-redis.readthedocs.io/en/latest/]?  Or how about the similar \naioredis [aioredis.readthedocs.org], another Asyncio Redis plug-in, which also\nincludes pure Python parsing, clustering support, and things I don’t even\nunderstand! There are truly more Python libraries for Redis\n[https://redis.io/clients#python]  than you could need.\n\nFinally, how could we ever forget Flask-Redis? We’ve already covered this\n[https://hackersandslackers.com/demystifying-flasks-application-context/], but\nis easily the first and last Redis library any Flask developer will use.\n\nYour Box, Your Treasure, Your World™\nNow that we’ve uncovered this niche between cached data and stored data, the\npossibilities are endless. The world is your oyster full of things which you may\nor may not choose to shove in your box.\n\nOk, fine. Perhaps this whole concept feels like a bit of an obscure niche hardly\nworthy of the words on this page. Just remember that feeling when the time comes\nthat you too need a little red cube, and it will be waiting with love and\ncompassion. A companion cube, if you will.","html":"<p>We’re hacking into the new year here at Hackers and Slackers, and in the process, we’ve received plenty of new gifts to play with. Nevermind how Santa manages to fit physically non-existent SaaS products under the Christmas tree. We ask for abstract enterprise software every year, and this time we happened to get a little red box.</p><p>If you've never personally used Redis, the name probably sounds familiar as you've been bombarded with obscure technology brand names in places like the Heroku marketplace, or your unacceptably nerdy Twitter account (I assure you, mine is worse). So what is Redis, you ask? Well, it's a NoSQL datasto- wait, where are you... NO! Don't leave! It's not like THAT, I swear!</p><h2 id=\"what-redis-is-and-when-to-use-it\">What Redis is and When to Use It</h2><p>Redis stores information in the familiar key/value pair format, but the term ‘NoSQL’ is more of a technicality than an indicator of use cases synonymous with NoSQL databases of the past. Redis looks the part for the very purpose it serves: a box that you fill with crap which may or may not be important down the line. It’s the perfect place to put a Starbucks gift card or the clothes you’ve already worn which aren’t quite ready to be washed yet.</p><h3 id=\"all-users-go-to-heaven-cloud-storage-for-user-sessions\">All Users go to Heaven: Cloud Storage for User Sessions</h3><p>Perhaps the most common use case is a glorified <strong>session cache</strong>. Similar to the way users might store temporary app information in cookies, Redis holds on to information which is fleeting. The difference is we now own this information inside our very own box, thus the Redis motto: “<em>your box, your rules</em>.”* </p><!--kg-card-begin: html--><span style=\"color:#9DA0A0;font-style:italic;margin-bottom:30px;display:block;text-align:right;width:100%;\">* I made this up: it holds zero truth.</span><!--kg-card-end: html--><p>Because temporary user information is in our hands as opposed to a fickle browser, we can decide just <em>how</em> temporary our “cache” is, having it persist across sessions or even devices. While local memory storage may as well be a place for throwaway information, and databases for persistent or eternal information, Redis is somewhere in between. As users interact and the information they create within our app evolves, we may choose at any point to promote information stored in Redis to a database, or perhaps have it stick around a little while longer. They’ll be thrilled to see their shopping cart still filled with the stupid things they almost bought while they were drunk yesterday.</p><h3 id=\"when-variables-are-on-a-bagel-you-can-have-variables-any-time\">When Variables are on a Bagel, You can Have Variables Any Time </h3><p>In other words, Redis is great for solving the need of globally accessible variables throughout an entire application, on a per-user basis. Users who accidentally quit your app, move to a new context, or merely use your app for longer than your QA team are easier to manage when their temporary information is in a safe and global environment. Compare this to saving a user’s Pac-Man score to a global variable:  the moment an app like Pac-Man crashes or restarts, that session is gone forever. Thus dies another three-letter app obscenity belonging to a leaderboard.</p><h3 id=\"speaking-of-leaderboards-\">Speaking of Leaderboards...</h3><p>Redis is great at counting in increments. This is probably made evident by the fact that it is a computer, and these are the things computers do. Something else that’s made great by counting: queues! Cues of tasks, notifications, chats, disappearing naked photos, etc: all of these things are ideally suited for our red box.</p><h2 id=\"getting-a-red-box-of-your-own\">Getting a Red Box of Your Own</h2><p>Playing around with a cloud-hosted Redis box will cost you maybe 5 bucks (monthly if you forget to cancel). Redis is open source so there are plenty of vendors to choose from with little differentiation between them. I’ll consider recommending whichever vendor offers to bribe me the most, but in the meantime I’ll leave the window shopping to you.</p><p>Setting up Redis should feel like setting up a cloud SQL database, except smaller and cuter. You’ll be able to pick adorable features for your box of possibly-but-not-surely-worthless stuff, such as hosted region, etc. Once you're set up you should have a host URL for reaching your instance:</p><!--kg-card-begin: code--><pre><code>redis-1738.c62.us-east-1-4.ec2.cloud.fakeredisprovider.com:42069</code></pre><!--kg-card-end: code--><p>Now we’re cooking with gas.</p><h2 id=\"using-the-redis-py-python-library\">Using the redis-py Python Library</h2><p>The main Python Redis library is typed as <code>redis</code>, as in <code>pip install Redis</code>. The easiest way to connect in any case is via a URI connection string, like such:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">r = redis.Redis( url='rediss://:password@hostname:port/0')\n</code></pre>\n<!--kg-card-end: markdown--><p>Note the unique structure of the URI above:</p><ul><li><strong>rediss://: </strong>precedes all Redis URIs; <em>NOTE THE TRAILING COLON.</em></li><li><strong>password</strong> comes next, with the interesting choice to bypass usernames.</li><li><strong>hostname</strong> is the instance's URL... almost always a thinly veiled repurposed EC2 instance. That's right, we're being sold simple open source software hosted on AWS. Don't think about it.</li><li><strong>port </strong>is your preferred port of call after pillaging British trade ships. Just making sure you're still here.</li><li><strong>/database </strong>brings up the rear, which is the name of your database.</li></ul><p>As with regular databases, other connection methods exist such as via SSL certificates, etc.</p><h3 id=\"storing-and-getting-values\">Storing and Getting Values</h3><p>This is your bread and butter for interacting with Redis:</p><ul><li><strong>.set():</strong> Set a key/value pair by either overwriting or creating a new value</li><li><strong>.get():</strong> Retrieve a value by naming the associated key</li><li><strong>hmget():</strong> Accepts a variable number of keys, and will return values for each if they exist</li><li><strong>hmset():</strong> Set multiple values to a single key.</li><li><strong>hgetall():</strong> Get all values for a key where a key has been assigned multiple values.</li></ul><p>It’s important to note that Redis by default returns bytes as opposed to strings. As a result, it is important to remember the encoding/decoding of values in order to retrieve them properly. For example:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\"># Setting a Value\nr.set('uri', str(app.config['SQLALCHEMY_DATABASE_URI']).encode('utf-8'))\n\n# Getting a Value\nr.get('uri').decode('utf-8')\n</code></pre>\n<!--kg-card-end: markdown--><p>If you happen to be remotely sane, you probably don't want to deal with encoding and decoding values over and again. Luckily we can ensure that responses are always decoded for us by setting the <code>decode_responses</code> parameter to <code>True</code> when setting up our Redis instance:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">redis.StrictRedis(host=&quot;localhost&quot;, port=6379, charset=&quot;utf-8&quot;, decode_responses=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>The <a href=\"https://redis-py.readthedocs.io/en/latest/\" rel=\"noopener\"><strong>redis-py</strong> documentation</a> actually goes wayyy deeper than the 5 methods listed above. If you ever somehow manage to cover all of it, I have many questions about the type of person you are.</p><h2 id=\"more-redis-libraries-for-python\">More Redis Libraries for Python</h2><p>If the above encoding/decoding seems annoying, you aren’t the first. That’s why libraries like <a href=\"https://github.com/seperman/redisworks\" rel=\"noopener\"><strong>Redisworks</strong></a> exist. Redisworks allows for the seamless exchange of Python data types to and from Redis. Want to shove a Python dict down your box’s throat? Go ahead! You won’t even have to think about it very hard. There are plenty of similar libraries all aimed to make sad lives easier.</p><p>Want more? How about Asyncio’s very own <a href=\"https://asyncio-redis.readthedocs.io/en/latest/\">asynchronous Redis library</a>?  Or how about the similar <strong><a href=\"aioredis.readthedocs.org\">aioredis</a></strong>, another Asyncio Redis plug-in, which also includes pure Python parsing, clustering support, and things I don’t even understand! There are truly <a href=\"https://redis.io/clients#python\">more Python libraries for Redis</a> than you could need.</p><p>Finally, how could we ever forget <strong>Flask-Redis</strong>? We’ve <a href=\"https://hackersandslackers.com/demystifying-flasks-application-context/\" rel=\"noopener\">already covered this</a>, but is easily the first and last Redis library any Flask developer will use.</p><h2 id=\"your-box-your-treasure-your-world-\">Your Box, Your Treasure, Your World<strong>™</strong></h2><p>Now that we’ve uncovered this niche between cached data and stored data, the possibilities are endless. The world is your oyster full of things which you may or may not choose to shove in your box.</p><p>Ok, fine. Perhaps this whole concept feels like a bit of an obscure niche hardly worthy of the words on this page. Just remember that feeling when the time comes that you too need a little red cube, and it will be waiting with love and compassion. A companion cube, if you will.</p>","url":"https://hackersandslackers.com/using-redis-with-python/","uuid":"fcf41325-f7d3-4f3f-b43f-8609e5dc6b07","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c307c9493bed0776a0a3d80"}},{"node":{"id":"Ghost__Post__5c27630bda392c696eab97de","title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","slug":"tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","excerpt":"Organizing a heist on Tableau Server to reclaim workbook data.","custom_excerpt":"Organizing a heist on Tableau Server to reclaim workbook data.","created_at_pretty":"29 December, 2018","published_at_pretty":"29 December, 2018","updated_at_pretty":"13 March, 2019","created_at":"2018-12-29T07:05:31.000-05:00","published_at":"2018-12-29T07:18:53.000-05:00","updated_at":"2019-03-13T05:53:25.000-04:00","meta_title":"Tableau's View Extraction REST API | Hackers and Slackers","meta_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","og_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","og_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","og_title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","twitter_description":"Our siege on Tableau Server continues as we organize a heist to reclaim our workbook data","twitter_image":"https://hackersandslackers.com/content/images/2018/12/hacktaleau-4@2x.jpg","twitter_title":"Tableau's REST API: Turning Tableau into an ETL Pipeline GUI","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Tableau","slug":"tableau","description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","feature_image":null,"meta_description":"Dissect Tableau server and implement hacks to improve your workflow, or simply become familiar with the Tableau desktop user interface.","meta_title":"Tableau Desktop & Server | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"BI","slug":"business-intelligence","description":"Business Intelligence, otherwise known as \"making nice reports for executives to ignore.\"","feature_image":null,"meta_description":null,"meta_title":"Business Intelligence Tools | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Hacking Tableau Server","slug":"hacking-tableau-server","description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","feature_image":"https://hackersandslackers.com/content/images/2019/03/tableauseries-2.jpg","meta_description":"Break free from the constraints of the TSM CLI to bend Tableau Server to your will. Uncover Superadmin privileges, or even rewire Tableau to handle ETL.","meta_title":"Hacking Tableau Server","visibility":"internal"}],"plaintext":"There's nothing I love more than exposing expensive enterprise software. \n\nIt may not seem obvious, but most SaaS products have an underlying core goal:\nshackle businesses to depend on proprietary, closed-source, costly software.\nWhen you pair a surplus of money with a reluctance to work, you've arrived at\nCorporate America: a prime victim yearning to marry itself to any vendor with a\nnice pitch deck and a vague promise.\n\nIn the case of Tableau, this becomes obvious when you attempt to do anything\nbesides create visuals. I don't like spending hours of my time cleaning data to\nbe rewarded with a shitty iframe embed: I want my data. As we've already seen by\nexposing Tableau's hidden Superadmin access\n[\thttps://hackersandslackers.com/hacking-linux-tableu-server/], it's pretty\nclear Tableau doesn't want you to do this. \n\nI realize Tableau is a BI tool, and some might argue we're barking up the wrong\ntree, and all data should be clean before reaching Tableau. My sentiment is\nthis: fuck that. If a single license costs one thousand dollars, and we have the\npower to manipulate data faster  as we visualize it, we should at least be able\nto own  that data: and by \"own,\" I don't mean a CSV export. I want it in my own \ndatabase of choice, not a locked down and hidden Postgres database living on a\nVPS filled with Tableau stuff.\n\nHere's how we'd do that.\n\n\"You Expect us to Just Walk out the Casino with Millions of Dollars on us?\"\nYou're looking at a Boeski, a Jim Brown, a Miss Daisy, two Jethros and a Leon\nSpinks, not to mention the biggest Ella Fitzgerald ever.You're here because\nyou're the best of the best. If you're feeling scared, feel free to back out\nnow.\n\nThis tutorial assumes you have a Tableau Server instance, with a workbook\npublished to a site within said instance. We're going to take a page out of that\nworkbook and turn the raw data into a database table. FAIR  WARNING: We're about\nto dive deep into the obscure world of the Tableau Server REST API\n[https://onlinehelp.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref.htm]\n. It's clunky, it's ugly, and it returns XML. Strap yourself in. \n\nWe're going to be working with 3 core endpoints. Let's walk through them, and\nI'll show you how to exploit said endpoints to create a ruthless data mining\nmachine in Python.\n\n'Tableau Authorization' Endpoint\nLike all obnoxious (aka useful) APIs, we need to authorize each API call with a\ntemporary token. Of course, we'll just have Python generate said token for every\ncall we make.\n\nPOST: http://[MyTaleauServerURL]/api/3.0/auth/signin\n\nHitting this endpoint successfully will result in an XML response (ugh). The\nresponse should look something like this:\n\n<?xml version='1.0' encoding='UTF-8'?>\n<tsResponse xmlns=\"http://tableau.com/api\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd\">\n    <credentials token=\"KBIuvu6FTViuyivuTUR^yfvgTUycvjGubgc\">\n        <site id=\"09Hiugv-345-45d0-b48b-34543giuyvg\" contentUrl=\"hackers\"/>\n        <user id=\"Uohiiyu-3455-8675-9b42-bugvdr876gv\"/>\n    </credentials>\n</tsResponse>\n\n\nThere are a number of things going on here that we should take note of. The\nfirst being a marvel of modern technology: this is perhaps the shittiest\nresponse to a token API call in modern history. Other than that, we need two\nthings from this response:\n\n * The token  is required for every API call from here on out. It is intended to\n   be passed as a header value with the key X-Tableau-Auth.\n * The site ID  is what we'll be using to look up the location of our workbooks\n   in our server instance. This is added to the URL of future API calls (again,\n   impressively shitty design here).\n\n'List All Views by Site' Endpoint\nThere are actually a number of methods we could use to retrieve views, but we're\nspecifically settling on listing our views by 'site,' in the Tableau sense of\nthe word. If you're unfamiliar, a Tableau site  is not a site at all: it's more\nof project within a greater Tableau instance. They probably should've named them\nthat.\n\nGET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views\n\nAs mentioned, we use the site ID  from step 1 to construct this endpoint. In my\nparticular instance, I've only saved a single workbook for simplicity's sake.\nThe response for such a case is as follows:\n\n<?xml version='1.0' encoding='UTF-8'?>\n<tsResponse xmlns=\"http://tableau.com/api\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd\">\n    <pagination pageNumber=\"1\" pageSize=\"100\" totalAvailable=\"1\"/>\n    <views>\n        <view id=\"9a4a1de9-b7af-4a4a-8556-fd5ac82f92bd\" name=\"Jira\" contentUrl=\"JiraIssues/sheets/Jira\" createdAt=\"2018-12-21T09:11:39Z\" updatedAt=\"2018-12-21T09:11:39Z\">\n            <workbook id=\"208a0c4e-e1d9-4852-9d19-7a2fe2717191\"/>\n            <owner id=\"Uohiiyu-3455-8675-9b42-bugvdr876gv\"/>\n            <project id=\"4d1ca337-20b4-442c-aa7b-1dfd470b68bd\"/>\n            <tags/>\n        </view>\n    </views>\n</tsResponse>\n\n\nCheck out the views  node: when we make this API call, <views>  will contain a\nlist of every view saved to the specified site. Keep in mind that a view is\nequivalent to a \"sheet\" of a workbook: in almost any case, you will have many\nviews listed here. \n\nMy sheet happens to be called \"Jira,\" as stated by name=\"Jira\". The thing we\nreally need however is the view id attribute: this will be used in our third and\nfinal API call.\n\n'Get View Data' Endpoint\nNow let's get the raw data from a view of our choice.\n\nGET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views/[MyViewID]/data\n\n\nHere's where we hit pay dirt. This request will result in an output of\ncomma-separated values; I don't need to tell you what we can do with\ncomma-separated values. Here's what my response looks like after formatting it\nas a table:\n\nCurrent AssigneeCurrent StatusDay of Updatedepic_colorepic_nameIssue Type\nissuetype_colorissuetype_urlkeyPriorityprojectsummaryTodd BirchardDoneJune 7,\n2018#42526EWidgetsBug#db5d5dhttps://hackers.nyc3.digitaloceanspaces.com/bug.png\nHACK-96LowestHackers and Slackers\"Recent Posts\" widget does not have link\nrolloverTodd BirchardBacklogJune 15, 2018#57D9A3Page TemplatesTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-32LowestHackers and\nSlackers“Join” pageTodd BirchardDoneNovember 13, 2018#42526EWidgetsTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-543MediumHackers and\nSlackersAdd “pro tip” boxTodd BirchardTo DoDecember 14, 2018#679EEFSEOMajor\nFunctionality#93d171https://hackers.nyc3.digitaloceanspaces.com/story.png\nHACK-656LowHackers and SlackersAdd alt attributes to images vis clarifai Todd\nBirchardBacklogOctober 16, 2018#FDDA3EAccountsMajor Functionality#93d171\nhttps://hackers.nyc3.digitaloceanspaces.com/story.pngHACK-473MediumHackers and\nSlackersAdd avatar selection to signupTodd BirchardDoneNovember 13, 2018#57D9A3\nPage TemplatesSub-task#92BFE5\nhttps://hackers.nyc3.digitaloceanspaces.com/subtask.pngHACK-231MediumHackers and\nSlackersAdd blurb to each post page explaining what these areTodd BirchardDone\nDecember 10, 2018#291BA9Code snippetsTask#73B0E1\nhttps://hackers.nyc3.digitaloceanspaces.com/task.pngHACK-452MediumHackers and\nSlackersAdd color styles for json snippetsThat's right, a table.  Databases are comprised of tables. Perhaps you see where\nI'm going with this.\n\n\"There's a Ninety-five Pound Chinese Man with a Hundred Sixty Million Dollars\nBehind this Door.\"\nLet's get him out.We've got the goods, but calling all these individual\nendpoints manually does nothing for us. We don't want to steal a single view, we\nwant to systematically rob Tableau of it's views on a scheduler and Shanghai\nthem off to a database of our choosing.\n\nIt would be a crime not to automate this, so I've created a class containing all\nthe relevant methods we'd want when it comes to interacting with Tableau's REST\nAPI:\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    \"\"\"Class for with the Tableau server API.\"\"\"\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        \"\"\"Extract contents of a single view.\"\"\"\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        \"\"\"List all views belonging to a Tableau Site.\"\"\"\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        \"\"\"Receive Auth token to perform API requests.\"\"\"\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        \"\"\"Retrieve ID of Tableau 'site' instance.\"\"\"\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        \"\"\"Retrieve core XML for interacting with Tableau.\"\"\"\n        headers = {'Content-Type': 'application/xml'}\n        body = '<tsRequest><credentials name=\"' + cls.__username + '\" password=\"' + cls.__password + '\" ><site contentUrl=\"' + cls.__contenturl + '\" /></credentials></tsRequest>'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n\n\nThe above snippet is a Python class utilizing all the API endpoints we explored\nin a mostly effortless manner. Instantiating the class immediately covers the\ngrunt work of:\n\n *   Generating a token\n * Getting your (unfriendly) site ID\n * Listing all views belonging to the provided site\n * Retrieving data from a worksheet of choice\n\nGet a list of views in your Tableau site by using the list_views()  method. When\nyou see the view you want, pass the view ID  to the .get_view()  method. This\nwill result in response of all raw data in the view in the form of a CSV. \n\nHow to Pull a Heist (Final Chapter): Storing in Offshore Accounts\nTo earn your title as a true con artist, I'm leaving the final step up to you.\nYou've escaped with the loot, but you'll need to put all that data somewhere.\nThis should be a trivial matter of automating a simple database query, but the\nspecifics are up to you.\n\nIf you're ready to liberate your data, feel free to grab the source off of\nGithub [https://gist.github.com/toddbirchard/ad1386b4334e7b22b2f7b38edca3bd5c] \nand go nuts.","html":"<p>There's nothing I love more than exposing expensive enterprise software. </p><p>It may not seem obvious, but most SaaS products have an underlying core goal: shackle businesses to depend on proprietary, closed-source, costly software. When you pair a surplus of money with a reluctance to work, you've arrived at Corporate America: a prime victim yearning to marry itself to any vendor with a nice pitch deck and a vague promise.</p><p>In the case of Tableau, this becomes obvious when you attempt to do anything besides create visuals. I don't like spending hours of my time cleaning data to be rewarded with a shitty iframe embed: I want my <em>data</em>. As we've already seen by exposing Tableau's hidden <a href=\"\thttps://hackersandslackers.com/hacking-linux-tableu-server/\">Superadmin access</a>, it's pretty clear Tableau doesn't want you to do this. </p><p>I realize Tableau is a BI tool, and some might argue we're barking up the wrong tree, and all data should be clean before reaching Tableau. My sentiment is this: <em>fuck that</em>. If a single license costs <em><strong>one thousand dollars</strong></em>, and we have the power to manipulate data <em>faster</em> as we visualize it, we should at least be able to <em>own</em> that data: and by \"own,\" I don't mean a CSV export. I want it in my <em>own</em> database of choice, not a locked down and hidden Postgres database living on a VPS filled with Tableau stuff.</p><p>Here's how we'd do that.</p><h2 id=\"you-expect-us-to-just-walk-out-the-casino-with-millions-of-dollars-on-us\">\"You Expect us to Just Walk out the Casino with Millions of Dollars on us?\"</h2><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/oceans.gif\" class=\"kg-image\"><figcaption>You're looking at a Boeski, a Jim Brown, a Miss Daisy, two Jethros and a Leon Spinks, not to mention the biggest Ella Fitzgerald ever.</figcaption></figure><!--kg-card-end: image--><p>You're here because you're the best of the best. If you're feeling scared, feel free to back out now.</p><p>This tutorial assumes you have a Tableau Server instance, with a workbook published to a site within said instance. We're going to take a page out of that workbook and turn the raw data into a database table. <strong>FAIR</strong> <strong>WARNING</strong>: We're about to dive deep into the obscure world of the <a href=\"https://onlinehelp.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref.htm\">Tableau Server REST API</a>. It's clunky, it's ugly, and it returns XML. Strap yourself in. </p><p>We're going to be working with 3 core endpoints. Let's walk through them, and I'll show you how to exploit said endpoints to create a ruthless data mining machine in Python.</p><h3 id=\"-tableau-authorization-endpoint\">'Tableau Authorization' Endpoint</h3><p>Like all obnoxious (aka useful) APIs, we need to authorize each API call with a temporary token. Of course, we'll just have Python generate said token for every call we make.</p><!--kg-card-begin: code--><pre><code>POST: http://[MyTaleauServerURL]/api/3.0/auth/signin</code></pre><!--kg-card-end: code--><p>Hitting this endpoint successfully will result in an XML response (ugh). The response should look something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-xml\">&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;tsResponse xmlns=&quot;http://tableau.com/api&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd&quot;&gt;\n    &lt;credentials token=&quot;KBIuvu6FTViuyivuTUR^yfvgTUycvjGubgc&quot;&gt;\n        &lt;site id=&quot;09Hiugv-345-45d0-b48b-34543giuyvg&quot; contentUrl=&quot;hackers&quot;/&gt;\n        &lt;user id=&quot;Uohiiyu-3455-8675-9b42-bugvdr876gv&quot;/&gt;\n    &lt;/credentials&gt;\n&lt;/tsResponse&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>There are a number of things going on here that we should take note of. The first being a marvel of modern technology: this is perhaps the shittiest response to a token API call in modern history. Other than that, we need two things from this response:</p><ul><li>The <strong>token</strong> is required for every API call from here on out. It is intended to be passed as a header value with the key <code>X-Tableau-Auth</code>.</li><li>The <strong>site ID</strong> is what we'll be using to look up the location of our workbooks in our server instance. This is added to the URL of future API calls (again, impressively shitty design here).</li></ul><h3 id=\"-list-all-views-by-site-endpoint\">'List All Views by Site' Endpoint</h3><p>There are actually a number of methods we could use to retrieve views, but we're specifically settling on listing our views by '<em>site,' </em>in the Tableau sense of the word<em>. </em>If you're unfamiliar, a Tableau <em>site</em> is not a site at all: it's more of project within a greater Tableau instance. They probably should've named them that.</p><!--kg-card-begin: code--><pre><code>GET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views</code></pre><!--kg-card-end: code--><p>As mentioned, we use the <strong>site ID</strong> from step 1 to construct this endpoint. In my particular instance, I've only saved a single workbook for simplicity's sake. The response for such a case is as follows:</p><!--kg-card-begin: markdown--><pre><code class=\"language-xml\">&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;tsResponse xmlns=&quot;http://tableau.com/api&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://tableau.com/api http://tableau.com/api/ts-api-3.0.xsd&quot;&gt;\n    &lt;pagination pageNumber=&quot;1&quot; pageSize=&quot;100&quot; totalAvailable=&quot;1&quot;/&gt;\n    &lt;views&gt;\n        &lt;view id=&quot;9a4a1de9-b7af-4a4a-8556-fd5ac82f92bd&quot; name=&quot;Jira&quot; contentUrl=&quot;JiraIssues/sheets/Jira&quot; createdAt=&quot;2018-12-21T09:11:39Z&quot; updatedAt=&quot;2018-12-21T09:11:39Z&quot;&gt;\n            &lt;workbook id=&quot;208a0c4e-e1d9-4852-9d19-7a2fe2717191&quot;/&gt;\n            &lt;owner id=&quot;Uohiiyu-3455-8675-9b42-bugvdr876gv&quot;/&gt;\n            &lt;project id=&quot;4d1ca337-20b4-442c-aa7b-1dfd470b68bd&quot;/&gt;\n            &lt;tags/&gt;\n        &lt;/view&gt;\n    &lt;/views&gt;\n&lt;/tsResponse&gt;\n</code></pre>\n<!--kg-card-end: markdown--><p>Check out the <strong>views</strong> node: when we make this API call, <code>&lt;views&gt;</code> will contain a list of every view saved to the specified site. Keep in mind that a view is equivalent to a \"sheet\" of a workbook: in almost any case, you will have many views listed here. </p><p>My sheet happens to be called \"Jira,\" as stated by <code>name=\"Jira\"</code>. The thing we really need however is the <strong>view id </strong>attribute: this will be used in our third and final API call.</p><h3 id=\"-get-view-data-endpoint\">'Get View Data' Endpoint</h3><p>Now let's get the raw data from a view of our choice.</p><!--kg-card-begin: code--><pre><code>GET: http://[MyTaleauServerURL]/api/3.0/sites/[MySiteID]/views/[MyViewID]/data\n</code></pre><!--kg-card-end: code--><p>Here's where we hit pay dirt. This request will result in an output of comma-separated values; I don't need to tell you what we can do with comma-separated values. Here's what my response looks like after formatting it as a table:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n<table class=\"table table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">Current Assignee</th>\n<th title=\"Field #2\">Current Status</th>\n<th title=\"Field #3\">Day of Updated</th>\n<th title=\"Field #4\">epic_color</th>\n<th title=\"Field #5\">epic_name</th>\n<th title=\"Field #6\">Issue Type</th>\n<th title=\"Field #7\">issuetype_color</th>\n<th title=\"Field #8\">issuetype_url</th>\n<th title=\"Field #9\">key</th>\n<th title=\"Field #10\">Priority</th>\n<th title=\"Field #11\">project</th>\n<th title=\"Field #12\">summary</th>\n</tr></thead>\n<tbody><tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>June 7, 2018</td>\n<td>#42526E</td>\n<td>Widgets</td>\n<td>Bug</td>\n<td>#db5d5d</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/bug.png</td>\n<td>HACK-96</td>\n<td>Lowest</td>\n<td>Hackers and Slackers</td>\n<td>&quot;Recent Posts&quot; widget does not have link rollover</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Backlog</td>\n<td>June 15, 2018</td>\n<td>#57D9A3</td>\n<td>Page Templates</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-32</td>\n<td>Lowest</td>\n<td>Hackers and Slackers</td>\n<td>“Join” page</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>November 13, 2018</td>\n<td>#42526E</td>\n<td>Widgets</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-543</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add “pro tip” box</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>To Do</td>\n<td>December 14, 2018</td>\n<td>#679EEF</td>\n<td>SEO</td>\n<td>Major Functionality</td>\n<td>#93d171</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/story.png</td>\n<td>HACK-656</td>\n<td>Low</td>\n<td>Hackers and Slackers</td>\n<td>Add alt attributes to images vis clarifai </td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Backlog</td>\n<td>October 16, 2018</td>\n<td>#FDDA3E</td>\n<td>Accounts</td>\n<td>Major Functionality</td>\n<td>#93d171</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/story.png</td>\n<td>HACK-473</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add avatar selection to signup</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>November 13, 2018</td>\n<td>#57D9A3</td>\n<td>Page Templates</td>\n<td>Sub-task</td>\n<td>#92BFE5</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/subtask.png</td>\n<td>HACK-231</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add blurb to each post page explaining what these are</td>\n</tr>\n<tr>\n<td>Todd Birchard</td>\n<td>Done</td>\n<td>December 10, 2018</td>\n<td>#291BA9</td>\n<td>Code snippets</td>\n<td>Task</td>\n<td>#73B0E1</td>\n<td>https://hackers.nyc3.digitaloceanspaces.com/task.png</td>\n<td>HACK-452</td>\n<td>Medium</td>\n<td>Hackers and Slackers</td>\n<td>Add color styles for json snippets</td>\n</tr>\n</tbody></table>\n</div><!--kg-card-end: html--><p>That's right, a <em>table.</em> Databases are comprised of tables. Perhaps you see where I'm going with this.</p><h2 id=\"there-s-a-ninety-five-pound-chinese-man-with-a-hundred-sixty-million-dollars-behind-this-door-\">\"There's a Ninety-five Pound Chinese Man with a Hundred Sixty Million Dollars Behind this Door.\"</h2><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/1379892761308689767.jpg\" class=\"kg-image\"><figcaption>Let's get him out.</figcaption></figure><!--kg-card-end: image--><p>We've got the goods, but calling all these individual endpoints manually does nothing for us. We don't want to steal a single view, we want to systematically rob Tableau of it's views on a scheduler and Shanghai them off to a database of our choosing.</p><p>It would be a crime not to automate this, so I've created a class containing all the relevant methods we'd want when it comes to interacting with Tableau's REST API:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import requests\nimport xml.etree.ElementTree as ET\nfrom . import r\nimport pandas as pd\nimport io\n\n\nclass ExtractTableauView:\n    &quot;&quot;&quot;Class for with the Tableau server API.&quot;&quot;&quot;\n\n    __baseurl = r.get('baseurl')\n    __username = r.get('username')\n    __password = r.get('password')\n    __database = r.get('uri')\n    __contenturl = r.get('contenturl')\n\n    @classmethod\n    def get_view(cls, site, xml, view, token):\n        &quot;&quot;&quot;Extract contents of a single view.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token,\n                   'Content-Type': 'text/csv'\n                   }\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + str(site) +'/views/' + str(view) + '/data', headers=headers, stream=True)\n        csv_text = req.text\n        view_df = pd.read_csv(io.StringIO(csv_text), header=0)\n        return view_df\n\n    @classmethod\n    def list_views(cls, site, xml, token):\n        &quot;&quot;&quot;List all views belonging to a Tableau Site.&quot;&quot;&quot;\n        headers = {'X-Tableau-Auth': token}\n        req = requests.get(cls.__baseurl + '/api/3.2/sites/' + site + '/views', auth=(cls.__username, cls.__password), headers=headers)\n        root = ET.fromstring(req.content)\n        views_arr = []\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}views':\n                for view in child:\n                    view_dict = {\n                        'name': view.attrib.get('name'),\n                        'id': view.attrib.get('id'),\n                        'url': cls.__baseurl + '/' + view.attrib.get('contentUrl'),\n                        'created': view.attrib.get('createdAt'),\n                        'updated': view.attrib.get('updatedAt')\n                    }\n                    views_arr.append(view_dict)\n        return views_arr\n\n    @classmethod\n    def get_token(cls, xml):\n        &quot;&quot;&quot;Receive Auth token to perform API requests.&quot;&quot;&quot;\n        for child in xml.iter('*'):\n            if child.tag == '{http://tableau.com/api}credentials':\n                token = child.attrib.get('token')\n                return token\n\n    @classmethod\n    def get_site(cls, xml):\n        &quot;&quot;&quot;Retrieve ID of Tableau 'site' instance.&quot;&quot;&quot;\n        root = xml\n        for child in root.iter('*'):\n            if child.tag == '{http://tableau.com/api}site':\n                site = child.attrib.get('id')\n                return site\n\n    @classmethod\n    def initialize_tableau_request(cls):\n        &quot;&quot;&quot;Retrieve core XML for interacting with Tableau.&quot;&quot;&quot;\n        headers = {'Content-Type': 'application/xml'}\n        body = '&lt;tsRequest&gt;&lt;credentials name=&quot;' + cls.__username + '&quot; password=&quot;' + cls.__password + '&quot; &gt;&lt;site contentUrl=&quot;' + cls.__contenturl + '&quot; /&gt;&lt;/credentials&gt;&lt;/tsRequest&gt;'\n        req = requests.post(cls.__baseurl + '/api/3.2/auth/signin', auth=(cls.__username, cls.__password), headers=headers, data=body)\n        root = ET.fromstring(req.content)\n        return root\n</code></pre>\n<!--kg-card-end: markdown--><p>The above snippet is a Python class utilizing all the API endpoints we explored in a mostly effortless manner. Instantiating the class immediately covers the grunt work of:</p><ul><li> Generating a token</li><li>Getting your (unfriendly) site ID</li><li>Listing all views belonging to the provided site</li><li>Retrieving data from a worksheet of choice</li></ul><p>Get a list of views in your Tableau site by using the <code>list_views()</code> method. When you see the view you want, pass the <strong>view ID</strong> to the <code>.get_view()</code> method. This will result in response of all raw data in the view in the form of a CSV. </p><h3 id=\"how-to-pull-a-heist-final-chapter-storing-in-offshore-accounts\">How to Pull a Heist (Final Chapter): Storing in Offshore Accounts</h3><p>To earn your title as a true con artist, I'm leaving the final step up to you. You've escaped with the loot, but you'll need to put all that data somewhere. This should be a trivial matter of automating a simple database query, but the specifics are up to you.</p><p>If you're ready to liberate your data, feel free to <a href=\"https://gist.github.com/toddbirchard/ad1386b4334e7b22b2f7b38edca3bd5c\">grab the source off of Github</a> and go nuts.</p>","url":"https://hackersandslackers.com/tableaus-rest-api-turning-tableau-into-an-etl-pipeline-gui/","uuid":"77d21a34-e5c1-4582-aade-ff92d8596387","page":false,"codeinjection_foot":null,"codeinjection_head":"<style>\n  .post-template .post-content img {\n    width: 100% !important;\n  }\n\n  figcaption {\n    width: -webkit-fill-available !important;\n    width: -moz-available !important;\n    margin: 0 auto 0;\n    margin-top: -10px;\n    padding: 10px !important;\n    background-color: rgba(33, 69, 138, .04);\n    color: #5e6167;\n    font-size: .8em !important;\n    font-style: italic;\n    text-align: center !important;\n    white-space: normal !important;\n    line-height: 1.5 !important;\n  }\n\n  .language-xml::before {\n    content: \"XML\" !important;\n  }\n\n  .language-html::before {\n    content: \"XML\" !important;\n  }\n\n  td {\n    display: table-cell;\n    padding: 15px 10px !important;\n    font-size: .7em !important;\n    line-height: 1.2 !important;\n    text-align: left !important;\n    text-align: center !important;\n    vertical-align: middle !important;\n    max-width: 150px !important;\n    overflow: hidden !important;\n    white-space: nowrap !important;\n  }\n\n  th {\n    padding: 10px !important;\n    font-size: .7em !important;\n    text-align: center !important;\n    min-width: none !important;\n  }\n\n  .tableContainer {\n    margin-top: 30px;\n    overflow: hidden;\n  }\n</style>","comment_id":"5c27630bda392c696eab97de"}},{"node":{"id":"Ghost__Post__5c1af93bffe54a660c58b85a","title":"Cracking Full Control Over Plot.ly Dash","slug":"gaining-full-control-over-plotly-dash","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/Dash@2x.jpg","excerpt":"Build apps with Plot.ly Dash on your own terms","custom_excerpt":"Build apps with Plot.ly Dash on your own terms","created_at_pretty":"20 December, 2018","published_at_pretty":"20 December, 2018","updated_at_pretty":"28 March, 2019","created_at":"2018-12-19T21:06:51.000-05:00","published_at":"2018-12-20T14:58:00.000-05:00","updated_at":"2019-03-28T05:19:31.000-04:00","meta_title":"Cracking Full Control Over Plot.ly Dash | Hackers and Slackers","meta_description":"Build apps with Plot.ly Dash on your own terms. Extend Flask functionality with Dash, not the other way around.","og_description":"Build apps with Plot.ly Dash on your own terms. Extend Flask functionality with Dash, not the other way around.","og_image":"https://hackersandslackers.com/content/images/2018/12/Dash@2x.jpg","og_title":"Cracking Full Control Over Plot.ly Dash","twitter_description":"Build apps with Plot.ly Dash on your own terms. Extend Flask functionality with Dash, not the other way around.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/Dash@2x.jpg","twitter_title":"Cracking Full Control Over Plot.ly Dash","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Plotly","slug":"plotly","description":"Get intimate with a staple product in data visualization. Create charts with Plot.ly's core product, or become a pro with Plot.ly Dash.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Dash.jpg","meta_description":"Get intimate with a staple product in data visualization. Create charts with Plot.ly's core product, or become a pro with Plot.ly Dash.","meta_title":"Plotly for Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Plotly","slug":"plotly","description":"Get intimate with a staple product in data visualization. Create charts with Plot.ly's core product, or become a pro with Plot.ly Dash.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Dash.jpg","meta_description":"Get intimate with a staple product in data visualization. Create charts with Plot.ly's core product, or become a pro with Plot.ly Dash.","meta_title":"Plotly for Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"},{"name":"#Using Plotly Dash Like a Pro","slug":"plotly-dash","description":"Push the limits of Plot.ly Dash to create data-driven applications with ease. Take on Python, Pandas, Flask, and Data Visualization all at once.","feature_image":"https://hackersandslackers.com/content/images/2019/03/Dash.jpg","meta_description":"Push the limits of Plot.ly Dash to create data-driven applications with ease. Take on Python, Pandas, Flask, and Data Visualization all at once.","meta_title":"Using Plotly Dash Like a Pro","visibility":"internal"}],"plaintext":"Ahh, Plot.ly [http://plot.ly/]; typing that name into a post headline triggers\nan emotional cocktail of pride and embarrassment. Over the years Plotly has been\nat the core of some of the most influential products I’ve worked on: a\nhodgepodge of Fintech and humanitarian clients, all of which are still proudly\nwaving their charts and dashboards around the world. Yet, my mind is boggled by\na simple question: what the hell  took us so long to write our first post about\nPlotly? We've been operating Hackers and Slackers for over a full year now...\ndid I seriously write a  post about JQuery\n[https://hackersandslackers.com/making-ajax-calls-with-jquery/]  in that time\nbefore reaching this point?\n\nMuch has changed in the last year or so for our friends in Montreal. Number 1 in\nmy book is the price reduction of their core product: from 300 dollars  to zero.\nI paid the 300 dollars. We really need to get a “donate” button around here. \n\nA close second is undoubtedly the introduction of Plot.ly Dash\n[https://plot.ly/products/dash/]. Dash  tickles a sentiment which has danced\nthrough many young and helplessly naïve Pythonistas' minds: what if we could\nwrite only  in Python, like, forever?  As awful of an idea it is to start\nGoogling Python-to-frontend code interpreters (they exist; I checked), Plotly's\nDash does a shockingly good job of breathing life into that romantic fantasy of\ncommitting to Python forever.\n\nBut we're not here to deliver a recycled 'What is Plotly?'  synopsis. We're not\neven interested in the obligatory 'How to Get Started Using This\nAlready-Well-Documented-Technology' post. Plotly deserves better than that.\nInstead, we're coming hot out of the gate swinging: we're going to show you how\nto beat Plotly down, break it, and make it bend to your will. Welcome to a\nmagical edition of Hacking Plotly. It must be Christmas, folks.\n\nLet's Make a Plotly + Flask Lovechild from Hell\nLike almost every single advancement to come out of Python-geared architecture\nthis year, Dash has a little secret: it's gotten here with a little help from\nFlask. Alright, perhaps more than a little: Dash actually extends Flask. Sounds\nsensible, and perhaps even exciting at first; its almost as though every crush\nyou've ever had decided it be best to simply put their differences aside to\nstart a group chat with you in the interest of making your sexual well-being an\nequal team effort out of sheer love. As you've already guessed, life doesn't\nwork like that.\n\nDash hijacks Flask from the beginning, starting with the way we instantiate the\napp. Any code monkey who has laid eyes upon a wsgi.py file can tell you\nsomething is up before you can even say app = dash.Dash(__name__). Check out the\nrecommended startup boilerplate:\n\nfrom dash import Dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\nexternal_stylesheets = ['https://codepen.io/fgdsgfhgfh/pen/IHvjvb.css']\n\napp = Dash(__name__, external_stylesheets=external_stylesheets)\n\napp.layout = html.Div(\n        id='example-div-element'\n        )\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\n\nIf you were to attempt to take this boilerplate and attempt to add core Flask\nlogic, such as authentication with Flask-Login, generating assets with \nFlask-Assets, or just creating a global database, where would you start? Plotly\ncleverly suggests reserving the app  namespace for your app- the very same that\nwe would do with Flask. Yet if we attempt to modify the app  object the same as\nwe would with Flask, nothing will work: Dash has declared an ecosystem, and\nnowhere in that ecosystem are you invited to add custom Flask application logic\nout of the box.\n\nDash does what it was intended to do very well: building dashboard-based\napplications. The issue is that applications which can only display data  aren't\nentirely useful as end products. What if we wanted to create a fully-featured\napp, where data visualization was simply a feature  of said app?\n\nCreating a Fully-Featured App (Where Data Vis is Simply a Feature of Said App)\nA common \"workaround\" you'll find in the community is passing Flask to Dash as\nthe underlying \"server\", something like this:\n\nfrom flask import Flask\nfrom dash import Dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\nserver = Flask(__name__)\napp = dash.Dash(__name__, server=server, url_base_pathname='/path')\napp.layout = html.Div(id='example-div-element')\n\n@server.route(\"/dash\")\ndef MyDashApp():\n    return app.index()\n\n\nMake no mistake: this method sucks. Sure, you've regained the ability to create\nroutes here and there, but let's not forget:\n\n * Your app will always start on a Dash-served page: if anything, we'd want our\n   start page to be something we have full control over to then dive into the\n   Dash components.\n * Access to globally available Flask plugins are still unavailable in this\n   method. Notice how we never set an application context?\n * Your ability to style your application with static assets and styles is\n   completely out of your hands.\n * Container architecture built on Flask, such as Google App Engine, won't play\n   nicely when we start something that isn't Flask. So there's a good chance\n   that playing by the rules means losing the ability to deploy.\n\nIf we want to do these things, we cannot start our app as an instance of Dash\nand attempt to work around it. Instead, we must create a Flask app, and put Dash\nin its place as an app embedded in our  app. This gives us full control over\nwhen users can enter the Dash interface, and even within that interface, we can\nstill manage database connections or user sessions as we see fit. Welcome to the\nbig leagues.\n\nTurning the Tables: Dash Inside Flask\nFirst things first, let's get our wsgi.py  file back. Pretty much any hosted\nPython application expects this, so please: enough with the app.py  nonsense.\n\nfrom plotly_flask_tutorial import create_app\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', debug=True)\n\n\nLook familiar? Not only do we get Flask back, but we get our entire application\nfactory and all that it includes. Take a look at application/__init__.py:\n\nfrom flask import Flask\nfrom . import dash_view\n\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n    dash_app = dash_view.Add_Dash(app)\n\n    with app.app_context():\n        # Construct the core application\n        from . import routes\n        app.register_blueprint(routes.main_bp)\n\n        return app\n\n\nIt's almost as though nothing changed! In fact, the only line we have regarding\nDash here is dash_app = plotly_dash_views.Add_Dash(app). \n\nWe import dash_view  at the start of __init.py__. What is this, you might ask?\nIt's actually a file which contains our Dash app! Dash apps typically like to\nhave a single .py  file per view, which turns out to work great for us. Let's\nlook at why this works by checking dash_view.py:\n\nimport glob\nfrom dash import Dash\nimport dash_table\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\n\n\ndef Add_Dash(server):\n    \"\"\"Plot.ly Dash view which populates the screen with loaded DataFrames.\"\"\"\n    external_stylesheets = ['https://hackers.nyc3.cdn.digitaloceanspaces.com/css/plotly-flask-tutorial.css']\n    dash_app = Dash(server=server,\n                    external_stylesheets=external_stylesheets,\n                    routes_pathname_prefix='/dash_view/')\n\n    # Create Dash Layout\n    dash_app.layout = html.Div(\n        id='dash-container'\n      )\n\n    return dash_app.server\n\n\nWe pass our Flask instance to Add_Dash  as a parameter called server. Unlike the\nprevious examples, it's actually server  running the show this time, with Dash\npiggybacking as a module. This is our most important line of code:\n\ndash_app = Dash(server=server, routes_pathname_prefix='/dash_view/')\n\n\nDash doesn't handle routes like Flask does (or at all, really). That's fine! We\nstart dash_app  with URL prefix, which means the Dash logic here is confined to\nthat single page. This means we can build a sprawling Flask app with hundreds of\nfeatures and views, and oh yeah, if we want a Dash view, we can just create a\nfile for that to chill on its own, not touching anything else.\n\nNow you're thinking with portals™.\n\nWhat Our App Looks Like\nIf you're following along, it would probably help to have a top-level view of\nwhat's going on so far:\n\nplotlydash-flask-tutorial\n├── /plotly_flask_tutorial\n│   ├── __init__.py\n│   ├── dash_view.py\n│   ├── routes.py\n│   ├── /data\n│   │   ├── chicago_taxis.csv\n│   │   ├── citibike_trips.csv\n│   │   ├── cities.csv\n│   │   └── pocket.csv\n│   ├── /static\n│   │   ├── dist\n│   │   │   └── css\n│   │   │       └── plotly-flask-tutorial.css\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── less\n│   │       ├── global.less\n│   │       ├── header.less\n│   │       └── table.less\n│   ├── /templates\n│   │   ├── index.html\n│   │   └── nav.html\n│   └── /data\n│       ├── chicago_taxis.csv\n│       ├── citibike_trips.csv\n│       ├── cities.csv\n│       └── pocket.csv\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── config.py\n├── requirements.txt\n├── setup.py\n├── start.sh\n└── wsgi.py\n\n\nWe're storing our app within a directory called plotly_flask_tutorial. In that\ndirectory, we have our typical Flask stuff (/templates, /static, etc) as well as\ntwo notable files: routes.py  and dash_view.py.\n\nroutes.py\nroutes.py  can contain anything we want. Our application will default to serving\na Flask page, not  a Dash page, so our routes can be an entire standalone\napplication. Here's what I tossed in there: \n\nimport os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask import current_app as app\nimport lesscpy\n\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\nEnvironment.auto_build = True\nEnvironment.debug = False\nless_bundle = Bundle('less/*.less',\n                     filters='less,cssmin',\n                     output='dist/css/plotly-flask-tutorial.css.css',\n                     extra={'rel': 'stylesheet/less'})\njs_bundle = Bundle('js/*.js',\n                   filters='jsmin',\n                   output='dist/js/main.js')\nassets.register('less_all', less_bundle)\nassets.register('js_all', js_bundle)\nless_bundle.build(force=True)\njs_bundle.build()\n\n\n# Landing Page\n@main_bp.route('/', methods=['GET'])\ndef home():\n    return render_template('index.html',\n                           title='Plotly Flask Tutorial.',\n                           template='home-template',\n                           body=\"This is an example homepage, served with Flask.\")\n\n\nAll this is doing is serving up index.html.\n\ndash_view.py\ndash_view.py  is the Dash app we have living within  our Flask app. But how does\nFlask know which url to serve our application at? Wasn't it missing from \nroutes.py? Indeed it was, good fellow! Because we set routes_pathname_prefix, we\n don't need  to create a route for dash_view.py: it will always be served\nwhenever we navigate to 127.0.01/dash_view. Thus, we can create a navigation\ntemplate like this:\n\n<nav>\n  <a href=\"/\"><i class=\"fas fa-home\"></i> Home</a>\n  <a href=\"/dash_view/\"><i class=\"fas fa-chart-line\"></i> Embdedded Plotly Dash</a>\n</nav>\n\n\nCreating Something Useful\nHere's a fun little thing I was able to do with Dash, while in the context of\nrunning under a Flask app. In our file dash_view.py, I have the app look at a\nfolder of extracted datasets (called  /data). For each dataset, I use Pandas to\ngenerate a preview, and Dash's \"data table\" component to render said previews in\nour Dash app. This lets us quickly cruise through the data an app depends on\nwith a cool interface:\n\nA bit rough around the edges, but you get the point.If you're hungry for some\nsource code to get started building your own Plotly Dash views, here's the\nsource I used to create the page above:\n\nimport glob\nfrom pathlib import Path, PurePath\nfrom dash import Dash\nimport dash_table\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\n\np = Path('.')\n\n\ndef Add_Dash(server):\n    \"\"\"Plot.ly Dash view which populates the screen with loaded DataFrames.\"\"\"\n    external_stylesheets = ['https://hackers.nyc3.cdn.digitaloceanspaces.com/css/plotly-flask-tutorial.css',\n                            'https://fonts.googleapis.com/css?family=Lato',\n                   'https://use.fontawesome.com/releases/v5.8.1/css/all.css']\n    dash_app = Dash(server=server,\n                    external_stylesheets=external_stylesheets,\n                    routes_pathname_prefix='/dash_view/')\n\n    # Override the underlying HTML template\n    dash_app.index_string = '''<!DOCTYPE html>\n        <html>\n            <head>\n                {%metas%}\n                <title>{%title%}</title>\n                {%favicon%}\n                {%css%}\n            </head>\n            <body>\n                <nav>\n                  <a href=\"/\"><i class=\"fas fa-home\"></i> Home</a>\n                  <a href=\"/dash_view/\"><i class=\"fas fa-chart-line\"></i> Embdedded Plotly Dash</a>\n                </nav>\n                {%app_entry%}\n                <footer>\n                    {%config%}\n                    {%scripts%}\n                    {%renderer%}\n                </footer>\n            </body>\n        </html>'''\n\n    # Create Dash Layout comprised of Data Tables\n    dash_app.layout = html.Div(\n        children=get_datasets(),\n        id='flex-container'\n      )\n\n    return dash_app.server\n\n\ndef get_datasets():\n    \"\"\"Returns previews of all CSVs saved in /data directory.\"\"\"\n    data_filepath = list(p.glob('plotly_flask_tutorial/data/*.csv'))\n    arr = ['This is an example Plot.ly Dash App.']\n    for index, csv in enumerate(data_filepath):\n        print(PurePath(csv))\n        df = pd.read_csv(data_filepath[index]).head(10)\n        table_preview = dash_table.DataTable(\n            id='table_' + str(index),\n            columns=[{\"name\": i, \"id\": i} for i in df.columns],\n            data=df.to_dict(\"rows\"),\n            sorting=True,\n        )\n        arr.append(table_preview)\n    return arr\n\n\nI've gone ahead and uploaded the source code for this working example up on\nGithub [https://github.com/toddbirchard/plotlydash-flask-tutorial]. Please steal\nit: it's all yours.\n\nNeedless to say, there's way more cool shit we can accomplish with Plotly Dash.\nStick around long enough, and chances are we'll cover all of them.","html":"<p>Ahh, <a href=\"http://plot.ly/\"><strong>Plot.ly</strong></a>; typing that name into a post headline triggers an emotional cocktail of pride and embarrassment. Over the years Plotly has been at the core of some of the most influential products I’ve worked on: a hodgepodge of Fintech and humanitarian clients, all of which are still proudly waving their charts and dashboards around the world. Yet, my mind is boggled by a simple question: what the <em>hell</em> took us so long to write our first post about Plotly? We've been operating Hackers and Slackers for over a full year now... did I seriously write a<a href=\"https://hackersandslackers.com/making-ajax-calls-with-jquery/\"> post about JQuery</a> in that time before reaching this point?</p><p>Much has changed in the last year or so for our friends in Montreal. Number 1 in my book is the price reduction of their core product: from <em><strong>300 dollars</strong></em> to <em><strong>zero</strong></em>. I paid the 300 dollars. We really need to get a “donate” button around here. </p><p>A close second is undoubtedly the introduction of <strong><a href=\"https://plot.ly/products/dash/\">Plot.ly Dash</a></strong>. <strong>Dash</strong> tickles a sentiment which has danced through many young and helplessly naïve Pythonistas' minds: <em>what if we could write </em><strong><em>only</em></strong><em> in Python, like, </em><strong><em>forever</em></strong><em>?</em> As awful of an idea it is to start Googling Python-to-frontend code interpreters (they exist; I checked), Plotly's Dash does a shockingly good job of breathing life into that romantic fantasy of committing to Python forever.</p><p>But we're not here to deliver a recycled 'W<em>hat is Plotly?'</em> synopsis. We're not even interested in the obligatory '<em>How to Get Started Using This Already-Well-Documented-Technology' </em>post<em>. </em>Plotly deserves better than that. Instead, we're coming hot out of the gate swinging: we're going to show you how to beat Plotly down, break it, and make it bend to your will. Welcome to a magical edition of Hacking Plotly. It must be Christmas, folks.</p><h2 id=\"let-s-make-a-plotly-flask-lovechild-from-hell\">Let's Make a Plotly + Flask Lovechild from Hell</h2><p>Like almost every single advancement to come out of Python-geared architecture this year, Dash has a little secret: it's gotten here with a little help from Flask. Alright, perhaps more than a little: Dash actually extends Flask. Sounds sensible, and perhaps even exciting at first; its almost as though every crush you've ever had decided it be best to simply put their differences aside to start a group chat with you in the interest of making your sexual well-being an equal team effort out of sheer love. As you've already guessed, life doesn't work like that.</p><p>Dash hijacks Flask from the beginning, starting with the way we instantiate the app. Any code monkey who has laid eyes upon a <strong>wsgi.py </strong>file can tell you something is up before you can even say <code>app = dash.Dash(__name__)</code>. Check out the recommended startup boilerplate:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from dash import Dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\nexternal_stylesheets = ['https://codepen.io/fgdsgfhgfh/pen/IHvjvb.css']\n\napp = Dash(__name__, external_stylesheets=external_stylesheets)\n\napp.layout = html.Div(\n        id='example-div-element'\n        )\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>If you were to attempt to take this boilerplate and attempt to add core Flask logic, such as authentication with <code>Flask-Login</code>, generating assets with <code>Flask-Assets</code>, or just creating a global database, where would you start? Plotly cleverly suggests reserving the <code>app</code> namespace for your app- the very same that we would do with Flask. Yet if we attempt to modify the <code>app</code> object the same as we would with Flask, nothing will work: Dash has declared an ecosystem, and nowhere in that ecosystem are you invited to add custom Flask application logic out of the box.</p><p>Dash does what it was intended to do very well: building dashboard-based applications. The issue is that applications which can <em>only display data</em> aren't entirely useful as end products. What if we wanted to create a fully-featured app, where data visualization was simply a <em>feature</em> of said app?</p><h2 id=\"creating-a-fully-featured-app-where-data-vis-is-simply-a-feature-of-said-app-\">Creating a Fully-Featured App (Where Data Vis is Simply a Feature of Said App)</h2><p>A common \"workaround\" you'll find in the community is passing Flask to Dash as the underlying \"server\", something like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom dash import Dash\nimport dash_core_components as dcc\nimport dash_html_components as html\n\nserver = Flask(__name__)\napp = dash.Dash(__name__, server=server, url_base_pathname='/path')\napp.layout = html.Div(id='example-div-element')\n\n@server.route(&quot;/dash&quot;)\ndef MyDashApp():\n    return app.index()\n</code></pre>\n<!--kg-card-end: markdown--><p>Make no mistake: this method <em>sucks. </em>Sure, you've regained the ability to create routes here and there, but let's not forget:</p><ul><li>Your app will always start on a Dash-served page: if anything, we'd want our start page to be something we have full control over to then dive into the Dash components.</li><li>Access to globally available Flask plugins are still unavailable in this method. Notice how we never set an application context?</li><li>Your ability to style your application with static assets and styles is completely out of your hands.</li><li>Container architecture built on Flask, such as Google App Engine, won't play nicely when we start something that isn't Flask. So there's a good chance that playing by the rules means losing the ability to deploy.</li></ul><p>If we want to do these things, we cannot start our app as an instance of Dash and attempt to work around it. Instead, we must create a Flask app, and put Dash in its place as an app embedded in <em>our</em> app. This gives us full control over when users can enter the Dash interface, and even within that interface, we can still manage database connections or user sessions as we see fit. Welcome to the big leagues.</p><h2 id=\"turning-the-tables-dash-inside-flask\">Turning the Tables: Dash Inside Flask</h2><p>First things first, let's get our <strong>wsgi.py</strong> file back. Pretty much any hosted Python application expects this, so please: enough with the <strong>app.py</strong> nonsense.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from plotly_flask_tutorial import create_app\n\napp = create_app()\n\nif __name__ == &quot;__main__&quot;:\n    app.run(host='0.0.0.0', debug=True)\n</code></pre>\n<!--kg-card-end: markdown--><p>Look familiar? Not only do we get Flask back, but we get our entire application factory and all that it includes. Take a look at <code>application/__init__.py</code><em>:</em></p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">from flask import Flask\nfrom . import dash_view\n\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n    dash_app = dash_view.Add_Dash(app)\n\n    with app.app_context():\n        # Construct the core application\n        from . import routes\n        app.register_blueprint(routes.main_bp)\n\n        return app\n</code></pre>\n<!--kg-card-end: markdown--><p>It's almost as though nothing changed! In fact, the only line we have regarding Dash here is <code>dash_app = plotly_dash_views.Add_Dash(app)</code>. </p><p>We import <code>dash_view</code> at the start of <code>__init.py__</code>. What is this, you might ask? It's actually a file which contains our Dash app! Dash apps typically like to have a single <em>.py</em> file per view, which turns out to work great for us. Let's look at why this works by checking <code>dash_view.py</code>:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import glob\nfrom dash import Dash\nimport dash_table\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\n\n\ndef Add_Dash(server):\n    &quot;&quot;&quot;Plot.ly Dash view which populates the screen with loaded DataFrames.&quot;&quot;&quot;\n    external_stylesheets = ['https://hackers.nyc3.cdn.digitaloceanspaces.com/css/plotly-flask-tutorial.css']\n    dash_app = Dash(server=server,\n                    external_stylesheets=external_stylesheets,\n                    routes_pathname_prefix='/dash_view/')\n\n    # Create Dash Layout\n    dash_app.layout = html.Div(\n        id='dash-container'\n      )\n\n    return dash_app.server\n</code></pre>\n<!--kg-card-end: markdown--><p>We pass our Flask instance to <code>Add_Dash</code> as a parameter called <em>server. </em>Unlike the previous examples, it's actually <em>server</em> running the show this time, with Dash piggybacking as a module. This is our most important line of code:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">dash_app = Dash(server=server, routes_pathname_prefix='/dash_view/')\n</code></pre>\n<!--kg-card-end: markdown--><p>Dash doesn't handle routes like Flask does (or at all, really). That's fine! We start <code>dash_app</code> with URL prefix, which means the Dash logic here is confined to that single page. This means we can build a sprawling Flask app with hundreds of features and views, and oh yeah, if we want a Dash view, we can just create a file for that to chill on its own, not touching anything else.</p><p>Now you're thinking with portals<strong>™.</strong></p><h2 id=\"what-our-app-looks-like\">What Our App Looks Like</h2><p>If you're following along, it would probably help to have a top-level view of what's going on so far:</p><!--kg-card-begin: markdown--><pre><code>plotlydash-flask-tutorial\n├── /plotly_flask_tutorial\n│   ├── __init__.py\n│   ├── dash_view.py\n│   ├── routes.py\n│   ├── /data\n│   │   ├── chicago_taxis.csv\n│   │   ├── citibike_trips.csv\n│   │   ├── cities.csv\n│   │   └── pocket.csv\n│   ├── /static\n│   │   ├── dist\n│   │   │   └── css\n│   │   │       └── plotly-flask-tutorial.css\n│   │   ├── js\n│   │   │   └── main.js\n│   │   └── less\n│   │       ├── global.less\n│   │       ├── header.less\n│   │       └── table.less\n│   ├── /templates\n│   │   ├── index.html\n│   │   └── nav.html\n│   └── /data\n│       ├── chicago_taxis.csv\n│       ├── citibike_trips.csv\n│       ├── cities.csv\n│       └── pocket.csv\n├── Pipfile\n├── Pipfile.lock\n├── README.md\n├── config.py\n├── requirements.txt\n├── setup.py\n├── start.sh\n└── wsgi.py\n</code></pre>\n<!--kg-card-end: markdown--><p>We're storing our app within a directory called <code>plotly_flask_tutorial</code>. In that directory, we have our typical Flask stuff (<strong>/templates</strong>, <strong>/static</strong>, etc) as well as two notable files: <code>routes.py</code> and <code>dash_view.py</code>.</p><h3 id=\"routes-py\">routes.py</h3><p><code>routes.py</code> can contain anything we want. Our application will default to serving a Flask page, <em>not</em> a Dash page, so our routes can be an entire standalone application. Here's what I tossed in there: </p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import os\nfrom flask import Blueprint, render_template\nfrom flask_assets import Environment, Bundle\nfrom flask import current_app as app\nimport lesscpy\n\nmain_bp = Blueprint('main_bp', __name__,\n                    template_folder='templates',\n                    static_folder='static')\nassets = Environment(app)\nEnvironment.auto_build = True\nEnvironment.debug = False\nless_bundle = Bundle('less/*.less',\n                     filters='less,cssmin',\n                     output='dist/css/plotly-flask-tutorial.css.css',\n                     extra={'rel': 'stylesheet/less'})\njs_bundle = Bundle('js/*.js',\n                   filters='jsmin',\n                   output='dist/js/main.js')\nassets.register('less_all', less_bundle)\nassets.register('js_all', js_bundle)\nless_bundle.build(force=True)\njs_bundle.build()\n\n\n# Landing Page\n@main_bp.route('/', methods=['GET'])\ndef home():\n    return render_template('index.html',\n                           title='Plotly Flask Tutorial.',\n                           template='home-template',\n                           body=&quot;This is an example homepage, served with Flask.&quot;)\n</code></pre>\n<!--kg-card-end: markdown--><p>All this is doing is serving up <code>index.html</code>.</p><h3 id=\"dash_view-py\">dash_view.py</h3><p><code>dash_view.py</code> is the Dash app we have living <em>within</em> our Flask app. But how does Flask know which url to serve our application at? Wasn't it missing from <code>routes.py</code>? Indeed it was, good fellow! Because we set <strong>routes_pathname_prefix</strong>, we <em>don't need</em> to create a route for <code>dash_view.py</code>: it will always be served whenever we navigate to <code>127.0.01/dash_view</code>. Thus, we can create a navigation template like this:</p><!--kg-card-begin: markdown--><pre><code class=\"language-html\">&lt;nav&gt;\n  &lt;a href=&quot;/&quot;&gt;&lt;i class=&quot;fas fa-home&quot;&gt;&lt;/i&gt; Home&lt;/a&gt;\n  &lt;a href=&quot;/dash_view/&quot;&gt;&lt;i class=&quot;fas fa-chart-line&quot;&gt;&lt;/i&gt; Embdedded Plotly Dash&lt;/a&gt;\n&lt;/nav&gt;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"creating-something-useful\">Creating Something Useful</h2><p>Here's a fun little thing I was able to do with Dash, while in the context of running under a Flask app. In our file <code>dash_view.py</code>, I have the app look at a folder of extracted datasets (called<em> /data</em>). For each dataset, I use Pandas to generate a preview, and Dash's \"data table\" component to render said previews in our Dash app. This lets us quickly cruise through the data an app depends on with a cool interface:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/dataframes.gif\" class=\"kg-image\"><figcaption>A bit rough around the edges, but you get the point.</figcaption></figure><!--kg-card-end: image--><p>If you're hungry for some source code to get started building your own Plotly Dash views, here's the source I used to create the page above:</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import glob\nfrom pathlib import Path, PurePath\nfrom dash import Dash\nimport dash_table\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\n\np = Path('.')\n\n\ndef Add_Dash(server):\n    &quot;&quot;&quot;Plot.ly Dash view which populates the screen with loaded DataFrames.&quot;&quot;&quot;\n    external_stylesheets = ['https://hackers.nyc3.cdn.digitaloceanspaces.com/css/plotly-flask-tutorial.css',\n                            'https://fonts.googleapis.com/css?family=Lato',\n                   'https://use.fontawesome.com/releases/v5.8.1/css/all.css']\n    dash_app = Dash(server=server,\n                    external_stylesheets=external_stylesheets,\n                    routes_pathname_prefix='/dash_view/')\n\n    # Override the underlying HTML template\n    dash_app.index_string = '''&lt;!DOCTYPE html&gt;\n        &lt;html&gt;\n            &lt;head&gt;\n                {%metas%}\n                &lt;title&gt;{%title%}&lt;/title&gt;\n                {%favicon%}\n                {%css%}\n            &lt;/head&gt;\n            &lt;body&gt;\n                &lt;nav&gt;\n                  &lt;a href=&quot;/&quot;&gt;&lt;i class=&quot;fas fa-home&quot;&gt;&lt;/i&gt; Home&lt;/a&gt;\n                  &lt;a href=&quot;/dash_view/&quot;&gt;&lt;i class=&quot;fas fa-chart-line&quot;&gt;&lt;/i&gt; Embdedded Plotly Dash&lt;/a&gt;\n                &lt;/nav&gt;\n                {%app_entry%}\n                &lt;footer&gt;\n                    {%config%}\n                    {%scripts%}\n                    {%renderer%}\n                &lt;/footer&gt;\n            &lt;/body&gt;\n        &lt;/html&gt;'''\n\n    # Create Dash Layout comprised of Data Tables\n    dash_app.layout = html.Div(\n        children=get_datasets(),\n        id='flex-container'\n      )\n\n    return dash_app.server\n\n\ndef get_datasets():\n    &quot;&quot;&quot;Returns previews of all CSVs saved in /data directory.&quot;&quot;&quot;\n    data_filepath = list(p.glob('plotly_flask_tutorial/data/*.csv'))\n    arr = ['This is an example Plot.ly Dash App.']\n    for index, csv in enumerate(data_filepath):\n        print(PurePath(csv))\n        df = pd.read_csv(data_filepath[index]).head(10)\n        table_preview = dash_table.DataTable(\n            id='table_' + str(index),\n            columns=[{&quot;name&quot;: i, &quot;id&quot;: i} for i in df.columns],\n            data=df.to_dict(&quot;rows&quot;),\n            sorting=True,\n        )\n        arr.append(table_preview)\n    return arr\n</code></pre>\n<!--kg-card-end: markdown--><p>I've gone ahead and uploaded the source code for this working example up <a href=\"https://github.com/toddbirchard/plotlydash-flask-tutorial\">on Github</a>. Please steal it: it's all yours.</p><p>Needless to say, there's way more cool shit we can accomplish with Plotly Dash. Stick around long enough, and chances are we'll cover all of them.</p>","url":"https://hackersandslackers.com/gaining-full-control-over-plotly-dash/","uuid":"535768b9-34b6-4a80-b5fa-b69b50cf3a68","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c1af93bffe54a660c58b85a"}},{"node":{"id":"Ghost__Post__5c192cdba632c8240cad3869","title":"Globally Accessible Variables in Flask: Demystifying the 'Application Context'","slug":"demystifying-flasks-application-context","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/04/flask_factory-2.jpg","excerpt":"Breaking down the nuances of the ‘app context’ in Flask's Application Factory.","custom_excerpt":"Breaking down the nuances of the ‘app context’ in Flask's Application Factory.","created_at_pretty":"18 December, 2018","published_at_pretty":"19 December, 2018","updated_at_pretty":"10 April, 2019","created_at":"2018-12-18T12:22:35.000-05:00","published_at":"2018-12-19T08:00:00.000-05:00","updated_at":"2019-04-09T23:49:27.000-04:00","meta_title":"Demystifying Flask's Application Context | Hackers and Slackers","meta_description":"A guide breaking down the cryptic nuances of Flask's 'Application Context.' Putting an end to \"RuntimeError: working outside of application context\".","og_description":"A guide breaking down the cryptic nuances of Flask's 'Application Context.' Putting an end to \"RuntimeError: working outside of application context\".","og_image":"https://hackersandslackers.com/content/images/2019/04/flask_factory-2-2.jpg","og_title":"Demystifying Flask's Application Context","twitter_description":"A guide breaking down the cryptic nuances of Flask's 'Application Context.' Putting an end to \"RuntimeError: working outside of application context\".","twitter_image":"https://hackersandslackers.com/content/images/2019/04/flask_factory-2-1.jpg","twitter_title":"Demystifying Flask's Application Context","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Building Flask Apps","slug":"building-flask-apps","description":"Python’s fast-growing and flexible microframework. Can handle apps as simple as API endpoints, to monoliths remininiscent of Django.","feature_image":"https://hackersandslackers.com/content/images/2019/03/flask-gettingstarted.jpg","meta_description":"Python’s fastest growing, most flexible, and perhaps most Pythonic framework.","meta_title":"Building Flask Apps","visibility":"internal"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"A 'skill' that's always fascinated me is just how long some engineers can make\nit in their career while carrying glaringly obvious gaps in their knowledge of\nthe systems they use every day. To my surprise, I've turned corners where I\nmyself have been that engineer all along, and there's perhaps no better example\nof this then the time I've spent with Flask.\n\nWARNING! Highly opinionated statement incoming: Flask is everything a framework\nshould be. That is to say, it isn't really  a framework a fully-fledged\nframework at all. Sure, the term microframework might seem like a cute PR term,\nbut that doesn't negate the fact that there's something about Flask that's\ndifferent. When I write apps in Flask,  I feel as though I'm writing apps in \nPython.  On the other hand, when I write apps in Django,  I feel like I'm just\nwriting apps in Django.  A disciplined programmer might feel that overly\nstructured frameworks damper creativity and they're probably right: these are\nthe backbones of businesses, thus it makes sense to keep people from deviating\nfrom the norm. \n\nThe upside of Flask is also its downside: there's nearly an infinite number of\nways to solve a single problem. Every Stackoverflow regular has their own\npreference, and sometimes, just none of them seem... right. The problem is\ncompounded by some of the phrasing coming from Flask's documentation itself.\nFlask touts the importance of structuring apps with factories and Blueprints,\nwhile simultaneously expressing the power behind the application context.  What\nyou'll notice over time is that in Flask's own examples, these two 'very\nimportant things' never both appear at the same time: that's because they're\nsimply incompatible with one another.  This is a highly understated\ncontradiction of philosophies.\n\nCommunication Breakdown?\nHere's Flask's take on Application factories\n[http://flask.pocoo.org/docs/1.0/patterns/appfactories/]:\n\n> If you are already using packages and blueprints for your application (Modular\nApplications with Blueprints) there are a couple of really nice ways to further\nimprove the experience. A common pattern is creating the application object when\nthe blueprint is imported.\n\n\nAnd here's their description of the Application context\n[http://flask.pocoo.org/docs/1.0/appcontext/]:\n\n> The application context keeps track of the application-level data during a\nrequest, CLI command, or other activity. Rather than passing the application\naround to each function, the current_app and g proxies are accessed instead.\n\n\nConsidering g  is intended to stand for \"global\" it seems safe from the previous\nstatements that setting attributes of g  can be accessed globally within an\napplication... but they can't. This is where we backpedal and get into messy\nterritory:\n\n> However, importing the app instance within the modules in your project is prone\nto circular import issues. When using the app factory pattern or writing\nreusable blueprints or extensions there won’t be an app instance to import at\nall.\n\nFlask solves this issue with the application context. Rather than referring to\nan app directly, you use the the current_app  proxy, which points to the\napplication handling the current activity.\n\n\nOkay, fine. So if I instantiate an application factory with app.app_context(): \n(which is the only sensible way to create a factory at all)  I should be able to\nregister blueprints within that context, and reference the app context, correct?\n\nI could be crazy but this never seems to work  within blueprints. Whether they\nexist as peer modules or submodules, the words 'from application import\ncurrent_app as app' always seems to result in the same \"missing application\ncontext\" error. Conveniently it seems, all working examples of the application\ncontext seem to be when the Flask developers opt to serve single-file app\nexamples. This stranger from Stackoverflow\n[https://stackoverflow.com/questions/50233118/access-to-flask-global-variables-in-blueprint-apps] \n clears things up a bit:\n\n> This happens because the data are lost when the context (with app.app_context())\nends (doc).\nInside the context, everything is ok :\n\nfrom flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n   print(g.my_db)\n\nthis prints 'database ok'\n\n\nBut outside, you cannot access the attribute:\n\nfrom flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n\nprint(g.my_db)\n\n\nthis throws RuntimeError: Working outside of application context\n\neven if you create a new context:\n\nfrom flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n\nwith app.app_context():\n   print(g.my_db)\n\n\nthis throws AttributeError: '_AppCtxGlobals' object has no attribute 'my_db'\n\n\nAlas, here I am. Doomed writing posts to fill in the blanks of documentation\nleft behind by others. \n\nFlask Sessions: The REAL Slim Shady\nFlask-Session  is the MVP when it comes sharing temporary information across\nmodularized parts of our program. In fact, it's a bit odd this isn't encouraged\nmore-so than g. But whatever. We're here to heal.\n\nSessions  can handled in a number of different ways besides cookies. Take a look\nat the choices we have for storing session-based values in an instance of Flask:\n\nSESSION_TYPE\n Specifies which type of session interface to\nuse. Built-in session types:\n\n * null: NullSessionInterface (default)\n * redis: RedisSessionInterface\n * memcached: MemcachedSessionInterface\n * filesystem: FileSystemSessionInterface\n * mongodb: MongoDBSessionInterface\n * sqlalchemy: SqlAlchemySessionInterface\n\nSESSION_PERMANENT\n Whether use permanent session or not, default\nto be True\n SESSION_USE_SIGNER\n Whether sign the session cookie sid or not,\nif set to True, you have to set\n flask.Flask.secret_key\n[http://flask.pocoo.org/docs/api/#flask.Flask.secret_key], default to be\n False\n SESSION_KEY_PREFIX\n A prefix that is added before all session keys.\nThis makes it possible to use the same backend\nstorage server for different apps, default\n“session:”\n SESSION_REDIS\n A redis.Redis  instance, default connect to\n 127.0.0.1:6379\n SESSION_MEMCACHED\n A memcache.Client  instance, default connect\nto 127.0.0.1:11211\n SESSION_FILE_DIR\n The directory where session files are stored.\nDefault to use flask_session  directory under\ncurrent working directory.\n SESSION_FILE_THRESHOLD\n The maximum number of items the session stores\nbefore it starts deleting some, default 500\n SESSION_FILE_MODE\n The file mode wanted for the session files,\ndefault 0600\n SESSION_MONGODB\n A pymongo.MongoClient  instance, default\nconnect to 127.0.0.1:27017\n SESSION_MONGODB_DB\n The MongoDB database you want to use, default\n“flask_session”\n SESSION_MONGODB_COLLECT\n The MongoDB collection you want to use, default\n“sessions”\n SESSION_SQLALCHEMY\n A flask.ext.sqlalchemy.SQLAlchemy  instance\nwhose database connection URI is configured\nusing the SQLALCHEMY_DATABASE_URI  parameter\n SESSION_SQLALCHEMY_TABLE\n The name of the SQL table you want to use,\ndefault “sessions”\n Using Redis For Cached Session Information\nFor the sake of trying something different, I've opted to pick up a tiny Redis\ninstance from redislabs [https://redislabs.com/]. I can't help myself but\nwasting money on new services to play with; after all, check out how cool this\nlittle red box looks:\n\nRedis Enterprise: A Unique Primary Database\nPerfomance at Scale\n * 50M ops/sec,\n    Symmetric shared–nothing architecture ensures no performance overheads while\n   scaling, auto-sharding and re-balancing\n    Enhanced connection management, pipeline execution and request scheduling\n    \n\nBuilt-in high performance search\n * High performance, real-time indexing with items available for search within\n   1ms\n * Predictable high performance querying while maintaining concurrent loads of\n   indexing and querying\n * Highly scalable across multiple nodes to billions of items per second \n\nFailsafe high availability\n * Cross-rack/zone/datacenter/geo replication\n * Instant auto-failover in single digit second\n * Zero impact on throughput and latency during cluster operations such as\n   scaling, upgrades, re-sharding and rebalancing\n * Out-of-the box support for backup, restore and DR\n\nActive-active geo distribution\n * Reads/Writes in multiple geo regions to the same dataset\n * Local latencies, global availability\n * Built-in conflict resolution for simple and complex data types\n * Based on revolutionary CRDT academic research\n\nBuilt-in persistence\n * Enhanced storage engine for parallel access to any persistent storage\n * Multiple options for enhanced data persistence\n * Reliable persistence configurations on both master and slave shards with zero\n   performance impact\n\nMulti-model\n * Graph, JSON, Machine Learning and Bloom filter modules set industry standards\n   for high performance\n * Multi-shard coordination\n * Extensibility with custom modules\n\nIntelligent tiered access to memory\n * Up to 80% lower infrastructure costs by running Redis on Flash\n * Automatic management of data tiering between RAM & Flash with no code changes\n * Supports all new persistent memory technologies\n\nFlexible deployment options\n * Hybrid clusters can span on-prem infrastructure and multiple clouds\n * Most efficient use of resources with maximized core usage, multi-tenancy,\n   re-sharding and re-balancing to avoid noisy neighbors in every environment\n\nPerfomance at Scale\n Built-in persistence\n Failsafe high availability\n Active-active geo distribution\n Built-in high performance search\n Multi-model\n Intelligent tiered access to memory\n \n(RAM and Flash)\n Flexible deployment options\n \n(cloud, on-prem, hybrid)\n Fast\nPerformance at scale\n Built-in high performance search\n Reliable\nBuilt-in persistence\n Failsafe high availability\n Active-active geo distribution\n Flexible\nMulti-model\n Flexible deployment options (cloud, hybrid, on-prem)\n Intelligent tiered access to memory (ram and flash)\n (Why am I not getting paid for this? Why did I take the time to even make that\nmodule?)Redis  is NoSQL datastore written in C intended to temporarily hold data in\nmemory for users as they blaze mindlessly through your site. Other use cases\ninclude serving as the foundation for real-time chat apps via the\npublish/subscribe messaging paradigm; popular amongst stupid chat or dating apps\nslowly destroying our abilities as human beings to interact face-to-face.\nPowerful stuff.\n\nStructuring init.py Correctly\nConsider this to be the guide to Flask Application factories I wish I had months\nago. A healthy application factory should:\n\n * Derive all app configuration values from a class or environment variables.\n * Allow Database interactions to occur at any point within the app.\n * Pass values globally outside of the application context.\n\nThis does all of those things:\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_session import Session\nfrom flask_redis import FlaskRedis\n\n# Globally accessible libraries\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    \"\"\"Initialize the core application.\"\"\"\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Set global session variables\n        r.set('endpoint', str(app.config['ENDPOINT']).encode('utf-8'))\n        r.set('post_query', str(app.config['POST_QUERY']).encode('utf-8'))\n        \n        # Initialize Global Libraries\n        redis_store.init_app(app)\n        db.init_app(app)\n\n        # Include our Routes\n        from . import routes\n\n        return app\n\n\nThe order of operations here is critical.\n\nBefore we do anything related to the app itself, we create instances of \nflask_sqlalchemy  and flask_redis. This will be initialized with our app once we\nactually have one created.\n\nThe first two lines of create_app()  should be no surprise: we're just creating\nour Flask app, and stating that it should be configured using a class called \nConfig  in a file named config.py.\n\napp = Flask(__name__, instance_relative_config=False)\napp.config.from_object('config.Config')\n\n\nMoving down the function comes the moment of truth: creating the app context. \nWhat happens in the app context stays in the app context... except for our sick\nnew Redis setup. By using the Redis .set()  method, we can assign key/value\npairs  for Redis hang on to, such as values from our app config which might be\nneeded elsewhere in our app: r.set('endpoint',\nstr(app.config['ENDPOINT']).encode('utf-8')).\n\nRedis stores information as bytes by default, thus attempting to pass values\nsuch as strings will result in the infamous `b'leading letter b'` phenomenon. Be\nsure to encode your values as utf-8 when using set(), and decode when using\nget().Making Redis Globally Available\nThe next part is important: we need to 'initialize' the services we want to use\nglobally (such as database access or Redis) by using init_app(). This must \nhappen inside the application context, with the parameter being app. This is our\nway of achieving singularity into inter-dimensional travel, thus breaking out of\nthe dreaded application context long after it dies.\n\nLet's Access Some Variables, Baby\nThe moment of truth: will this actually work? Or am I actually a filthy liar\nflooding the internet with more uselessly outdated Flask advice? Let's see:\n\n# routes.py\n\nfrom flask import current_app as app\nfrom flask import make_response\nimport json\nfrom . import models\nfrom . import r\n\nheaders = { 'Access-Control-Allow-Headers': 'Content-Type' }\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef entry():\n    readers = models.Readers.query.filter_by(username='john').all()\n    print(readers)\n    print(r.get('uri').decode('utf-8'))\n    return make_response(str('readers'), 200, headers)\n\n\nEureka! This worthless entry-point prints two things: the value we assigned to\nour Redis block, and all records in our database of people named John:\n\n>> [<User john>]\n>> https://us1-hackersandslackers-543.cloudfunctions.net/link-endpoint?url=\n\n\nAs simple and stupid as it seems, developing an app to this point while\nunderstanding why it works  is a victory for any developer. I complain about\nthis nearly every post, but the fact of the matter is that the heroes who build\nmuch of today's technologies commonly fail to explain their own art in\nunderstandable terms. It's an understandable phenomenon resulting from isolated\nspurts of genius, perhaps, but it damages the growth of companies and humanity\nalike.\n\nSo I guess this is my calling: writing documentation for other people's\naccomplishments. \"Marginally less confusing than 4 open Stackoverflow tabs.\" \nThat's what I hope to have engraved on my gravestone.\n\nMerry Christmas.","html":"<p>A 'skill' that's always fascinated me is just how long some engineers can make it in their career while carrying glaringly obvious gaps in their knowledge of the systems they use every day. To my surprise, I've turned corners where I myself have been that engineer all along, and there's perhaps no better example of this then the time I've spent with Flask.</p><p><strong>WARNING! Highly opinionated statement incoming</strong>: Flask is everything a framework should be. That is to say, it <em>isn't really</em> a framework a fully-fledged framework at all. Sure, the term <em>microframework </em>might seem like a cute PR term, but that doesn't negate the fact that there's something about Flask that's different. When I write apps in <strong>Flask,</strong> I feel as though I'm writing apps in <strong>Python.</strong> On the other hand, when I write apps in <strong>Django,</strong> I feel like I'm just writing apps in <strong>Django.</strong> A disciplined programmer might feel that overly structured frameworks damper creativity and they're probably right: these are the backbones of businesses, thus it makes sense to keep people from deviating from the norm. </p><p>The upside of Flask is also its downside: there's nearly an infinite number of ways to solve a single problem. Every Stackoverflow regular has their own preference, and sometimes, just none of them seem... <em>right. </em>The problem is compounded by some of the phrasing coming from Flask's documentation itself. Flask touts the importance of structuring apps with factories and Blueprints, while simultaneously expressing the power behind the <strong><em>application context.</em></strong> What you'll notice over time is that in Flask's own examples, these two 'very important things' never both appear at the same time: that's because they're simply <em>incompatible with one another.</em> This is a highly understated contradiction of philosophies.</p><h2 id=\"communication-breakdown\">Communication Breakdown?</h2><p>Here's <a href=\"http://flask.pocoo.org/docs/1.0/patterns/appfactories/\">Flask's take on Application factories</a>:</p><blockquote>\n<p>If you are already using packages and blueprints for your application (Modular Applications with Blueprints) there are a couple of really nice ways to further improve the experience. A common pattern is creating the application object when the blueprint is imported.</p>\n</blockquote>\n<p>And here's their <a href=\"http://flask.pocoo.org/docs/1.0/appcontext/\">description of the Application context</a>:</p><blockquote>\n<p>The application context keeps track of the application-level data during a request, CLI command, or other activity. Rather than passing the application around to each function, the current_app and g proxies are accessed instead.</p>\n</blockquote>\n<p>Considering <code>g</code> is intended to stand for \"global\" it seems safe from the previous statements that setting attributes of <code>g</code> can be accessed globally within an application... but they can't. This is where we backpedal and get into messy territory:</p><blockquote>\n<p>However, importing the app instance within the modules in your project is prone to circular import issues. When using the app factory pattern or writing reusable blueprints or extensions there won’t be an app instance to import at all.</p>\n<p>Flask solves this issue with the application context. Rather than referring to an app directly, you use the the <strong>current_app</strong> proxy, which points to the application handling the current activity.</p>\n</blockquote>\n<p>Okay, fine. So if I instantiate an application factory with <code>app.app_context():</code> (which is the only sensible way to create a factory at all)  I should be able to register blueprints within that context, and reference the app context, correct?</p><p>I could be crazy but this <em>never seems to work</em> within blueprints. Whether they exist as peer modules or submodules, the words 'from application import current_app as app' always seems to result in the same \"missing application context\" error. Conveniently it seems, all working examples of the application context seem to be when the Flask developers opt to serve single-file app examples. This <a href=\"https://stackoverflow.com/questions/50233118/access-to-flask-global-variables-in-blueprint-apps\">stranger from Stackoverflow</a> clears things up a bit:</p><blockquote>\n<p>This happens because the data are lost when the context (with app.app_context()) ends (doc).<br>\nInside the context, everything is ok :</p>\n<pre><code class=\"language-python\">from flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n   print(g.my_db)\n\nthis prints 'database ok'\n</code></pre>\n<p>But outside, you cannot access the attribute:</p>\n<pre><code class=\"language-python\">from flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n\nprint(g.my_db)\n</code></pre>\n<p>this throws RuntimeError: Working outside of application context</p>\n<p>even if you create a new context:</p>\n<pre><code class=\"language-python\">from flask import Flask, g\napp = Flask(__name__)\nwith app.app_context():\n   g.my_db = 'database ok'\n\nwith app.app_context():\n   print(g.my_db)\n</code></pre>\n<p>this throws AttributeError: '_AppCtxGlobals' object has no attribute 'my_db'</p>\n</blockquote>\n<p>Alas, here I am. Doomed writing posts to fill in the blanks of documentation left behind by others. </p><h2 id=\"flask-sessions-the-real-slim-shady\">Flask Sessions: The REAL Slim Shady</h2><p><code>Flask-Session</code> is the MVP when it comes sharing temporary information across modularized parts of our program. In fact, it's a bit odd this isn't encouraged more-so than <code>g</code>. But whatever. We're here to heal.</p><p><strong>Sessions</strong> can handled in a number of different ways besides cookies. Take a look at the choices we have for storing session-based values in an instance of Flask:</p><style>\n    tr td:first-child{\n    text-align: left;\n    text-align: top;\n    }\n    \n    tr td:first-child {\n    text-align: left;\n    text-align: top;\n    font-weight: 500;\n    background: #646c82 !important;\n    color: white;\n    border-bottom: 1px solid #747d92;\n    max-width: 70px;\n}\n    \n    table td {\n        font-size:.9em;\n    }\n    \n    td {\n       text-align: left;\n        font-size:.9em;\n        \n    }\n   \n    \n    tr td:nth-of-type(2){\n        font-weight: 100;\n            padding: 20px;\n    }\n    @media (max-width: 800px) {\n        \n        tr td {\n    \t\tpadding: 10px 0;\n        }\n        \n        tbody {\n            margin-left: 0 !important;\n        }\n        \n      tr td:first-child {\n       width: 100%;\n       white-space: nowrap;\n    padding: 10px 0 !important;\n    text-overflow: ellipsis;\n          max-width: none;\n    }\n        \n        tr:first-child td{\n       \t    min-width: 300px;\n            max-width: -webkit-fill-available !important;\n        }\n        \n        th {\n            \n        }\n        \n        tr {\n            padding: 0px !important;\n            overflow-x: hidden;\n        }\n        \n        td {\n            line-height:1.5;\n        }\n        \n        td:nth-of-type(2) {\n            width: 100%;\n            padding: 20px !important;\n        }\n        \n        tr td:nth-of-type(2){\n        font-weight: 100;\n        padding: 15px !important;\n    \t}\n    }\n    \n        \n</style>\n\n<div class=\"tableContainer\">\n  <table>\n  <tbody valign=\"top\">\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_TYPE</span></td>\n      <td>\n        <p class=\"first\">Specifies which type of session interface to\n          use. Built-in session types:</p>\n        <ul class=\"last simple\">\n          <li><strong>null</strong>: NullSessionInterface (default)</li>\n          <li><strong>redis</strong>: RedisSessionInterface</li>\n          <li><strong>memcached</strong>: MemcachedSessionInterface</li>\n          <li><strong>filesystem</strong>: FileSystemSessionInterface</li>\n          <li><strong>mongodb</strong>: MongoDBSessionInterface</li>\n          <li><strong>sqlalchemy</strong>: SqlAlchemySessionInterface</li>\n        </ul>\n      </td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_PERMANENT</span></td>\n      <td>Whether use permanent session or not, default\n        to be <span class=\"pre\">True</span></td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_USE_SIGNER</span></td>\n      <td>Whether sign the session cookie sid or not,\n        if set to <span class=\"pre\">True</span>, you have to set\n        <a class=\"reference external\" href=\"http://flask.pocoo.org/docs/api/#flask.Flask.secret_key\" title=\"(in Flask v0.12-dev)\"><tt class=\"xref py py-attr docutils literal\"><span class=\"pre\">flask.Flask.secret_key</span></tt></a>, default to be\n        <span class=\"pre\">False</span></td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_KEY_PREFIX</span></td>\n      <td>A prefix that is added before all session keys.\n        This makes it possible to use the same backend\n        storage server for different apps, default\n        “session:”</td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_REDIS</span></td>\n      <td>A <span class=\"pre\">redis.Redis</span> instance, default connect to\n        <span class=\"pre\">127.0.0.1:6379</span></td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_MEMCACHED</span></td>\n      <td>A <span class=\"pre\">memcache.Client</span> instance, default connect\n        to <span class=\"pre\">127.0.0.1:11211</span></td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_FILE_DIR</span></td>\n      <td>The directory where session files are stored.\n        Default to use <cite>flask_session</cite> directory under\n        current working directory.</td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_FILE_THRESHOLD</span></td>\n      <td>The maximum number of items the session stores\n        before it starts deleting some, default 500</td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_FILE_MODE</span></td>\n      <td>The file mode wanted for the session files,\n        default 0600</td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_MONGODB</span></td>\n      <td>A <span class=\"pre\">pymongo.MongoClient</span> instance, default\n        connect to <span class=\"pre\">127.0.0.1:27017</span></td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_MONGODB_DB</span></td>\n      <td>The MongoDB database you want to use, default\n        “flask_session”</td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_MONGODB_COLLECT</span></td>\n      <td>The MongoDB collection you want to use, default\n        “sessions”</td>\n    </tr>\n    <tr class=\"row-odd\">\n      <td><span class=\"pre\">SESSION_SQLALCHEMY</span></td>\n      <td>A <span class=\"pre\">flask.ext.sqlalchemy.SQLAlchemy</span> instance\n        whose database connection URI is configured\n        using the <span class=\"pre\">SQLALCHEMY_DATABASE_URI</span> parameter</td>\n    </tr>\n    <tr class=\"row-even\">\n      <td><span class=\"pre\">SESSION_SQLALCHEMY_TABLE</span></td>\n      <td>The name of the SQL table you want to use,\n        default “sessions”</td>\n    </tr>\n  </tbody>\n    </table>\n</div>\n<h3 id=\"using-redis-for-cached-session-information\">Using Redis For Cached Session Information</h3><p>For the sake of trying something different, I've opted to pick up a tiny Redis instance from <a href=\"https://redislabs.com/\"><strong>redislabs</strong></a>. I can't help myself but wasting money on new services to play with; after all, check out how cool this little red box looks:</p>\n<!-- Strengths -->\n<div id=\"unique\">\n  <!-- Headline -->\n  <h2>Redis Enterprise: A Unique Primary Database</h2>\n  <div class=\"item-scale item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-performance-reverse.svg\">\n    <h3>Perfomance at Scale</h3>\n    <ul>\n      <li>50M ops/sec,\n        <1ms 26=\"\" latency,=\"\" with=\"\" only=\"\" cloud=\"\" instances<=\"\" li=\"\"> <li>Symmetric shared–nothing architecture ensures no performance overheads while scaling, auto-sharding and re-balancing</li>\n      <li>Enhanced connection management, pipeline execution and request scheduling</li>\n    </1ms></li></ul>\n  </div>\n  <div class=\"item-search item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-search-reverse.svg\">\n    <h3>Built-in high performance search</h3>\n    <ul>\n      <li>High performance, real-time indexing with items available for search within 1ms</li>\n      <li>Predictable high performance querying while maintaining concurrent loads of indexing and querying</li>\n      <li>Highly scalable across multiple nodes to billions of items per second </li>\n    </ul>\n  </div>\n  <div class=\"item-fail item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-fail-reverse.svg\" class=\"popup-image\">\n    <h3>Failsafe high availability</h3>\n    <ul>\n      <li>Cross-rack/zone/datacenter/geo replication</li>\n      <li>Instant auto-failover in single digit second</li>\n      <li>Zero impact on throughput and latency during cluster operations such as scaling, upgrades, re-sharding and rebalancing</li>\n      <li>Out-of-the box support for backup, restore and DR</li>\n    </ul>\n  </div>\n  <div class=\"item-geo item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-geo-reverse.svg\" class=\"popup-image\">\n    <h3>Active-active geo distribution</h3>\n    <ul>\n      <li>Reads/Writes in multiple geo regions to the same dataset</li>\n      <li>Local latencies, global availability</li>\n      <li>Built-in conflict resolution for simple and complex data types</li>\n      <li>Based on revolutionary CRDT academic research</li>\n    </ul>\n  </div>\n  <div class=\"item-persist item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-persist-reverse.svg\" class=\"popup-image\">\n    <h3>Built-in persistence</h3>\n    <ul>\n      <li>Enhanced storage engine for parallel access to any persistent storage</li>\n      <li>Multiple options for enhanced data persistence</li>\n      <li>Reliable persistence configurations on both master and slave shards with zero performance impact</li>\n    </ul>\n  </div>\n  <div class=\"item-multi item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-multi-reverse.svg\" class=\"popup-image\">\n    <h3>Multi-model</h3>\n    <ul>\n      <li>Graph, JSON, Machine Learning and Bloom filter modules set industry standards for high performance</li>\n      <li>Multi-shard coordination</li>\n      <li>Extensibility with custom modules</li>\n    </ul>\n  </div>\n  <div class=\"item-tiered item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-tiered-reverse.svg\" class=\"popup-image\">\n    <h3>Intelligent tiered access to memory</h3>\n    <ul>\n      <li>Up to 80% lower infrastructure costs by running Redis on Flash</li>\n      <li>Automatic management of data tiering between RAM &amp; Flash with no code changes</li>\n      <li>Supports all new persistent memory technologies</li>\n    </ul>\n  </div>\n  <div class=\"item-deploy item hidden\">\n    <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-deploy-reverse.svg\" class=\"popup-image\">\n    <h3>Flexible deployment options</h3>\n    <ul>\n      <li>Hybrid clusters can span on-prem infrastructure and multiple clouds</li>\n      <li>Most efficient use of resources with maximized core usage, multi-tenancy, re-sharding and re-balancing to avoid noisy neighbors in every environment</li>\n    </ul>\n  </div>\n  <!-- Categories -->\n  <!-- Desktop Grid -->\n  <div class=\"strengths\">\n    <!-- Grid - Row -->\n\n    <div class=\"main-flex\">\n      <div class=\"columns medium-4 left parent\">\n        <div class=\"columns feature item-fast scale\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Perfomance at Scale\n          </div>\n        </div>\n\n        <div class=\"columns feature item-durable persistence\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Built-in persistence\n          </div>\n        </div>\n\n        <div class=\"columns feature item-durable failsafe\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Failsafe high availability\n          </div>\n        </div>\n\n        <div class=\"columns feature item-durable geo\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Active-active geo distribution\n          </div>\n        </div>\n\n      </div>\n      <div class=\"columns medium-4 center parent\">\n        <div class=\"redis red-strengths\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/redis-e-logo.svg\" alt=\"Redis Labs\">\n        </div>\n      </div>\n      <div class=\"columns medium-4 right parent\">\n        <div class=\"columns feature right item-fast search\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Built-in high performance search\n          </div>\n        </div>\n\n        <div class=\"columns feature item-flex multi\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Multi-model\n          </div>\n        </div>\n\n        <div class=\"columns feature item-flex vert tiered\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Intelligent tiered access to memory\n            <br>(RAM and Flash)\n          </div>\n        </div>\n\n        <div class=\"columns feature item-flex vert deploy\">\n          <div class=\"img\">\n          </div>\n          <div class=\"text\">\n            Flexible deployment options\n            <br>(cloud, on-prem, hybrid)\n          </div>\n        </div>\n\n\n      </div>\n\n    </div>\n\n    <!-- Grid - Redis Logo-->\n\n  </div>\n  <!-- End Desktop Grid -->\n\n\n\n  <div class=\"grid-container mobile-grid\">\n    <div class=\"redis-mobile red-strengths\">\n      <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/redis-e-logo.svg\" alt=\"Redis Labs\">\n    </div>\n    <!-- Column 1 -->\n    <div class=\"mobile-flex\">\n      <div class=\"columns small-12 medium-4\">\n        <h3>Fast</h3>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-performance.svg\">\n          <span class=\"text short\">\n            Performance at scale\n          </span>\n        </div>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-search.svg\">\n          <span class=\"text med\">\n            Built-in high performance search\n          </span>\n        </div>\n      </div>\n      <!-- End Column 1 -->\n      <!-- Column 2 -->\n      <div class=\"columns small-12 medium-4\">\n        <h3>Reliable</h3>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-persist.svg\">\n          <span class=\"text short\">\n            Built-in persistence\n          </span>\n        </div>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-fail.svg\">\n          <span class=\"text short med\">\n            Failsafe high availability\n          </span>\n        </div>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-geo.svg\">\n          <span class=\"text med\">\n            Active-active geo distribution\n          </span>\n        </div>\n      </div>\n      <!-- End Column 2 -->\n      <!-- Column 3 -->\n      <div class=\"columns small-12 medium-4\">\n        <h3>Flexible</h3>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-multi.svg\">\n          <span class=\"text short\">\n            Multi-model\n          </span>\n        </div>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-deploy.svg\">\n          <span class=\"text long\">\n            Flexible deployment options <span class=\"small\">(cloud, hybrid, on-prem)</span>\n          </span>\n        </div>\n        <div class=\"columns\">\n          <img src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/ico-front-tiered.svg\">\n          <span class=\"text long\">\n            Intelligent tiered access to memory <span class=\"small\">(ram and flash)</span>\n          </span>\n        </div>\n      </div>\n    </div>\n    <!-- End Column 3 -->\n  </div>\n</div>\n<span style=\"color: #969696;\n    text-align: center;\n    display: block;\n    font-weight: 100;\n    font-style: italic;\n    margin-bottom: 30px;\n    font-size: .9em;\">(Why am I not getting paid for this? Why did I take the time to even make that module?)</span><p><strong>Redis</strong> is NoSQL datastore written in C intended to temporarily hold data in memory for users as they blaze mindlessly through your site. Other use cases include serving as the foundation for real-time chat apps via the publish/subscribe messaging paradigm; popular amongst stupid chat or dating apps slowly destroying our abilities as human beings to interact face-to-face. Powerful stuff.</p><h2 id=\"structuring-init-py-correctly\">Structuring <strong>init</strong>.py Correctly</h2><p>Consider this to be the guide to Flask Application factories I wish I had months ago. A healthy application factory should:</p><ul><li>Derive all app configuration values from a class or environment variables.</li><li>Allow Database interactions to occur at any point within the app.</li><li>Pass values globally outside of the application context.</li></ul><p>This does all of those things:</p><pre><code class=\"language-python\">from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_session import Session\nfrom flask_redis import FlaskRedis\n\n# Globally accessible libraries\ndb = SQLAlchemy()\nr = FlaskRedis()\n\n\ndef create_app():\n    &quot;&quot;&quot;Initialize the core application.&quot;&quot;&quot;\n    app = Flask(__name__, instance_relative_config=False)\n    app.config.from_object('config.Config')\n\n    with app.app_context():\n        # Set global session variables\n        r.set('endpoint', str(app.config['ENDPOINT']).encode('utf-8'))\n        r.set('post_query', str(app.config['POST_QUERY']).encode('utf-8'))\n        \n        # Initialize Global Libraries\n        redis_store.init_app(app)\n        db.init_app(app)\n\n        # Include our Routes\n        from . import routes\n\n        return app\n</code></pre>\n<p>The order of operations here is critical.</p><p>Before we do anything related to the app itself, we create instances of <code>flask_sqlalchemy</code> and <code>flask_redis</code>. This will be initialized with our app once we actually have one created.</p><p>The first two lines of <code>create_app()</code> should be no surprise: we're just creating our Flask app, and stating that it should be configured using a class called <strong>Config</strong> in a file named <strong>config.py.</strong></p><pre><code class=\"language-python\">app = Flask(__name__, instance_relative_config=False)\napp.config.from_object('config.Config')\n</code></pre>\n<p>Moving down the function comes the moment of truth: <strong>creating the app context.</strong> What happens in the app context stays in the app context... except for our sick new Redis setup. By using the Redis <code>.set()</code> method, we can assign <em>key/value pairs</em> for Redis hang on to, such as values from our app config which might be needed elsewhere in our app: <code>r.set('endpoint', str(app.config['ENDPOINT']).encode('utf-8'))</code>.</p><div class=\"protip\">\n    Redis stores information as bytes by default, thus attempting to pass values such as strings will result in the infamous `b'leading letter b'` phenomenon. Be sure to encode your values as utf-8 when using set(), and decode when using get().\n</div><h3 id=\"making-redis-globally-available\">Making Redis Globally Available</h3><p>The next part is important: we need to 'initialize' the services we want to use globally (such as database access or Redis) by using <code>init_app()</code>. This <em>must </em>happen inside the application context, with the parameter being <code>app</code>. This is our way of achieving singularity into inter-dimensional travel, thus breaking out of the dreaded application context long after it dies.</p><h2 id=\"let-s-access-some-variables-baby\">Let's Access Some Variables, Baby</h2><p>The moment of truth: will this actually work? Or am I actually a filthy liar flooding the internet with more uselessly outdated Flask advice? Let's see:</p><pre><code class=\"language-python\"># routes.py\n\nfrom flask import current_app as app\nfrom flask import make_response\nimport json\nfrom . import models\nfrom . import r\n\nheaders = { 'Access-Control-Allow-Headers': 'Content-Type' }\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef entry():\n    readers = models.Readers.query.filter_by(username='john').all()\n    print(readers)\n    print(r.get('uri').decode('utf-8'))\n    return make_response(str('readers'), 200, headers)\n</code></pre>\n<p>Eureka! This worthless entry-point prints two things: the value we assigned to our Redis block, and all records in our database of people named John:</p><pre><code class=\"language-bash\">&gt;&gt; [&lt;User john&gt;]\n&gt;&gt; https://us1-hackersandslackers-543.cloudfunctions.net/link-endpoint?url=\n</code></pre>\n<p>As simple and stupid as it seems, developing an app to this point <em>while understanding why it works</em> is a victory for any developer. I complain about this nearly every post, but the fact of the matter is that the heroes who build much of today's technologies commonly fail to explain their own art in understandable terms. It's an understandable phenomenon resulting from isolated spurts of genius, perhaps, but it damages the growth of companies and humanity alike.</p><p>So I guess this is my calling: writing documentation for other people's accomplishments. <strong>\"Marginally less confusing than 4 open Stackoverflow tabs.\"</strong> That's what I hope to have engraved on my gravestone.</p><p>Merry Christmas.</p>","url":"https://hackersandslackers.com/demystifying-flasks-application-context/","uuid":"ede882df-a696-43ef-a392-9430d98a961e","page":false,"codeinjection_foot":"<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/autoNumeric.min.js\"></script>\n\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/underscore-min.js\"></script>\n\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/mustache.min.js\"></script>\n\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/owl.js\"></script>\n\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/jquery.counterup.min.js\"></script>\n\n\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/foundation.js\"></script>\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/docker.js\"></script>\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/jquery.fancybox.pack.js\"></script>\n<script type=\"text/javascript\" async=\"\" src=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/scripts.js\"></script>","codeinjection_head":"<link rel=\"stylesheet\" href=\"https://hackers.nyc3.cdn.digitaloceanspaces.com/redis/redislast2.css\">","comment_id":"5c192cdba632c8240cad3869"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673756","title":"Geocoding Raw Datasets for Mapbox","slug":"preparing-data-for-mapbox-geocoding","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","excerpt":"Make sense of unstructured data with enough precision to put it on a map.","custom_excerpt":"Make sense of unstructured data with enough precision to put it on a map.","created_at_pretty":"10 December, 2018","published_at_pretty":"18 December, 2018","updated_at_pretty":"02 April, 2019","created_at":"2018-12-10T17:16:29.000-05:00","published_at":"2018-12-18T08:00:00.000-05:00","updated_at":"2019-04-01T20:27:23.000-04:00","meta_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","meta_description":"Make sense of unstructured data with enough precision to put it on a map.","og_description":"Make sense of unstructured data with enough precision to put it on a map.","og_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","og_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","twitter_description":"Make sense of unstructured data with enough precision to put it on a map.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/mapbox2_o-2@2x.jpg","twitter_title":"Geocoding Raw Datasets for Mapbpox | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Analysis","slug":"data-analysis","description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","feature_image":null,"meta_description":"Drawing meaningful conclusions from data. Includes interpretation, dashboard creation, and data manipulation.","meta_title":"Data Analysis | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"This wouldn't be a proper data blog unless we spend a vast majority of our time\ntalking about cleaning data. Chances are if you're pursuing analysis that's\ngroundbreaking (or worthwhile), we're probably starting with some ugly, untapped\ninformation. It turns out Mapbox has an API specifically for this purpose: the \nMapbox Geocoding API [https://www.mapbox.com/api-documentation/#geocoding].\n\nGeocoding  is a blanket term for turning vague information into specific\nLat/Long coordinates. How vague, you ask? The API covers:\n\n * Pinpointing exact location via street address.\n * Locating regions or cities by recognizable name (ie: Rio de Janeiro).\n * Locating cities by highly unspecific name (Geocoding for \"Springfield\" will\n   return results for 41 American cities)\n * Locating cities or venues by name within a given region (such as searching\n   for Ray's Pizza in NYC).\n\nWe can also use Geocoding to do the reverse of this, where passing in\ncoordinates will return location names. If you find this useful, I'm assuming\nyou're a spy.\n\nChipping Away at a Real Use Case\nIn a real-life example, I have two sets of data: one represents general places\nof residence for a particular sample group. The goal is to see how they interact\nwith the second dataset: a list of locations they will be traveling to. I'd love\nto go into more detail, but:\n\nI get one cliche meme per year.So how can we use the Mapbox Geocoding API to\nsystematically extract coordinates for thousands of addresses, from multiple\ndatasets? With Pandas, of course!\n\nI'm Just Happy to be Writing About Pandas Right Now\nPardon my excitement; I've been far overdue for posting anything Pandas-related.\nIt's been killing me on the inside.\n\nWe need to make sense of some vague data. As seen in our Citibike example\n[https://hackersandslackers.com/map-data-visualization-with-mapbox/], New York\nhas plenty of public datasets with information like Taxi pickup/dropoffs, public\ntransit, etc. These start and end points are typically too fluid to have\nLat/Long coordinates associated with them, so we'll add them in ourselves. Given\nthat we're about to pass hundreds or thousands of addresses and locations, we'll\nuse Pandas .apply()  to fill out the missing Lat/Long columns in our dataset. \n\nInstead of using Mapbox's Python SDK, I'll actually be using requests  to hit\nthe Mapbox REST API. For some reason, the Python SDK was a bit unpredictable on\nmy last run.*\n\n*UPDATE: the Python SDK \"wasn't working\" because I apparently don't know the\ndifference between longitude and latitude. Awesome, so I'm a moron.\n\nimport sys\nimport os\nimport pandas as pd\nimport requests\nimport json\n\n\nclass GeocodeAddresses:\n    \"\"\"Add missing lat/long information to exisiting dataset.\"\"\"\n\n    def __init__(self, address_data):\n        self.data = address_data\n        self.address_df = pd.read_csv(self.data)\n        self.complete_data = self.get_coords(self.address_df)\n\n\n    @classmethod\n    def get_coords(self, employee_address_df):\n        \"\"\"Fill Dataframe lat/long columns.\"\"\"\n\n        def fill_coords(row):\n            \"\"\"Create a route object by passing GeoJSON start/end objects.\"\"\"\n            base_url = 'https://api.mapbox.com/geocoding/v5/mapbox.places/'\n            address = str(row.home_address)\n            format = '.json'\n            endpoint = base_url + address + format\n            params = {\n                'access_token':  'pk.eyJ1IjNoYXJkd2VthisisreallolaXdyNHQ3OTUifQ.VTAUrmzD91Ppxr1AJww'\n            }\n            headers = {\n                'Content-Type': 'application/json'\n            }\n            r = requests.get(endpoint, params=params, headers=headers)\n            try:\n                Lat = r.json()['features'][0]['geometry']['coordinates'][0]\n                Long = r.json()['features'][0]['geometry']['coordinates'][1]\n                print(pd.Series([Lat, Long]))\n                return pd.Series([Lat, Long])\n            except IndexError:\n                pass\n\n        address_df[['Lat', 'Long']] = address_df.head(100).apply(fill_coords, axis=1)\n        address_df.to_csv('geocoded.csv')\n\n\nIn the above example, we're using .apply()  against an empty series (our\nLat/Long columns) as opposed to our entire Dataframe. When get_coords()  returns\ntwo values, these values will fill the empty columns on a per-row basis.\n\nFor the scope of this tutorial, we'll simply focus on getting these points\nplotted. Don't worry, this is only part 2 of our Mapbox series! Yes, an entire\nseries!\n\nTurning Your Datasets into Tilesets\nIn Mapbox terms, a Tileset  is essentially a layer of data we can overlay on top\nof our blank map. The map style  we created last time was really just the\naesthetic unpinning of all the interesting data we can pile on time.\n\nTilesets can be stacked on one another, thus giving us infinite possibilities of\nthe types of data we can communicate: especially when you consider that Mapbox\nsupports Heatmaps and topology - far more than just plotted points.\n\nFirst, we'll plot our origins. I've put together a dataset of completely\nfalsified names (with presumably real addresses?) to demonstrate how we'd plot\nthese points. Here's a sample of the garbage I'll be feeding into Mapbox:\n\naddressnamelonglat761 ST ANNS AVE NY NY 10451Royal Hiett40.754466-73.97945525\nCOLUMBUS CIR NY NY 10019Yolanda Antonio40.8201997-73.91103241145 LENOX RD NY NY\n11212Marguerita Autry40.7667595-73.98157042800 VICTORY BLVD NY NY 10314Alyse\nPeranio40.6597804-73.9183181750 LEXINGTON AVE NY NY 10022Sina Walberg40.6080557\n-74.153241829 BAY RIDGE AVE NY NY 11220Ignacia Frasher40.7625148-73.9685564550\nRIVERSIDE DR NY NY 10027Marta Haymond40.6386587-74.034633808 W END AVE NY NY\n10025Angie Tseng40.8159612-73.96031841-03 69 ST NY NY 11377Marcella Weinstock\n40.797233-73.971324550 PARK AVE NY NY 10016Filiberto Everett40.7444514\n-73.8956728739 BROOK AVE NY NY 10451Vernia Mcgregor40.7492656-73.9803386777 W\nEND AVE NY NY 10025Michelina Althoff40.8199675-73.9122757866 E 165 ST NY NY\n10459Dave Tauber40.7965956-73.9726135130 E 37 ST NY NY 10016Tandra Gowen\n40.8237011-73.8990202797 ST ANNS AVE NY NY 10451Toby Philbrick40.7482336\n-73.97856641 AARON LN NY NY 10309Aisha Grief40.82089-73.9109118641 LEXINGTON AVE\nNY NY 10022Tarah Sinkler40.5541368-74.21266534201 4 AVE NY NY 11232Coletta\nJeansonne40.7590297-73.97032191021 PARK AVE NY NY 10028Lorie Shriver40.650317\n-74.0081672127 RIVERSIDE DR NY NY 10024Antwan Fullilove40.7794132-73.95724755120\nBROADWAY NY NY 10034Normand Beerman40.7890613-73.98065697124 20 AVE NY NY 11204\nWes Nieman40.8714856-73.91303623506 BEDFORD AVE NY NY 11210Marlen Hutcherson\n40.6127972-73.9901551550 GRAND ST NY NY 10002Leonie Lablanc40.6168306-73.9501481\n1711 GROVE ST NY NY 11385Doris Herrman40.7143151-73.9800558785 W END AVE NY NY\n10025Cyndy Kossman40.7032053-73.91119426040 HUXLEY AVE NY NY 10471Donya Ponte\n40.796763-73.972483Head Over to Mapbox Studio\nWhile we can technically do everything programmatically, Mapbox's GUI is simply\ntoo easy to ignore. Using Mapbox Studio\n[https://www.mapbox.com/studio/datasets/], we can upload our data and turn it\ninto a tileset; the heart and soul of what makes our maps interesting. \n\nOnce you've uploaded your CSV (or JSON, or whatever) as a dataset, we can\nimmediately see what this information looks like on a map by previewing it as a\ntileset. Mapbox is surprisingly intelligent in that it can deduce lat/long\nvalues from poorly named or formatted columns (such as Lat/Long, \nLatitutde/Longitude, start_longitude_lol/start_latitude_lmao, etc). Mapbox gets\nit right most of the time.\n\nIf y'all went well you should see a cluster of points on a map - this is a\npreview of your Tileset. Think of this as a layer in Photoshop: we can stack\nthese layers of information atop one another continuously to make something\ngreater than the sum of its parts.\n\nIf all looks good, export your Tileset via the \"export\" button on the top right.\n\nUpload your dataset and click \"edit\"Switch Over to Your Map \"Style\"\nYou map 'style' is your blank canvas. Get in there and add a layer, and from\nthere select the Tileset you just created. Once your Tileset is loaded, you can\nstyle the points themselves and even label them with the data in your dataset as\nyou see fit:\n\nSo many colorful layers.Simply clicking around the preloaded Tilesets should\nstart giving you ideas of what's possible down the line. Just look at those\nhorrifically bright Miami Vice themed streets.\n\nFeel free to get creative with Mapbox's tools to clarify the visual story you're\ntrying to tell. I've distinguished points from others after adding a third data\nset: Every Starbucks in New York City.  Yes, those map pins have been replaced\nwith that terrifying Starbucks Logo Mermaid Sea-demon\n\nTake a look at that perfect grid of mocha frappa-whatevers and tell me these\nguys don't have a business strategy:\n\nGod that's an ugly map.For all it's worth, I'd like to sincerely apologize for\nblinding your eyes with classless use of gifs paired with the useless corporate\nmonstrosity of a map I've created. I have faith that you'll do better.\n\nNow that we've spent enough time covering the n00b stuff, it's time to take the\ngloves off. While Mapbox studio's GUI serves as an amazing crutch and way to\ncustomize the look of our data, we must not forget: we're programmers, God damn\nit! True magic lies in 1s and 0s, not WYSIWYG editors.\n\nUntil we start using Plot.ly Dash, that is.\n\n(Suddenly, thousands of fans erupt into a roaring cheer at the very mention of\nPlot.ly. It's about time.™)","html":"<p>This wouldn't be a proper data blog unless we spend a vast majority of our time talking about cleaning data. Chances are if you're pursuing analysis that's groundbreaking (or worthwhile), we're probably starting with some ugly, untapped information. It turns out Mapbox has an API specifically for this purpose: the <a href=\"https://www.mapbox.com/api-documentation/#geocoding\">Mapbox Geocoding API</a>.</p><p><strong>Geocoding</strong> is a blanket term for turning vague information into specific Lat/Long coordinates. How vague, you ask? The API covers:</p><ul><li>Pinpointing exact location via street address.</li><li>Locating regions or cities by recognizable name (ie: Rio de Janeiro).</li><li>Locating cities by highly unspecific name (Geocoding for \"Springfield\" will return results for 41 American cities)</li><li>Locating cities or venues by name within a given region (such as searching for Ray's Pizza in NYC).</li></ul><p>We can also use Geocoding to do the reverse of this, where passing in coordinates will return location names. If you find this useful, I'm assuming you're a spy.</p><h3 id=\"chipping-away-at-a-real-use-case\">Chipping Away at a Real Use Case</h3><p>In a real-life example, I have two sets of data: one represents general places of residence for a particular sample group. The goal is to see how they interact with the second dataset: a list of locations they will be traveling to. I'd love to go into more detail, but:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-1.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/tenor.gif\" class=\"kg-image\"><figcaption>I get one cliche meme per year.</figcaption></figure><!--kg-card-end: image--><p>So how can we use the Mapbox Geocoding API to systematically extract coordinates for thousands of addresses, from multiple datasets? With Pandas, of course!</p><h2 id=\"i-m-just-happy-to-be-writing-about-pandas-right-now\">I'm Just Happy to be Writing About Pandas Right Now</h2><p>Pardon my excitement; I've been far overdue for posting anything Pandas-related. It's been killing me on the inside.</p><p>We need to make sense of some vague data. As seen in <a href=\"https://hackersandslackers.com/map-data-visualization-with-mapbox/\">our Citibike example</a>, New York has plenty of public datasets with information like Taxi pickup/dropoffs, public transit, etc. These start and end points are typically too fluid to have Lat/Long coordinates associated with them, so we'll add them in ourselves. Given that we're about to pass hundreds or thousands of addresses and locations, we'll use Pandas <code>.apply()</code> to fill out the missing Lat/Long columns in our dataset. </p><p>Instead of using Mapbox's Python SDK, I'll actually be using <code>requests</code> to hit the Mapbox REST API. For some reason, the Python SDK was a bit unpredictable on my last run.<strong>*</strong></p><p><strong>*UPDATE: </strong>the Python SDK \"wasn't working\" because I apparently don't know the difference between longitude and latitude. Awesome, so I'm a moron.</p><!--kg-card-begin: markdown--><pre><code class=\"language-python\">import sys\nimport os\nimport pandas as pd\nimport requests\nimport json\n\n\nclass GeocodeAddresses:\n    &quot;&quot;&quot;Add missing lat/long information to exisiting dataset.&quot;&quot;&quot;\n\n    def __init__(self, address_data):\n        self.data = address_data\n        self.address_df = pd.read_csv(self.data)\n        self.complete_data = self.get_coords(self.address_df)\n\n\n    @classmethod\n    def get_coords(self, employee_address_df):\n        &quot;&quot;&quot;Fill Dataframe lat/long columns.&quot;&quot;&quot;\n\n        def fill_coords(row):\n            &quot;&quot;&quot;Create a route object by passing GeoJSON start/end objects.&quot;&quot;&quot;\n            base_url = 'https://api.mapbox.com/geocoding/v5/mapbox.places/'\n            address = str(row.home_address)\n            format = '.json'\n            endpoint = base_url + address + format\n            params = {\n                'access_token':  'pk.eyJ1IjNoYXJkd2VthisisreallolaXdyNHQ3OTUifQ.VTAUrmzD91Ppxr1AJww'\n            }\n            headers = {\n                'Content-Type': 'application/json'\n            }\n            r = requests.get(endpoint, params=params, headers=headers)\n            try:\n                Lat = r.json()['features'][0]['geometry']['coordinates'][0]\n                Long = r.json()['features'][0]['geometry']['coordinates'][1]\n                print(pd.Series([Lat, Long]))\n                return pd.Series([Lat, Long])\n            except IndexError:\n                pass\n\n        address_df[['Lat', 'Long']] = address_df.head(100).apply(fill_coords, axis=1)\n        address_df.to_csv('geocoded.csv')\n</code></pre>\n<!--kg-card-end: markdown--><p>In the above example, we're using <code>.apply()</code> against an empty series (our Lat/Long columns) as opposed to our entire Dataframe. When <code>get_coords()</code> returns two values, these values will fill the empty columns on a per-row basis.</p><p>For the scope of this tutorial, we'll simply focus on getting these points plotted. Don't worry, this is only part 2 of our Mapbox series! Yes, an entire series!</p><h2 id=\"turning-your-datasets-into-tilesets\">Turning Your Datasets into Tilesets</h2><p>In Mapbox terms, a <strong>Tileset</strong> is essentially a layer of data we can overlay on top of our blank map. The map <strong>style</strong> we created last time was really just the aesthetic unpinning of all the interesting data we can pile on time.</p><p>Tilesets can be stacked on one another, thus giving us infinite possibilities of the types of data we can communicate: especially when you consider that Mapbox supports Heatmaps and topology - far more than just plotted points.</p><p>First, we'll plot our origins. I've put together a dataset of completely falsified names (with presumably real addresses?) to demonstrate how we'd plot these points. Here's a sample of the garbage I'll be feeding into Mapbox:</p><!--kg-card-begin: html--><div class=\"tableContainer\">\n\t\t\t<table>\n\t\t\t\t<thead>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<th>address</th>\n\t\t\t\t\t\t<th>name</th>\n\t\t\t\t\t\t<th>long</th>\n\t\t\t\t\t\t<th>lat</th>\n\t\t\t\t\t</tr>\n\t\t\t\t</thead>\n\t\t\t\t<tbody>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>761 ST ANNS AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Royal Hiett</td>\n\t\t\t\t\t\t<td>40.754466</td>\n\t\t\t\t\t\t<td>-73.9794552</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>5 COLUMBUS CIR NY NY 10019</td>\n\t\t\t\t\t\t<td>Yolanda Antonio</td>\n\t\t\t\t\t\t<td>40.8201997</td>\n\t\t\t\t\t\t<td>-73.9110324</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1145 LENOX RD NY NY 11212</td>\n\t\t\t\t\t\t<td>Marguerita Autry</td>\n\t\t\t\t\t\t<td>40.7667595</td>\n\t\t\t\t\t\t<td>-73.9815704</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>2800 VICTORY BLVD NY NY 10314</td>\n\t\t\t\t\t\t<td>Alyse Peranio</td>\n\t\t\t\t\t\t<td>40.6597804</td>\n\t\t\t\t\t\t<td>-73.9183181</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>750 LEXINGTON AVE NY NY 10022</td>\n\t\t\t\t\t\t<td>Sina Walberg</td>\n\t\t\t\t\t\t<td>40.6080557</td>\n\t\t\t\t\t\t<td>-74.1532418</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>29 BAY RIDGE AVE NY NY 11220</td>\n\t\t\t\t\t\t<td>Ignacia Frasher</td>\n\t\t\t\t\t\t<td>40.7625148</td>\n\t\t\t\t\t\t<td>-73.9685564</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>550 RIVERSIDE DR NY NY 10027</td>\n\t\t\t\t\t\t<td>Marta Haymond</td>\n\t\t\t\t\t\t<td>40.6386587</td>\n\t\t\t\t\t\t<td>-74.034633</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>808 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Angie Tseng</td>\n\t\t\t\t\t\t<td>40.8159612</td>\n\t\t\t\t\t\t<td>-73.960318</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>41-03 69 ST NY NY 11377</td>\n\t\t\t\t\t\t<td>Marcella Weinstock</td>\n\t\t\t\t\t\t<td>40.797233</td>\n\t\t\t\t\t\t<td>-73.9713245</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>50 PARK AVE NY NY 10016</td>\n\t\t\t\t\t\t<td>Filiberto Everett</td>\n\t\t\t\t\t\t<td>40.7444514</td>\n\t\t\t\t\t\t<td>-73.8956728</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>739 BROOK AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Vernia Mcgregor</td>\n\t\t\t\t\t\t<td>40.7492656</td>\n\t\t\t\t\t\t<td>-73.9803386</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>777 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Michelina Althoff</td>\n\t\t\t\t\t\t<td>40.8199675</td>\n\t\t\t\t\t\t<td>-73.9122757</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>866 E 165 ST NY NY 10459</td>\n\t\t\t\t\t\t<td>Dave Tauber</td>\n\t\t\t\t\t\t<td>40.7965956</td>\n\t\t\t\t\t\t<td>-73.9726135</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>130 E 37 ST NY NY 10016</td>\n\t\t\t\t\t\t<td>Tandra Gowen</td>\n\t\t\t\t\t\t<td>40.8237011</td>\n\t\t\t\t\t\t<td>-73.8990202</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>797 ST ANNS AVE NY NY 10451</td>\n\t\t\t\t\t\t<td>Toby Philbrick</td>\n\t\t\t\t\t\t<td>40.7482336</td>\n\t\t\t\t\t\t<td>-73.978566</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>41 AARON LN NY NY 10309</td>\n\t\t\t\t\t\t<td>Aisha Grief</td>\n\t\t\t\t\t\t<td>40.82089</td>\n\t\t\t\t\t\t<td>-73.9109118</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>641 LEXINGTON AVE NY NY 10022</td>\n\t\t\t\t\t\t<td>Tarah Sinkler</td>\n\t\t\t\t\t\t<td>40.5541368</td>\n\t\t\t\t\t\t<td>-74.2126653</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>4201 4 AVE NY NY 11232</td>\n\t\t\t\t\t\t<td>Coletta Jeansonne</td>\n\t\t\t\t\t\t<td>40.7590297</td>\n\t\t\t\t\t\t<td>-73.9703219</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1021 PARK AVE NY NY 10028</td>\n\t\t\t\t\t\t<td>Lorie Shriver</td>\n\t\t\t\t\t\t<td>40.650317</td>\n\t\t\t\t\t\t<td>-74.0081672</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>127 RIVERSIDE DR NY NY 10024</td>\n\t\t\t\t\t\t<td>Antwan Fullilove</td>\n\t\t\t\t\t\t<td>40.7794132</td>\n\t\t\t\t\t\t<td>-73.9572475</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>5120 BROADWAY NY NY 10034</td>\n\t\t\t\t\t\t<td>Normand Beerman</td>\n\t\t\t\t\t\t<td>40.7890613</td>\n\t\t\t\t\t\t<td>-73.9806569</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>7124 20 AVE NY NY 11204</td>\n\t\t\t\t\t\t<td>Wes Nieman</td>\n\t\t\t\t\t\t<td>40.8714856</td>\n\t\t\t\t\t\t<td>-73.9130362</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>3506 BEDFORD AVE NY NY 11210</td>\n\t\t\t\t\t\t<td>Marlen Hutcherson</td>\n\t\t\t\t\t\t<td>40.6127972</td>\n\t\t\t\t\t\t<td>-73.9901551</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>550 GRAND ST NY NY 10002</td>\n\t\t\t\t\t\t<td>Leonie Lablanc</td>\n\t\t\t\t\t\t<td>40.6168306</td>\n\t\t\t\t\t\t<td>-73.9501481</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>1711 GROVE ST NY NY 11385</td>\n\t\t\t\t\t\t<td>Doris Herrman</td>\n\t\t\t\t\t\t<td>40.7143151</td>\n\t\t\t\t\t\t<td>-73.9800558</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>785 W END AVE NY NY 10025</td>\n\t\t\t\t\t\t<td>Cyndy Kossman</td>\n\t\t\t\t\t\t<td>40.7032053</td>\n\t\t\t\t\t\t<td>-73.9111942</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td>6040 HUXLEY AVE NY NY 10471</td>\n\t\t\t\t\t\t<td>Donya Ponte</td>\n\t\t\t\t\t\t<td>40.796763</td>\n\t\t\t\t\t\t<td>-73.972483</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</tbody>\n\t\t\t</table>\n</div><!--kg-card-end: html--><h3 id=\"head-over-to-mapbox-studio\">Head Over to Mapbox Studio</h3><p>While we can technically do everything programmatically, Mapbox's GUI is simply too easy to ignore. Using <a href=\"https://www.mapbox.com/studio/datasets/\">Mapbox Studio</a>, we can upload our data and turn it into a <em><strong>tileset; </strong></em>the heart and soul of what makes our maps interesting. </p><p>Once you've uploaded your CSV (or JSON, or whatever) as a dataset, we can immediately see what this information looks like on a map by previewing it as a tileset. Mapbox is surprisingly intelligent in that it can deduce lat/long values from poorly named or formatted columns (such as <em>Lat/Long</em>, <em>Latitutde/Longitude</em>, <em>start_longitude_lol/start_latitude_lmao</em>, etc). Mapbox gets it right most of the time.</p><p>If y'all went well you should see a cluster of points on a map - this is a preview of your Tileset. Think of this as a layer in Photoshop: we can stack these layers of information atop one another continuously to make something greater than the sum of its parts.</p><p>If all looks good, export your Tileset via the \"export\" button on the top right.</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-4.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/tileset.gif\" class=\"kg-image\"><figcaption>Upload your dataset and click \"edit\"</figcaption></figure><!--kg-card-end: image--><h3 id=\"switch-over-to-your-map-style\">Switch Over to Your Map \"Style\"</h3><p>You map 'style' is your blank canvas. Get in there and add a layer, and from there select the Tileset you just created. Once your Tileset is loaded, you can style the points themselves and even label them with the data in your dataset as you see fit:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-2.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/layers.gif\" class=\"kg-image\"><figcaption>So many colorful layers.</figcaption></figure><!--kg-card-end: image--><p>Simply clicking around the preloaded Tilesets should start giving you ideas of what's possible down the line. Just look at those horrifically bright Miami Vice themed streets.</p><p>Feel free to get creative with Mapbox's tools to clarify the visual story you're trying to tell. I've distinguished points from others after adding a third data set: <strong>Every Starbucks in New York City.</strong> Yes, those map pins have been replaced with that terrifying Starbucks Logo Mermaid Sea-demon</p><p>Take a look at that perfect grid of mocha frappa-whatevers and tell me these guys don't have a business strategy:</p><!--kg-card-begin: image--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://res-3.cloudinary.com/hackers-and-slackers/image/upload/q_auto/v1/images/mapstarbucks2.gif\" class=\"kg-image\"><figcaption>God that's an ugly map.</figcaption></figure><!--kg-card-end: image--><p>For all it's worth, I'd like to sincerely apologize for blinding your eyes with classless use of gifs paired with the useless corporate monstrosity of a map I've created. I have faith that you'll do better.</p><p>Now that we've spent enough time covering the n00b stuff, it's time to take the gloves off. While Mapbox studio's GUI serves as an amazing crutch and way to customize the look of our data, we must not forget: we're programmers, God damn it! True magic lies in 1s and 0s, not WYSIWYG editors.</p><p>Until we start using <strong>Plot.ly Dash</strong>, that is.</p><!--kg-card-begin: html--><span class=\"subtext\">(Suddenly, thousands of fans erupt into a roaring cheer at the very mention of Plot.ly. It's about time.™)</span><!--kg-card-end: html-->","url":"https://hackersandslackers.com/preparing-data-for-mapbox-geocoding/","uuid":"b2e24775-df44-464e-839a-be24e2a3eb42","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c0ee5bd8687896e154a9376"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673743","title":"Geographic Data Visualization with Mapbox","slug":"map-data-visualization-with-mapbox","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/12/mapbox@2x.jpg","excerpt":"Visualizing Geodata with Mapbox's API and Tools.","custom_excerpt":"Visualizing Geodata with Mapbox's API and Tools.","created_at_pretty":"07 December, 2018","published_at_pretty":"11 December, 2018","updated_at_pretty":"31 January, 2019","created_at":"2018-12-07T12:36:58.000-05:00","published_at":"2018-12-11T08:00:00.000-05:00","updated_at":"2019-01-31T17:52:32.000-05:00","meta_title":"Geographical Data Visualization with Mapbox | Hackers and Slackers","meta_description":"A collection of Map APIs that rivals Google Maps, plus Beautiful GeoData Visualization.","og_description":"A collection of Map APIs that rivals Google Maps, plus Beautiful GeoData Visualization.","og_image":"https://hackersandslackers.com/content/images/2018/12/mapbox@2x.jpg","og_title":"Geographic Data Visualization with Mapbox","twitter_description":"A collection of Map APIs that rivals Google Maps, plus Beautiful GeoData Visualization.","twitter_image":"https://hackersandslackers.com/content/images/2018/12/mapbox@2x.jpg","twitter_title":"Geographic Data Visualization with Mapbox","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Data Vis","slug":"datavis","description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Primarily focused on programmatic visualization as opposed to Business Intelligence software.","feature_image":null,"meta_description":"Visualize your data with charting tools like Matplotlib, Plotly, D3, Chart.js, Muze, Seaborn, and countless others. Focused on programmatic visualization.","meta_title":"Data Visualization | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"SaaS Products","slug":"saas","description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","feature_image":null,"meta_description":"Third-party products and services we’ve discovered to be diamonds in the rough. These are products we’ve included in our stack based on price and value add.","meta_title":"Our Picks: SaaS Products | Hackers and Slackers","visibility":"public"},{"name":"#Mapping Data with Mapbox","slug":"mapping-data-with-mapbox","description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","feature_image":"https://hackersandslackers.com/content/images/2019/03/mapbox.jpg","meta_description":"A full exploration into Mapbox: the sweetheart of geovisualization amongst data scientists. Learn the core product or see why the API rivals Google Maps.","meta_title":"Mapping Data with Mapbox","visibility":"internal"}],"plaintext":"There's a trend among those using Jupyter Notebooks (or equivalent) which leads\nme to believe humanity is coming to an important realization: Google Maps,  as\nan API is expensive.\n\nRegardless if Google maps is embedded as a consumer-facing widget, or part of a\nroutine data-pipeline, a single surge of high-traffic can leave enterprises with\nprice tags in the hundreds of thousands of dollars. In fact, I can hardly\nremember a product where this hadn't  become the case. One can hardly blame the\nsearch engine; after all, our tendency to ignore the Terms and Service\nagreements (as well as payment policies) has always been core to the Google\nbusiness model.  Even then, there are enough enterprises to go around to turn a\nblind eye and actually pay such a bill willingly without exploring alternatives.\n\nData Scientists in particular have no excuse for inaction when it comes to\nseeking a better alternative. As it turns out, there is  one, and it is Cheaper,\n Easier, and perhaps more Fully Featured  than its Google Maps counterpart. That\nproduct is Mapbox. \n\nMapbox  is much more than a Google API clone. The web product offers a plethora\nof UI-driven features that we can use to customize maps as well as save or\neffortlessly transform raw data into workable GeoJSON data without even touching\nan API (which, mind you, there is.... with SDKs in every conceivable language).\nWe're going to create a quick map visualization incorporating some real data to\nget introduced to Mapbox's functionality, but this is only the beginning.\nDownload the line we'll see just how easy it is to incorporate Mapbox in\nproducts like Plot.ly Dash  or even Jupyter Notebooks.\n\nX Marks the Spot\nBefore straying from reigning champion Google Maps, it's worth exploring the\nsignificance of the metric that brought us here first: price.\n\nMurphy's law clearly states \"Cash Rules Everything Around Me, C.R.E.A.M; get the\nmoney, Dolla dolla bill y'all.\"  Given this reality, a minimum requirement for\nMapbox should be it's pricing model when compared to Google's.\n\nMapbox Pricing Tiers\nPrice\n Web apps\n Mobile SDKs\n Free to Start\n\n$0\n\n50,000  map views /mo\n\n50,000  Geocoding requests /mo\n\n50,000  Directions requests /mo\n\n50,000  Matrix elements /mo\n\n50,000  Tilequery requests /mo\n\n50,000  map views /mo\n\n50,000  Geocoding requests /mo\n\n50,000  Directions requests /mo\n\n50,000  Matrix elements /mo\n\n50,000  Tilequery requests /mo\n\nAdditional Usage\n\n$0.50\n\n50,000  map views /mo\n\n50,000  Geocoding requests /mo\n\n50,000  Directions requests /mo\n\n50,000  Matrix elements /mo\n\n50,000  Tilequery requests /mo\n\n50,000  map views /mo\n\n50,000  Geocoding requests /mo\n\n50,000  Directions requests /mo\n\n50,000  Matrix elements /mo\n\n50,000  Tilequery requests /mo\n\nCompare this to Google's transparent pricing structure:\n\nGoogle API Pricing Tiers\nPrice\n Web apps\n Mobile SDKs\n Starter Pack\n\nBrown Paper Bag full of $20s\n\n5  and a half map views /mo\n\n11  times thinking about the API /mo\n\n6  verbal mentions of \"Google\" /mo\n\n2  directions to shitty parties /mo\n\n8  visits to anywhere /mo\n\n9  Android unlocks /mo\n\n12  Google queries for restaurants /mo\n\n3  \"OK Google\" queries /mo\n\n7  Accidental app opens /mo\n\n1  Creating the next \"Uber for X\" /mo\n\nAdditional Usage\n\nEleventy Billion Dollars\n\nUnlimited Requests!* \n\n*See Pricing \n\nUnlimited Requests!*\n\n*See Pricing\n\nSeems like a convincing point in the win column for Mapbox. If we stay within\nreason, Mapbox can essentially serve us as an entirely free service.\n\nSurely we must be missing something  since we're opting for free services\nthough, right? How do Mapbox visualizations stack up against Google Maps?\n\n\n[https://codepen.io/ro-ka/pen/ENoOjz/]  [https://codepen.io/ro-ka] \n[https://codepen.io]\n\nPardon my French here, but hot damn that map is dope.  There are plenty more\nexamples where that came from, but it's clear that Mapbox has lowkey stolen the\nhearts of the scientific analysis  market, while Google  concerns itself on the \nconsumer  and business  markets.\n\nTonight's Itinerary: Creating Dope Maps\nTo make some data art, we have a few items on our checklist:\n\n * Obtain a dataset with location-based data: In our case of routing, we need a\n   dataset with a set origin and destination per row.\n * Create direction object routes by running our dataset through the Mapbox API.\n    \n * Create a styled map for our presentation by using Mapbox's style editor.\n * Overlay our route data on our beautiful map.\n\nStep 1: Get Some Free Data\nNow that we've properly shit-talked Google, let's use Google. We're going to\nneed to get some good data, and BigQuery  has some awesome free datasets that we\ncan run wild with. I'll be opting for NYC's dataset on Citibike trips, as it\nprovides a clean set of data where starting and ending coordinates are always\npresent.\n\nGoogle Cloud's Free Dataset of Citibike TripsAs a side note, BigQuery is great.\nEven if you're only somewhat versed in SQL, BigQuery's syntax is essentially\nwhatever your first guess would be.\n\nGranted we only need the start and end locations to make our map, but i decided\nto take a bit extra for curiosity's sake:\n\nstart_namestart_latitudestart_longitudeend_nameend_latitudeend_longitude1 Ave &\nE 15 St40.732218530-73.9816555701 Ave & E 18 St40.733812192-73.9805442091 Ave &\nE 30 St40.741443870-73.975360820E 39 St & 2 Ave40.747803730-73.9734419001 Ave &\nE 62 St40.761227400-73.960940220E 75 St & 3 Ave40.771129270-73.9577229702 Ave &\nE 99 St40.786258600-73.9455257903 Ave & E 112 St40.795508000-73.9416060003 St &\n3 Ave40.675070500-73.98775226010 St & 7 Ave40.666207800-73.9819988603 St &\nProspect Park West40.668132000-73.9736383103 St & Prospect Park West40.668132000\n-73.9736383106 Ave & W 33 St40.749012710-73.988483950W 37 St & 5 Ave40.750380090\n-73.9833898808 Ave & W 52 St40.763707390-73.985161500Central Park S & 6 Ave\n40.765909360-73.97634151011 Ave & W 41 St40.760300960-73.998842220W 34 St & 11\nAve40.755941590-74.00211630012 Ave & W 40 St40.760875020-74.002776680W 42 St & 8\nAve40.757569900-73.990985070Allen St & E Houston St40.722055000-73.989111000Mott\nSt & Prince St40.723179580-73.994800120Allen St & Hester St40.716058660\n-73.991907590Greenwich St & N Moore St40.720434110-74.010206090Amsterdam Ave & W\n73 St40.779668090-73.980930448E 85 St & 3 Ave40.778012030-73.954071490Bank St &\nHudson St40.736528890-74.006180260MacDougal St & Prince St40.727102580\n-74.002970880Bank St & Washington St40.736196700-74.008592070W 4 St & 7 Ave S\n40.734011430-74.002938770Barclay St & Church St40.712912240-74.010202340Clinton\nSt & Tillary St40.696192000-73.991218000Berkeley Pl & 7 Ave40.675146839\n-73.975232095West Drive & Prospect Park West40.661063372-73.979452550Bialystoker\nPl & Delancey St40.716226440-73.982612060Reade St & Broadway40.714504510\n-74.005627890Broadway & W 24 St40.742354300-73.989150760South End Ave & Liberty\nSt40.711512000-74.015756000Broadway & W 29 St40.746200900-73.988557230Stanton St\n& Chrystie St40.722293460-73.991475350Broadway & W 56 St40.765265400\n-73.981923380Broadway & W 49 St40.760683271-73.984527290Broadway & W 58 St\n40.766953170-73.9816933305 Ave & E 78 St40.776321422-73.964273930Cadman Plaza E\n& Red Cross Pl40.699917550-73.989717730Leonard St & Church St40.717571000\n-74.005549000Cadman Plaza E & Tillary St40.695976830-73.990148920Lawrence St &\nWilloughby St40.692361780-73.986317460Carmine St & 6 Ave40.730385990\n-74.002149880W 27 St & 7 Ave40.746647000-73.993915000Central Park W & W 96 St\n40.791270000-73.964839000W 52 St & 6 Ave40.761329831-73.979820013Central Park\nWest & W 76 St40.778967840-73.973747370Central Park S & 6 Ave40.765909360\n-73.976341510Step 2: Style a Sexy Map in Mapbox Studio\nMapbox provides a superb web UI labeled “studio” interface to help us get\nstarted. The “studio” web UI is separated into three parts: custom map styles, \ntilesets, and datasets.\n\nAll of these can we saved for later use.These three sections can be summarized\nas:\n\n * Styles: Custom map styles editable via a GUI, which produce a stylesheet for\n   convenience \n * Tilesets: Map overlays we can apply from our own data or otherwise to segment\n   geographical areas\n * Datasets:  Data containing anything from points on a map to complex direction\n   routes we can overlay atop our map.\n\nHere's a quick look at the Map style editor:\n\nI want to wake up, in a city that doesn't sleep.Save your styled map once you\nfind it to be adequately attractive. We'll need it for later.\n\nStep 4: Start a Flask App\nOf course we're making a Flask app; is there even any other kind? We'll be using\nthe Flask Application Factory setup as we usually do, so we should end up with a\nfile structure as below. If you feel like you're getting ahead of ourself,\ncheckout our post on structuring Flask applications\n[https://hackersandslackers.com/structuring-your-flask-app/].\n\nmapbox-app\n├── /application\n│   └── __init__.py\n├── /datasets\n│   ├── data.json\n│   └── output.csv   \n├── /maps\n│   ├── __init__.py\n│   ├── /templates\n│   │    └── index.html\n│   ├── views.py\n│   └── plots.py\n├── start.sh\n├── settings.py\n├── wsgi.py\n├── Pipfile\n├── README.md\n└── requirements.txt\n\n\nTo mix things up a bit we'll use a shell script this time to handle envars and\nrunning our script. Start by creating start.sh:\n\n# start.sh\n\nexport FLASK_APP=wsgi.py\nexport FLASK_DEBUG=1\nexport APP_CONFIG_FILE=settings.py\nflask run\n\n\nYes, we'll be using settings.py  as our config file for a change. Ahhh, just\nlike the Django days. This file should contain a Mapbox access token. Mapbox\nprovides you with a public token by default in many of its tutorials (noted by\nthe pk  prefix for 'public key' - contrast this with sk  for 'secret key'). If\nyou'd like to do anything meaningful with Mapbox, you'll have to retrieve a\nsecret key via the UI. Then we can add this token to settings.py  as such:\n\nMAPBOX_ACCESS_TOKEN=\"sk.eyJ1IB&F^&f^R&DFRUYFTRUctyrcTYRUFrtCFTYDYTuEg\"\n\n\nFinally, here's a look at application/__init__.py  just to make sure we're on\nthe same page:\n\n# application/__init__.py\n\nimport os\nfrom flask import Flask, g\n\ndef create_app():\n    \"\"\"Construct the core application.\"\"\"\n    app = Flask(__name__)\n    app.config.from_envvar('APP_CONFIG_FILE', silent=True)\n\n    with app.app_context():\n        # Construct map blueprint\n        from maps import mapviews\n        app.register_blueprint(mapviews.map_blueprint)\n\n        return app\n\n\nStep 5: Create a Blueprint for Your Map\nYou may have noticed we registered this Blueprint in the previous step. Create a\n /maps  directory which we'll set as a module; we'll need this to handle the \nview, model (or just data),  and controller (routes.py as seen below).\n\nroutes.py\nimport os\nfrom flask import Blueprint, render_template, request\nfrom flask import current_app as app\nfrom . import locations\n\nmap_blueprint = Blueprint('map', __name__, template_folder='templates', static_folder='static')\nplot_locations = locations.LocationData()\n\n\n# Landing Page\n@map_blueprint.route('/', methods=['GET'])\ndef map():\n    return render_template('index.html', ACCESS_KEY=app.MAPBOX_ACCESS_KEY,  locations=plot_locations.get_plots, title=\"CitiBike Mapbox App.\")\n\n\n\ntemplates/index.html\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset='utf-8' />\n  <title>{{title}}</title>\n  <meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' />\n  <script src='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.js'></script>\n  <link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.css' rel='stylesheet' />\n  <style>\n    body { margin:0; padding:0; }\n    #map { position:absolute; top:0; bottom:0; width:100%; }\n  </style>\n</head>\n<body>\n\n<div id='map'></div>\n<script>\nmapboxgl.accessToken = {{MAPBOX_ACCESS_TOKEN}};\nconst map = new mapboxgl.Map({\n  container: 'map',\n  style: 'mapbox://styles/toddbirchard/cjpij1gfhghiy2spetf5w998w',\n  center: [-73.981856, 40.703820],\n  zoom: 11.1,\n\n});\n\nmap.on('load', function(e) {\n  // Add the data to your map as a layer\n  map.addLayer({\n    id: 'locations',\n    type: 'symbol',\n    // Add a GeoJSON source containing place coordinates and information.\n    source: {\n      type: 'geojson',\n      data: {{locations}}\n    },\n    layout: {\n      'icon-image': 'restaurant-15',\n      'icon-allow-overlap': true,\n    }\n  });\n});\n</script>\n\n</body>\n</html>\n\n\ndata.py\nNormally this is where we'd use the magic of the Mapbox API to get coordinates,\nroute objects, or whatever it is your heart hopes to plot. This is intended to\nbe intro post, so let's break that logic out for another time and use a dataset\nMapbox would be happy to receive for the sake of results.\n\nStep 6: Uploading our Dataset via Mapbox Studio\nMapbox graciously lets us upload our data via their Studio UI, which does the\nunthinkable; immediately upon upload, Mapbox will take the data we give it\n(whether it be CSV, GeoJSON, etc) and immediately parse it in a way that makes\nsense. Upload your dataset at https://www.mapbox.com/studio/datasets/:\n\nUploading the raw data of our Citibike CSV.Next, Mapbox shows us a preview of\nour data before we even know what happened:\n\nIt's like they don't even want us to do work.Step 7: Do It in Flask\nAfter uploading your dataset via mapbox studio, you can actually redownload  the\ndata with a subtle twist: your data will be automatically formatted as GeoJSON:\nthe format of JSON objects Mapbox uses to plot points, draw routes, etc.\n\nSince we've had a long day, I'll allow you to download this pre-formatted data\nand hardcore the values in to your Map view. You're getting off easy for now,\nbut next time we're doing this programmatically ;).\n\n<!DOCTYPE html>\n<html>\n\n<head>\n  <meta charset='utf-8' />\n  <title>{{title}}</title>\n  <meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' />\n  <script src='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.js'></script>\n  <link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.css' rel='stylesheet' />\n  <style>\n    body {\n      margin: 0;\n      padding: 0;\n    }\n\n    #map {\n      position: absolute;\n      top: 0;\n      bottom: 0;\n      width: 100%;\n    }\n  </style>\n</head>\n\n<body>\n\n  <div id='map'></div>\n  <script>\n    mapboxgl.accessToken = '{{ACCESS_KEY}}';\n    const map = new mapboxgl.Map({\n      container: 'map',\n      style: 'mapbox://styles/toddbirchard/cjpij1oxl3hiy2spetf5w998w',\n      center: [-73.981856, 40.703820],\n      zoom: 11.1,\n\n    });\n\n\n    map.addLayer({\n      \"id\": \"points\",\n      \"type\": \"symbol\",\n      \"source\": {\n        \"features\": [{\n            \"type\": \"Feature\",\n            \"properties\": {\n              \"start_station_name\": \"Central Park West & W 76 St\",\n              \"end_station_name\": \"Central Park S & 6 Ave\",\n              \"end_station_latitude\": \"40.76590936\",\n              \"end_station_longitude\": \"-73.97634151\"\n            },\n            \"geometry\": {\n              \"coordinates\": [\n                -73.973747,\n                40.778967\n              ],\n              \"type\": \"Point\"\n            },\n            \"id\": \"000a1f944d4dd786d9e7ed04620af02b\"\n          },\n          {\n            \"type\": \"Feature\",\n            \"properties\": {\n              \"start_station_name\": \"W 64 St & West End Ave\",\n              \"end_station_name\": \"W 70 St & Amsterdam Ave\",\n              \"end_station_latitude\": \"40.77748046\",\n              \"end_station_longitude\": \"-73.98288594\"\n            },\n            \"geometry\": {\n              \"coordinates\": [\n                -73.987537,\n                40.774528\n              ],\n              \"type\": \"Point\"\n            },\n            \"id\": \"01d8c19524f067a3f4712653265e0a49\"\n          },\n          {\n            \"type\": \"Feature\",\n            \"properties\": {\n              \"start_station_name\": \"E 20 St & FDR Drive\",\n              \"end_station_name\": \"W 13 St & 7 Ave\",\n              \"end_station_latitude\": \"40.73781509\",\n              \"end_station_longitude\": \"-73.99994661\"\n            },\n            \"geometry\": {\n              \"coordinates\": [\n                -73.975738,\n                40.733142\n              ],\n              \"type\": \"Point\"\n            },\n            \"id\": \"038ac5403b136e34874a7278f64d4e95\"\n          },\n          {\n              \\\\ --------------------------------------\n              (etc etc....)\n               \\\\ --------------------------------------\n          },\n          {\n            \"type\": \"Feature\",\n            \"properties\": {\n              \"start_station_name\": \"Mercer St & Bleecker St\",\n              \"end_station_name\": \"1 Ave & E 30 St\",\n              \"end_station_latitude\": \"40.74144387\",\n              \"end_station_longitude\": \"-73.97536082\"\n            },\n            \"geometry\": {\n              \"coordinates\": [\n                -73.996621,\n                40.727063\n              ],\n              \"type\": \"Point\"\n            },\n            \"id\": \"ff1daf9aadbf0cd6b788bd76f0a3f333\"\n          }\n        ],\n        \"type\": \"FeatureCollection\"\n      },\n      \"layout\": {\n        \"icon-image\": \"{icon}-15\",\n        \"text-field\": \"{title}\",\n        \"text-font\": [\"Open Sans Semibold\", \"Arial Unicode MS Bold\"],\n        \"text-offset\": [0, 0.6],\n        \"text-anchor\": \"top\"\n      }\n    });\n  </script>\n\n</body>\n\n</html>\n\n\nUncharted Territory\nThere's way more for us to explore in Mapbox. Stay tuned for the rest of this\nseries as we explore generating GeoData programmatically, and build interactive\napplications to really get users involved in map data by letting them control\nconstraints such as time, etc.","html":"<p>There's a trend among those using Jupyter Notebooks (or equivalent) which leads me to believe humanity is coming to an important realization: <strong>Google Maps,</strong> as an API is <em>expensive.</em></p><p>Regardless if Google maps is embedded as a consumer-facing widget, or part of a routine data-pipeline, a single surge of high-traffic can leave enterprises with price tags in the hundreds of thousands of dollars. In fact, I can hardly remember a product where this <em>hadn't</em> become the case. One can hardly blame the search engine; after all, our tendency to ignore the Terms and Service agreements (as well as payment policies) has always been core to the Google business model.  Even then, there are enough enterprises to go around to turn a blind eye and actually pay such a bill willingly without exploring alternatives.</p><p>Data Scientists in particular have no excuse for inaction when it comes to seeking a better alternative. As it turns out, there <em>is</em> one, and it is <strong>Cheaper</strong>, <strong>Easier, </strong>and perhaps more <strong>Fully Featured</strong> than its Google Maps counterpart. That product is <strong>Mapbox</strong>. </p><p><strong>Mapbox</strong> is much more than a Google API clone. The web product offers a plethora of UI-driven features that we can use to customize maps as well as save or effortlessly transform raw data into workable GeoJSON data without even touching an API (which, mind you, there is.... with SDKs in every conceivable language). We're going to create a quick map visualization incorporating some real data to get introduced to Mapbox's functionality, but this is only the beginning. Download the line we'll see just how easy it is to incorporate Mapbox in products like <strong>Plot.ly Dash</strong> or even <strong>Jupyter Notebooks</strong>.</p><h2 id=\"x-marks-the-spot\">X Marks the Spot</h2><p>Before straying from reigning champion Google Maps, it's worth exploring the significance of the metric that brought us here first: price.</p><p>Murphy's law clearly states <strong>\"Cash Rules Everything Around Me, C.R.E.A.M; get the money, Dolla dolla bill y'all.\"</strong> Given this reality, a minimum requirement for Mapbox should be it's pricing model when compared to Google's.</p><h3 id=\"mapbox-pricing-tiers\">Mapbox Pricing Tiers</h3><style>\n  p {\n    line-height: 1.2;\n    color: #444350;\n    margin: 0 0 0 0 !important;\n  }\n   \n  td > p {\n      margin: 0 0 5px 0 !important;\n    }\n    \n  .introbox {\n    text-align: left;\n    vertical-align: top;\n    padding: 3%;\n    max-width: 200px;\n   }\n\n  .tier {\n    font-weight: 900 !important;\n    font-size: 16px !important;\n    display: block;\n    line-height: 1 !important;\n    margin: 0 0 10px !important;\n  }\n\n  .price {\n    font-weight: 500 !important;\n\tfont-size: 18px !important;\n    display: block;\n    color: #60afe6 !important;\n    line-height: 1.3 !important;\n  }\n    \n    thead th {\n        text-align: left !important;\n    }\n    \n    th strong {\n      font-size: 18px;\n      text-align: left;\n    }\n    \n</style>\n\n<div class=\"tableContainer\">\n  <table class=\"table left\">\n    <thead>\n      <tr>\n        <th>\n          <strong>Price</strong>\n        </th>\n        <th>\n          <strong>Web apps</strong>\n        </th>\n        <th>\n          <strong>Mobile SDKs</strong>\n        </th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td class=\"introbox\">\n          <p class=\"tier\">Free to Start</p>\n          <p class=\"price\">$0</p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>50,000</strong> map views <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Geocoding requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Directions requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Matrix elements <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Tilequery requests <strong>/mo</strong></p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>50,000</strong> map views <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Geocoding requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Directions requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Matrix elements <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Tilequery requests <strong>/mo</strong></p>\n        </td>\n      </tr>\n      <tr>\n        <td class=\"introbox\">\n          <p class=\"tier\">Additional Usage</p>\n          <p class=\"price\">$0.50</p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>50,000</strong> map views <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Geocoding requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Directions requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Matrix elements <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Tilequery requests <strong>/mo</strong></p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>50,000</strong> map views <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Geocoding requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Directions requests <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Matrix elements <strong>/mo</strong></p>\n          <p><strong>50,000</strong> Tilequery requests <strong>/mo</strong></p>\n        </td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<p>Compare this to Google's transparent pricing structure:</p><h3 id=\"google-api-pricing-tiers\">Google API Pricing Tiers</h3><style>\n  p {\n    line-height: 1.2;\n    color: #444350;\n    margin: 0 0 0 0 !important;\n  }\n    \n  .introbox {\n      text-align: left;\n      vertical-align: top;\n      padding: 3%;\n      max-width: 200px;\n    }\n\n  .tier {\n    font-weight: 900 !important;\n    font-size: 16px !important;\n    display: block;\n    line-height: 1 !important;\n    margin: 0 0 10px !important;\n  }\n\n  .price {\n    font-weight: 500 !important;\n\tfont-size: 18px !important;\n    display: block;\n    color: #60afe6 !important;\n    line-height: 1.3 !important;\n  }\n    \n    thead th {\n        text-align: left !important;\n    }\n    \n    th strong {\n      font-size: 18px;\n      text-align: left;\n    }\n\n</style>\n\n<div class=\"tableContainer\">\n  <table class=\"table left\">\n    <thead>\n      <tr>\n        <th>\n          <strong>Price</strong>\n        </th>\n        <th>\n          <strong>Web apps</strong>\n        </th>\n        <th>\n          <strong>Mobile SDKs</strong>\n        </th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td class=\"introbox\">\n          <p class=\"tier\">Starter Pack</p>\n          <p class=\"price\">Brown Paper Bag full of $20s</p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>5</strong> and a half map views  <strong>/mo</strong></p>\n          <p><strong>11</strong> times thinking about the API <strong>/mo</strong></p>\n          <p><strong>6</strong> verbal mentions of \"Google\"  <strong>/mo</strong></p>\n          <p><strong>2</strong> directions to shitty parties <strong>/mo</strong></p>\n          <p><strong>8</strong> visits to anywhere <strong>/mo</strong></p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p><strong>9</strong> Android unlocks <strong>/mo</strong></p>\n          <p><strong>12</strong> Google queries for restaurants  <strong>/mo</strong></p>\n          <p><strong>3</strong> \"OK Google\" queries  <strong>/mo</strong></p>\n          <p><strong>7</strong> Accidental app opens <strong>/mo</strong></p>\n          <p><strong>1</strong> Creating the next \"Uber for X\" <strong>/mo</strong></p>\n        </td>\n      </tr>\n      <tr>\n        <td class=\"introbox\">\n          <p class=\"tier\">Additional Usage</p>\n          <p class=\"price\">Eleventy Billion Dollars</p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p>Unlimited Requests!*  </p>\n          <p><small>*See Pricing </small></p>\n        </td>\n        <td style=\"padding: 20px 15px;\">\n          <p>Unlimited Requests!*</p>\n          <p><small>*See Pricing</small></p>\n        </td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<p>Seems like a convincing point in the win column for <strong>Mapbox</strong>. If we stay within reason, Mapbox can essentially serve us as an entirely free service.</p><p>Surely we must be missing <em>something</em> since we're opting for free services though, right? How do Mapbox visualizations stack up against Google Maps?</p><p data-height=\"511\" data-theme-id=\"0\" data-slug-hash=\"ENoOjz\" data-default-tab=\"result\" data-user=\"ro-ka\" data-pen-title=\"3D map visualizations with Mapbox GL JS\" class=\"codepen\"><a href=\"https://codepen.io/ro-ka/pen/ENoOjz/\"><br></a><a href=\"https://codepen.io/ro-ka\"></a><a href=\"https://codepen.io\"></a></p>\n<script async src=\"https://static.codepen.io/assets/embed/ei.js\"></script><p>Pardon my French here, but hot damn that map is <em>dope.</em> There are plenty more examples where that came from, but it's clear that <strong>Mapbox </strong>has lowkey stolen the hearts of the <em>scientific analysis</em> market, while <strong>Google</strong> concerns itself on the <em>consumer</em> and <em>business</em> markets.</p><h2 id=\"tonight-s-itinerary-creating-dope-maps\">Tonight's Itinerary: Creating Dope Maps</h2><p>To make some data art, we have a few items on our checklist:</p><ul><li>Obtain a dataset with location-based data: In our case of routing, we need a dataset with a set origin and destination per row.</li><li>Create <strong>direction </strong>object routes by running our dataset through the Mapbox API. </li><li>Create a styled map for our presentation by using Mapbox's style editor.</li><li>Overlay our route data on our beautiful map.</li></ul><h2 id=\"step-1-get-some-free-data\">Step 1: Get Some Free Data</h2><p>Now that we've properly shit-talked Google, let's use Google. We're going to need to get some good data, and <strong>BigQuery</strong> has some awesome free datasets that we can run wild with. I'll be opting for <strong>NYC's dataset on Citibike trips</strong>, as it provides a clean set of data where starting and ending coordinates are always present.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/Screen-Shot-2018-12-07-at-2.53.34-AM.png\" class=\"kg-image\"><figcaption>Google Cloud's Free Dataset of Citibike Trips</figcaption></figure><p>As a side note, BigQuery is great. Even if you're only somewhat versed in SQL, BigQuery's syntax is essentially whatever your first guess would be.</p><p>Granted we only need the start and end locations to make our map, but i decided to take a bit extra for curiosity's sake:</p><div class=\"tableContainer\">\n<table border=\"1\" class=\"table table-striped table-bordered table-hover table-condensed\">\n<thead><tr><th title=\"Field #1\">start_name</th>\n<th title=\"Field #2\">start_latitude</th>\n<th title=\"Field #3\">start_longitude</th>\n<th title=\"Field #4\">end_name</th>\n<th title=\"Field #5\">end_latitude</th>\n<th title=\"Field #6\">end_longitude</th>\n</tr></thead>\n<tbody><tr><td>1 Ave &amp; E 15 St</td>\n<td align=\"right\">40.732218530</td>\n<td align=\"right\">-73.981655570</td>\n<td>1 Ave &amp; E 18 St</td>\n<td align=\"right\">40.733812192</td>\n<td align=\"right\">-73.980544209</td>\n</tr>\n<tr><td>1 Ave &amp; E 30 St</td>\n<td align=\"right\">40.741443870</td>\n<td align=\"right\">-73.975360820</td>\n<td>E 39 St &amp; 2 Ave</td>\n<td align=\"right\">40.747803730</td>\n<td align=\"right\">-73.973441900</td>\n</tr>\n<tr><td>1 Ave &amp; E 62 St</td>\n<td align=\"right\">40.761227400</td>\n<td align=\"right\">-73.960940220</td>\n<td>E 75 St &amp; 3 Ave</td>\n<td align=\"right\">40.771129270</td>\n<td align=\"right\">-73.957722970</td>\n</tr>\n<tr><td>2 Ave &amp; E 99 St</td>\n<td align=\"right\">40.786258600</td>\n<td align=\"right\">-73.945525790</td>\n<td>3 Ave &amp; E 112 St</td>\n<td align=\"right\">40.795508000</td>\n<td align=\"right\">-73.941606000</td>\n</tr>\n<tr><td>3 St &amp; 3 Ave</td>\n<td align=\"right\">40.675070500</td>\n<td align=\"right\">-73.987752260</td>\n<td>10 St &amp; 7 Ave</td>\n<td align=\"right\">40.666207800</td>\n<td align=\"right\">-73.981998860</td>\n</tr>\n<tr><td>3 St &amp; Prospect Park West</td>\n<td align=\"right\">40.668132000</td>\n<td align=\"right\">-73.973638310</td>\n<td>3 St &amp; Prospect Park West</td>\n<td align=\"right\">40.668132000</td>\n<td align=\"right\">-73.973638310</td>\n</tr>\n<tr><td>6 Ave &amp; W 33 St</td>\n<td align=\"right\">40.749012710</td>\n<td align=\"right\">-73.988483950</td>\n<td>W 37 St &amp; 5 Ave</td>\n<td align=\"right\">40.750380090</td>\n<td align=\"right\">-73.983389880</td>\n</tr>\n<tr><td>8 Ave &amp; W 52 St</td>\n<td align=\"right\">40.763707390</td>\n<td align=\"right\">-73.985161500</td>\n<td>Central Park S &amp; 6 Ave</td>\n<td align=\"right\">40.765909360</td>\n<td align=\"right\">-73.976341510</td>\n</tr>\n<tr><td>11 Ave &amp; W 41 St</td>\n<td align=\"right\">40.760300960</td>\n<td align=\"right\">-73.998842220</td>\n<td>W 34 St &amp; 11 Ave</td>\n<td align=\"right\">40.755941590</td>\n<td align=\"right\">-74.002116300</td>\n</tr>\n<tr><td>12 Ave &amp; W 40 St</td>\n<td align=\"right\">40.760875020</td>\n<td align=\"right\">-74.002776680</td>\n<td>W 42 St &amp; 8 Ave</td>\n<td align=\"right\">40.757569900</td>\n<td align=\"right\">-73.990985070</td>\n</tr>\n<tr><td>Allen St &amp; E Houston St</td>\n<td align=\"right\">40.722055000</td>\n<td align=\"right\">-73.989111000</td>\n<td>Mott St &amp; Prince St</td>\n<td align=\"right\">40.723179580</td>\n<td align=\"right\">-73.994800120</td>\n</tr>\n<tr><td>Allen St &amp; Hester St</td>\n<td align=\"right\">40.716058660</td>\n<td align=\"right\">-73.991907590</td>\n<td>Greenwich St &amp; N Moore St</td>\n<td align=\"right\">40.720434110</td>\n<td align=\"right\">-74.010206090</td>\n</tr>\n<tr><td>Amsterdam Ave &amp; W 73 St</td>\n<td align=\"right\">40.779668090</td>\n<td align=\"right\">-73.980930448</td>\n<td>E 85 St &amp; 3 Ave</td>\n<td align=\"right\">40.778012030</td>\n<td align=\"right\">-73.954071490</td>\n</tr>\n<tr><td>Bank St &amp; Hudson St</td>\n<td align=\"right\">40.736528890</td>\n<td align=\"right\">-74.006180260</td>\n<td>MacDougal St &amp; Prince St</td>\n<td align=\"right\">40.727102580</td>\n<td align=\"right\">-74.002970880</td>\n</tr>\n<tr><td>Bank St &amp; Washington St</td>\n<td align=\"right\">40.736196700</td>\n<td align=\"right\">-74.008592070</td>\n<td>W 4 St &amp; 7 Ave S</td>\n<td align=\"right\">40.734011430</td>\n<td align=\"right\">-74.002938770</td>\n</tr>\n<tr><td>Barclay St &amp; Church St</td>\n<td align=\"right\">40.712912240</td>\n<td align=\"right\">-74.010202340</td>\n<td>Clinton St &amp; Tillary St</td>\n<td align=\"right\">40.696192000</td>\n<td align=\"right\">-73.991218000</td>\n</tr>\n<tr><td>Berkeley Pl &amp; 7 Ave</td>\n<td align=\"right\">40.675146839</td>\n<td align=\"right\">-73.975232095</td>\n<td>West Drive &amp; Prospect Park West</td>\n<td align=\"right\">40.661063372</td>\n<td align=\"right\">-73.979452550</td>\n</tr>\n<tr><td>Bialystoker Pl &amp; Delancey St</td>\n<td align=\"right\">40.716226440</td>\n<td align=\"right\">-73.982612060</td>\n<td>Reade St &amp; Broadway</td>\n<td align=\"right\">40.714504510</td>\n<td align=\"right\">-74.005627890</td>\n</tr>\n<tr><td>Broadway &amp; W 24 St</td>\n<td align=\"right\">40.742354300</td>\n<td align=\"right\">-73.989150760</td>\n<td>South End Ave &amp; Liberty St</td>\n<td align=\"right\">40.711512000</td>\n<td align=\"right\">-74.015756000</td>\n</tr>\n<tr><td>Broadway &amp; W 29 St</td>\n<td align=\"right\">40.746200900</td>\n<td align=\"right\">-73.988557230</td>\n<td>Stanton St &amp; Chrystie St</td>\n<td align=\"right\">40.722293460</td>\n<td align=\"right\">-73.991475350</td>\n</tr>\n<tr><td>Broadway &amp; W 56 St</td>\n<td align=\"right\">40.765265400</td>\n<td align=\"right\">-73.981923380</td>\n<td>Broadway &amp; W 49 St</td>\n<td align=\"right\">40.760683271</td>\n<td align=\"right\">-73.984527290</td>\n</tr>\n<tr><td>Broadway &amp; W 58 St</td>\n<td align=\"right\">40.766953170</td>\n<td align=\"right\">-73.981693330</td>\n<td>5 Ave &amp; E 78 St</td>\n<td align=\"right\">40.776321422</td>\n<td align=\"right\">-73.964273930</td>\n</tr>\n<tr><td>Cadman Plaza E &amp; Red Cross Pl</td>\n<td align=\"right\">40.699917550</td>\n<td align=\"right\">-73.989717730</td>\n<td>Leonard St &amp; Church St</td>\n<td align=\"right\">40.717571000</td>\n<td align=\"right\">-74.005549000</td>\n</tr>\n<tr><td>Cadman Plaza E &amp; Tillary St</td>\n<td align=\"right\">40.695976830</td>\n<td align=\"right\">-73.990148920</td>\n<td>Lawrence St &amp; Willoughby St</td>\n<td align=\"right\">40.692361780</td>\n<td align=\"right\">-73.986317460</td>\n</tr>\n<tr><td>Carmine St &amp; 6 Ave</td>\n<td align=\"right\">40.730385990</td>\n<td align=\"right\">-74.002149880</td>\n<td>W 27 St &amp; 7 Ave</td>\n<td align=\"right\">40.746647000</td>\n<td align=\"right\">-73.993915000</td>\n</tr>\n<tr><td>Central Park W &amp; W 96 St</td>\n<td align=\"right\">40.791270000</td>\n<td align=\"right\">-73.964839000</td>\n<td>W 52 St &amp; 6 Ave</td>\n<td align=\"right\">40.761329831</td>\n<td align=\"right\">-73.979820013</td>\n</tr>\n<tr><td>Central Park West &amp; W 76 St</td>\n<td align=\"right\">40.778967840</td>\n<td align=\"right\">-73.973747370</td>\n<td>Central Park S &amp; 6 Ave</td>\n<td align=\"right\">40.765909360</td>\n<td align=\"right\">-73.976341510</td>\n</tr>\n</tbody></table>\n</div><h2 id=\"step-2-style-a-sexy-map-in-mapbox-studio\">Step 2: Style a Sexy Map in Mapbox Studio</h2><p>Mapbox provides a superb web UI labeled “studio” interface to help us get started. The “studio” web UI is separated into three parts: <strong>custom map styles</strong>, <strong>tilesets</strong>, and <strong>datasets</strong>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mapboxstudio.gif\" class=\"kg-image\"><figcaption>All of these can we saved for later use.</figcaption></figure><p>These three sections can be summarized as:</p><ul><li><strong>Styles: </strong>Custom map styles editable via a GUI, which produce a stylesheet for convenience </li><li><strong>Tilesets: </strong>Map overlays we can apply from our own data or otherwise to segment geographical areas</li><li><strong>Datasets:</strong> Data containing anything from points on a map to complex direction routes we can overlay atop our map.</li></ul><p>Here's a quick look at the Map style editor:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/Screen-Shot-2018-12-07-at-4.29.43-PM.png\" class=\"kg-image\"><figcaption>I want to wake up, in a city that doesn't sleep.</figcaption></figure><p>Save your styled map once you find it to be adequately attractive. We'll need it for later.</p><h2 id=\"step-4-start-a-flask-app\">Step 4: Start a Flask App</h2><p>Of course we're making a Flask app; is there even any other kind? We'll be using the Flask Application Factory setup as we usually do, so we should end up with a file structure as below. If you feel like you're getting ahead of ourself, checkout our post on <a href=\"https://hackersandslackers.com/structuring-your-flask-app/\">structuring Flask applications</a>.</p><pre><code class=\"language-bash\">mapbox-app\n├── /application\n│   └── __init__.py\n├── /datasets\n│   ├── data.json\n│   └── output.csv   \n├── /maps\n│   ├── __init__.py\n│   ├── /templates\n│   │    └── index.html\n│   ├── views.py\n│   └── plots.py\n├── start.sh\n├── settings.py\n├── wsgi.py\n├── Pipfile\n├── README.md\n└── requirements.txt\n</code></pre>\n<p>To mix things up a bit we'll use a shell script this time to handle envars and running our script. Start by creating <strong>start.sh</strong>:</p><pre><code class=\"language-bash\"># start.sh\n\nexport FLASK_APP=wsgi.py\nexport FLASK_DEBUG=1\nexport APP_CONFIG_FILE=settings.py\nflask run\n</code></pre>\n<p>Yes, we'll be using <code>settings.py</code> as our config file for a change. Ahhh, just like the Django days. This file should contain a Mapbox <strong>access token</strong>. Mapbox provides you with a public token by default in many of its tutorials (noted by the <em><strong>pk</strong> </em>prefix for <em>'public key' - </em>contrast this with <strong><em>sk</em></strong> for <em>'secret key'</em>). If you'd like to do anything meaningful with Mapbox, you'll have to retrieve a secret key via the UI. Then we can add this token to <code>settings.py</code> as such:</p><pre><code class=\"language-bash\">MAPBOX_ACCESS_TOKEN=&quot;sk.eyJ1IB&amp;F^&amp;f^R&amp;DFRUYFTRUctyrcTYRUFrtCFTYDYTuEg&quot;\n</code></pre>\n<p>Finally, here's a look at <code>application/__init__.py</code> just to make sure we're on the same page:</p><pre><code class=\"language-python\"># application/__init__.py\n\nimport os\nfrom flask import Flask, g\n\ndef create_app():\n    &quot;&quot;&quot;Construct the core application.&quot;&quot;&quot;\n    app = Flask(__name__)\n    app.config.from_envvar('APP_CONFIG_FILE', silent=True)\n\n    with app.app_context():\n        # Construct map blueprint\n        from maps import mapviews\n        app.register_blueprint(mapviews.map_blueprint)\n\n        return app\n</code></pre>\n<h2 id=\"step-5-create-a-blueprint-for-your-map\">Step 5: Create a Blueprint for Your Map</h2><p>You may have noticed we registered this Blueprint in the previous step. Create a <code>/maps</code> directory which we'll set as a module; we'll need this to handle the <strong>view</strong>, <strong>model </strong>(or just data),  and <strong>controller </strong>(routes.py as seen below).</p><h3 id=\"routes-py\">routes.py</h3><pre><code class=\"language-python\">import os\nfrom flask import Blueprint, render_template, request\nfrom flask import current_app as app\nfrom . import locations\n\nmap_blueprint = Blueprint('map', __name__, template_folder='templates', static_folder='static')\nplot_locations = locations.LocationData()\n\n\n# Landing Page\n@map_blueprint.route('/', methods=['GET'])\ndef map():\n    return render_template('index.html', ACCESS_KEY=app.MAPBOX_ACCESS_KEY,  locations=plot_locations.get_plots, title=&quot;CitiBike Mapbox App.&quot;)\n\n</code></pre>\n<h3 id=\"templates-index-html\">templates/index.html</h3><pre><code class=\"language-html\">&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta charset='utf-8' /&gt;\n  &lt;title&gt;{{title}}&lt;/title&gt;\n  &lt;meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' /&gt;\n  &lt;script src='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.js'&gt;&lt;/script&gt;\n  &lt;link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.css' rel='stylesheet' /&gt;\n  &lt;style&gt;\n    body { margin:0; padding:0; }\n    #map { position:absolute; top:0; bottom:0; width:100%; }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;div id='map'&gt;&lt;/div&gt;\n&lt;script&gt;\nmapboxgl.accessToken = {{MAPBOX_ACCESS_TOKEN}};\nconst map = new mapboxgl.Map({\n  container: 'map',\n  style: 'mapbox://styles/toddbirchard/cjpij1gfhghiy2spetf5w998w',\n  center: [-73.981856, 40.703820],\n  zoom: 11.1,\n\n});\n\nmap.on('load', function(e) {\n  // Add the data to your map as a layer\n  map.addLayer({\n    id: 'locations',\n    type: 'symbol',\n    // Add a GeoJSON source containing place coordinates and information.\n    source: {\n      type: 'geojson',\n      data: {{locations}}\n    },\n    layout: {\n      'icon-image': 'restaurant-15',\n      'icon-allow-overlap': true,\n    }\n  });\n});\n&lt;/script&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h3 id=\"data-py\">data.py</h3><p>Normally this is where we'd use the magic of the Mapbox API to get coordinates, route objects, or whatever it is your heart hopes to plot. This is intended to be intro post, so let's break that logic out for another time and use a dataset Mapbox would be happy to receive for the sake of results.</p><h2 id=\"step-6-uploading-our-dataset-via-mapbox-studio\">Step 6: Uploading our Dataset via Mapbox Studio</h2><p>Mapbox graciously lets us upload our data via their Studio UI, which does the unthinkable; immediately upon upload, Mapbox will take the data we give it (whether it be CSV, GeoJSON, etc) and immediately parse it in a way that makes sense. Upload your dataset at <a href=\"https://www.mapbox.com/studio/datasets/\">https://www.mapbox.com/studio/datasets/</a>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/Screen-Shot-2018-12-10-at-5.29.24-PM.png\" class=\"kg-image\"><figcaption>Uploading the raw data of our Citibike CSV.</figcaption></figure><p>Next, Mapbox shows us a preview of our data before we even know what happened:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/Screen-Shot-2018-12-10-at-5.28.24-PM.png\" class=\"kg-image\"><figcaption>It's like they don't even want us to do work.</figcaption></figure><h2 id=\"step-7-do-it-in-flask\">Step 7: Do It in Flask</h2><p>After uploading your dataset via mapbox studio, you can actually <em>redownload</em> the data with a subtle twist: your data will be automatically formatted as GeoJSON: the format of JSON objects Mapbox uses to plot points, draw routes, etc.</p><p>Since we've had a long day, I'll allow you to download this pre-formatted data and hardcore the values in to your Map view. You're getting off easy for now, but next time we're doing this programmatically ;).</p><pre><code class=\"language-html\">&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset='utf-8' /&gt;\n  &lt;title&gt;{{title}}&lt;/title&gt;\n  &lt;meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' /&gt;\n  &lt;script src='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.js'&gt;&lt;/script&gt;\n  &lt;link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.49.0/mapbox-gl.css' rel='stylesheet' /&gt;\n  &lt;style&gt;\n    body {\n      margin: 0;\n      padding: 0;\n    }\n\n    #map {\n      position: absolute;\n      top: 0;\n      bottom: 0;\n      width: 100%;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n  &lt;div id='map'&gt;&lt;/div&gt;\n  &lt;script&gt;\n    mapboxgl.accessToken = '{{ACCESS_KEY}}';\n    const map = new mapboxgl.Map({\n      container: 'map',\n      style: 'mapbox://styles/toddbirchard/cjpij1oxl3hiy2spetf5w998w',\n      center: [-73.981856, 40.703820],\n      zoom: 11.1,\n\n    });\n\n\n    map.addLayer({\n      &quot;id&quot;: &quot;points&quot;,\n      &quot;type&quot;: &quot;symbol&quot;,\n      &quot;source&quot;: {\n        &quot;features&quot;: [{\n            &quot;type&quot;: &quot;Feature&quot;,\n            &quot;properties&quot;: {\n              &quot;start_station_name&quot;: &quot;Central Park West &amp; W 76 St&quot;,\n              &quot;end_station_name&quot;: &quot;Central Park S &amp; 6 Ave&quot;,\n              &quot;end_station_latitude&quot;: &quot;40.76590936&quot;,\n              &quot;end_station_longitude&quot;: &quot;-73.97634151&quot;\n            },\n            &quot;geometry&quot;: {\n              &quot;coordinates&quot;: [\n                -73.973747,\n                40.778967\n              ],\n              &quot;type&quot;: &quot;Point&quot;\n            },\n            &quot;id&quot;: &quot;000a1f944d4dd786d9e7ed04620af02b&quot;\n          },\n          {\n            &quot;type&quot;: &quot;Feature&quot;,\n            &quot;properties&quot;: {\n              &quot;start_station_name&quot;: &quot;W 64 St &amp; West End Ave&quot;,\n              &quot;end_station_name&quot;: &quot;W 70 St &amp; Amsterdam Ave&quot;,\n              &quot;end_station_latitude&quot;: &quot;40.77748046&quot;,\n              &quot;end_station_longitude&quot;: &quot;-73.98288594&quot;\n            },\n            &quot;geometry&quot;: {\n              &quot;coordinates&quot;: [\n                -73.987537,\n                40.774528\n              ],\n              &quot;type&quot;: &quot;Point&quot;\n            },\n            &quot;id&quot;: &quot;01d8c19524f067a3f4712653265e0a49&quot;\n          },\n          {\n            &quot;type&quot;: &quot;Feature&quot;,\n            &quot;properties&quot;: {\n              &quot;start_station_name&quot;: &quot;E 20 St &amp; FDR Drive&quot;,\n              &quot;end_station_name&quot;: &quot;W 13 St &amp; 7 Ave&quot;,\n              &quot;end_station_latitude&quot;: &quot;40.73781509&quot;,\n              &quot;end_station_longitude&quot;: &quot;-73.99994661&quot;\n            },\n            &quot;geometry&quot;: {\n              &quot;coordinates&quot;: [\n                -73.975738,\n                40.733142\n              ],\n              &quot;type&quot;: &quot;Point&quot;\n            },\n            &quot;id&quot;: &quot;038ac5403b136e34874a7278f64d4e95&quot;\n          },\n          {\n              \\\\ --------------------------------------\n              (etc etc....)\n               \\\\ --------------------------------------\n          },\n          {\n            &quot;type&quot;: &quot;Feature&quot;,\n            &quot;properties&quot;: {\n              &quot;start_station_name&quot;: &quot;Mercer St &amp; Bleecker St&quot;,\n              &quot;end_station_name&quot;: &quot;1 Ave &amp; E 30 St&quot;,\n              &quot;end_station_latitude&quot;: &quot;40.74144387&quot;,\n              &quot;end_station_longitude&quot;: &quot;-73.97536082&quot;\n            },\n            &quot;geometry&quot;: {\n              &quot;coordinates&quot;: [\n                -73.996621,\n                40.727063\n              ],\n              &quot;type&quot;: &quot;Point&quot;\n            },\n            &quot;id&quot;: &quot;ff1daf9aadbf0cd6b788bd76f0a3f333&quot;\n          }\n        ],\n        &quot;type&quot;: &quot;FeatureCollection&quot;\n      },\n      &quot;layout&quot;: {\n        &quot;icon-image&quot;: &quot;{icon}-15&quot;,\n        &quot;text-field&quot;: &quot;{title}&quot;,\n        &quot;text-font&quot;: [&quot;Open Sans Semibold&quot;, &quot;Arial Unicode MS Bold&quot;],\n        &quot;text-offset&quot;: [0, 0.6],\n        &quot;text-anchor&quot;: &quot;top&quot;\n      }\n    });\n  &lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre>\n<h2 id=\"uncharted-territory\">Uncharted Territory</h2><p>There's way more for us to explore in Mapbox. Stay tuned for the rest of this series as we explore generating GeoData programmatically, and build interactive applications to really get users involved in map data by letting them control constraints such as time, etc. </p>","url":"https://hackersandslackers.com/map-data-visualization-with-mapbox/","uuid":"3b64bf8d-b545-469c-b2df-83ee7a816e31","page":false,"codeinjection_foot":"","codeinjection_head":"  <script src='https://api.tiles.mapbox.com/mapbox-gl-js/v0.51.0/mapbox-gl.js'></script>\n    <link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.51.0/mapbox-gl.css' rel='stylesheet' />","comment_id":"5c0aafba5da6c4479ab70ff1"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673740","title":"The Many Faces and Filetypes of Python Configs","slug":"simplify-your-python-projects-configuration","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/config@2x.jpg","excerpt":"Cleverly (or uncleverly) configure your Python project using .ini, .yaml, or .env files.","custom_excerpt":"Cleverly (or uncleverly) configure your Python project using .ini, .yaml, or .env files.","created_at_pretty":"29 November, 2018","published_at_pretty":"29 November, 2018","updated_at_pretty":"27 December, 2018","created_at":"2018-11-29T02:24:13.000-05:00","published_at":"2018-11-29T16:40:26.000-05:00","updated_at":"2018-12-26T23:23:54.000-05:00","meta_title":"Simplify Your Python Project Configuration | Hackers and Slackers","meta_description":"Cleverly (or uncleverly) configure your Python project using .ini, .yaml, or .env files. Adopt a feel for when certain files or classes fit your needs best.","og_description":"Cleverly (or uncleverly) configure your Python project using .ini, .yaml, or .env files. Adopt a feel for when certain files or classes fit your needs best.","og_image":"https://hackersandslackers.com/content/images/2018/11/config@2x.jpg","og_title":"The Many Faces and Files of Python Configs","twitter_description":"Cleverly (or uncleverly) configure your Python project using .ini, .yaml, or .env files. Adopt a feel for when certain files or classes fit your needs best.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/config@2x.jpg","twitter_title":"The Many Faces and Files of Python Configs","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Software Development","slug":"software-development","description":"General software development principals and tools. Receive insights applicable to building any application.","feature_image":null,"meta_description":"General software development principals and tools. Receive insights applicable to building any application.","meta_title":"Software Development | Hackers and Slackers","visibility":"public"}],"plaintext":"As we cling harder and harder to Dockerfiles, Kubernetes, or any modern\npreconfigured app environment, our dependency on billable boilerplate grows.\nWhether or not that is a problem is a conversation in itself. The longer I keep\nmy projects self-hosted, the more  I'm consumed by the open-ended approaches\npeople take to manage their project configuration variables.\n\nFull disclosure here: this post is probably about as boring as where you see\nthis heading. Today, I'm here to talk about Python Environment and general\nconfiguration variable handling.\n\nPick Your Poison\nSomeday, each and every one of us will die. I'm referring of course to the part\ninside of us that slowly withers away as we're forced to maintain projects we've\nhanded off. We can do our best to avoid these situations by isolating the\nvariables most subject to change in separate, easy-to-edit files for Person\nNumber 2 to pick up on.\n\nOption 1: Project Config via .ini Files\n.ini  files are simple, making them perfect for simple projects- especially\nthose to be handled by others why may not have development backgrounds. These\nare configuration files with a single-level hierarchy:\n\n[GLOBAL]\nPROJECT: Fake Example Project\nREGION: us-east-1\nINPUT_FOLDER: data/zip/\nOUTPUT_FOLDER: data/output/\nTIMEOUT: 200\nMEMORY: 512\n\n[PROD]\nDATABASE = postgresql://loser:weakpassword@localhost:5432/mydatabase\nENDPOINT = https://production.endpoint.example.com\nUSER = PROD_USERNAME\n\n[DEV]\nDATABASE = postgresql://loser:weakpassword@localhost:5432/mydatabase\nENDPOINT = https://dev.endpoint.example.com\nUSER = DEV_USERNAME\n\n\nAnother example, for instance, may be to specify AWS Services:\n\n[S3]\nBUCKET_NAME: public-bucket\nBUCKET_FOLDER: /\n\n[RDS]\nNAME: rds/prod/sensitivedata\nARN: arn:aws:rds:us-east-1:66574567568434896:secret:rds/prod/peopledata-ZvJ3Ys\nREGION: us-east-1\n\n[LAMBDA]\nFUNCTION_NAME: handler\nHANDLER: lambda.handler\nDESCRIPTION: Performs a task every now and then.\nRUNTIME: python3.7\nROLE: lambda_role\nDIST_FOLDER: lambda/dist\n\n[SECRETS]\nSECRET_NAME: rds/prod/totallysecret\nSECRET_ARN: arn:aws:secretsmanager:us-east-1:769979969:secret:rds/prod/stupidproject-5647\n\n\n.ini  files are handled in Python by the configparser  library; this is our way\nof doing something with the essentially static text in these files. Since we're\nkeeping vars separate from app source code, we now need to create a file and a\nclass which exists merely to access these values.\n\nCreating a Python Class to Extract Variables\nInstead of explicitly hardcoding a dump of all variables, we're going to create\na class that provides an easy syntax for accessing variables on demand. Check it\nout:\n\n# config_loader.py\nfrom configparser import SafeConfigParser\nimport os\n\n\nclass Config:\n    \"\"\"Interact with configuration variables.\"\"\"\n\n    configParser = SafeConfigParser()\n    configFilePath = (os.path.join(os.getcwd(), 'config.ini'))\n\n    @classmethod\n    def initialize(cls, newhire_table):\n        \"\"\"Start config by reading config.ini.\"\"\"\n        cls.configParser.read(cls.configFilePath)\n\n    @classmethod\n    def prod(cls, key):\n        \"\"\"Get prod values from config.ini.\"\"\"\n        return cls.configParser.get('PROD', key)\n\n    @classmethod\n    def dev(cls, key):\n        \"\"\"Get dev values from config.ini.\"\"\"\n        return cls.configParser.get('DEV', key)\n\n\nThis simple class goes a long way to simplify grabbing variables. The class\nnever needs to be instantiated, so we can import Config  wherever we please and\nimmediately start pulling values.\n\nTo separate variables by concern, each block in config.ini  receives its own\nclass method. Now retrieving the proper variables is as simple as \nConfig.prod('DATABASE')  will return the URI for a production database. Easy to\nuse, simple to understand.\n\nOption 2: Complex YAML Configurations\nUnless you're developing apps in isolation in an isolated third-world nation or\nunder a dictatorship which blocks internet access, you already know that .yaml \nfiles are all the rage when it comes to storing static values in text files\n(wow, this really is  an obscure topic for a post).\n\nYAML  files provide plenty of upsides to alternative file types. Where .ini \nfiles are simply grouped variables, YAML  provides a hierarchy structure. This\nmakes YAML files much easier to understand and maintain for larger applications,\nas some variables only make sense in the context of being a sub-variable (?).\n\nCheck out what a sample YAML config might look like:\n\n---\n\nappName: appName\nlogLevel: WARN\n\nAWS:\n    Region: us-east-1\n    Resources:\n      EC2: \n        Type: \"AWS::EC2::Instance\"\n        Properties: \n          ImageId: \"ami-0ff8a91507f77f867\"\n          InstanceType: t2.micro\n          KeyName: testkey\n          BlockDeviceMappings:\n            -\n              DeviceName: /dev/sdm\n              Ebs:\n                VolumeType: io1\n                Iops: 200\n                DeleteOnTermination: false\n                VolumeSize: 20\n      Lambda:\n          Type: \"AWS::Lambda::Function\"\n          Properties: \n            Handler: \"index.handler\"\n            Role: \n              Fn::GetAtt: \n                - \"LambdaExecutionRole\"\n                - \"Arn\"\n            Runtime: \"python3.7\"\n            Timeout: 25\n            TracingConfig:\n              Mode: \"Active\"\n\nroutes:\n  admin:\n    url: /admin\n    template: admin.html\n    assets:\n        templates: /templates\n        static: /static\n  dashboard:\n    url: /dashboard\n    template: dashboard.html\n    assets:\n        templates: /templates\n        static: /static\n  account:\n    url: /account\n    template: account.html\n    assets:\n        templates: /templates\n        static: /static\n        \ndatabases:\n  cassandra:\n    host: example.cassandra.db\n    username: user\n    password: password\n  redshift:\n    jdbcURL: jdbc:redshift://<IP>:<PORT>/file?user=username&password=pass\n    tempS3Dir: s3://path/to/redshift/temp/dir/ \n  redis:\n    host: hostname\n    port: port-number\n    auth: authentication\n    db: database\n\n\nThis would read horribly if we tried to fit this in an .ini  file. A more fair\ncomparison would be to JSON  configurations: JSON objects indeed share the same\nhierarchy advantages of YAML, but JSON syntax is prone to errors and unhelpful\nerror messages, thanks to being a brainchild of Old Man JavaScript. YAML doesn't\ncare if you open and close with brackets, use double quotes, or leave a trailing\ncomma. All of these stupid things are why I prefer Python.\n\nParsing YAML in Python\nI recommend the Python Confuse library [https://github.com/sampsyo/confuse]  (a\npackage name that's sure to raise some eyebrows by your company's information\nsecurity team).\n\nConfuse  allows use to interact with YAML files almost identically to how we\nwould with JSON, with the exception that we specify .get()  at the end of\nwalking through the tree hierarchy, like so:\n\nconfig = confuse.Configuration('MyApp', __name__)\n\nconfig['AWS']['Lambda']['Runtime'].get()\n\n\n.get()  can accept a datatype value such as int. Doing so ensures that the value\nwe're getting is actually of the schema we're expecting, which is a neat\nfeature.\n\nValidators\nConfuse's documentation [https://confuse.readthedocs.io/en/latest/]details\nadditional validation methods for values we pull from YAML files. Methods like \nas_filename(), as_number(), and as_str_seq()  do basically what you'd expect\nthem to.\n\nCLI Configuration\nConfuse also gets into the realm of building CLIs, allowing use to use our YAML\nfile to inform arguments which can be passed to a CLI and their potential\nvalues:\n\nconfig = confuse.Configuration('myapp')\nparser = argparse.ArgumentParser()\nparser.add_argument('--foo', help='a parameter')\nargs = parser.parse_args()\nconfig.set_args(args)\nprint(config['foo'].get())\n\n\nThere's plenty of things you can go nuts with here.\n\nOption 3: Using .env Config Files\nLastly, we can leverage the already well-known .env  format to set variables.\nWorking this way is pretty equivalent to working with .ini  files, but we're\nhuman beings so we're stupid and do things like build the same protocols over\nand over. In .env, we get to store beautiful values such as these:\n\nCONFIG_PATH=${HOME}/.config/foo\nDOMAIN=example.org\nEMAIL=admin@${DOMAIN}\n\nTo read these values, we'll be using the python-dotenv library\n[https://github.com/theskumar/python-dotenv]. This gets you started:\n\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nload_dotenv(verbose=True)\n\nenv_path = Path('.') / '.env'\nload_dotenv(dotenv_path=env_path)\n\n\nAfter that, it's a matter of setting variables in Python to values you extract\nfrom .env:\n\nimport os\nSECRET_KEY = os.getenv(\"EMAIL\")\nDATABASE_PASSWORD = os.getenv(\"DATABASE_PASSWORD\")\n\n\nSo Yeah, Basically Just Use What You Want\nClearly there are plenty of ways to set environment and project variables in\nPython. We could spend all day investigating the nuances of each and how their\naccompanying Python configuration class should be structured, but we've got apps\nto build. \n\nBesides, I need to go reflect on my life after writing a thousand words about\nloading variables in Python.","html":"<p>As we cling harder and harder to Dockerfiles, Kubernetes, or any modern preconfigured app environment, our dependency on billable boilerplate grows. Whether or not that is a problem is a conversation in itself. The longer I keep my projects self-hosted, the more  I'm consumed by the open-ended approaches people take to manage their project configuration variables.</p><p>Full disclosure here: this post is probably about as boring as where you see this heading. Today, I'm here to talk about Python Environment and general configuration variable handling.</p><h2 id=\"pick-your-poison\">Pick Your Poison</h2><p>Someday, each and every one of us will die. I'm referring of course to the part inside of us that slowly withers away as we're forced to maintain projects we've handed off. We can do our best to avoid these situations by isolating the variables most subject to change in separate, easy-to-edit files for Person Number 2 to pick up on.</p><h2 id=\"option-1-project-config-via-ini-files\">Option 1: Project Config via .ini Files</h2><p><code>.ini</code> files are simple, making them perfect for simple projects- especially those to be handled by others why may not have development backgrounds. These are configuration files with a single-level hierarchy:</p><pre><code class=\"language-ini\">[GLOBAL]\nPROJECT: Fake Example Project\nREGION: us-east-1\nINPUT_FOLDER: data/zip/\nOUTPUT_FOLDER: data/output/\nTIMEOUT: 200\nMEMORY: 512\n\n[PROD]\nDATABASE = postgresql://loser:weakpassword@localhost:5432/mydatabase\nENDPOINT = https://production.endpoint.example.com\nUSER = PROD_USERNAME\n\n[DEV]\nDATABASE = postgresql://loser:weakpassword@localhost:5432/mydatabase\nENDPOINT = https://dev.endpoint.example.com\nUSER = DEV_USERNAME\n</code></pre>\n<p>Another example, for instance, may be to specify AWS Services:</p><pre><code class=\"language-ini\">[S3]\nBUCKET_NAME: public-bucket\nBUCKET_FOLDER: /\n\n[RDS]\nNAME: rds/prod/sensitivedata\nARN: arn:aws:rds:us-east-1:66574567568434896:secret:rds/prod/peopledata-ZvJ3Ys\nREGION: us-east-1\n\n[LAMBDA]\nFUNCTION_NAME: handler\nHANDLER: lambda.handler\nDESCRIPTION: Performs a task every now and then.\nRUNTIME: python3.7\nROLE: lambda_role\nDIST_FOLDER: lambda/dist\n\n[SECRETS]\nSECRET_NAME: rds/prod/totallysecret\nSECRET_ARN: arn:aws:secretsmanager:us-east-1:769979969:secret:rds/prod/stupidproject-5647\n</code></pre>\n<p><code>.ini</code> files are handled in Python by the <strong>configparser</strong> library; this is our way of doing something with the essentially static text in these files. Since we're keeping vars separate from app source code, we now need to create a file and a class which exists merely to access these values.</p><h3 id=\"creating-a-python-class-to-extract-variables\">Creating a Python Class to Extract Variables</h3><p>Instead of explicitly hardcoding a dump of all variables, we're going to create a class that provides an easy syntax for accessing variables on demand. Check it out:</p><pre><code class=\"language-python\"># config_loader.py\nfrom configparser import SafeConfigParser\nimport os\n\n\nclass Config:\n    &quot;&quot;&quot;Interact with configuration variables.&quot;&quot;&quot;\n\n    configParser = SafeConfigParser()\n    configFilePath = (os.path.join(os.getcwd(), 'config.ini'))\n\n    @classmethod\n    def initialize(cls, newhire_table):\n        &quot;&quot;&quot;Start config by reading config.ini.&quot;&quot;&quot;\n        cls.configParser.read(cls.configFilePath)\n\n    @classmethod\n    def prod(cls, key):\n        &quot;&quot;&quot;Get prod values from config.ini.&quot;&quot;&quot;\n        return cls.configParser.get('PROD', key)\n\n    @classmethod\n    def dev(cls, key):\n        &quot;&quot;&quot;Get dev values from config.ini.&quot;&quot;&quot;\n        return cls.configParser.get('DEV', key)\n</code></pre>\n<p>This simple class goes a long way to simplify grabbing variables. The class never needs to be instantiated, so we can <code>import Config</code> wherever we please and immediately start pulling values.</p><p>To separate variables by concern, each block in <code>config.ini</code> receives its own class method. Now retrieving the proper variables is as simple as <code>Config.prod('DATABASE')</code> will return the URI for a production database. Easy to use, simple to understand.</p><h2 id=\"option-2-complex-yaml-configurations\">Option 2: Complex YAML Configurations</h2><p>Unless you're developing apps in isolation in an isolated third-world nation or under a dictatorship which blocks internet access, you already know that <code>.yaml</code> files are all the rage when it comes to storing static values in text files (wow, this really <em>is</em> an obscure topic for a post).</p><p><strong>YAML</strong> files provide plenty of upsides to alternative file types. Where <strong>.ini</strong> files are simply grouped variables, <strong>YAML</strong> provides a hierarchy structure. This makes YAML files much easier to understand and maintain for larger applications, as some variables only make sense in the context of being a sub-variable (?).</p><p>Check out what a sample YAML config might look like:</p><pre><code class=\"language-yaml\">---\n\nappName: appName\nlogLevel: WARN\n\nAWS:\n    Region: us-east-1\n    Resources:\n      EC2: \n        Type: &quot;AWS::EC2::Instance&quot;\n        Properties: \n          ImageId: &quot;ami-0ff8a91507f77f867&quot;\n          InstanceType: t2.micro\n          KeyName: testkey\n          BlockDeviceMappings:\n            -\n              DeviceName: /dev/sdm\n              Ebs:\n                VolumeType: io1\n                Iops: 200\n                DeleteOnTermination: false\n                VolumeSize: 20\n      Lambda:\n          Type: &quot;AWS::Lambda::Function&quot;\n          Properties: \n            Handler: &quot;index.handler&quot;\n            Role: \n              Fn::GetAtt: \n                - &quot;LambdaExecutionRole&quot;\n                - &quot;Arn&quot;\n            Runtime: &quot;python3.7&quot;\n            Timeout: 25\n            TracingConfig:\n              Mode: &quot;Active&quot;\n\nroutes:\n  admin:\n    url: /admin\n    template: admin.html\n    assets:\n        templates: /templates\n        static: /static\n  dashboard:\n    url: /dashboard\n    template: dashboard.html\n    assets:\n        templates: /templates\n        static: /static\n  account:\n    url: /account\n    template: account.html\n    assets:\n        templates: /templates\n        static: /static\n        \ndatabases:\n  cassandra:\n    host: example.cassandra.db\n    username: user\n    password: password\n  redshift:\n    jdbcURL: jdbc:redshift://&lt;IP&gt;:&lt;PORT&gt;/file?user=username&amp;password=pass\n    tempS3Dir: s3://path/to/redshift/temp/dir/ \n  redis:\n    host: hostname\n    port: port-number\n    auth: authentication\n    db: database\n</code></pre>\n<p>This would read horribly if we tried to fit this in an <strong>.ini</strong> file. A more fair comparison would be to <strong>JSON</strong> configurations: JSON objects indeed share the same hierarchy advantages of YAML, but JSON syntax is prone to errors and unhelpful error messages, thanks to being a brainchild of Old Man JavaScript. YAML doesn't care if you open and close with brackets, use double quotes, or leave a trailing comma. All of these stupid things are why I prefer Python.</p><h3 id=\"parsing-yaml-in-python\">Parsing YAML in Python</h3><p>I recommend the <a href=\"https://github.com/sampsyo/confuse\">Python <em>Confuse</em> library</a> (a package name that's sure to raise some eyebrows by your company's information security team).</p><p><strong>Confuse</strong> allows use to interact with YAML files almost identically to how we would with JSON, with the exception that we specify <code>.get()</code> at the end of walking through the tree hierarchy, like so:</p><pre><code class=\"language-python\">config = confuse.Configuration('MyApp', __name__)\n\nconfig['AWS']['Lambda']['Runtime'].get()\n</code></pre>\n<p><strong>.get()</strong> can accept a datatype value such as <em>int. </em>Doing so ensures that the value we're getting is actually of the schema we're expecting, which is a neat feature.</p><h4 id=\"validators\">Validators</h4><p><a href=\"https://confuse.readthedocs.io/en/latest/\">Confuse's documentation </a>details additional validation methods for values we pull from YAML files. Methods like <code>as_filename()</code>, <code>as_number()</code>, and <code>as_str_seq()</code> do basically what you'd expect them to.</p><h4 id=\"cli-configuration\">CLI Configuration</h4><p>Confuse also gets into the realm of building CLIs, allowing use to use our YAML file to inform arguments which can be passed to a CLI and their potential values:</p><pre><code class=\"language-python\">config = confuse.Configuration('myapp')\nparser = argparse.ArgumentParser()\nparser.add_argument('--foo', help='a parameter')\nargs = parser.parse_args()\nconfig.set_args(args)\nprint(config['foo'].get())\n</code></pre>\n<p>There's plenty of things you can go nuts with here.</p><h2 id=\"option-3-using-env-config-files\">Option 3: Using .env Config Files</h2><p>Lastly, we can leverage the already well-known <code>.env</code> format to set variables. Working this way is pretty equivalent to working with <strong>.ini</strong> files, but we're human beings so we're stupid and do things like build the same protocols over and over. In <strong>.env</strong>, we get to store beautiful values such as these:</p><pre><code>CONFIG_PATH=${HOME}/.config/foo\nDOMAIN=example.org\nEMAIL=admin@${DOMAIN}</code></pre><p>To read these values, we'll be using the <a href=\"https://github.com/theskumar/python-dotenv\"><strong>python-dotenv</strong> library</a>. This gets you started:</p><pre><code class=\"language-python\">from dotenv import load_dotenv\nfrom pathlib import Path\n\nload_dotenv(verbose=True)\n\nenv_path = Path('.') / '.env'\nload_dotenv(dotenv_path=env_path)\n</code></pre>\n<p>After that, it's a matter of setting variables in Python to values you extract from <code>.env</code>:</p><pre><code class=\"language-python\">import os\nSECRET_KEY = os.getenv(&quot;EMAIL&quot;)\nDATABASE_PASSWORD = os.getenv(&quot;DATABASE_PASSWORD&quot;)\n</code></pre>\n<h2 id=\"so-yeah-basically-just-use-what-you-want\">So Yeah, Basically Just Use What You Want</h2><p>Clearly there are plenty of ways to set environment and project variables in Python. We could spend all day investigating the nuances of each and how their accompanying Python configuration class should be structured, but we've got apps to build. </p><p>Besides, I need to go reflect on my life after writing a thousand words about loading variables in Python.</p>","url":"https://hackersandslackers.com/simplify-your-python-projects-configuration/","uuid":"48fb64b7-b9f2-4605-ac1f-c1c45ffc5964","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bff941deae98c3b9d4c25f4"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673734","title":"Scraping Data on the Web with BeautifulSoup","slug":"scraping-urls-with-beautifulsoup","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","excerpt":"The honest act of systematically stealing data without permission.","custom_excerpt":"The honest act of systematically stealing data without permission.","created_at_pretty":"11 November, 2018","published_at_pretty":"11 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-11T04:53:44.000-05:00","published_at":"2018-11-11T08:35:09.000-05:00","updated_at":"2019-01-05T13:21:06.000-05:00","meta_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","meta_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","og_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","og_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","twitter_description":"Using Python's BeautifulSoup library to scrape the web. This tutorial covers scraping links for metadata to generate link previews.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/beauitfulsoup2@2x.jpg","twitter_title":"Scraping URLs with BeautifulSoup | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"Flask","slug":"flask","description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/flaskroutes-2-1_o.jpg","meta_description":"All things Flask ranging from core framework to all conceivable libraries. Tips on how to utilize Flask’s flexibility to create expressive applications.","meta_title":"Building Python Apps in Flask | Hackers and Slackers","visibility":"public"}],"plaintext":"There are plenty of reliable and open sources of data on the web. Datasets are\nfreely released to the public domain by the likes of Kaggle, Google Cloud, and\nof course local & federal government. Like most things free and open, however,\nfollowing the rules to obtain public data can be a bit... boring. I'm not\nsuggesting we go and blatantly break some grey-area laws by stealing data, but\nthis blog isn't exactly called People Who Play It Safe And Slackers, either. \n\nMy personal Python roots can actually be traced back to an ambitious\nside-project: to aggregate all new music from across the web and deliver it the\nmasses. While that project may have been abandoned (after realizing it already\nexisted), BeautifulSoup  was more-or-less my first ever experience with Python. \n\nThe Tool(s) for the Job(s)\nBefore going any further, we'd be ill-advised to not at least mention Python's\nother web-scraping behemoth, Scrapy [https://scrapy.org/]. BeautifulSoup  and \nScrapy  have two very different agendas. BeautifulSoup is intended to parse or\nextract data one page at a time, with each page being served up via the requests \n library or equivalent. Scrapy,  on the other hand, is for creating crawlers: or\nrather absolute monstrosities unleashed upon the web like a swarm, loosely\nfollowing links and haste-fully grabbing data where data exists to be grabbed.\nTo put this in perspective, Google Cloud functions will not even let you import\nScrapy as a usable library.\n\nThis isn't to say that BeautifulSoup  can't be made into a similar monstrosity\nof its own. For now, we'll focus on a modest task: generating link previews for\nURLs by grabbing their metadata.\n\nStep 1: Stalk Your Prey\nBefore we steal any data, we should take a look at the data we're hoping to\nsteal.\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape URLs to generate previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n\n\nThe above is the minimum needed to retrieve the DOM structure of an HTML page. \nBeautifulSoup  accepts the .content  output from a request, from which we can\ninvestigate the contents.\n\nUsing BeauitfulSoup will often result in different results for your scaper than\nyou might see as a human, such as 403 errors or blocked content. An easy way\naround this faking your headers into looking like normal browser agents, as we\ndo here: \nheaders.update({\n'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101\nFirefox/52.0',\n})`The result of print(soup.prettify())  will predictably output a \"pretty\" printed\nversion of your target DOM structure:\n\n<html class=\"gr__example_com\"><head>\n    <title>Example Domain</title>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta property=\"og:site_name\" content=\"Example dot com\">\n    <meta property=\"og:type\" content=\"website\">\n    <meta property=\"og:title\" content=\"Example\">\n    <meta property=\"og:description\" content=\"An Example website.\">\n    <meta property=\"og:image\" content=\"http://example.com/img/image.jpg\">\n    <meta name=\"twitter:title\" content=\"Hackers and Slackers\">\n    <meta name=\"twitter:description\" content=\"An Example website.\">\n    <meta name=\"twitter:url\" content=\"http://example.com/\">\n    <meta name=\"twitter:image\" content=\"http://example.com/img/image.jpg\">\n</head>\n\n<body data-gr-c-s-loaded=\"true\">\n  <div>\n    <h1>Example Domain</h1>\n      <p>This domain is established to be used for illustrative examples in documents.</p>\n      <p>You may use this domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n  </div>\n</body>\n    \n</html>\n\n\nStep 2: The Extraction\nAfter turning our request content into a BeautifulSoup object, we access items\nin the DOM via dot notation as such:\n\ntitle = soup.title.string\n\n\n.string  gives us the actual content of the tag which is Example Domain, whereas\n soup.title  would return the entirety of the tag as <title>Example\nDomain</title>. \n\nDot notation is fine when pages have predictable hierarchies or structures, but\nbecomes much less useful for extracting patterns we see in the document. soup.a \nwill only return the first instance of a link, and probably isn't what we want.\n\nIf we wanted to extract all  <a>  tags of a page's content while avoiding the\nnoise of nav links etc, we can use CSS selectors to return a list of all\nelements matching the selection. soup.select('body p > a')  retrieves all links\nembedded in paragraph text, limited to the body of the page. \n\nSome other methods of grabbing elements:\n\n * soup.find(id=\"example\"): Useful for when a single element is expected.\n * soup.find_all('a'):  Returns a list of all elements matching the selection\n   after searching the document recursively.\n * .parent and .child: Relative selectors to a currently engaged element.\n\nGet Some Attributes\nChances are we'll almost always want the contents or the attributes of a tag, as\nopposed to the entire <a>  tag's HTML. A common example of going after a tag's\nattributes would be in the cases of img  and a  tags. Chances are we're most\ninterested in the src  and href  attributes of such tags, respectively. \n\nThe .get  method refers specifically to getting the value of attributes on a\ntag. For example, soup.find('.logo').get('href')  would find an element with the\nclass \"logo\", and return the url to that image.\n\nPesky Tags to Deal With\nIn our example of creating link previews, a good first source of information\nwould obviously be the page's meta tags: specifically the og  tags they've\nspecified to openly provide the bite-sized information we're looking for.\nGrabbing these tags are a bit more difficult to deal with:\n\nsoup.find(\"meta\", property=\"og:description\").get('content')\n\n\nOh yeah, now that's some ugly shit right there. Meta tags are especially\ninteresting because they're all uselessly dubbed 'meta', thus we need a second\ndifferentiator in addition to the tag name to specify which meta tag we care\nabout. Only then can we bother to get  the actual content of said tag.\n\nStep 3: Realizing Something Will Always Break\nIf we were to try the above selector on an HTML page which did not contain an \nog:description, our script would break unforgivingly. Not only do we miss this\ndata, but we miss out on everything entirely - this means we always need to\nbuild in a plan B, and at the very least deal with a lack of tag altogether.\n\nIt's best to break out this logic one tag at a time. First, let's look at an\nexample for a base scraper with all the knowledge we have so far:\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p > a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n\n\nGreat - there's a base function for snatching all links out of the body of a\npage. Ultimately we'll create a JSON object for each of these links containing\npreview data, link_preview_dict.\n\nTo handle each value of our dict, we have individual functions:\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get a preview image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\nIn case you're wondering:\n\n * getTitle tries to get the <title>  tag, and falls back to the page's first \n   <h1>  tag (surprisingly enough some pages are in fact missing a title).\n * getDescription  looks for the OG description, and falls back to the content\n   of the page's first paragraph.\n * getImage looks for the OG image, and falls back to the page's first image.\n * getSiteName similarly tries to grab the OG attribute, otherwise it does it's\n   best to extract the domain name from the URL string under the assumption that\n   this is the origin's name (look, it ain't perfect).\n\nWhat Did We Just Build?\nBelieve it or not, the above is considered to be enough logic to be a paid\nservice with a monthly fee. Go ahead and Google it; or better yet, just steal my\nsource code entirely:\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    \"\"\"Attempt to get a title.\"\"\"\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(\"h1\") is not None:\n        title = link.find(\"h1\")\n    return title\n\n\ndef getDescription(link):\n    \"\"\"Attempt to get description.\"\"\"\n    description = ''\n    if link.find(\"meta\", property=\"og:description\") is not None:\n        description = link.find(\"meta\", property=\"og:description\").get('content')\n    elif link.find(\"p\") is not None:\n        description = link.find(\"p\").content\n    return description\n\n\ndef getImage(link):\n    \"\"\"Attempt to get image.\"\"\"\n    image = ''\n    if link.find(\"meta\", property=\"og:image\") is not None:\n        image = link.find(\"meta\", property=\"og:image\").get('content')\n    elif link.find(\"img\") is not None:\n        image = link.find(\"img\").get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    \"\"\"Attempt to get the site's base name.\"\"\"\n    sitename = ''\n    if link.find(\"meta\", property=\"og:site_name\") is not None:\n        sitename = link.find(\"meta\", property=\"og:site_name\").get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    \"\"\"Scrape scheduled link previews.\"\"\"\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p > a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)","html":"<p>There are plenty of reliable and open sources of data on the web. Datasets are freely released to the public domain by the likes of Kaggle, Google Cloud, and of course local &amp; federal government. Like most things free and open, however, following the rules to obtain public data can be a bit... boring. I'm not suggesting we go and blatantly break some grey-area laws by stealing data, but this blog isn't exactly called <strong>People Who Play It Safe And Slackers</strong>, either. </p><p>My personal Python roots can actually be traced back to an ambitious side-project: to aggregate all new music from across the web and deliver it the masses. While that project may have been abandoned (after realizing it already existed), <strong>BeautifulSoup</strong> was more-or-less my first ever experience with Python. </p><h2 id=\"the-tool-s-for-the-job-s-\">The Tool(s) for the Job(s)</h2><p>Before going any further, we'd be ill-advised to not at least mention Python's other web-scraping behemoth, <strong><a href=\"https://scrapy.org/\">Scrapy</a></strong>. <strong>BeautifulSoup</strong> and <strong>Scrapy</strong> have two very different agendas. BeautifulSoup is intended to parse or extract data one page at a time, with each page being served up via the <strong>requests</strong> library or equivalent. <strong>Scrapy,</strong> on the other hand, is for creating crawlers: or rather absolute monstrosities unleashed upon the web like a swarm, loosely following links and haste-fully grabbing data where data exists to be grabbed. To put this in perspective, Google Cloud functions will not even let you import Scrapy as a usable library.</p><p>This isn't to say that <strong>BeautifulSoup</strong> can't be made into a similar monstrosity of its own. For now, we'll focus on a modest task: generating link previews for URLs by grabbing their metadata.</p><h2 id=\"step-1-stalk-your-prey\">Step 1: Stalk Your Prey</h2><p>Before we steal any data, we should take a look at the data we're hoping to steal.</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape URLs to generate previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url, headers)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    print(soup.prettify())\n</code></pre>\n<p>The above is the minimum needed to retrieve the DOM structure of an HTML page. <strong>BeautifulSoup</strong> accepts the <code>.content</code> output from a request, from which we can investigate the contents.</p><div class=\"protip\">\n    Using BeauitfulSoup will often result in different results for your scaper than you might see as a human, such as 403 errors or blocked content. An easy way around this faking your headers into looking like normal browser agents, as we do here: <br><code>headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })`</code>\n</div><p>The result of <code>print(soup.prettify())</code> will predictably output a \"pretty\" printed version of your target DOM structure:</p><pre><code class=\"language-html\">&lt;html class=&quot;gr__example_com&quot;&gt;&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n    &lt;meta charset=&quot;utf-8&quot;&gt;\n    &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;\n    &lt;meta property=&quot;og:site_name&quot; content=&quot;Example dot com&quot;&gt;\n    &lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n    &lt;meta property=&quot;og:title&quot; content=&quot;Example&quot;&gt;\n    &lt;meta property=&quot;og:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n    &lt;meta name=&quot;twitter:title&quot; content=&quot;Hackers and Slackers&quot;&gt;\n    &lt;meta name=&quot;twitter:description&quot; content=&quot;An Example website.&quot;&gt;\n    &lt;meta name=&quot;twitter:url&quot; content=&quot;http://example.com/&quot;&gt;\n    &lt;meta name=&quot;twitter:image&quot; content=&quot;http://example.com/img/image.jpg&quot;&gt;\n&lt;/head&gt;\n\n&lt;body data-gr-c-s-loaded=&quot;true&quot;&gt;\n  &lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n      &lt;p&gt;This domain is established to be used for illustrative examples in documents.&lt;/p&gt;\n      &lt;p&gt;You may use this domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&quot;http://www.iana.org/domains/example&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n    \n&lt;/html&gt;\n</code></pre>\n<h2 id=\"step-2-the-extraction\">Step 2: The Extraction</h2><p>After turning our request content into a BeautifulSoup object, we access items in the DOM via dot notation as such:</p><pre><code class=\"language-python\">title = soup.title.string\n</code></pre>\n<p><code>.string</code> gives us the actual content of the tag which is <code>Example Domain</code>, whereas <code>soup.title</code> would return the entirety of the tag as <code>&lt;title&gt;Example Domain&lt;/title&gt;</code>. </p><p>Dot notation is fine when pages have predictable hierarchies or structures, but becomes much less useful for extracting patterns we see in the document. <code>soup.a</code> will only return the first instance of a link, and probably isn't what we want.</p><p>If we wanted to extract <em>all</em> <code>&lt;a&gt;</code> tags of a page's content while avoiding the noise of nav links etc, we can use CSS selectors to return a list of all elements matching the selection. <code>soup.select('body p &gt; a')</code> retrieves all links embedded in paragraph text, limited to the body of the page. </p><p>Some other methods of grabbing elements:</p><ul><li><strong>soup.find(id=\"example\")</strong>: Useful for when a single element is expected.</li><li><strong>soup.find_all('a')</strong>:<strong> </strong>Returns a list of all elements matching the selection after searching the document recursively.</li><li><strong>.parent </strong>and <strong>.child</strong>: Relative selectors to a currently engaged element.</li></ul><h3 id=\"get-some-attributes\">Get Some Attributes</h3><p>Chances are we'll almost always want the contents or the attributes of a tag, as opposed to the entire <code>&lt;a&gt;</code> tag's HTML. A common example of going after a tag's attributes would be in the cases of <code>img</code> and <code>a</code> tags. Chances are we're most interested in the <code>src</code> and <code>href</code> attributes of such tags, respectively. </p><p>The <code>.get</code> method refers specifically to getting the value of attributes on a tag. For example, <code>soup.find('.logo').get('href')</code> would find an element with the class \"logo\", and return the url to that image.</p><h3 id=\"pesky-tags-to-deal-with\">Pesky Tags to Deal With</h3><p>In our example of creating link previews, a good first source of information would obviously be the page's meta tags: specifically the <code>og</code> tags they've specified to openly provide the bite-sized information we're looking for. Grabbing these tags are a bit more difficult to deal with:</p><pre><code class=\"language-python\">soup.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n</code></pre>\n<p>Oh yeah, now that's some ugly shit right there. Meta tags are especially interesting because they're all uselessly dubbed 'meta', thus we need a second differentiator in addition to the tag name to specify <em>which </em>meta tag we care about. Only then can we bother to <em>get</em> the actual content of said tag.</p><h2 id=\"step-3-realizing-something-will-always-break\">Step 3: Realizing Something Will Always Break</h2><p>If we were to try the above selector on an HTML page which did not contain an <code>og:description</code>, our script would break unforgivingly. Not only do we miss this data, but we miss out on everything entirely - this means we always need to build in a plan B, and at the very least deal with a lack of tag altogether.</p><p>It's best to break out this logic one tag at a time. First, let's look at an example for a base scraper with all the knowledge we have so far:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    headers = requests.utils.default_headers()\n    headers.update({\n        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n    })\n    r = requests.get(url)\n    raw_html = r.content\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    links = soup.select('body p &gt; a')\n    previews = []\n    for link in links:\n        url = link.get('href')\n        r2 = requests.get(url, headers=headers)\n        link_html = r2.content\n        embedded_link = BeautifulSoup(link_html, 'html.parser')\n        link_preview_dict = {\n            'title': getTitle(embedded_link),\n            'description': getDescription(embedded_link),\n            'image': getImage(embedded_link),\n            'sitename': getSiteName(embedded_link, url),\n            'url': url\n            }\n        previews.append(link_preview_dict)\n        print(link_preview_dict)\n</code></pre>\n<p>Great - there's a base function for snatching all links out of the body of a page. Ultimately we'll create a JSON object for each of these links containing preview data, <code>link_preview_dict</code>.</p><p>To handle each value of our dict, we have individual functions:</p><pre><code class=\"language-python\">def getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get a preview image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n</code></pre>\n<p>In case you're wondering:</p><ul><li><strong>getTitle </strong>tries to get the <code>&lt;title&gt;</code> tag, and falls back to the page's first <code>&lt;h1&gt;</code> tag (surprisingly enough some pages are in fact missing a title).</li><li><strong>getDescription</strong> looks for the OG description, and falls back to the content of the page's first paragraph.</li><li><strong>getImage </strong>looks for the OG image, and falls back to the page's first image.</li><li><strong>getSiteName </strong>similarly tries to grab the OG attribute, otherwise it does it's best to extract the domain name from the URL string under the assumption that this is the origin's name (look, it ain't perfect).</li></ul><h2 id=\"what-did-we-just-build\">What Did We Just Build?</h2><p>Believe it or not, the above is considered to be enough logic to be a paid service with a monthly fee. Go ahead and Google it; or better yet, just steal my source code entirely:</p><pre><code class=\"language-python\">import requests\nfrom bs4 import BeautifulSoup\nfrom flask import make_response\n\n\ndef getTitle(link):\n    &quot;&quot;&quot;Attempt to get a title.&quot;&quot;&quot;\n    title = ''\n    if link.title.string is not None:\n        title = link.title.string\n    elif link.find(&quot;h1&quot;) is not None:\n        title = link.find(&quot;h1&quot;)\n    return title\n\n\ndef getDescription(link):\n    &quot;&quot;&quot;Attempt to get description.&quot;&quot;&quot;\n    description = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:description&quot;) is not None:\n        description = link.find(&quot;meta&quot;, property=&quot;og:description&quot;).get('content')\n    elif link.find(&quot;p&quot;) is not None:\n        description = link.find(&quot;p&quot;).content\n    return description\n\n\ndef getImage(link):\n    &quot;&quot;&quot;Attempt to get image.&quot;&quot;&quot;\n    image = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:image&quot;) is not None:\n        image = link.find(&quot;meta&quot;, property=&quot;og:image&quot;).get('content')\n    elif link.find(&quot;img&quot;) is not None:\n        image = link.find(&quot;img&quot;).get('href')\n    return image\n\n\ndef getSiteName(link, url):\n    &quot;&quot;&quot;Attempt to get the site's base name.&quot;&quot;&quot;\n    sitename = ''\n    if link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;) is not None:\n        sitename = link.find(&quot;meta&quot;, property=&quot;og:site_name&quot;).get('content')\n    else:\n        sitename = url.split('//')[1]\n        name = sitename.split('/')[0]\n        name = sitename.rsplit('.')[1]\n        return name.capitalize()\n    return sitename\n\n\ndef scrape(request):\n    &quot;&quot;&quot;Scrape scheduled link previews.&quot;&quot;&quot;\n    if request.method == 'POST':\n        # Allows POST requests from any origin with the Content-Type\n        # header and caches preflight response for an 3600s\n        headers = {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Methods': 'POST',\n            'Access-Control-Allow-Headers': 'Content-Type',\n            'Access-Control-Max-Age': '3600'\n        }\n        request_json = request.get_json()\n        target_url = request_json['url']\n        headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n        })\n        r = requests.get(target_url)\n        raw_html = r.content\n        soup = BeautifulSoup(raw_html, 'html.parser')\n        links = soup.select('.post-content p &gt; a')\n        previews = []\n        for link in links:\n            url = link.get('href')\n            r2 = requests.get(url, headers=headers)\n            link_html = r2.content\n            embedded_link = BeautifulSoup(link_html, 'html.parser')\n            preview_dict = {\n                'title': getTitle(embedded_link),\n                'description': getDescription(embedded_link),\n                'image': getImage(embedded_link),\n                'sitename': getSiteName(embedded_link, url),\n                'url': url\n                }\n            previews.append(preview_dict)\n        return make_response(str(previews), 200, headers)\n    return make_response('bruh pls', 400, headers)\n</code></pre>\n","url":"https://hackersandslackers.com/scraping-urls-with-beautifulsoup/","uuid":"c933218e-6bbf-44b7-8f01-bfd188c71d89","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be7fc282ec6e0035b4b16bc"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673733","title":"Python-Lambda: The Essential Library for AWS Cloud Functions","slug":"improve-your-aws-lambda-workflow-with-python-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","custom_excerpt":"Deploy AWS Lambda functions with ease with the help of a single Python library.","created_at_pretty":"07 November, 2018","published_at_pretty":"08 November, 2018","updated_at_pretty":"05 January, 2019","created_at":"2018-11-07T16:01:48.000-05:00","published_at":"2018-11-07T19:13:20.000-05:00","updated_at":"2019-01-05T13:22:03.000-05:00","meta_title":"Simplify Lambda Deployment with python-lambda | Hackers and Slackers","meta_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","og_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","og_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","twitter_description":"Create and deploy AWS Lambda functions with ease in Python. Use python-lambda to initialize your lambdas, run locally, and automate deployment. ","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pythonlambda3@2x.jpg","twitter_title":"Improve your AWS Lambda Workflow with python-lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"REST APIs","slug":"restapis","description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","feature_image":null,"meta_description":"Get the most out of REST APIs by example, or build your own. Discover new APIs and how to interact with them, regardless of proffered programming language.","meta_title":"Consuming and Building REST APIs | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"In our series about building AWS APIs\n[https://hackersandslackers.com/tag/aws-api/], we've covered a lot of ground\naround learning the AWS ecosystem. Now that we're all feeling a bit more\ncomfortable, it may be time to let everybody in on the world's worst-kept\nsecret: Almost nobody builds architecture by interacting with the AWS UI\ndirectly. There are plenty examples of how this is done, with the main example\nbeing HashiCorp:  an entire business model based around the premise that AWS has\na shitty UI, to the point where it's easier to write code to make things which\nwill host your code. What a world.\n\nIn the case of creating Python Lambda functions, the \"official\" (aka: manual)\nworkflow of deploying your code to AWS is something horrible like this:\n\n * You start a project locally and begin development.\n * You opt to use virtualenv, because you're well aware that you're going to\n   need the source for any packages you use available.\n * When you're ready to 'deploy' to AWS, you copy all your dependencies from \n   /site-packages  and move them into your root directory, temporarily creating\n   an abomination of a project structure.\n * With your project fully bloated and confused, you cherry pick the files\n   needed to zip into an archive.\n * Finally, you upload your code via zip either to Lambda directory or to S3,\n   only to run your code, realize its broken, and need to start all over.\n\nThere Must be a Better Way\nIndeed there is, and surprisingly enough the solution is 100% Python (sorry\nHashiCorp, we'll talk another time). This \"better way\" is my personal method of\nleveraging the following:\n\n * The official AWS CLI\n   [https://docs.aws.amazon.com/cli/latest/userguide/installing.html].\n * Pipenv [https://pipenv.readthedocs.io/en/latest/]  as an environment manager.\n * Python's python-lambda [https://github.com/nficano/python-lambda]  package:\n   the magic behind it all.\n\nObligatory \"Installing the CLI\" Recap\nFirst off, make sure you're using a compatible version of Python on your system,\nas AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by\nthe way, Python 2.7 doesn't count as compatible - let it die before your career\ndoes).\n\n$ pip3 install awscli --upgrade --user\n\n\nIf you're working off an EC2 instance, it has come to my attention pip3 does not\ncome preinstalled. Remember to run: * $ apt update\n * $ apt upgrade\n * $ apt install python3-pip\n\nYou may be prompted to run apt install awscli  as well.Awesome, now that we have\nthe CLI installed on the real  version of Python, we need to store your\ncredentials. Your Access Key ID and Secret Access Key can be found in your IAM\npolicy manager.\n\n$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:\n\nOn both Linux and OSX, this should generate files found under cd ~/.aws  which\nwill be referenced by default whenever you use an AWS service moving forward.\n\nSet Up Your Environment\nAs mentioned, we'll use pipenv  for easy environment management. We'll create an\nenvironment using Lambda's preferred Python version:\n\n$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n\n\nSomething you should be aware of at the time of writing: Pip's latest version,\n18.1, is actually a breaking change  for Pipenv. Thus, the first thing we should\ndo is force usage of pip 18.0 (is there even a fix for this yet?). This is\nsolved by typing pip3 install pip==18.0  with the Pipenv shell activated. Now\nlet's get to the easy part.\n\npython-lambda: The Savior of AWS\nSo far we've made our lives easier in two ways: we're keeping our AWS\ncredentials safe and far away from ourselves, and we have what is by far the\nsuperior Python package management solution. But this is all foreplay leading up\nto python-lambda:\n\n$ pip3 install python-lambda\n\n\nThis library alone is about to do you the following favors:\n\n * Initiate your Lambda project structure for you.\n * Isolate Lambda configuration to a  config.yaml  file, covering everything\n   from the name of your entry point, handler function, and even\n   program-specific variables.\n * Allow you to run tests locally, where a test.json  file simulates a request\n   being made to your function locally.\n * Build a production-ready zip file with all dependencies completely separated \n   from your beautiful file structure.\n * The ability to deploy directly  to S3 or Lambda with said zip file from\n   command-line.\n\nCheck out the commands for yourself:\n\nCommands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n\n\nInitiate your project\nRunning lambda init  will generate the following file structure:\n\n.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n\n\nChecking out the entry point: service.py\npython-lambda starts you off with a basic handler as an example of a working\nproject. Feel free to rename service.py  and its handler function to whatever\nyou please, as we can configure that in a bit.\n\n# -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n\n\nEasy configuration via configure.yaml\nThe base config generated by lambda init  looks like this:\n\nregion: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n\n\nLook familiar? These are all the properties you would normally have to set up\nvia the UI. As an added bonus, you can store values (such as S3 bucket names for\nboto3) in this file as well. That's dope.\n\nSetting up event.json\nThe default event.json  is about as simplistic as you can get, and naturally not\nvery helpful at first (it isn't meant to be). These are the contents:\n\n{\n  \"pi\": 3.14,\n  \"e\": 2.718\n}\n\n\nWe can replace this a real test JSON which we can grab from Lambda itself.\nHere's an example of a Cloudwatch event we can use instead:\n\n{\n  \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n  \"detail-type\": \"Scheduled Event\",\n  \"source\": \"aws.events\",\n  \"account\": \"{{account-id}}\",\n  \"time\": \"1970-01-01T00:00:00Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n  ],\n  \"pi\": 3.14,\n  \"e\": 2.718\n  \"detail\": {}\n}\n\n\nRemember that event.json  is what is being passed to our handler as the event \nparameter. Thus, now we can run our Lambda function locally  to see if it works:\n\n$ lambda invoke\n5.8580000000000005\n\n\nPretty cool if you ask me.\n\nDeploy it, Ship it, Roll Credits\nAfter you express your coding genius, remember to output pip freeze >\nrequirements.txt. python-lambda  will use this as a reference for which packages\nneed to be included. This is neat because we can use Pipenv and the benefits of\nthe workflow it provides while still easily outputting what we need to deploy. \n\nBecause we already specified which Lambda we're going to deploy to in \nconfig.yaml, we can deploy to that Lambda immediately. lambda deploy  will use\nthe zip upload method, whereas lambda deploy-s3  will store your source on S3.\n\nIf you'd like to deploy the function yourself, run with lambda build  which will\nzip your source code plus dependencies  neatly into a /dist  directory. Suddenly\nwe never have to compromise our project structure, and now we can easily source\ncontrol our Lambdas by .gitignoring our build folders while hanging on to our\nPipfiles.\n\nHere's to hoping you never need to deploy Lambdas using any other method ever\nagain. Cheers.","html":"<p>In our series about building <a href=\"https://hackersandslackers.com/tag/aws-api/\">AWS APIs</a>, we've covered a lot of ground around learning the AWS ecosystem. Now that we're all feeling a bit more comfortable, it may be time to let everybody in on the world's worst-kept secret: Almost nobody builds architecture by interacting with the AWS UI directly. There are plenty examples of how this is done, with the main example being <strong>HashiCorp:</strong> an entire business model based around the premise that AWS has a shitty UI, to the point where it's easier to write code to make things which will host your code. What a world.</p><p>In the case of creating Python Lambda functions, the \"official\" (aka: manual) workflow of deploying your code to AWS is something horrible like this:</p><ul><li>You start a project locally and begin development.</li><li>You opt to use <strong>virtualenv, </strong>because you're well aware that you're going to need the source for any packages you use available.</li><li>When you're ready to 'deploy' to AWS, you <em>copy all your dependencies from </em><code>/site-packages</code> <em>and move them into your root directory</em>, temporarily creating an abomination of a project structure.</li><li>With your project fully bloated and confused, you cherry pick the files needed to zip into an archive.</li><li>Finally, you upload your code via zip either to Lambda directory or to S3, only to run your code, realize its broken, and need to start all over.</li></ul><h2 id=\"there-must-be-a-better-way\">There Must be a Better Way</h2><p>Indeed there is, and surprisingly enough the solution is 100% Python (sorry HashiCorp, we'll talk another time). This \"better way\" is my personal method of leveraging the following:</p><ul><li>The official <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/installing.html\">AWS CLI</a>.</li><li><a href=\"https://pipenv.readthedocs.io/en/latest/\">Pipenv</a> as an environment manager.</li><li>Python's <strong><a href=\"https://github.com/nficano/python-lambda\">python-lambda</a></strong> package: the magic behind it all.</li></ul><h3 id=\"obligatory-installing-the-cli-recap\">Obligatory \"Installing the CLI\" Recap</h3><p>First off, make sure you're using a compatible version of Python on your system, as AWS is still stuck on version 3.6. Look, we can't all be Google Cloud (and by the way, <em>Python 2.7 </em>doesn't count as compatible - let it die before your career does).</p><pre><code class=\"language-python\">$ pip3 install awscli --upgrade --user\n</code></pre>\n<div class=\"protip\">\n    If you're working off an EC2 instance, it has come to my attention pip3 does not come preinstalled. Remember to run:\n<ul>\n    <li><code>$ apt update</code></li>\n    <li><code>$ apt upgrade</code></li>\n    <li><code>$ apt install python3-pip</code></li>\n</ul>\n    \n    You may be prompted to run <code>apt install awscli</code> as well.\n</div><p>Awesome, now that we have the CLI installed on the <em>real</em> version of Python, we need to store your credentials. Your Access Key ID and Secret Access Key can be found in your IAM policy manager.</p><pre><code>$ aws configure\nAWS Access Key ID [None]: YOURKEY76458454535\nAWS Secret Access Key [None]: SECRETKEY*^R(*$76458397045609365493\nDefault region name [None]:\nDefault output format [None]:</code></pre><p>On both Linux and OSX, this should generate files found under <code>cd ~/.aws</code> which will be referenced by default whenever you use an AWS service moving forward.</p><h2 id=\"set-up-your-environment\">Set Up Your Environment</h2><p>As mentioned, we'll use <code>pipenv</code> for easy environment management. We'll create an environment using Lambda's preferred Python version:</p><pre><code class=\"language-python\">$ pip3 install pipenv\n$ pipenv shell --python 3.6\n\nCreating a virtualenv for this project…\nPipfile: /home/example/Pipfile\nUsing /usr/bin/python3 (3.6.6) to create virtualenv…\n⠇Already using interpreter /usr/bin/python3\nUsing base prefix '/usr'\nNew python executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python3\nAlso creating executable in /root/.local/share/virtualenvs/example-RnlD17kd/bin/python\nInstalling setuptools, pip, wheel...done.\n</code></pre>\n<p>Something you should be aware of at the time of writing: Pip's latest version, 18.1, is actually a <em>breaking change</em> for Pipenv. Thus, the first thing we should do is force usage of pip 18.0 (is there even a fix for this yet?). This is solved by typing <code>pip3 install pip==18.0</code> with the Pipenv shell activated. Now let's get to the easy part.</p><h2 id=\"python-lambda-the-savior-of-aws\">python-lambda: The Savior of AWS</h2><p>So far we've made our lives easier in two ways: we're keeping our AWS credentials safe and far away from ourselves, and we have what is by far the superior Python package management solution. But this is all foreplay leading up to <code>python-lambda</code>:</p><pre><code class=\"language-bash\">$ pip3 install python-lambda\n</code></pre>\n<p>This library alone is about to do you the following favors:</p><ul><li>Initiate your Lambda project structure for you.</li><li>Isolate Lambda configuration to a<em> config.yaml</em> file, covering everything from the name of your entry point, handler function, and even program-specific variables.</li><li>Allow you to run tests locally, where a <em>test.json</em> file simulates a request being made to your function locally.</li><li>Build a production-ready zip file with all dependencies <em>completely separated </em>from your beautiful file structure.</li><li>The ability to deploy <em>directly</em> to S3 or Lambda with said zip file from command-line.</li></ul><p>Check out the commands for yourself:</p><pre><code class=\"language-bash\">Commands:\n  build      Bundles package for deployment.\n  cleanup    Delete old versions of your functions\n  deploy     Register and deploy your code to lambda.\n  deploy-s3  Deploy your lambda via S3.\n  init       Create a new function for Lambda.\n  invoke     Run a local test of your function.\n  upload     Upload your lambda to S3.\n</code></pre>\n<h3 id=\"initiate-your-project\">Initiate your project</h3><p>Running <code>lambda init</code> will generate the following file structure:</p><pre><code class=\"language-bash\">.\n├── Pipfile\n├── config.yaml\n├── event.json\n└── service.py\n</code></pre>\n<h3 id=\"checking-out-the-entry-point-service-py\">Checking out the entry point: service.py</h3><p>python-lambda starts you off with a basic handler as an example of a working project. Feel free to rename <code>service.py</code> and its handler function to whatever you please, as we can configure that in a bit.</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\n\ndef handler(event, context):\n    # Your code goes here!\n    e = event.get('e')\n    pi = event.get('pi')\n    return e + pi\n</code></pre>\n<h3 id=\"easy-configuration-via-configure-yaml\">Easy configuration via configure.yaml</h3><p>The base config generated by <code>lambda init</code> looks like this:</p><pre><code class=\"language-yaml\">region: us-east-1\n\nfunction_name: my_lambda_function\nhandler: service.handler\ndescription: My first lambda function\nruntime: python3.6\n# role: lambda_basic_execution\n\n# S3 upload requires appropriate role with s3:PutObject permission\n# (ex. basic_s3_upload), a destination bucket, and the key prefix\n# bucket_name: 'example-bucket'\n# s3_key_prefix: 'path/to/file/'\n\n# if access key and secret are left blank, boto will use the credentials\n# defined in the [default] section of ~/.aws/credentials.\naws_access_key_id:\naws_secret_access_key:\n\n# dist_directory: dist\n# timeout: 15\n# memory_size: 512\n# concurrency: 500\n#\n\n# Experimental Environment variables\nenvironment_variables:\n    env_1: foo\n    env_2: baz\n\n# If `tags` is uncommented then tags will be set at creation or update\n# time.  During an update all other tags will be removed except the tags\n# listed here.\n#tags:\n#    tag_1: foo\n#    tag_2: bar\n</code></pre>\n<p>Look familiar? These are all the properties you would normally have to set up via the UI. As an added bonus, you can store values (such as S3 bucket names for boto3) in this file as well. That's dope.</p><h3 id=\"setting-up-event-json\">Setting up event.json</h3><p>The default <code>event.json</code> is about as simplistic as you can get, and naturally not very helpful at first (it isn't meant to be). These are the contents:</p><pre><code class=\"language-json\">{\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n}\n</code></pre>\n<p>We can replace this a real test JSON which we can grab from Lambda itself. Here's an example of a Cloudwatch event we can use instead:</p><pre><code class=\"language-json\">{\n  &quot;id&quot;: &quot;cdc73f9d-aea9-11e3-9d5a-835b769c0d9c&quot;,\n  &quot;detail-type&quot;: &quot;Scheduled Event&quot;,\n  &quot;source&quot;: &quot;aws.events&quot;,\n  &quot;account&quot;: &quot;{{account-id}}&quot;,\n  &quot;time&quot;: &quot;1970-01-01T00:00:00Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [\n    &quot;arn:aws:events:us-east-1:123456789012:rule/ExampleRule&quot;\n  ],\n  &quot;pi&quot;: 3.14,\n  &quot;e&quot;: 2.718\n  &quot;detail&quot;: {}\n}\n</code></pre>\n<p>Remember that <code>event.json</code> is what is being passed to our handler as the <code>event</code> parameter. Thus, now we can run our Lambda function <em>locally</em> to see if it works:</p><pre><code class=\"language-bash\">$ lambda invoke\n5.8580000000000005\n</code></pre>\n<p>Pretty cool if you ask me.</p><h2 id=\"deploy-it-ship-it-roll-credits\">Deploy it, Ship it, Roll Credits</h2><p>After you express your coding genius, remember to output <code>pip freeze &gt; requirements.txt</code>. <strong>python-lambda</strong> will use this as a reference for which packages need to be included. This is neat because we can use Pipenv and the benefits of the workflow it provides while still easily outputting what we need to deploy. </p><p>Because we already specified which Lambda we're going to deploy to in <code>config.yaml</code>, we can deploy to that Lambda immediately. <code>lambda deploy</code> will use the zip upload method, whereas <code>lambda deploy-s3</code> will store your source on S3.</p><p>If you'd like to deploy the function yourself, run with <code>lambda build</code> which will zip your source code <em>plus dependencies</em> neatly into a /<em>dist</em> directory. Suddenly we never have to compromise our project structure, and now we can easily source control our Lambdas by <em>.gitignoring </em>our build folders while hanging on to our Pipfiles.</p><p>Here's to hoping you never need to deploy Lambdas using any other method ever again. Cheers.</p>","url":"https://hackersandslackers.com/improve-your-aws-lambda-workflow-with-python-lambda/","uuid":"08ad7706-8dd7-4475-875e-880c017de8d5","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5be352bc2aa81b1606ab77a7"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673731","title":"Liberating Data from PDFs with Tabula and Pandas","slug":"liberating-data-from-pdfs-with-tabula-and-pandas","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","excerpt":"Making 'open' data more open.","custom_excerpt":"Making 'open' data more open.","created_at_pretty":"03 November, 2018","published_at_pretty":"04 November, 2018","updated_at_pretty":"02 February, 2019","created_at":"2018-11-03T13:04:50.000-04:00","published_at":"2018-11-04T14:23:41.000-05:00","updated_at":"2019-02-02T08:19:55.000-05:00","meta_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","meta_description":"Making 'open' data more open: use Python's Pandas library and Tabula to extract data from PDFs.","og_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","og_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","og_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","twitter_description":"Making 'open' data more open: use Python's Pandas library to extract data from PDFs.","twitter_image":"https://hackersandslackers.com/content/images/2018/11/pandaspdf@2x.jpg","twitter_title":"Liberating Data from PDFs with Tabula and Pandas | Hackers and Slackers","authors":[{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Graham Beckley","slug":"graham","bio":"Loves Python; loves pandas; leaves every project more Pythonic than he found it.","profile_image":"https://hackersandslackers.com/content/images/2019/03/graham2.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Pandas","slug":"pandas","description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","feature_image":"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/pandasmerge.jpg","meta_description":"Analyze data with the Pandas data analysis library for Python. Start from the basics or see real-life examples of pros using Pandas to solve problems.","meta_title":"Pythons and Pandas | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"#Data Analysis with Pandas","slug":"data-analysis-pandas","description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pandasseries-1.jpg","meta_description":"Analyze data with Python's Pandas. Start from the basics or see real-life examples of using Pandas to solve problems.","meta_title":"Data Analysis with Pandas","visibility":"internal"}],"plaintext":"Check out the accompanying GitHub repo for this article here\n[https://github.com/grahamalama/school_budget_aggregator].\n\nTechnically, the School District of Philadelphia's budget data for the 2019\nfiscal year is \"open\". It is, after all, made available through the district's \nOpen Data portal  and is freely available to download.\n\nBut just because data is freely available, doesn't mean it's easy to work with.\nThat's what found out when I downloaded the zipped folder, opened it up, and\nfound a heap of PDFs. Joy.\n\nAs a member of Code for Philly [https://codeforphilly.org/], I thought of my\ncompatriots who might want to use school district data in their projects. I knew\nwith a bit of data munging, I could provide a data set that would be more easily\nusable.\n\nData Liberation\nThe first hurdle was to find a way to get the data from the PDFs. After a bit\nGoogling, I came across tabula-py [https://github.com/chezou/tabula-py], a\nPython wrapper for Tabula [https://tabula.technology/].\n\nEach budget is composed of 5 tables:\n\n * General information about the school\n * Enrollment information\n * Operating Funded budget allotments\n * Grant Funded budget allotments\n * A summary table of allotment totals\n\nExtracting these tables from a budget with Tabula was as simple as:\n\ntabula.read_pdf(path_to_budget, multiple_tables=True)\n\n\nWhich returned a list of DataFrames, one for each table mentioned above.\nPerfect! \nSo, I iterated over all of the files in folder and appended them to a list:\n\nimport os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f\"{directory}/{filename}\", \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(\"SY1819_School_Budgets\")\n\n\nInitial Cleaning\nWhile this gave me a good start, I knew it wouldn't be that easy to liberate the\ndata from the PDFs. I took a look at each of the DataFrames to see what I'd be\nworking with. \n\n# an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n\n\nAfter I saw the output, I wrote a function to perform the same cleaning\noperation for each table in each budget.\n\nFor each table below, first I'll introduce the \"raw\" output that Tabula\nreturned, then I'll show the function that I wrote to fix that output.\n\nBasic Information\nRaw Output\nbasic_information = sample_budget[0] #basic information\nbasic_information\n\n\n0\n 1\n 0\n Basic Information\n NaN\n 1\n Council District\n 2nd\n 2\n Organization Code\n 1380\n 3\n School Level\n Elementary School\n 4\n Economically\n NaN\n 5\n Disadvantaged Rate*\n NaN\n 6\n NaN\n 83.44%\n Cleanup Function\ndef generate_basic_information_table(df):\n    '''Series representing the \"basic information\" table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [\"Economically Disadvantaged Rate\", df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n\n\nCleaned\nbasic_information = generate_basic_information_table(basic_information)\nbasic_information\n\n\n# Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n\n\nEnrollment\nRaw Output\n# Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n\n\n0\n 1\n 2\n 3\n 0\n NaN\n FY14\n FY18\n FY19 Projected\n 1\n Enrollment**\n 842\n 640\n 602\n Cleanup Function\ndef generate_enrollment_table(df):\n    '''returns a series representing the \"enrollment\" table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=\"enrollment\")\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n\n\nCleaned\n# Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n\n\nAllotments\nLuckily, both allotment tables were identical, so I could apply to the same\ncleanup steps to both.\n\nRaw Output\noperating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n\n\n0\n 1\n 2\n 3\n 0\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n 2\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n 3\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n 4\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n 5\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n 6\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n 7\n Secretaries\n 1.0\n 1.0\n 1.0\n 8\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n 9\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n 10\n Other\n 0.0\n 1.2\n 1.0\n 11\n Total Positions\n 60.0\n 43.8\n 50.1\n 12\n Supplies/Equipment/Non Full‐Time Salaries/Other\n $32,272\n $100,159\n $97,553\n grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n\n\nCleanup Function\nI decided to merge the two allotment tables into one DataFrame while building a\nMultiIndex to keep things in order. This would allow me to ask some more\ninteresting questions further on down the road.\n\ndef generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + \"ed_allotments\"\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(\"org_code\", \"fund\", \"allotment\")\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n\n\nCleaned\npd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, \"1410\", \"operating_fund\"\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, \"1410\", \"grant_fund\"\n    )\n])\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment\n 1410\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n Totals\nSince the final \"totals\" table could be calculated from the data already in the\nnew allotment table, I didn't bother transforming it in any way.\n\n# same data can be derived from the allotments table directly\nsample_budget[4]\n\n\n0\n 1\n 2\n 3\n 4\n 0\n NaN\n Position/Expenditure\n FY14 Budget\n FY18 Budget\n FY19 Budget\n 1\n Total Positions\n NaN\n 76.1\n 64.8\n 67.8\n 2\n Total Supplies/Equipment/Non Full‐Time Salarie...\n NaN\n $230,726\n $120,136\n $112,719\n Once I figured out what transformations I needed for each table, I combined\nthem into a function so that, given a list of DataFames from Tabula, I'd get\nthose same tables back neatly formatted.\n\ndef generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n\n\nAggregation Time\nNow that I had cleaned the tables that Tabula produced, it was time to combine\nthem into some aggregated tables.\n\nFirst I wrote a function that would output a Series (representing one row) of\ninformation from all tables for a given school in a given fiscal year. \n\ndef generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +\": \" + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == \"FY19\" else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == \"FY19\" else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(\"FY18\", basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n\n\n# Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n\n\nThen, I applied this function to each list of budgets in the collection and\ncompiled them into a DataFrame.\n\ndef generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = \"FY14\"\nfy14.to_csv(\"output/combined_fy14.csv\")\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = \"FY18\"\nfy18.to_csv(\"output/combined_fy18.csv\")\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = \"FY19\"\nfy19.to_csv(\"output/combined_fy19.csv\")\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(\"output/all_budgets_tabular.csv\")\n\n\nFinally, I wanted to output a CSV that would preserve some of the multi-indexed\nnature of the allotment tables. Here's what I wrote for that.\n\ndef generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(\"output/all_budgets_hierarchical.csv\")\n\nhierarchical_budget\n\n\nFY14\n FY18\n FY19\n org_code\n fund\n allotment  \n 1380\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n grant_fund\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n operating_fund\n Principals/Assistant Principals\n 2.0\n 1.0\n 1.0\n Teachers ‐ Regular Education\n 30.2\n 25.0\n 24.0\n Teachers ‐ Special Education\n 6.0\n 2.8\n 5.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 1.2\n 0.8\n 0.1\n Nurses/Health Services\n 0.6\n 1.0\n 1.0\n Classroom Assistants/Teacher Assistants\n 11.0\n 8.0\n 9.0\n Secretaries\n 1.0\n 1.0\n 1.0\n Support Services Assistants\n 0.0\n 2.0\n 5.0\n Student Climate Staff\n 8.0\n 1.0\n 3.0\n Other\n 0.0\n 1.2\n 1.0\n Total Positions\n 60.0\n 43.8\n 50.1\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 32272.0\n 100159.0\n 97553.0\n grant_fund\n Principals/Assistant Principals\n 0.0\n 0.0\n 0.0\n Teachers ‐ Regular Education\n 8.1\n 8.6\n 9.6\n Teachers ‐ Special Education\n 0.0\n 0.2\n 0.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 0.0\n 0.2\n 1.1\n Nurses/Health Services\n 0.0\n 0.0\n 0.0\n Classroom Assistants/Teacher Assistants\n 0.0\n 0.0\n 0.0\n Secretaries\n 0.0\n 0.0\n 0.0\n Support Services Assistants\n 7.0\n 5.0\n 3.0\n Student Climate Staff\n 0.0\n 7.0\n 4.0\n Other\n 1.0\n 0.0\n 0.0\n Total Positions\n 16.1\n 21.0\n 17.7\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 198454.0\n 19977.0\n 15166.0\n 5160 rows × 3 columnsThis makes it easier to aggregate in interesting ways:\n\nhierarchical_budget.groupby('allotment').sum()\n\n\nFY14\n FY18\n FY19\n allotment\n Classroom Assistants/Teacher Assistants\n 2365.0\n 1720.0\n 1935.0\n Counselors/Student Adv./ Soc. Serv. Liaisons\n 258.0\n 215.0\n 258.0\n Nurses/Health Services\n 129.0\n 215.0\n 215.0\n Other\n 215.0\n 258.0\n 215.0\n Principals/Assistant Principals\n 430.0\n 215.0\n 215.0\n Secretaries\n 215.0\n 215.0\n 215.0\n Student Climate Staff\n 1720.0\n 1720.0\n 1505.0\n Supplies/Equipment/Non Full‐Time Salaries/Other\n 49606090.0\n 25829240.0\n 24234585.0\n Support Services Assistants\n 1505.0\n 1505.0\n 1720.0\n Teachers ‐ Regular Education\n 8234.5\n 7224.0\n 7224.0\n Teachers ‐ Special Education\n 1290.0\n 645.0\n 1075.0\n Total Positions\n 16361.5\n 13932.0\n 14577.0\n More Cleaning to be Done\nMy work here is done. I saved the data from their not-so-accessible PDF prisons.\nBut now it's time for someone with some domain-specific knowledge to make it\nactionable.\n\nThe biggest weakness with the data in its current form is that there is some\namount of ambiguity as to what the different allotments numbers represent in\nreal-dollar amounts. Only the Supplies/Equipment/Non Full‐Time Salaries/Other \nallotment category came in currency notation – the rest of the allotments were\nrepresented as simple decimal amounts with no context to help interpret what\nthey mean. Do they represent FTE\n[https://en.wikipedia.org/wiki/Full-time_equivalent]? Dollar amounts in\nscientific notation? I'm not sure, but I hope by handing this work off to the\nright people, these questions and more can be answered more easily thanks to a\ncleaner, more accessible data set.","html":"<p><em>Check out the accompanying GitHub repo for this article <a href=\"https://github.com/grahamalama/school_budget_aggregator\">here</a>.</em></p><p>Technically, the School District of Philadelphia's budget data for the 2019 fiscal year is \"open\". It is, after all, made available through the district's <a href=\"https://www.philasd.org/performance/programsservices/open-data/district-information/#budget\">Open Data portal</a> and is freely available to download.</p><p>But just because data is freely available, doesn't mean it's easy to work with. That's what found out when I downloaded the zipped folder, opened it up, and found a heap of PDFs. Joy.</p><p>As a member of <a href=\"https://codeforphilly.org/\">Code for Philly</a>, I thought of my compatriots who might want to use school district data in their projects. I knew with a bit of data munging, I could provide a data set that would be more easily usable.</p><h2 id=\"data-liberation\">Data Liberation</h2><p>The first hurdle was to find a way to get the data from the PDFs. After a bit Googling, I came across <a href=\"https://github.com/chezou/tabula-py\"><strong>tabula-py</strong></a>, a Python wrapper for <a href=\"https://tabula.technology/\">Tabula</a>.</p><p>Each budget is composed of 5 tables:</p><ul><li>General information about the school</li><li>Enrollment information</li><li>Operating Funded budget allotments</li><li>Grant Funded budget allotments</li><li>A summary table of allotment totals</li></ul><p>Extracting these tables from a budget with Tabula was as simple as:</p><pre><code class=\"language-python\">tabula.read_pdf(path_to_budget, multiple_tables=True)\n</code></pre>\n<p>Which returned a list of DataFrames, one for each table mentioned above. Perfect! <br>So, I iterated over all of the files in folder and appended them to a list:</p><pre><code class=\"language-python\">import os\nimport pandas as pd\nimport tabula\n\ndef read_budgets(directory):\n    budgets = []\n    for filename in os.listdir(directory):\n        budget_tables = tabula.read_pdf(\n            f&quot;{directory}/{filename}&quot;, \n            multiple_tables=True\n        )\n        budgets.append(budget_tables)\n\n    return budgets\n\n\n# this takes a while\nbudgets = read_budgets(&quot;SY1819_School_Budgets&quot;)\n</code></pre>\n<h2 id=\"initial-cleaning\">Initial Cleaning</h2><p>While this gave me a good start, I knew it wouldn't be that easy to liberate the data from the PDFs. I took a look at each of the DataFrames to see what I'd be working with. </p><pre><code class=\"language-python\"># an example list of budgets\nsample_budget = budgets[0]\nsample_budget\n\n[    0                  1\n     0    Basic Information                NaN\n     1     Council District                2nd\n     2    Organization Code               1380\n     3         School Level  Elementary School\n     4         Economically                NaN\n     5  Disadvantaged Rate*                NaN\n     6                  NaN             83.44%,\n                   0     1     2               3\n     0           NaN  FY14  FY18  FY19 Projected\n     1  Enrollment**   842   640             602,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          2.0          1.0   \n     2                      Teachers ‐ Regular Education         30.2         25.0   \n     3                      Teachers ‐ Special Education          6.0          2.8   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          1.2          0.8   \n     5                            Nurses/Health Services          0.6          1.0   \n     6           Classroom Assistants/Teacher Assistants         11.0          8.0   \n     7                                       Secretaries          1.0          1.0   \n     8                       Support Services Assistants          0.0          2.0   \n     9                             Student Climate Staff          8.0          1.0   \n     10                                            Other          0.0          1.2   \n     11                                  Total Positions         60.0         43.8   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other      $32,272     $100,159   \n     \n                   3  \n     0   FY19 Budget  \n     1           1.0  \n     2          24.0  \n     3           5.0  \n     4           0.1  \n     5           1.0  \n     6           9.0  \n     7           1.0  \n     8           5.0  \n     9           3.0  \n     10          1.0  \n     11         50.1  \n     12      $97,553  ,\n                                                       0            1            2  \\\n     0                              Position/Expenditure  FY14 Budget  FY18 Budget   \n     1                   Principals/Assistant Principals          0.0          0.0   \n     2                      Teachers ‐ Regular Education          8.1          8.6   \n     3                      Teachers ‐ Special Education          0.0          0.2   \n     4      Counselors/Student Adv./ Soc. Serv. Liaisons          0.0          0.2   \n     5                            Nurses/Health Services          0.0          0.0   \n     6           Classroom Assistants/Teacher Assistants          0.0          0.0   \n     7                                       Secretaries          0.0          0.0   \n     8                       Support Services Assistants          7.0          5.0   \n     9                             Student Climate Staff          0.0          7.0   \n     10                                            Other          1.0          0.0   \n     11                                  Total Positions         16.1         21.0   \n     12  Supplies/Equipment/Non Full‐Time Salaries/Other     $198,454      $19,977   \n     \n                   3  \n     0   FY19 Budget  \n     1           0.0  \n     2           9.6  \n     3           0.0  \n     4           1.1  \n     5           0.0  \n     6           0.0  \n     7           0.0  \n     8           3.0  \n     9           4.0  \n     10          0.0  \n     11         17.7  \n     12      $15,166  ,\n                                                        0                     1  \\\n     0                                                NaN  Position/Expenditure   \n     1                                    Total Positions                   NaN   \n     2  Total Supplies/Equipment/Non Full‐Time Salarie...                   NaN   \n     \n                  2            3            4  \n     0  FY14 Budget  FY18 Budget  FY19 Budget  \n     1         76.1         64.8         67.8  \n     2     $230,726     $120,136     $112,719  ]     \n</code></pre>\n<p>After I saw the output, I wrote a function to perform the same cleaning operation for each table in each budget.</p><p>For each table below, first I'll introduce the \"raw\" output that Tabula returned, then I'll show the function that I wrote to fix that output.</p><h2 id=\"basic-information\">Basic Information</h2><h3 id=\"raw-output\">Raw Output</h3><pre><code class=\"language-python\">basic_information = sample_budget[0] #basic information\nbasic_information\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Basic Information</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Council District</td>\n      <td>2nd</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Organization Code</td>\n      <td>1380</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>School Level</td>\n      <td>Elementary School</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Economically</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Disadvantaged Rate*</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>83.44%</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_basic_information_table(df):\n    '''Series representing the &quot;basic information&quot; table.'''\n\n    # budgets with a comment near the basic information table, e.g. 2050\n    if df.shape[1] == 3:\n        df = df.iloc[1:, 1:]\n        df = df.reset_index(drop=True)\n        df = df.T.reset_index(drop=True).T\n\n    # After that, Tabula did pretty well for this table, but didn't get the\n    # Economically Disadvanted Rate quite right.\n\n    df.loc[4] = [&quot;Economically Disadvantaged Rate&quot;, df.loc[6, 1]]\n    df = df.loc[1:4, :]\n    return pd.Series(list(df[1]), index=list(df[0]), name='basic_information')\n</code></pre>\n<h3 id=\"cleaned\">Cleaned</h3><pre><code class=\"language-python\">basic_information = generate_basic_information_table(basic_information)\nbasic_information\n</code></pre>\n<pre><code class=\"language-python\"># Basic information output\nCouncil District                                 2nd\nOrganization Code                               1380\nSchool Level                       Elementary School\nEconomically Disadvantaged Rate               83.44%\nName: basic_information, dtype: object\n</code></pre>\n<h2 id=\"enrollment\">Enrollment</h2><h4 id=\"raw-output-1\">Raw Output</h4><pre><code class=\"language-python\"># Getting the enrollment output\nenrollment = sample_budget[1]\nenrollment\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>FY14</td>\n      <td>FY18</td>\n      <td>FY19 Projected</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Enrollment**</td>\n      <td>842</td>\n      <td>640</td>\n      <td>602</td>\n    </tr>\n  </tbody>\n</table>\n</div><h4 id=\"cleanup-function-1\">Cleanup Function</h4><pre><code class=\"language-python\">def generate_enrollment_table(df):\n    '''returns a series representing the &quot;enrollment&quot; table'''\n    # nothing too crazy here\n    df = df.T.loc[1:, :]\n    df_to_series = pd.Series(list(df[1]), index=list(df[0]), name=&quot;enrollment&quot;)\n    return df_to_series.str.replace(',', '').astype(float)\n\ngenerate_enrollment_table(enrollment)\n</code></pre>\n<h4 id=\"cleaned-1\">Cleaned</h4><pre><code class=\"language-python\"># Enrollment table\nFY14              842.0\nFY18              640.0\nFY19 Projected    602.0\nName: enrollment, dtype: float64\n</code></pre>\n<h2 id=\"allotments\">Allotments</h2><p>Luckily, both allotment tables were identical, so I could apply to the same cleanup steps to both.</p><h4 id=\"raw-output-2\">Raw Output</h4><pre><code class=\"language-python\">operating_funded_allotments = sample_budget[2]\noperating_funded_allotments\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Principals/Assistant Principals</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Teachers ‐ Regular Education</td>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Teachers ‐ Special Education</td>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Counselors/Student Adv./ Soc. Serv. Liaisons</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nurses/Health Services</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Classroom Assistants/Teacher Assistants</td>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Secretaries</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Support Services Assistants</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Student Climate Staff</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Other</td>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Total Positions</td>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Supplies/Equipment/Non Full‐Time Salaries/Other</td>\n      <td>$32,272</td>\n      <td>$100,159</td>\n      <td>$97,553</td>\n    </tr>\n  </tbody>\n</table>\n</div><pre><code class=\"language-python\">grant_funded_allotments = sample_budget[3]\ngrant_funded_allotments\n</code></pre>\n<h3 id=\"cleanup-function-2\">Cleanup Function</h3><p>I decided to merge the two allotment tables into one DataFrame while building a MultiIndex to keep things in order. This would allow me to ask some more interesting questions further on down the road.</p><pre><code class=\"language-python\">def generate_allotments_table(df, code, fund):\n    '''Multiindex DF of org code, fund, and budget category by budget year'''\n    df.columns = df.iloc[0]\n    df = df.drop(0)\n    df = df.set_index(['Position/Expenditure'])\n    df = (df.apply(lambda x: x.str.replace('$', '').str.replace(',', ''))\n            .astype(float)\n          )\n    df.name = fund + &quot;ed_allotments&quot;\n\n    df_index_arrays = [\n        [code] * len(df),\n        [fund] * len(df),\n        list(df.index),\n    ]\n\n    df.index = pd.MultiIndex.from_arrays(\n        df_index_arrays,\n        names=(&quot;org_code&quot;, &quot;fund&quot;, &quot;allotment&quot;)\n    )\n    df.columns = [column[:4] for column in df.columns]\n\n    return df\n</code></pre>\n<h4 id=\"cleaned-2\">Cleaned</h4><pre><code class=\"language-python\">pd.concat([\n    generate_allotments_table(\n        operating_funded_allotments, &quot;1410&quot;, &quot;operating_fund&quot;\n    ),\n    generate_allotments_table(\n        grant_funded_allotments, &quot;1410&quot;, &quot;grant_fund&quot;\n    )\n])\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"24\" valign=\"top\">1410</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"totals\">Totals</h2><p>Since the final \"totals\" table could be calculated from the data already in the new allotment table, I didn't bother transforming it in any way.</p><pre><code class=\"language-python\"># same data can be derived from the allotments table directly\nsample_budget[4]\n</code></pre>\n<div class=\"tableContainer\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>Position/Expenditure</td>\n      <td>FY14 Budget</td>\n      <td>FY18 Budget</td>\n      <td>FY19 Budget</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Total Positions</td>\n      <td>NaN</td>\n      <td>76.1</td>\n      <td>64.8</td>\n      <td>67.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total Supplies/Equipment/Non Full‐Time Salarie...</td>\n      <td>NaN</td>\n      <td>$230,726</td>\n      <td>$120,136</td>\n      <td>$112,719</td>\n    </tr>\n  </tbody>\n</table>\n</div><p>Once I figured out what transformations I needed for each table, I combined them into a function so that, given a list of DataFames from Tabula, I'd get those same tables back neatly formatted.</p><pre><code class=\"language-python\">def generate_all_tables(list_of_df):\n    basic_information = generate_basic_information_table(list_of_df[0])\n    enrollment = generate_enrollment_table(list_of_df[1])\n\n    operating_funded_allotments = generate_allotments_table(\n        list_of_df[2],\n        basic_information['Organization Code'],\n        'operating_fund'\n    )\n    grant_funded_allotments = generate_allotments_table(\n        list_of_df[3],\n        basic_information['Organization Code'],\n        'grant_fund'\n    )\n    operating_and_grant_funded_allotments = pd.concat(\n        [operating_funded_allotments, grant_funded_allotments]\n    )\n\n    return basic_information, enrollment, operating_and_grant_funded_allotments\n\nbasic_information, enrollment, operating_and_grant_funded_allotments = \ngenerate_all_tables(sample_budget)\n</code></pre>\n<h2 id=\"aggregation-time\">Aggregation Time</h2><p>Now that I had cleaned the tables that Tabula produced, it was time to combine them into some aggregated tables.</p><p>First I wrote a function that would output a Series (representing one row) of information from all tables for a given school in a given fiscal year. </p><pre><code class=\"language-python\">def generate_row(budget_year, basic_information, allotments, enrollment):\n    '''School budget series for fiscal year.'''\n \t# budget_year should be FY14, FY18, or FY19\n    \n    flattened_allotments = pd.DataFrame(allotments.to_records())\n    flattened_allotments.index = flattened_allotments['fund'] +&quot;: &quot; + flattened_allotments['allotment']\n    flattened_allotments = flattened_allotments.drop(\n        ['fund','allotment'], axis=1\n    )\n    budget_allotments = flattened_allotments[budget_year]\n    \n    enrollment_label = budget_year + ' Projected' if budget_year == &quot;FY19&quot; else budget_year\n    enrollment_index = 'projected_enrollment' if budget_year == &quot;FY19&quot; else 'enrollment'\n    enrollment_row = pd.Series(\n        enrollment[enrollment_label], index=[enrollment_index]\n    )\n    \n    return pd.concat(\n            [basic_information,budget_allotments,enrollment_row],\n            axis=0\n           )\n\ngenerate_row(&quot;FY18&quot;, basic_information,\n             operating_and_grant_funded_allotments, enrollment)\n</code></pre>\n<pre><code class=\"language-python\"># Output\nCouncil District 2 nd\nOrganization Code 1380\nSchool Level Elementary School\nEconomically Disadvantaged Rate 83.44 %\noperating_fund: Principals / Assistant Principal.1\noperating_fund: Teachers‐ Regular Education 25\noperating_fund: Teachers‐ Special Education 2.8\noperating_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.8\noperating_fund: Nurses / Health Services 1\noperating_fund: Classroom Assistants / Teacher Assistants 8\noperating_fund: Secretaries 1\noperating_fund: Support Services Assistants 2\noperating_fund: Student Climate Staff 1\noperating_fund: Other 1.2\noperating_fund: Total Positions 43.8\noperating_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 100159\ngrant_fund: Principals / Assistant Principals 0\ngrant_fund: Teachers‐ Regular Education 8.6\ngrant_fund: Teachers‐ Special Education 0.2\ngrant_fund: Counselors / Student Adv. / Soc.Serv.Liaisons 0.2\ngrant_fund: Nurses / Health Services 0\ngrant_fund: Classroom Assistants / Teacher Assistants 0\ngrant_fund: Secretaries 0\ngrant_fund: Support Services Assistants 5\ngrant_fund: Student Climate Staff 7\ngrant_fund: Other 0\ngrant_fund: Total Positions 21\ngrant_fund: Supplies / Equipment / Non Full‐ Time Salaries / Other 19977\nenrollment 640\ndtype: object\n</code></pre>\n<p>Then, I applied this function to each list of budgets in the collection and compiled them into a DataFrame.</p><pre><code class=\"language-python\">def generate_tabular_budget(budget_year, budgets):\n    '''generate a tabular budget summary for a budget year. Budget year must be FY14,\n    FY18, or FY19. Enrollemnt values for budget year 2019 are projected.'''\n    school_budget_series = []\n    for budget_tables in budgets:\n        basic_information, enrollment, operating_and_grant_funded_allotments = generate_all_tables(\n            budget_tables\n        )\n        budget_row = generate_row(\n            budget_year, basic_information, operating_and_grant_funded_allotments, enrollment\n        )\n        budget_row = budget_row\n        school_budget_series.append(budget_row)\n\n    return pd.DataFrame(school_budget_series)\n\n\nfy14 = generate_tabular_budget('FY14', budgets)\nfy14['budget_year'] = &quot;FY14&quot;\nfy14.to_csv(&quot;output/combined_fy14.csv&quot;)\n\nfy18 = generate_tabular_budget('FY18', budgets)\nfy18['budget_year'] = &quot;FY18&quot;\nfy18.to_csv(&quot;output/combined_fy18.csv&quot;)\n\nfy19 = generate_tabular_budget('FY19', budgets)\nfy19['budget_year'] = &quot;FY19&quot;\nfy19.to_csv(&quot;output/combined_fy19.csv&quot;)\n\n\ncombined_tabular_budgets = pd.concat([fy14, fy18, fy19])\ncombined_tabular_budgets.to_csv(&quot;output/all_budgets_tabular.csv&quot;)\n</code></pre>\n<p>Finally, I wanted to output a CSV that would preserve some of the multi-indexed nature of the allotment tables. Here's what I wrote for that.</p><pre><code class=\"language-python\">def generate_hierarchical_budget(budgets):\n    school_budgets_dfs = []\n    for budget_tables in budgets:\n        school_budgets_dfs.append(operating_and_grant_funded_allotments)\n    return pd.concat(school_budgets_dfs)\n\nhierarchical_budget = generate_hierarchical_budget(budgets)\nhierarchical_budget.to_csv(&quot;output/all_budgets_hierarchical.csv&quot;)\n\nhierarchical_budget\n</code></pre>\n<div class=\"tableContainer\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>org_code</th>\n      <th>fund</th>\n      <th>allotment</th>       \n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"61\" valign=\"top\">1380</th>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">grant_fund</th>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">operating_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>30.2</td>\n      <td>25.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>6.0</td>\n      <td>2.8</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>11.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>0.0</td>\n      <td>1.2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>60.0</td>\n      <td>43.8</td>\n      <td>50.1</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>32272.0</td>\n      <td>100159.0</td>\n      <td>97553.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">grant_fund</th>\n      <th>Principals/Assistant Principals</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8.1</td>\n      <td>8.6</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16.1</td>\n      <td>21.0</td>\n      <td>17.7</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>198454.0</td>\n      <td>19977.0</td>\n      <td>15166.0</td>\n    </tr>\n  </tbody>\n</table>\n<div style=\"text-align: right;\n    width: 100%;\n    font-family: Gordita-Medium,sans-serif;\n    font-size: .9em;\n    margin-top: -20px;\">5160 rows × 3 columns</div>\n</div><p>This makes it easier to aggregate in interesting ways:</p><pre><code class=\"language-python\">hierarchical_budget.groupby('allotment').sum()\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>FY14</th>\n      <th>FY18</th>\n      <th>FY19</th>\n      <th>allotment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Classroom Assistants/Teacher Assistants</th>\n      <td>2365.0</td>\n      <td>1720.0</td>\n      <td>1935.0</td>\n    </tr>\n    <tr>\n      <th>Counselors/Student Adv./ Soc. Serv. Liaisons</th>\n      <td>258.0</td>\n      <td>215.0</td>\n      <td>258.0</td>\n    </tr>\n    <tr>\n      <th>Nurses/Health Services</th>\n      <td>129.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Other</th>\n      <td>215.0</td>\n      <td>258.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Principals/Assistant Principals</th>\n      <td>430.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Secretaries</th>\n      <td>215.0</td>\n      <td>215.0</td>\n      <td>215.0</td>\n    </tr>\n    <tr>\n      <th>Student Climate Staff</th>\n      <td>1720.0</td>\n      <td>1720.0</td>\n      <td>1505.0</td>\n    </tr>\n    <tr>\n      <th>Supplies/Equipment/Non Full‐Time Salaries/Other</th>\n      <td>49606090.0</td>\n      <td>25829240.0</td>\n      <td>24234585.0</td>\n    </tr>\n    <tr>\n      <th>Support Services Assistants</th>\n      <td>1505.0</td>\n      <td>1505.0</td>\n      <td>1720.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Regular Education</th>\n      <td>8234.5</td>\n      <td>7224.0</td>\n      <td>7224.0</td>\n    </tr>\n    <tr>\n      <th>Teachers ‐ Special Education</th>\n      <td>1290.0</td>\n      <td>645.0</td>\n      <td>1075.0</td>\n    </tr>\n    <tr>\n      <th>Total Positions</th>\n      <td>16361.5</td>\n      <td>13932.0</td>\n      <td>14577.0</td>\n    </tr>\n  </tbody>\n</table>\n</div><h2 id=\"more-cleaning-to-be-done\">More Cleaning to be Done</h2><p>My work here is done. I saved the data from their not-so-accessible PDF prisons. But now it's time for someone with some domain-specific knowledge to make it actionable.</p><p>The biggest weakness with the data in its current form is that there is some amount of ambiguity as to what the different allotments numbers represent in real-dollar amounts. Only the <strong>Supplies/Equipment/Non Full‐Time Salaries/Other</strong> allotment category came in currency notation – the rest of the allotments were represented as simple decimal amounts with no context to help interpret what they mean. Do they represent <a href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\">FTE</a>? Dollar amounts in scientific notation? I'm not sure, but I hope by handing this work off to the right people, these questions and more can be answered more easily thanks to a cleaner, more accessible data set.</p>","url":"https://hackersandslackers.com/liberating-data-from-pdfs-with-tabula-and-pandas/","uuid":"ab1a4ee3-9cc3-43a6-9ebe-5a885ae264a2","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bddd5323ea1e4769817c4c9"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb8673730","title":"Create a REST API Endpoint Using AWS Lambda","slug":"create-a-rest-api-endpoint-using-aws-lambda","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","excerpt":"Use Python and MySQL to Build an Endpoint.","custom_excerpt":"Use Python and MySQL to Build an Endpoint.","created_at_pretty":"29 October, 2018","published_at_pretty":"30 October, 2018","updated_at_pretty":"06 January, 2019","created_at":"2018-10-29T19:26:03.000-04:00","published_at":"2018-10-29T22:08:06.000-04:00","updated_at":"2019-01-05T19:57:04.000-05:00","meta_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","meta_description":"Use Python and MySQL to Build an Endpoint","og_description":"Use Python and MySQL to Build an Endpoint","og_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","og_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","twitter_description":"Use Python and MySQL to Build an Endpoint","twitter_image":"https://hackersandslackers.com/content/images/2018/10/apigateway4-3@2x.jpg","twitter_title":"Create a REST API Endpoint Using AWS Lambda | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},"tags":[{"name":"AWS","slug":"aws","description":"Monolithic cloud architecture via microservices. Become familiar with AWS products, account administration, security practices, and tips to make it all easier.","feature_image":null,"meta_description":"Become familiar with AWS services, account administration, security practices, and tips to make it all easier.","meta_title":"Learn AWS | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"MySQL","slug":"mysql","description":"Database configuration, building queries, and cloud hosting options for MySQL.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/mysqlrevamp_o.jpg","meta_description":"Database configuration, building queries, and cloud hosting options for MySQL.","meta_title":"Working with MySQL | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"},{"name":"#Creating APIs in AWS","slug":"create-an-aws-api","description":"Create a REST API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/pythonlambda.jpg","meta_description":"Create an API in AWS with industry-standard services such as Lambda Functions, RDS, and API Gateway.","meta_title":"Create a REST API in AWS","visibility":"internal"}],"plaintext":"Now that you know your way around API Gateway,  you have the power to create\nvast collections of endpoints. If only we could get those endpoints to actually\nreceive and return some stuff. \n\nWe'll create a GET function which will solve the common task of retrieving data\nfrom a database. The sequence will look something like:\n\n * Connect to the database\n * Execute the relevant SQL query\n * Map values returned by the query to a key/value dictionary \n * Return a response body containing the prepared response\n\nTo get started, create a project on your local machine (this is necessary as\nwe'll need to upload a library to import). We're ultimately going to have 3\nitems:\n\n * rds_config.py: Credentials for your RDS database\n * lambda_function.py: The main logic of your function, via the 'handler'\n * pymysql: A lightweight Python library to run SQL queries\n\nStoring Credentials Like an Idiot\nFor the sake of this tutorial and to avoid a security best-practices tangent,\nI'm going to do something very bad: store credentials in plain text. Don't ever\ndo this:  there are much better ways to handle secrets like these, such as using\nAWS Secrets Manager.\n\n# rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n\n\nThe Holy lambda_function.py\nThis is where the magic happens. For this GET call, we're simply going to get\nall records from a table in a database and return them in a consumable way for\nwhomever will ultimately use the API.\n\nRemember that Lambda expects you to specify the function upon initialization.\nThis can be set in the \"Handler\" field here:\n\nWhere 'lambda_function' is the file, and 'handler' is the function.Let's build\nthis thing:\n\nimport sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(\"select * from employees\")\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n\n\nCheck out what's happening in our handler function. We're:\n\n * Establishing a DB connection\n * Running a select all  query for a table in our database\n * Iterating over each row returned by the query\n * Mapping values to a dict\n * Appending each generated dict to an array\n * Returning the array as our response body\n\nPyMySQL\nThe shitty thing about the AWS console is there's no way to install python\nlibraries via the UI, so we need to do this locally. In your project folder,\ninstall PyMySQL by using something like virtualenv:\n\n$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n\n\nThat will install the pymysql library in your environment bin. Copy that into\nyour main directory where lambda_function.py lives.\n\nGame time\nIn your project folder, make a zip file of lambda_function.py, rds_config.py,\nand PyMySQL. Upload your ZIP file via the \"Code entry type\" field:\n\nS3 could also work.Save your function and run a test via the top right menu.\nWhen asked to specify a test type, select a standard API call. Your results\nshould look like this:\n\nTest results always appear at the top of the Lambda editor page.Post Functions\nCreating a POST function isn't much more complicated. Obviously we're\nessentially doing the reverse of before: we're expecting information to be\npassed, which we'll add to a database.\n\nlambda_function.py\nimport sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = \"myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com\"\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(\"ERROR: Unexpected error: Could not connect to MySql instance.\")\n    sys.exit()\n\nlogger.info(\"SUCCESS: Connection to RDS mysql instance succeeded\")\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = \"INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)\"\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n\n\nParameters in a post function are contained in the event parameter we pass tot\nhe handler. We first create a dict to associate these values. Pay attention to\nhow we structured our sql query for best PyMySQL best practice.\n\nPost functions expect a response body to contain (at the very least) a status\ncode as well as a body. We'll stick to bare minimums here and tell the user is\ngood to go, and recap what was added.\n\nFor the sake of this demo we kept things simple with an insert query, but keep\nin mind this means the same record can never be added twice or updated in this\nmanner- you might be better suited by something such as REPLACE. Just something\nto keep in mind as you're building your app.","html":"<p>Now that you know your way around <strong>API Gateway,</strong> you have the power to create vast collections of endpoints. If only we could get those endpoints to actually receive and return some stuff. </p><p>We'll create a GET function which will solve the common task of retrieving data from a database. The sequence will look something like:</p><ul><li>Connect to the database</li><li>Execute the relevant SQL query</li><li>Map values returned by the query to a key/value dictionary </li><li>Return a response body containing the prepared response</li></ul><p>To get started, create a project on your local machine (this is necessary as we'll need to upload a library to import). We're ultimately going to have 3 items:</p><ul><li><strong>rds_config.py</strong>: Credentials for your RDS database</li><li><strong>lambda_function.py</strong>: The main logic of your function, via the 'handler'</li><li><strong>pymysql</strong>: A lightweight Python library to run SQL queries</li></ul><h3 id=\"storing-credentials-like-an-idiot\">Storing Credentials Like an Idiot</h3><p>For the sake of this tutorial and to avoid a security best-practices tangent, I'm going to do something very bad: store credentials in plain text. <strong>Don't ever do this:</strong> there are much better ways to handle secrets like these, such as using AWS Secrets Manager.</p><pre><code class=\"language-python\"># rds_config.py\n\ndb_username = 'myUser'\ndb_password = 'jigheu896vf7bd'\ndb_name = 'myDatabase'\n</code></pre>\n<h3 id=\"the-holy-lambda_function-py\">The Holy lambda_function.py</h3><p>This is where the magic happens. For this GET call, we're simply going to get all records from a table in a database and return them in a consumable way for whomever will ultimately use the API.</p><p>Remember that Lambda expects you to specify the function upon initialization. This can be set in the \"Handler\" field here:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.11.09-PM.png\" class=\"kg-image\"><figcaption>Where 'lambda_function' is the file, and 'handler' is the function.</figcaption></figure><p>Let's build this thing:</p><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# connect using creds from rds_config.py\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\n# array to store values to be returned\nrecords = []\n\n# executes upon API event\ndef handler(event, context):\n   with conn.cursor() as cur:\n   cur.execute(&quot;select * from employees&quot;)\n   conn.commit()\n   for row in cur:\n            record = {\n                    'employee_id': row[1],\n                    'employee_info': {\n                        'firstname': row[2],\n                        'lastname': row[3],\n                        'email': row[4],\n                    }\n                }\n            records.append(record)\n    return records\n\n</code></pre>\n<p>Check out what's happening in our handler function. We're:</p><ul><li>Establishing a DB connection</li><li>Running a <em>select all</em> query for a table in our database</li><li>Iterating over each row returned by the query</li><li>Mapping values to a dict</li><li>Appending each generated dict to an array</li><li>Returning the array as our response body</li></ul><h3 id=\"pymysql\">PyMySQL</h3><p>The shitty thing about the AWS console is there's no way to install python libraries via the UI, so we need to do this locally. In your project folder, install PyMySQL by using something like virtualenv:</p><pre><code class=\"language-python\">$ virtualenv lambdaenv\n$ source lambdaenv/bin/activate\n$ pip3 install pymysql\n</code></pre>\n<p>That will install the pymysql library in your environment bin. Copy that into your main directory where lambda_function.py lives.</p><h3 id=\"game-time\">Game time</h3><p>In your project folder, make a zip file of lambda_function.py, rds_config.py, and PyMySQL. Upload your ZIP file via the \"Code entry type\" field:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-2.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.28.18-PM.png\" class=\"kg-image\"><figcaption>S3 could also work.</figcaption></figure><p>Save your function and run a test via the top right menu. When asked to specify a test type, select a standard API call. Your results should look like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-05-13-at-6.21.23-PM.png\" class=\"kg-image\"><figcaption>Test results always appear at the top of the Lambda editor page.</figcaption></figure><h2 id=\"post-functions\">Post Functions</h2><p>Creating a POST function isn't much more complicated. Obviously we're essentially doing the reverse of before: we're expecting information to be passed, which we'll add to a database.</p><h3 id=\"lambda_function-py\">lambda_function.py</h3><pre><code class=\"language-python\">import sys\nimport logging\nimport rds_config\nimport pymysql\nimport json\n\n# rds settings\nrds_host  = &quot;myDatabase.ghfghghgf.us-east-1.rds.amazonaws.com&quot;\nname = rds_config.db_username\npassword = rds_config.db_password\ndb_name = rds_config.db_name\n\n# logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ntry:\n    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)\nexcept:\n    logger.error(&quot;ERROR: Unexpected error: Could not connect to MySql instance.&quot;)\n    sys.exit()\n\nlogger.info(&quot;SUCCESS: Connection to RDS mysql instance succeeded&quot;)\n\ndef handler(event, context):\n    data = {\n        json.dumps({\n        'key': event['id'],\n        'email': event['email'],\n        'firstname': event['firstname'],\n        'lastname': event['lastname'],\n    }\n    with conn.cursor() as cur:\n        sql = &quot;INSERT INTO `workers` (`key`, `email`, `firstname`, `lastname`) VALUES (%s, %s, %s, %s)&quot;\n        cur.execute(sql, (data['key'], data['email'], data['firstname'], data['lastname']))\n        conn.commit()\n    \n    return {\n        'statusCode': 200,\n        'body': data,\n        })\n    }\n\n</code></pre>\n<p>Parameters in a post function are contained in the event parameter we pass tot he handler. We first create a dict to associate these values. Pay attention to how we structured our sql query for best PyMySQL best practice.</p><p>Post functions expect a response body to contain (at the very least) a status code as well as a body. We'll stick to bare minimums here and tell the user is good to go, and recap what was added.</p><p>For the sake of this demo we kept things simple with an insert query, but keep in mind this means the same record can never be added twice or updated in this manner- you might be better suited by something such as <code>REPLACE</code>. Just something to keep in mind as you're building your app.</p>","url":"https://hackersandslackers.com/create-a-rest-api-endpoint-using-aws-lambda/","uuid":"143ebe65-2939-4930-be08-a6bbe6fc09cf","page":false,"codeinjection_foot":"","codeinjection_head":"","comment_id":"5bd7970b97b9c46d478e36f5"}},{"node":{"id":"Ghost__Post__5c12d7bfe875ad7bb867372d","title":"Create Google Cloud Functions Running Python 3.7","slug":"creating-a-python-google-cloud-function","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","excerpt":"GCP scores another victory by trivializing serverless functions.","custom_excerpt":"GCP scores another victory by trivializing serverless functions.","created_at_pretty":"18 October, 2018","published_at_pretty":"19 October, 2018","updated_at_pretty":"14 February, 2019","created_at":"2018-10-18T19:44:02.000-04:00","published_at":"2018-10-18T22:33:07.000-04:00","updated_at":"2019-02-13T23:13:40.000-05:00","meta_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","meta_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","og_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","og_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","og_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","twitter_description":"Create cloud functions and endpoints with ease using Google Cloud's Cloud Functions and Source Repositories.","twitter_image":"https://hackersandslackers.com/content/images/2018/10/googlefunc-1@2x.jpg","twitter_title":"Creating Google Cloud Functions Running Python | Hackers and Slackers","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Architecture","slug":"architecture","description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to Lambda functions, Docker containers, Google Cloud functions,  Kubernetes, Heroku, etc.","feature_image":null,"meta_description":"Advancements in software architecture, serverless and beyond. Examples include equivalents to cloud functions, Docker containers, Kubernetes, Heroku, etc.","meta_title":"Software Architecture | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"The more I explore Google Cloud's endless catalog of cloud services, the more I\nreally like Google Cloud. This is why before moving forward, I'd like to be\ntransparent that this blog has become little more than thinly veiled Google\npropaganda, where I will henceforth bombard you with persuasive and subtle\nmessaging to sell your soul to Google. Let's be honest; they've probably\nsimulated it anyway.\n\nIt should be safe to assume that you're fairly familiar with AWS Lambda\nFunctions [https://hackersandslackers.com/creating-endpoints-with-lambda/],\nwhich have served as the backbone of what we refer to as \"serverless.\" These\ncode snippets in the cloud have restructured entire IT departments and are\npartially why almost nobody knows enough basic Linux to configure a web server\nor build anything without a vendor. In my opinion, Google Cloud Functions are\nbetter than that, so strap in.\n\nAWS vs GCP Comparison\nFirst off, let's talk about a big one: price. AWS charges based on Lambda usage,\nwhereas Google Cloud Functions are free. The only exception to this is when you\nbreak 2 million invocations/month, at which point you'll be hemorrhaging as\nghastly 40 cents per additional million. That's ridiculous. I think we've just\ndiscovered Google Cloud's lead generation strategy.\n\nWhat about in terms of workflow? AWS holds an architecture philosophy of\nchaining services together, into what inevitably becomes a web of self-contained\nbillable items on your invoice. A fine illustration of this is a fine post on \ncommon AWS patterns\n[https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/]  which\nprovides a decent visual of this complexity, while also revealing how much\npeople apparently love this kind of shit, as though SaaS is the new Legos. To\ninteract with a Lambda function in AWS via HTTP requests, you need to set up an\nAPI Gateway in front. I hate setting up API Gateways: it's a feat more\nconvoluted and difficult than actually coding. Pair this with an inevitable user\npermission struggle just to get the right Lambda roles set up, and you quickly\nhave yourself a nightmare- especially  if you're just trying to get a single\nfunction live.  Eventually you’ll get to write some code or upload a horrendous\nzip file like some sort of neanderthal (friendly reminder: I am entirely\nbiased).\n\nGCP has clearly been taking notes on the sidelines on how to improve this\nprocess by removing red tape around service setup or policy configuration. AWS\nand GCP are tackling opposites approaches; AWS allows you to build a Robust API\ncomplete with staging and testing with the intent that some of these APIs can\neven be sold as standalone products to consumers. GCP takes the opposite\napproach: cloud functions are services intended for developers to develop, which\ncovers the vast majority of use cases in my opinion.\n\nCloud Function Deployment\nTo create our first function to serve as an endpoint, we'll utilize the\nfollowing:\n\n * A new Cloud Function  running Python 3.7\n * Google's Source Repositories: AKA a Github/Bitbucket clone with auto-syncing\n   to your real repos, along with direct access to GCP services (think Heroku's\n   source control).\n * The gcloud  CLI to enable us to work locally.\n\nYou should immediately notice the glaring lack of any mentions of API endpoints,\nmethods, stages, or anything related to handling web requests. It should not be\nunderstated that Cloud Functions are preconfigured with an endpoint, and all\nnonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is\nmissing entirely. These things are handled in the function itself, and because\nGoogle Cloud functions running Python are preconfigured with Flask, all of that\nstuff is really trivially easy.  That's right, we've got Flask, Python,  and GCP \n all in a single post. Typing these words feels like eating cake while Dwyane\nThe Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's\ngreat.\n\nCreate your Function\nOur function will intend to serve as a Python HTTP endpoint:\n\nSingle-page setup. Easy. * Trigger  specifies what will have access to this function. By selecting HTTP,\n   we will immediately receive a URL.\n * Source code  gives us a few options to deploy our code with cloud source\n   repository  being by far the easiest solution, especially when working\n   locally.\n * Runtime  allows you to select NodeJS by accident.\n\nBefore we get to code, let's talk Python libraries.\n\nIncluding Dependencies in your Function\nOur function comes prepared with two files: main.py  and our friend \nrequirements.txt. These files do exactly what you'd expect, as per every project\never:\n\nUnfortunately, ease-of-use ensures that GCP certifications will be in low\ndemand.Our function immediately installs all dependencies in requirements.txt  for use\nupon deployment. Once deployed, we can import these libraries as expected. So,\nlet's deploy something.\n\nGoogle Source Repositories\nGoogle's source repositories can serve as a stand-in replacement for Github\n(unlikely), or auto-sync to any repo on the version control behemoth of your\nchoice. The advantage of this extra layer is mostly to trigger deployments upon\ncommits, which in turn feed into GCP's own CI/CD processes (which remain young\nfor now). Create a repo locally using gcloud:\n\n$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n\n\nNow make that puppy go live with gcloud functions deploy totally-dope-function,\nwhere totally-dope-function  is name of your function, as it should be.\n\nNow let's get to the coding part.\n\nThe Coding Part (ft. Flask)\nHere's perhaps the most basic endpoint you'll ever create:\n\nimport requests\n\ndef endpoint(request):\n    \"\"\"Does the things.\"\"\"\n    if request.method == 'POST':\n    # Allows POST requests from any origin with the Content-Type\n    # header and caches preflight response for an 3600s\n    headers = {\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'POST',\n        'Access-Control-Allow-Headers': 'Content-Type',\n        'Access-Control-Max-Age': '3600'\n    }\n    request_json = request.get_json()\n    if request_json:\n        plaintext = request_json['plain']\n        html = request_json['html']\n        return html\n    else:\n        return 'You didn't pass a JSON body you idiot.'\n\n\nIf you're familiar with Flask (you are, because you're on this blog) you already\nknow what all of this does. Look at that simple copy-paste of headers, as\nopposed to working them into a horrible web interface. Gasp in disbelief as you\nrealize that typing if request.method == 'POST':  would be a 10-minute task in a\nvisual API building tool. We've made it fam.\n\nEase of Logging\nBecause we have a real endpoint to work with, we don't need to waste any time\nsimulating stupid fucking tests where we send fake JSON to our function. We can\nuse postman or anything to immediately interact with our endpoint, and the logs\nare a click away:\n\nEasy. Breezy. Beautiful.There's no point in me droning on at this point because\nyou've surely already ventured into the Google Cloud console in blissful\ndisbelief as a good obedient drone would. If adopting Google Cloud is the last\nshred of hope we have to resist Google's all-knowing algorithms which have\nalready replaced the illusion of free will, I'd gladly take that dystopia over\nsetting up monolithic API Gateways any day.\n\nThe Downsides\nTime for the asterisks to kill that euphoric buzz you might've experienced for a\nbrief moment. My sole purpose as an engineer is to have my dreams crushed\nfull-time; I simply cant resist returning the favor.\n\nFirst notable drawback of Cloud functions is a lack of out-of-the-box custom DNS \n configuration. Firebase has workarounds for this, but Firebase is a beast of\nits own.\n\nWhen it comes to debugging, functions tend to fall short in comparison to their\nLambda rivals. Most Cloud Function debugging involves deploying, testing in dev,\nand sifting through cryptic error logs (they can be quite bad). There's nearly\nno UI mock testing  to speak of. You'd better brush up on PyTest.\n\nMy best advice is to be careful with what services you play around with on GCP.\nLet's not forget this is a platform geared exclusively towards enterprises; the\nfact that we're even playing ball here makes us weirdos in the first place.\nDon't let yourself hemorrhage money like an enterprise.","html":"<p>The more I explore Google Cloud's endless catalog of cloud services, the more I really like Google Cloud. This is why before moving forward, I'd like to be transparent that this blog has become little more than thinly veiled Google propaganda, where I will henceforth bombard you with persuasive and subtle messaging to sell your soul to Google. Let's be honest; they've probably simulated it anyway.</p><p>It should be safe to assume that you're fairly familiar with AWS <a href=\"https://hackersandslackers.com/creating-endpoints-with-lambda/\">Lambda Functions</a>, which have served as the backbone of what we refer to as \"serverless.\" These code snippets in the cloud have restructured entire IT departments and are partially why almost nobody knows enough basic Linux to configure a web server or build anything without a vendor. In my opinion, Google Cloud Functions are better than <em>that</em>, so strap in.</p><h2 id=\"aws-vs-gcp-comparison\">AWS vs GCP Comparison</h2><p>First off, let's talk about a big one: price. AWS charges based on Lambda usage, whereas Google Cloud Functions are <strong>free</strong>. The only exception to this is when you break 2 million invocations/month, at which point you'll be hemorrhaging as ghastly <strong>40 cents per additional million. </strong>That's ridiculous. I think we've just discovered Google Cloud's lead generation strategy.</p><p>What about in terms of workflow? AWS holds an architecture philosophy of chaining services together, into what inevitably becomes a web of self-contained billable items on your invoice. A fine illustration of this is a fine post on <a href=\"https://www.jeremydaly.com/serverless-microservice-patterns-for-aws/\">common AWS patterns</a> which provides a decent visual of this complexity, while also revealing how much people apparently love this kind of shit, as though SaaS is the new Legos. To interact with a Lambda function in AWS via HTTP requests, you need to set up an API Gateway in front. I hate setting up API Gateways: it's a feat more convoluted and difficult than actually coding. Pair this with an inevitable user permission struggle just to get the right Lambda roles set up, and you quickly have yourself a nightmare- <em>especially</em> if you're just trying to get a single function live.<em> </em>Eventually you’ll get to write some code or upload a horrendous zip file like some sort of neanderthal (friendly reminder: I am entirely biased).</p><p>GCP has clearly been taking notes on the sidelines on how to improve this process by removing red tape around service setup or policy configuration. AWS and GCP are tackling opposites approaches; AWS allows you to build a Robust API complete with staging and testing with the intent that some of these APIs can even be sold as standalone products to consumers. GCP takes the opposite approach: cloud functions are services intended for developers to develop, which covers the vast majority of use cases in my opinion.</p><h2 id=\"cloud-function-deployment\">Cloud Function Deployment</h2><p>To create our first function to serve as an endpoint, we'll utilize the following:</p><ul><li>A new <strong>Cloud Function</strong> running Python 3.7</li><li>Google's <strong>Source Repositories: </strong>AKA a Github/Bitbucket clone with auto-syncing to your real repos, along with direct access to GCP services (think Heroku's source control).</li><li>The <strong>gcloud</strong> CLI to enable us to work locally.</li></ul><p>You should immediately notice the glaring lack of any mentions of API endpoints, methods, stages, or anything related to handling web requests. It should not be understated that <em>Cloud Functions are preconfigured with an endpoint</em>, and all nonsense regarding whether endpoints accept GET or POST or AUTH or OPTIONs is missing entirely. These things are handled in the function itself, and because Google Cloud functions running Python are preconfigured with <strong>Flask, </strong>all of that stuff is <em>really trivially easy.</em> That's right, we've got <em>Flask</em>, <em>Python</em>,<em> </em>and <em>GCP</em> all in a single post. Typing these words feels like eating cake while Dwyane The Rock Johnson reads me bedtime stories and caresses me as I fall asleep. It's great.</p><h3 id=\"create-your-function\">Create your Function</h3><p>Our function will intend to serve as a Python HTTP endpoint:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/func.gif\" class=\"kg-image\"><figcaption>Single-page setup. Easy.</figcaption></figure><ul><li><strong>Trigger</strong> specifies what will have access to this function. By selecting HTTP, we will immediately receive a URL.</li><li><strong>Source code</strong> gives us a few options to deploy our code with <em>cloud source repository</em> being by far the easiest solution, especially when working locally.</li><li><strong>Runtime</strong> allows you to select NodeJS by accident.</li></ul><p>Before we get to code, let's talk Python libraries.</p><h3 id=\"including-dependencies-in-your-function\">Including Dependencies in your Function</h3><p>Our function comes prepared with two files: <code>main.py</code> and our friend <code>requirements.txt</code>. These files do exactly what you'd expect, as per every project ever:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-4.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2018-10-18-at-9.14.17-PM.png\" class=\"kg-image\"><figcaption>Unfortunately, ease-of-use ensures that GCP certifications will be in low demand.</figcaption></figure><p>Our function immediately installs all dependencies in <code>requirements.txt</code> for use upon deployment. Once deployed, we can import these libraries as expected. So, let's deploy something.</p><h3 id=\"google-source-repositories\">Google Source Repositories</h3><p>Google's source repositories can serve as a stand-in replacement for Github (unlikely), or auto-sync to any repo on the version control behemoth of your choice. The advantage of this extra layer is mostly to trigger deployments upon commits, which in turn feed into GCP's own CI/CD processes (which remain young for now). Create a repo locally using gcloud:</p><pre><code class=\"language-bash\">$ gcloud source repos create real-repo\n$ cd myproject/\n$ git init\n--------------------------------------------------------\n(take a moment to write or save some actual code here)\n--------------------------------------------------------\n$ git add --all\n$ git remote add google https://source.developers.google.com/p/hackers/r/real-repo\n$ git commit -m 'cheesey init message'\n$ git push --all google\n</code></pre>\n<p>Now make that puppy go live with <code>gcloud functions deploy totally-dope-function</code>, where <em><strong>totally-dope-function</strong> </em>is name of your function, as it should be.</p><p>Now let's get to the coding part.</p><h2 id=\"the-coding-part-ft-flask-\">The Coding Part (ft. Flask)</h2><p>Here's perhaps the most basic endpoint you'll ever create:</p><pre><code class=\"language-python\">import requests\n\ndef endpoint(request):\n    &quot;&quot;&quot;Does the things.&quot;&quot;&quot;\n    if request.method == 'POST':\n    # Allows POST requests from any origin with the Content-Type\n    # header and caches preflight response for an 3600s\n    headers = {\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Methods': 'POST',\n        'Access-Control-Allow-Headers': 'Content-Type',\n        'Access-Control-Max-Age': '3600'\n    }\n    request_json = request.get_json()\n    if request_json:\n        plaintext = request_json['plain']\n        html = request_json['html']\n        return html\n    else:\n        return 'You didn't pass a JSON body you idiot.'\n</code></pre>\n<p>If you're familiar with Flask (you are, because you're on this blog) you already know what all of this does. Look at that simple copy-paste of headers, as opposed to working them into a horrible web interface. Gasp in disbelief as you realize that typing <code>if request.method == 'POST':</code> would be a 10-minute task in a visual API building tool. We've made it fam.</p><h3 id=\"ease-of-logging\">Ease of Logging</h3><p>Because we have a real endpoint to work with, we don't need to waste any time simulating stupid fucking tests where we send fake JSON to our function. We can use postman or anything to immediately interact with our endpoint, and the logs are a click away:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-3.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/logs.gif\" class=\"kg-image\"><figcaption>Easy. Breezy. Beautiful.</figcaption></figure><p>There's no point in me droning on at this point because you've surely already ventured into the Google Cloud console in blissful disbelief as a good obedient drone would. If adopting Google Cloud is the last shred of hope we have to resist Google's all-knowing algorithms which have already replaced the illusion of free will, I'd gladly take that dystopia over setting up monolithic <em>API Gateways </em>any day.</p><h2 id=\"the-downsides\">The Downsides</h2><p>Time for the asterisks to kill that euphoric buzz you might've experienced for a brief moment. My sole purpose as an engineer is to have my dreams crushed full-time; I simply cant resist returning the favor.</p><p>First notable drawback of Cloud functions is a <strong>lack of out-of-the-box custom DNS</strong> configuration. Firebase has workarounds for this, but Firebase is a beast of its own.</p><p>When it comes to debugging, functions tend to fall short in comparison to their Lambda rivals. Most Cloud Function debugging involves deploying, testing in dev, and sifting through cryptic error logs (they can be quite bad). There's nearly no <strong>UI mock testing</strong> to speak of. You'd better brush up on PyTest.</p><p>My best advice is to <em>be careful </em>with what services you play around with on GCP. Let's not forget this is a platform geared exclusively towards enterprises; the fact that we're even playing ball here makes us weirdos in the first place. Don't let yourself hemorrhage money like an enterprise.</p>","url":"https://hackersandslackers.com/creating-a-python-google-cloud-function/","uuid":"ec428cb9-976e-4578-a3de-9120a0dd7352","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5bc91ac23d1eab214413b12b"}}]}},"pageContext":{"slug":"python","limit":12,"skip":12,"numberOfPages":7,"humanPageNumber":2,"prevPageNumber":1,"nextPageNumber":3,"previousPagePath":"/tag/python/","nextPagePath":"/tag/python/page/3/"}}