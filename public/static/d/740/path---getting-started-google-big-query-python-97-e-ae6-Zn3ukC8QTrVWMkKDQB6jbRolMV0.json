{"data":{"ghostPost":{"id":"Ghost__Post__5c47584f4f3823107c9e8f23","title":"Google BigQuery's Python SDK: Creating Tables Programmatically","slug":"getting-started-google-big-query-python","featured":false,"feature_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","custom_excerpt":"Create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","created_at_pretty":"22 January, 2019","published_at_pretty":"02 February, 2019","updated_at_pretty":"28 March, 2019","created_at":"2019-01-22T12:52:15.000-05:00","published_at":"2019-02-02T09:24:00.000-05:00","updated_at":"2019-03-28T17:06:20.000-04:00","meta_title":"Google BigQuery's Python SDK: Creating Tables | Hackers and Slackers","meta_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","og_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","og_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","twitter_description":"Leverage Google Cloud's Python SDK to create tables in Google BigQuery, auto-generate their schemas, and retrieve said schemas.","twitter_image":"https://hackersandslackers.com/content/images/2019/02/bigquery101@2x.jpg","twitter_title":"Google BigQuery's Python SDK: Creating Tables Programmatically","authors":[{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"}],"primary_author":{"name":"Todd Birchard","slug":"todd","bio":"Product manager turned engineer with an ongoing identity crisis. Breaks everything before learning best practices. Completely normal and emotionally stable.","profile_image":"https://hackersandslackers.com/content/images/2019/03/todd3.jpg","twitter":"@ToddRBirchard","facebook":null,"website":"https://toddbirchard.com"},"primary_tag":{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},"tags":[{"name":"Big Data","slug":"bigdata","description":"Work with unstructured data across file types and schemas. Tools such as data warehouses, Hadoop, Spark, BigQuery, etc.","feature_image":null,"meta_description":"Work with massive amounts of unstandardized data across file types and schemas. Includes working with data warehouses, Hadoop, Spark, BigQuery, etc.","meta_title":"Big Data | Hackers and Slackers","visibility":"public"},{"name":"Python","slug":"python","description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold.","feature_image":null,"meta_description":"Let us feed your endless Python addiction! Regardless of where you stand as a Pythonista, our team of pros are constantly teaching and sharing pythonic gold","meta_title":"Python Tricks, Hacks, and Snippets | Hackers and Slackers","visibility":"public"},{"name":"Google Cloud","slug":"googlecloud","description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","feature_image":"https://res.cloudinary.com/hackers-and-slackers/image/upload/q_auto:good/v1/images/googleseries2.jpg","meta_description":"Evaluating Google Cloud Platform’s offerings. Get introduced with tutorials, see our vendor comparisons, and endure biased opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud | Hackers and Slackers","visibility":"public"},{"name":"#The Rise of Google Cloud","slug":"the-rise-of-google-cloud","description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","feature_image":"https://res-5.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/ADF7E324-9EAD-4F15-8670-AF205E6804EA.png","meta_description":"Build robust serverless architecture on Google Cloud Platform. Learn through tutorials, make comparisons, and hear opinions on GCP as a whole.","meta_title":"The Rise of Google Cloud","visibility":"internal"},{"name":"Data Engineering","slug":"dataengineering","description":"The systematic collection and transformation of data via the creation of tools and pipelines.","feature_image":null,"meta_description":null,"meta_title":"Data Engineering | Hackers and Slackers","visibility":"public"}],"plaintext":"GCP is on the rise, and it's getting harder and harder to have conversations\naround data without addressing the 500-pound gorilla in the room: Google\nBigQuery. With most enterprises comfortably settled into their Apache-based Big\nData stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers\nAnd Slackers is no such enterprise. Thus, we aren't afraid to ask the Big\nquestion: how much easier would life be with BigQuery?\n\nBig Data, BigQuery\nIn short, BigQuery trivializes the act of querying against multiple,\nunpredictable data sources. To better understand when this is useful, it would\nbetter serve us to identify the types of questions BigQuery can answer. Such as:\n\n * What are our users doing across our multiple systems? How do we leverage log\n   files outputted by multiple systems to find out?\n * How can we consolidate information about employee information, payroll, and\n   benefits, when these all live in isolated systems?\n * What the hell am I supposed to do with all these spreadsheets?\n\nUnlike previous solutions, BigQuery solves these problems in a single product\nand does so with SQL-like query syntax,  a web interface, and 7 native Client\nLibraries.  There are plenty of reasons to love BigQuery, but let's start with\none we've recently already talked about: the auto-generation of table schemas. \n\nMatt has demonstrated how to approach this problem manually with the help of\nPandas\n[https://hackersandslackers.com/downcast-numerical-columns-python-pandas/]. I\nprovided a more gimmicky approach by leveraging the Python table-schema library\n[https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/]. With\nBigQuery, we find yet another alternative which is neither manual or gimmicky:\nperfect for those who are lazy, rich, and demand perfection (AKA: your clients,\nprobably).\n\nFirst, we'll need to get our data into BigQuery\n\nUploading Data into Google Cloud Storage via the Python SDK\nBigQuery requires us to go through Google Cloud Storage as a buffer before\ninputting data into tables. No big deal, we'll write a script!\n\nWe're assuming that you have a basic knowledge of Google Cloud, Google Cloud\nStorage, and how to download a JSON Service Account key\n[https://cloud.google.com/bigquery/docs/reference/libraries]  to store locally\n(hint: click the link).\n\nfrom google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n\n\nThe above is nearly a copy + paste of Google Cloud's sample code for the Google\nCloud Storage Python SDK:\n\n * bucket_uri  is found by inspecting any bucket's information on Google Cloud.\n * bucket_name  is... well, you know.\n * bucket_target  represents the resulting file structure representing the saved\n   CSV when completed.\n * local_dataset  is the path to a CSV we've stored locally: we can assume that\n   we've grabbed some data from somewhere, like an API, and tossed into a local\n   file temporarily.\n\nSuccessfully executing the above results in the following message:\n\nFile data/test.csv uploaded to datasets/data_upload.csv.\n\n\nInserting Data from Cloud Storage to BigQuery\nThat was the easy part. Let's move on to the good stuff:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n\n\nWe've added the function insert_bigquery()  to handle creating a BigQuery table\nout of a CSV.\n\nAfter we set our client, we create a dataset reference. In BigQuery, tables can\nbelong to a 'dataset,' which is a grouping of tables. Compare this concept to\nMongoDB's collections, or PostgreSQL's schemas. Note that this process is made\nmuch easier by the fact that we stored our project key locally: otherwise, we'd\nhave to specify which Google Cloud project we're looking for, etc.\n\nWith the dataset specified, we begin to build our \"job\" object with \nLoadJobConfig. This is like loading a gun before unleashing a shotgun blast into\nthe face of our problems. Alternatively, a more relevant comparison could be\nwith the Python requests  library and the act of prepping an API request before\nexecution.\n\nWe set job_config.autodetect  to be True, obviously. \njob_config.skip_leading_rows  reserves our header row from screwing things up.\n\nload_job  puts our request together, and load_job.result()  executes said job.\nThe .result()  method graciously puts the rest of our script on hold until the\nspecified job is completed. In our case, we want this happen: it simplifies our\nscript so that we don't need to verify this manually before moving on.\n\nLet's see what running that job with our fake data looks like in the BigQuery\nUI:\n\nAll my fake friends are here!Getting Our Flawlessly Inferred Table Schema\nBigQuery surely gets table schemas wrong some of the time. That said, I have yet\nto see it happen. Let's wrap this script up:\n\nfrom google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    \"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    \"\"\"Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    \"\"\"Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    \"\"\"\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n\n\nWith the addition of get_bigquery_schema(), our script is complete!\n\nTableReference()  is similar to the dataset reference we went over earlier, only\nfor tables (duh). This allows us to call upon get_table(), which returns a Table\nclass representing the table we just created. Amongst the methods of that class,\nwe can call .schema(), which gives us precisely what we want: a beautiful\nrepresentation of a Table schema, generated from raw CSV information, where\nthere previously was none.\n\nBehold the fruits of your labor:\n\n[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n\n\nThere you have it; a correctly inferred schema, from data which wasn't entirely\nclean in the first place (our dates are in MM/DD/YY  format as opposed to \nMM/DD/YYYY, but Google still gets it right. How? Because Google).\n\nIt Doesn't End Here\nI hope it goes without saying that abusing Google BigQuery's API to generate\nschemas for you is only a small, obscure use case of what Google BigQuery is\nintended to do, and what it can do for you. That said, I need to stop this\nfanboying post before anybody realizes I'll promote their products for free\nforever (I think I may have passed that point).\n\nIn case you're interested, the source code for this script has been uploaded as\na Gist here\n[https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e]. Have at\nit, and remember to think Big™*.\n\n*Not a real trademark, I'm making things up again.","html":"<p>GCP is on the rise, and it's getting harder and harder to have conversations around data without addressing the 500-pound gorilla in the room: Google BigQuery. With most enterprises comfortably settled into their Apache-based Big Data stacks, BigQuery rattles the cages of convention for many. Luckily, Hackers And Slackers is no such enterprise. Thus, we aren't afraid to ask the Big question: how much easier would life be with BigQuery?</p><h2 id=\"big-data-bigquery\">Big Data, BigQuery</h2><p>In short, BigQuery trivializes the act of querying against multiple, unpredictable data sources. To better understand when this is useful, it would better serve us to identify the types of questions BigQuery can answer. Such as:</p><ul><li>What are our users doing across our multiple systems? How do we leverage log files outputted by multiple systems to find out?</li><li>How can we consolidate information about employee information, payroll, and benefits, when these all live in isolated systems?</li><li>What the hell am I supposed to do with all these spreadsheets?</li></ul><p>Unlike previous solutions, BigQuery solves these problems in a single product and does so with <strong>SQL-like query syntax,</strong> a <strong>web interface</strong>, and <strong>7 native Client Libraries.</strong> There are plenty of reasons to love BigQuery, but let's start with one we've recently already talked about: the <em>auto-generation of table schemas</em>. </p><p>Matt has demonstrated how to approach this problem <a href=\"https://hackersandslackers.com/downcast-numerical-columns-python-pandas/\">manually with the help of Pandas</a>. I provided a more gimmicky approach by leveraging the <a href=\"https://hackersandslackers.com/infer-datatypes-from-csvs-to-create/\">Python table-schema library</a>. With BigQuery, we find yet another alternative which is neither manual or gimmicky: perfect for those who are lazy, rich, and demand perfection (AKA: your clients, probably).</p><p>First, we'll need to get our data into BigQuery</p><h2 id=\"uploading-data-into-google-cloud-storage-via-the-python-sdk\">Uploading Data into Google Cloud Storage via the Python SDK</h2><p>BigQuery requires us to go through Google Cloud Storage as a buffer before inputting data into tables. No big deal, we'll write a script!</p><p>We're assuming that you have a basic knowledge of Google Cloud, Google Cloud Storage, and how to download a <a href=\"https://cloud.google.com/bigquery/docs/reference/libraries\">JSON Service Account key</a> to store locally (hint: click the link).</p><pre><code class=\"language-python\">from google.cloud import storage\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n        \n        \nupload_blob(bucket_name, local_dataset, bucket_target)\n</code></pre>\n<p>The above is nearly a copy + paste of Google Cloud's sample code for the Google Cloud Storage Python SDK:</p><ul><li><code>bucket_uri</code> is found by inspecting any bucket's information on Google Cloud.</li><li><code>bucket_name</code> is... well, you know.</li><li><code>bucket_target</code><strong> </strong>represents the resulting file structure representing the saved CSV when completed.</li><li><code>local_dataset</code> is the path to a CSV we've stored locally: we can assume that we've grabbed some data from somewhere, like an API, and tossed into a local file temporarily.</li></ul><p>Successfully executing the above results in the following message:</p><pre><code class=\"language-shell\">File data/test.csv uploaded to datasets/data_upload.csv.\n</code></pre>\n<h2 id=\"inserting-data-from-cloud-storage-to-bigquery\">Inserting Data from Cloud Storage to BigQuery</h2><p>That was the easy part. Let's move on to the good stuff:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\n</code></pre>\n<p>We've added the function <code>insert_bigquery()</code> to handle creating a BigQuery table out of a CSV.</p><p>After we set our client, we create a <strong>dataset reference</strong>. In BigQuery, tables can belong to a 'dataset,' which is a grouping of tables. Compare this concept to MongoDB's <strong>collections, </strong>or PostgreSQL's <strong>schemas</strong>. Note that this process is made much easier by the fact that we stored our project key locally: otherwise, we'd have to specify which Google Cloud project we're looking for, etc.</p><p>With the dataset specified, we begin to build our \"job\" object with <code>LoadJobConfig</code>. This is like loading a gun before unleashing a shotgun blast into the face of our problems. Alternatively, a more relevant comparison could be with the Python <code>requests</code> library and the act of prepping an API request before execution.</p><p>We set <code>job_config.autodetect</code> to be <code>True</code>, obviously. <code>job_config.skip_leading_rows</code> reserves our header row from screwing things up.</p><p><code>load_job</code> puts our request together, and <code>load_job.result()</code> executes said job. The <code>.result()</code> method graciously puts the rest of our script on hold until the specified job is completed. In our case, we want this happen: it simplifies our script so that we don't need to verify this manually before moving on.</p><p>Let's see what running that job with our fake data looks like in the BigQuery UI:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://res-1.cloudinary.com/hackers-and-slackers/image/upload/f_auto,q_auto/v1/images/Screen-Shot-2019-02-01-at-7.42.52-PM.png\" class=\"kg-image\"><figcaption>All my fake friends are here!</figcaption></figure><h2 id=\"getting-our-flawlessly-inferred-table-schema\">Getting Our Flawlessly Inferred Table Schema</h2><p>BigQuery surely gets table schemas wrong <em>some </em>of the time. That said, I have yet to see it happen. Let's wrap this script up:</p><pre><code class=\"language-python\">from google.cloud import storage\nfrom google.cloud import bigquery\nimport pprint\n\nbucket_uri = 'gs://your-bucket/'\nbucket_name = 'your-bucket'\nbucket_target = 'datasets/data_upload.csv'\nlocal_dataset = 'data/test.csv'\nbucket_target_uri = bucket_uri + bucket_target\nbigquery_dataset = 'uploadtest'\nbigquery_table = 'my_table'\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    &quot;&quot;&quot;Upload a CSV to Google Cloud Storage.\n\n    1. Retrieve the target bucket.\n    2. Set destination of data to be uploaded.\n    3. Upload local CSV.\n    &quot;&quot;&quot;\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    # Commence Upload\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef insert_bigquery(target_uri, dataset_id, table_id):\n    &quot;&quot;&quot;Insert CSV from Google Storage to BigQuery Table.\n\n    1. Specify target dataset within BigQuery.\n    2. Create a Job configuration.\n    3. Specify that we are autodetecting datatypes.\n    4. Reserve row #1 for headers.\n    5. Specify the source format of the file (defaults to CSV).\n    6. Pass the URI of the data storage on Google Cloud Storage from.\n    7. Load BigQuery Job.\n    8. Execute BigQuery Job.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    job_config = bigquery.LoadJobConfig()\n    job_config.autodetect = True\n    job_config.skip_leading_rows = 1\n    job_config.source_format = bigquery.SourceFormat.CSV\n    uri = target_uri\n    load_job = bigquery_client.load_table_from_uri(\n        uri,\n        dataset_ref.table(table_id),\n        job_config=job_config)  # API request\n    print('Starting job {}'.format(load_job.job_id))\n    # Waits for table load to complete.\n    load_job.result()\n    print('Job finished.')\n\n\ndef get_schema(dataset_id, table_id):\n    &quot;&quot;&quot;Get BigQuery Table Schema.\n\n    1. Specify target dataset within BigQuery.\n    2. Specify target table within given dataset.\n    3. Create Table class instance from existing BigQuery Table.\n    4. Print results to console.\n    5. Return the schema dict.\n    &quot;&quot;&quot;\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    bg_tableref = bigquery.table.TableReference(dataset_ref, table_id)\n    bg_table = bigquery_client.get_table(bg_tableref)\n    # Print Schema to Console\n    pp = pprint.PrettyPrinter(indent=4)\n    pp.pprint(bg_table.schema)\n    return bg_table.schema\n\n\nupload_blob(bucket_name, local_dataset, bucket_target)\ninsert_bigquery(bucket_target_uri, bigquery_dataset, bigquery_table)\nbigquery_table_schema = get_schema(bigquery_dataset, bigquery_table)\n</code></pre>\n<p>With the addition of <code>get_bigquery_schema()</code>, our script is complete!</p><p><code>TableReference()</code> is similar to the dataset reference we went over earlier, only for tables (duh). This allows us to call upon <code>get_table()</code>, which returns a Table class representing the table we just created. Amongst the methods of that class, we can call <code>.schema()</code>, which gives us precisely what we want: a beautiful representation of a Table schema, generated from raw CSV information, where there previously was none.</p><p>Behold the fruits of your labor:</p><pre><code class=\"language-python\">[   SchemaField('id', 'INTEGER', 'NULLABLE', None, ()),\n    SchemaField('initiated', 'TIMESTAMP', 'NULLABLE', None, ()),\n    SchemaField('hiredate', 'DATE', 'NULLABLE', None, ()),\n    SchemaField('email', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('firstname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('lastname', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('title', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('department', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('location', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('country', 'STRING', 'NULLABLE', None, ()),\n    SchemaField('type', 'STRING', 'NULLABLE', None, ())]\n</code></pre>\n<p>There you have it; a correctly inferred schema, from data which wasn't entirely clean in the first place (our dates are in <strong>MM/DD/YY</strong> format as opposed to <strong>MM/DD/YYYY</strong>, but Google still gets it right. How? Because Google).</p><h3 id=\"it-doesn-t-end-here\">It Doesn't End Here</h3><p>I hope it goes without saying that abusing Google BigQuery's API to generate schemas for you is only a small, obscure use case of what Google BigQuery is intended to do, and what it can do for you. That said, I need to stop this fanboying post before anybody realizes I'll promote their products for free forever (I think I may have passed that point).</p><p>In case you're interested, the source code for this script has been uploaded as a Gist <a href=\"https://gist.github.com/toddbirchard/a743db3b8805dfe9834e73c530dc8a6e\">here</a>. Have at it, and remember to think Big<strong>™*</strong>.</p><span style=\"color: #a6a6a6;font-style: italic; font-size: .8em;\">*Not a real trademark, I'm making things up again.</span>","url":"https://hackersandslackers.com/getting-started-google-big-query-python/","uuid":"30051eb2-7fa7-4a09-91fd-c3f11966b398","page":false,"codeinjection_foot":null,"codeinjection_head":null,"comment_id":"5c47584f4f3823107c9e8f23"}},"pageContext":{"slug":"getting-started-google-big-query-python"}}